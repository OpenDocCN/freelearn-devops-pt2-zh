- en: Monitoring and Logging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和日志记录
- en: This chapter will cover the usage and customization of both built-in and third-party
    monitoring tools on our Kubernetes cluster. We will cover how to use the tools
    to monitor the health and performance of our cluster. In addition, we will look
    at built-in logging, the **Google Cloud Logging** service, and **Sysdig**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖对我们的 Kubernetes 集群中内置和第三方监控工具的使用和自定义。我们将介绍如何使用这些工具来监视我们集群的健康和性能。此外，我们还将研究内置日志记录、**Google
    Cloud Logging** 服务和 **Sysdig**。
- en: 'This chapter will discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下主题：
- en: How Kuberentes uses cAdvisor, Heapster, InfluxDB, and Grafana
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 如何使用 cAdvisor、Heapster、InfluxDB 和 Grafana
- en: Customizing the default Grafana dashboard
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义默认 Grafana 仪表盘
- en: Using FluentD and Grafana
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 FluentD 和 Grafana
- en: Installing and using logging tools
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和使用日志记录工具
- en: Working with popular third-party tools, such as StackDriver and Sysdig, to extend
    our monitoring capabilities
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流行的第三方工具，如 StackDriver 和 Sysdig，来扩展我们的监控能力
- en: Monitoring operations
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控操作
- en: Real-world monitoring goes far beyond checking whether a system is up and running.
    Although health checks, like those you learned in [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml),
    *Pods, Services, Replication Controllers, and Labels*, in the *Health checks*
    section, can help us isolate problem applications. Operation teams can best serve
    the business when they can anticipate the issues and mitigate them before a system
    goes offline.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 实际监控远不止检查系统是否正常运行。尽管像你在[第二章](281f1b00-8685-4614-895f-df5ae1518373.xhtml)中学到的那样，*Pods、Services、Replication
    Controllers 和 Labels* 中的 *健康检查* 部分所学的可以帮助我们隔离问题应用程序。但是，只有在系统下线之前能够预见问题并加以缓解时，运营团队才能最好地为业务服务。
- en: Best practices in monitoring are to measure the performance and usage of core
    resources and watch for trends that stray from the normal baseline. Containers
    are not different here, and a key component to managing our Kubernetes cluster
    is having a clear view into performance and availability of the OS, network, system
    (CPU and memory), and storage resources across all nodes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 监控的最佳实践是测量核心资源的性能和使用情况，并观察是否存在偏离正常基线的趋势。容器在这方面也不例外，管理我们的 Kubernetes 集群的一个关键组件是清晰地了解所有节点上的操作系统、网络、系统（CPU
    和内存）和存储资源的性能和可用性。
- en: In this chapter, we will examine several options to monitor and measure the
    performance and availability of all our cluster resources. In addition, we will
    look at a few options for alerting and notifications when irregular trends start
    to emerge.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究几种选项，以监控和测量所有集群资源的性能和可用性。此外，当出现异常趋势时，我们还将查看一些警报和通知选项。
- en: Built-in monitoring
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内置监控
- en: 'If you recall from [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to Kubernetes*, we noted that our nodes were already running a number
    of monitoring services. We can see these once again by running the `get pods`
    command with the `kube-system` namespace specified as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回忆一下[第一章](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml)中关于 Kubernetes 的介绍，我们注意到我们的节点已经运行了许多监控服务。我们可以通过以下方式再次运行
    `get pods` 命令，并指定 `kube-system` 命名空间来查看这些服务：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是上述命令的结果：
- en: '![](img/B06302_08_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_01.png)'
- en: System pod listing
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 系统 Pod 列表
- en: Again, we see a variety of services, but how does this all fit together? If
    you recall the *Node (formerly minions)* section from [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml),
    *Pods, Services, Replication Controllers, and Labels*, each node is running a
    kublet. The kublet is the main interface for nodes to interact and update the
    API server. One such update is the **metrics** of the node resources. The actual
    reporting of the resource usage is performed by a program named **cAdvisor**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 再次看到了各种各样的服务，但这一切又是如何结合在一起的呢？如果你回忆一下[第二章](281f1b00-8685-4614-895f-df5ae1518373.xhtml)中关于
    *节点（原名为 minions）* 部分，每个节点都在运行一个 kublet。Kublet 是节点与 API 服务器交互和更新的主要接口。其中一个更新是节点资源的
    **度量值**。实际上，资源使用情况的报告由一个名为 **cAdvisor** 的程序执行。
- en: cAdvisor is another open-source project from Google, which provides various
    metrics on container resource use. Metrics include CPU, memory, and network statistics.
    There is no need to tell cAdvisor about individual containers; it collects the
    metrics for all containers on a node and reports this back to the kublet, which
    in turn reports to Heapster.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: cAdvisor 是 Google 的另一个开源项目，提供了有关容器资源使用情况的各种指标。指标包括 CPU、内存和网络统计信息。不需要告诉 cAdvisor
    关于单个容器的信息；它会收集节点上所有容器的指标，并将其报告给 kublet，然后再报告给 Heapster。
- en: '**Google''s open-source projects** Google has a variety of open-source projects
    related to Kubernetes. Check them out, use them, and even contribute your own
    code!'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google 的开源项目** Google 有许多与 Kubernetes 相关的开源项目。查看它们，使用它们，甚至贡献您自己的代码！'
- en: 'cAdvisor and Heapster are mentioned in the following section:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: cAdvisor 和 Heapster 在下一节中提到：
- en: '**cAdvisor**: [https://github.com/google/cadvisor](https://github.com/google/cadvisor)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cAdvisor**: [https://github.com/google/cadvisor](https://github.com/google/cadvisor)'
- en: '**Heapster**: [https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Heapster**: [https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)'
- en: '**Contrib** is a catch-all for a variety of components that are not part of
    core Kubernetes. It is found at:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**Contrib** 是各种不属于核心 Kubernetes 的组件的集合。其位置在：'
- en: '[https://github.com/kubernetes/contrib](https://github.com/kubernetes/contrib).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kubernetes/contrib](https://github.com/kubernetes/contrib).'
- en: '**LevelDB** is a key store library that was used in the creation of InfluxDB.
    It is found at:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**LevelDB** 是 InfluxDB 创建时使用的键存储库。其位置在：'
- en: '[https://github.com/google/leveldb](https://github.com/google/leveldb).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/google/leveldb](https://github.com/google/leveldb).'
- en: '**Heapster** is yet another open-source project from Google; you may start
    to see a theme emerging here (see the preceding information box). Heapster runs
    in a container on one of the minion nodes and aggregates the data from kublet.
    A simple REST interface is provided to query the data.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Heapster** 是 Google 的又一个开源项目；你可能开始看到这里出现了一个主题（参见前面的信息框）。Heapster 在一个 minion
    节点上的容器中运行，并从 kublet 聚合数据。提供了一个简单的 REST 接口来查询数据。'
- en: When using the GCE setup, a few additional packages are set up for us, which
    saves us time and gives us a complete package to monitor our container workloads.
    As we can see from the preceding *System pod listing* screenshot, there is another
    pod with `influx-grafana` in the title.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GCE 设置时，为我们设置了一些额外的包，这为我们节省了时间，并提供了一个完整的包来监视我们的容器工作负载。从前面的 *系统 pod 列表* 截图中可以看到，还有另一个带有
    `influx-grafana` 的 pod。
- en: '**InfluxDB** is described on its official website as follows (you can refer
    to more details about this in point 1 in the *References* section at the end of
    the chapter):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**InfluxDB** 在其官方网站上描述如下（您可以在本章末尾的 *参考* 部分的第 1 点中找到更多详细信息）：'
- en: An open-source distributed time series database with no external dependencies.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个没有外部依赖的开源分布式时间序列数据库。
- en: InfluxDB is based on a key store package (refer to the previous *Google's open-source
    projects* information box) and is perfect to store and query event—or time-based
    statistics such as those provided by Heapster.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: InfluxDB 基于一个键存储包（参考前面的 *Google 的开源项目* 信息框）构建，并且非常适合存储和查询事件或者时间统计信息，例如由 Heapster
    提供的那些。
- en: Finally, we have **Grafana**, which provides a dashboard and graphing interface
    for the data stored in InfluxDB. Using Grafana, users can create a custom monitoring
    dashboard and get immediate visibility into the health of their Kubernetes cluster
    and therefore their entire container infrastructure.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 **Grafana**，它为存储在 InfluxDB 中的数据提供了仪表板和图形界面。使用 Grafana，用户可以创建自定义监控仪表板，并立即查看其
    Kubernetes 集群的健康状况，因此也可以查看其整个容器基础架构的健康状况。
- en: Exploring Heapster
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Heapster
- en: 'Let''s quickly look at the REST interface by running SSH to the node with the
    Heapster pod. First, we can list the pods to find the one running Heapster, as
    follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 SSH 快速查看 Heapster pod 所在节点的 REST 接口。首先，我们可以列出 pod 来找到运行 Heapster 的 pod，如下所示：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The name of the pod should start with `monitoring-heapster`. Run a `describe`
    command to see which node it is running on, as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: pod 的名称应以 `monitoring-heapster` 开头。运行 `describe` 命令查看它运行在哪个节点上，如下所示：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'From the output in the following screenshot, we can see that the pod is running
    in `kubernetes-minion-merd`. Also note the IP for the pod, a few lines down, as
    we will need that in a moment:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从下面的截图输出中，我们可以看到该 pod 正在 `kubernetes-minion-merd` 上运行。另外，请注意下方几行中的 pod IP，因为我们稍后会用到它：
- en: '![](img/B06302_08_02.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_02.png)'
- en: Heapster pod details
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Heapster Pod 详细信息
- en: 'Next, we can SSH to this box with the familiar `gcloud ssh` command, as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用熟悉的 `gcloud ssh` 命令 SSH 到这台机器，如下所示：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: From here, we can access the Heapster REST API directly using the pod's IP address.
    Remember that pod IPs are routable not only in the containers but also on the
    nodes themselves. The `Heapster` API is listening on port `8082`, and we can get
    a full list of metrics at `/api/v1/metric-export-schema/`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以直接使用 pod 的 IP 地址访问 Heapster REST API。请记住，Pod IP 不仅在容器中路由，而且还在节点本身上路由。`Heapster`
    API 在端口 `8082` 上监听，并且我们可以在 `/api/v1/metric-export-schema/` 获取完整的指标列表。
- en: 'Let''s see the list now by issuing a `curl` command to the pod IP address we
    saved from the `describe` command, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过向我们从 `describe` 命令保存的 Pod IP 地址发出 `curl` 命令来查看列表，如下所示：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will see a listing that is quite long. The first section shows all the metrics
    available. The last two sections list fields by which we can filter and group.
    For your convenience, I''ve added the following tables that are a little bit easier
    to read:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到一个相当长的列表。第一部分显示所有可用的指标。最后两部分列出了我们可以按其过滤和分组的字段。为了您的方便，我添加了以下略微易读一些的表格：
- en: '| **Metric** | **Description** | **Unit** | **Type** |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **指标** | **描述** | **单位** | **类型** |'
- en: '| uptime | The number of milliseconds since the container was started | ms
    | cumulative |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 正常运行时间 | 容器启动以来的毫秒数 | 毫秒 | 累计 |'
- en: '| cpu/usage | The cumulative CPU usage on all cores | ns | cumulative |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| cpu/使用率 | 所有核心上的累计 CPU 使用率 | 纳秒 | 累计 |'
- en: '| cpu/limit | The CPU limit in millicores | - | gauge |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| cpu/限制 | 毫核 CPU 限制 | - | 测量值 |'
- en: '| memory/usage | Total memory usage | bytes | gauge |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 内存/使用量 | 总内存使用量 | 字节 | 测量值 |'
- en: '| memory/working_set | Total working set usage; the working set is the memory
    being used and not easily dropped by the kernel | bytes | gauge |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 内存/工作集 | 总工作集使用量；工作集是内核正在使用且不容易丢弃的内存 | 字节 | 测量值 |'
- en: '| memory/limit | The memory limit | bytes | gauge |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 内存/限制 | 内存限制 | 字节 | 测量值 |'
- en: '| memory/page_faults | The number of page faults | - | cumulative |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 内存/页面错误 | 页面错误数 | - | 累计 |'
- en: '| memory/major_page_faults | The number of major page faults | - | cumulative
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 内存/主要页面错误 | 主要页面错误数 | - | 累计 |'
- en: '| network/rx | The cumulative number of bytes received over the network | bytes
    | cumulative |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 网络/接收 | 累计接收到网络的字节数 | 字节 | 累计 |'
- en: '| network/rx_errors | The cumulative number of errors while receiving over
    the network | - | cumulative |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 网络/接收错误 | 接收到网络时的累计错误数 | - | 累计 |'
- en: '| network/tx | The cumulative number of bytes sent over the network | bytes
    | cumulative |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 网络/发送 | 累计发送到网络的字节数 | 字节 | 累计 |'
- en: '| network/tx_errors | The cumulative number of errors while sending over the
    network | - | cumulative |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 网络/发送错误 | 发送到网络时的累计错误数 | - | 累计 |'
- en: '| filesystem/usage | The total number of bytes consumed on a filesystem | bytes
    | gauge |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 文件系统/使用量 | 文件系统上消耗的总字节数 | 字节 | 测量值 |'
- en: '| filesystem/limit | The total size of filesystem in bytes | bytes | gauge
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 文件系统/限制 | 文件系统总大小（以字节为单位） | 字节 | 测量值 |'
- en: '| filesystem/available | The number of available bytes remaining in a the filesystem
    | bytes | gauge |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 文件系统/可用 | 文件系统中剩余的可用字节数 | 字节 | 测量值 |'
- en: Table 6.1\. Available Heapster metrics
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1。可用的 Heapster 指标
- en: '| **Field** | **Description** | **Label type** |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **字段** | **描述** | **标签类型** |'
- en: '| `nodename` | The nodename where the container ran | Common |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `nodename` | 容器运行的节点名称 | 通用 |'
- en: '| `hostname` | The hostname where the container ran | Common |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `hostname` | 容器运行的主机名 | 通用 |'
- en: '| `host_id` | An identifier specific to a host, which is set by the cloud provider
    or user | Common |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `host_id` | 特定于主机的标识符，由云提供商或用户设置 | 通用 |'
- en: '| `container_base_image` | The user-defined image name that is run inside the
    container | Common |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `container_base_image` | 在容器内运行的用户定义的镜像名称 | 通用 |'
- en: '| `container_name` | The user-provided name of the container or full container
    name for system containers | Common |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `container_name` | 容器的用户提供的名称或系统容器的完整容器名称 | 通用 |'
- en: '| `pod_name` | The name of the pod | Pod |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `pod_name` | Pod 的名称 | Pod |'
- en: '| `pod_id` | The unique ID of the pod | Pod |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `pod_id` | Pod 的唯一 ID | Pod |'
- en: '| `pod_namespace` | The namespace of the pod | Pod |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `pod_namespace` | Pod 的命名空间 | Pod |'
- en: '| `namespace_id` | The unique ID of the namespace of the pod | Pod |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `namespace_id` | Pod 命名空间的唯一 ID | Pod |'
- en: '| `labels` | A comma-separated list of user-provided labels | Pod |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `labels` | 用户提供的标签的逗号分隔列表 | Pod |'
- en: Table 6.2\. Available Heapster fields
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.2。可用的 Heapster 字段
- en: Customizing our dashboards
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义我们的仪表板
- en: 'Now that we have the fields, we can have some fun. Recall the Grafana page
    we looked at in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml), *Introduction to Kubernetes*.
    Let''s pull that up again by going to our cluster''s monitoring URL. Note that
    you may need to log in with your cluster credentials. Refer to the following format
    of the link you need to use:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了字段，我们可以玩得开心了。回想一下我们在[第1章](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml)中看到的Grafana页面，*Kubernetes入门*。让我们再次打开它，方法是转到我们集群的监控URL。请注意，您可能需要使用您的集群凭据登录。请参考您需要使用的链接的以下格式：
- en: '`https://**<your master IP>**/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`https://**<your master IP>**/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana`'
- en: We'll see the default Home dashboard. Click on the down arrow next to Home and
    select Cluster. This shows the Kubernetes cluster dashboard, and now we can add
    our own statistics to the board. Scroll all the way to the bottom and click on
    Add a Row. This should create a space for a new row and present a green tab on
    the left-hand side of the screen.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到默认的主页仪表板。点击主页旁边的向下箭头，选择集群。这将显示Kubernetes集群仪表板，现在我们可以向面板添加我们自己的统计数据。滚动到底部，点击添加一行。这应该会创建一个新行的空间，并在屏幕左侧显示一个绿色标签。
- en: Let's start by adding a view into the filesystem usage for each node (minion).
    Click on the *green* tab to expand and then select Add Panel and then graph. An
    empty graph should appear on the screen along with a query panel for our custom
    graph.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先为每个节点（minion）添加一个文件系统使用情况的视图。点击*绿色*标签以展开，然后选择 添加面板，然后选择图表。屏幕上应该会出现一个空图表以及我们自定义图表的查询面板。
- en: The first field in this panel should show a query that starts with 'SELECT mean("value")
    FROM ...'. Click on the A character next to this field to expand it. Leave the
    first field next to FROM as default and then click on the next field with the select
    measurement value. A dropdown menu will appear with the Heapster metrics we saw
    in the previous tables. Select `filesystem/usage_bytes_gauge`. Now in the SELECT
    row, click on mean() and then on the x symbol to remove it. Next, click on the
    + symbol on the end of the row and add selectors -> max. Then, you'll see a GROUP
    BY row with time($interval) and fill(none). Carefully click on fill and not on
    the (none) portion and again on x to remove it. Then, click on the + symbol at
    the end of the row and select tag(hostname).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此面板中的第一个字段应显示以“SELECT mean("value") FROM ...”开头的查询。点击该字段旁边的 A 字符以展开它。将下一个字段留在FROM 旁边默认设置，并点击下一个字段以选择测量值。下拉菜单中将显示我们在前面表格中看到的Heapster指标。选择 `filesystem/usage_bytes_gauge`。现在，在SELECT行中，点击 mean()，然后点击x 符号将其删除。接下来，点击该行末尾的+ 符号并添加选择器
    -> max。然后，您会看到一个GROUP BY行，其中包含time($interval)和fill(none)。小心地点击 fill 而不是 (none) 部分，然后再次点击x 符号将其删除。然后，点击该行末尾的+ 符号并选择标签（hostname）。
- en: 'Finally, at the bottom of the screen we should see a Group by time interval.
    Enter `5s` there and you should have something similar to the following screenshot:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在屏幕底部，我们应该看到一个按时间间隔分组。在那里输入`5s`，你应该会看到类似以下截图的东西：
- en: '![](img/B06302_08_03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_03.png)'
- en: Heapster pod details
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Heapster pod 详细信息
- en: Next, let's click on the Axes tab, so that we can set the units and legend.
    Under Left Y Axis, click on the field next to Unit and set it to data -> bytes
    and Label to Disk Space Used. Under Right Y Axis, set Unit to none -> none. Next,
    on the Legend tab, make sure to check Show in Options and Max in Values.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们点击轴标签，以便设置单位和图例。在左Y轴下，点击单位旁边的字段，将其设置为data -> bytes，标签设置为磁盘空间已用。在右Y轴下，将单位设置为none
    -> none。接下来，在 图例 标签下，确保选中在选项中显示和在值中最大化。
- en: Now, let's quickly go to the General tab and choose a title. In my case, I named
    mine `Filesystem Disk Usage by Node (max)`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们快速转到通用标签并选择一个标题。在我的情况下，我将其命名为`节点的文件系统磁盘使用情况（最大值）`。
- en: We don't want to lose this nice new graph we've created, so let's click on the
    save icon in the top right corner. It looks like a *floppy disk* (you can do a
    Google image search if you don't know what this is).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想丢失我们创建的这张漂亮的新图表，所以让我们点击右上角的保存图标。它看起来像一个*软盘*（如果你不知道这是什么，你可以进行谷歌图片搜索）。
- en: After we click on the save icon, we will see a green dialog box that verifies
    the dashboard was saved. We can now click the x symbol above the graph details
    panel and below the graph itself.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们点击保存图标后，我们会看到一个绿色的对话框，确认仪表板已保存。现在我们可以点击位于图表详细信息面板上方和图表本身下方的 x 符号。
- en: This will return us to the dashboard page. If we scroll all the way down, we
    will see our new graph. Let's add another panel to this row. Again use the *green*
    tab and then select Add Panel -> singlestat. Once again, an empty panel will appear
    with a setting form below it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将带我们返回仪表板页面。如果我们一直往下滚动，我们会看到我们的新图形。让我们在这一行再添加另一个面板。再次使用*绿色*标签，然后选择Add Panel
    -> singlestat。又一次，一个空面板将出现，下面是一个设置表单。
- en: Let's say, we want to watch a particular node and monitor network usage. We
    can easily do this by first going to the Metrics tab. Then expand the query field
    and set the second value in the FROM field to network/rx. Now we can specify the
    WHERE clause by clicking the + symbol at the end of the row and choosing hostname
    from the dropdown. After hostname = click on select tag value and choose one of
    the minion nodes from the list.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要监视特定节点并监视网络使用情况。我们可以首先转到Metrics选项卡来轻松完成这项任务。然后展开查询字段，并将FROM字段中的第二个值设置为network/rx。现在，我们可以通过点击行末尾的+符号并从下拉菜单中选择主机名，在WHERE子句中指定条件。在hostname
    = 后点击select tag value，并从列表中选择一个 minion 节点。
- en: 'Finally, leave mean() for the second SELECT field:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将**mean()**留给第二个SELECT字段：
- en: '![](img/B06302_08_04.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_04.png)'
- en: Singlestat options
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Singlestat选项
- en: In the Options tab, make sure that Unit format is set to data -> bytes and check
    the Show box next to Spark lines. The **sparkline** gives us a quick history view
    of the recent variation in the value. We can use Background mode to take up the
    entire background; by default, it uses the area below the value.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在Options选项卡中，确保Unit format设置为data -> bytes，然后在Spark lines旁的Show框中打勾。**sparkline**可以让我们快速查看价值的最近变化历史。我们可以使用Background
    mode来占据整个背景；默认情况下，它使用值下面的区域。
- en: In Coloring, we can optionally check the Value or Background box and choose
    Thresholds and Colors. This will allow us to choose different colors for the value
    based on the threshold tier we specify. Note that an unformatted version of the
    number must be used for threshold values.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在Coloring中，我们可以选择Value或Background框并选择Thresholds and Colors。这将使我们能够根据我们指定的阈值层选择不同颜色的值。请注意，阈值数必须使用未格式化版本。
- en: 'Now, let''s go back to the General tab and set the title as `Network bytes
    received (Node35ao)`. Use the identifier for your minion node. Once again, let''s
    save our work and return to the dashboard. We should now have a row that looks
    like the following screenshot:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们返回到General选项卡，并将标题设置为`Network bytes received (Node35ao)`。使用您的 minion 节点的标识符。再次保存我们的工作并返回仪表板。我们现在应该有一个类似以下截图的行：
- en: '![](img/B06302_08_05.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_05.png)'
- en: Custom dashboard panels
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义仪表板面板
- en: Grafana has a number of other panel types you can play with such as Dashboard
    list, Plugin list, Table, and Text.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 还有其他许多不同类型的面板供您尝试，例如仪表板列表、插件列表、表格和文本。
- en: As we can see, it is pretty easy to build a custom dashboard and monitor the
    health of our cluster at a glance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，构建自定义仪表板并一目了然地监视我们集群的健康状况非常容易。
- en: FluentD and Google Cloud Logging
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FluentD和Google Cloud Logging
- en: Looking back at the *System pod listing* screenshot at the beginning of the
    chapter, you may have noted a number of pods starting with the words `fluentd-cloud-logging-kubernetes...`
    . These pods appear when using the GCE provider for your K8s cluster. A pod like
    this exists on every node in our cluster and its sole purpose is to handle the
    processing of Kubernetes logs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下本章一开始的*System pod listing*截图，你可能会注意到一些以`fluentd-cloud-logging-kubernetes...`开头的
    pod。在使用 GCE 供应商为您的 K8s 集群提供服务时，这些 pod 会出现。我们集群中的每个节点都有一个这样的 pod，其唯一目的是处理 Kubernetes
    日志的处理。
- en: If we log in to our Google Cloud Platform account, we can see some of the logs
    processed there. Simply use the left side, under Stackdriver select Logging. This
    will take us to a log listing page with a number of drop-down menus on the top.
    If this is your first time visiting the page, the first dropdown will likely be
    set to Cloud HTTP Load Balancer.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们登录到我们的 Google Cloud Platform 帐户，就可以看到一些在那里处理的日志。只需在左侧，在 Stackdriver 下选择
    Logging。这将带我们到一个带有顶部多个下拉菜单的日志列表页面。如果这是您第一次访问该页面，第一个下拉菜单可能会被设定为 Cloud HTTP Load
    Balancer。
- en: 'In this drop-down menu, we''ll see a number of GCE types of entries. Select GCE
    VM Instances and then the Kubernetes master or one of the nodes. In the second
    dropdown, we can choose various log groups, including kublet. We can also filter
    by the event log level and date. Additionally, we can use the *play* button to
    watch events stream in live:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在此下拉菜单中，我们将看到许多 GCE 类型的条目。选择 GCE VM 实例，然后选择 Kubernetes 主节点或其中一个节点。在第二个下拉菜单中，我们可以选择各种日志组，包括
    kublet。我们还可以按事件日志级别和日期进行过滤。此外，我们可以使用*播放*按钮实时观看事件流：
- en: '![](img/B06302_08_06.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_06.png)'
- en: The Google Cloud Logging filter
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Logging 过滤器
- en: FluentD
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FluentD
- en: Now we know that the `fluentd-cloud-logging-kubernetes` pods are sending the
    data to the Google Cloud, but why do we need FluentD? Simply put, **FluentD**
    is a collector. It can be configured to have multiple sources to collect and tag
    logs, which are then sent to various output points for analysis, alerting, or
    archiving. We can even transform data using plugins before it is passed on to
    its destination.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道 `fluentd-cloud-logging-kubernetes` pods 正在将数据发送到 Google Cloud，但是我们为什么需要
    FluentD？简而言之，**FluentD** 是一个收集器。它可以配置为具有多个来源来收集和标记日志，然后将其发送到各种输出点进行分析、报警或存档。我们甚至可以在将数据传递给目的地之前使用插件转换数据。
- en: Not all provider setups have FluentD installed by default, but it is one of
    the recommended approaches to give us greater flexibility for future monitoring
    operations. The AWS Kubernetes setup also uses FluentD, but instead forwards events
    to **Elasticsearch**.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的提供商设置都默认安装了 FluentD，但这是一种推荐的方法，可以为我们未来的监控运营提供更大的灵活性。AWS Kubernetes 设置也使用
    FluentD，但是将事件转发到**Elasticsearch**。
- en: '**Exploring FluentD** If you are curious about the inner workings of the FluentD
    setup or just want to customize the log collection, we can explore quite easily
    using the `kubectl exec` command and one of the pod names from the command we
    ran earlier in the chapter.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索 FluentD** 如果你对 FluentD 设置的内部工作原理感到好奇，或者只是想自定义日志收集，我们可以很容易地使用 `kubectl
    exec` 命令和本章前面运行的一个 pod 名称进行探索。'
- en: 'First, let''s see if we can find the FluentD `config` file:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看是否可以找到 FluentD 的 `config` 文件：
- en: '`**$ kubectl exec fluentd-cloud-logging-kubernetes-minion-group-r4qt --namespace=kube-system
    -- ls /etc/td-agent**`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`**$ kubectl exec fluentd-cloud-logging-kubernetes-minion-group-r4qt --namespace=kube-system
    -- ls /etc/td-agent**`'
- en: 'We will look in the `etc` folder and then `td-agent`, which is the `fluent`
    subfolder. While searching in this directory, we should see a `td-agent.conf`
    file. We can view that file with a simple `cat` command, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 `etc` 文件夹中查找，然后在 `td-agent` 文件夹中查找，这是 `fluent` 子文件夹。在这个目录中搜索时，我们应该看到一个
    `td-agent.conf` 文件。我们可以使用简单的 `cat` 命令查看该文件，如下所示：
- en: '`**$ kubectl exec fluentd-cloud-logging-kubernetes-minion-group-r4qt --namespace=kube-system
    -- cat /etc/td-agent/td-agent.conf**`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`**$ kubectl exec fluentd-cloud-logging-kubernetes-minion-group-r4qt --namespace=kube-system
    -- cat /etc/td-agent/td-agent.conf**`'
- en: We should see a number of sources including the various Kubernetes components,
    Docker, and some GCP elements.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到许多来源，包括各种 Kubernetes 组件、Docker 和一些 GCP 元素。
- en: While we can make changes here, remember that it is a running container and
    our changes won't be saved if the pod dies or is restarted. If we really want
    to customize, it's best to use this container as a base and build a new container
    which we can push to a repository for later use.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以在这里进行更改，但请记住这是一个正在运行的容器，如果 pod 死亡或重新启动，我们的更改将不会被保存。如果我们真的想自定义，最好使用这个容器作为基础构建一个新的容器，将其推送到存储库以供以后使用。
- en: Maturing our monitoring operations
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完善我们的监控运营
- en: While Grafana gives us a great start to monitor our container operations, it
    is still a work in progress. In the real world of operations, having a complete
    dashboard view is great once we know there is a problem. However, in everyday
    scenarios, we'd prefer to be proactive and actually receive notifications when
    issues arise. This kind of alerting capability is a must to keep the operations
    team ahead of the curve and out of *reactive mode*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Grafana 为我们提供了一个监控容器运营的良好起点，但它仍然是一个正在进行中的工作。在运营的真实世界中，一旦我们知道有问题，拥有完整的仪表板视图就很棒。然而，在日常场景中，我们更愿意采取积极主动的方式，实际上在问题出现时收到通知。这种报警能力对于让运营团队保持领先并避免*被动模式*至关重要。
- en: There are many solutions available in this space, and we will take a look at
    two in particular—GCE monitoring (StackDriver) and Sysdig.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个空间中有许多可用的解决方案，我们将特别关注两个——GCE 监控（StackDriver）和 Sysdig。
- en: GCE (StackDriver)
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GCE（StackDriver）
- en: '**StackDriver** is a great place to start for infrastructure in the public
    cloud. It is actually owned by Google, so it''s integrated as the Google Cloud
    Platform monitoring service. Before your lock-in alarm bells start ringing, StackDriver
    also has solid integration with AWS. In addition, StackDriver has alerting capability
    with support for notification to a variety of platforms and webhooks for anything
    else.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**StackDriver** 是公共云基础设施的绝佳起点。实际上，它由 Google 拥有，因此作为 Google 云平台监控服务进行集成。在您的锁定警报开始响起之前，StackDriver
    还具有与 AWS 的良好集成。此外，StackDriver 还具有警报功能，支持向各种平台发送通知，并支持用于其他内容的 Webhook。'
- en: Sign-up for GCE monitoring
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注册 GCE 监控
- en: In the GCE console, in the Stackdriver section click on Monitoring. This will
    open a new window, where we can sign up for a free trial of Stackdriver. We can
    then add our GCP project and optionally an AWS account as well. This requires
    a few more steps, but instructions are included on the page. Finally, we'll be
    given instructions on how to install the agents on our cluster nodes. We can skip
    this for now, but will come back to it in a minute.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GCE 控制台中，在**Stackdriver**部分点击**监控**。这将打开一个新窗口，我们可以在其中注册 Stackdriver 的免费试用。然后，我们可以添加我们的
    GCP 项目，以及可选的 AWS 帐户。这需要一些额外的步骤，但页面上包含了说明。最后，我们将收到有关如何在我们的集群节点上安装代理的说明。我们现在可以跳过这一步，但一会儿会回来。
- en: Click on Continue, set up your daily alerts, and click on Continue again.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**继续**，设置您的每日警报，然后再次点击**继续**。
- en: Click on Launch Monitoring to proceed. We'll be taken to the main dashboard
    page, where we will see some basic statistics on our node in the cluster. If we
    select Resources from the side menu and then Instances, we'll be taken to a page
    with all our nodes listed. By clicking on the individual node, we can again see
    some basic information even without an agent installed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**启动监控**继续。我们将被带到主仪表板页面，在那里我们将看到集群中节点的一些基本统计信息。如果我们从侧边栏中选择**资源**，然后选择**实例**，我们将被带到列出所有节点的页面。通过点击单个节点，即使没有安装代理，我们也可以再次看到一些基本信息。
- en: Stackdriver also offers monitoring and logging agents that can be installed
    on the nodes. However, it currently does not support the container OS that is
    used by default in the GCE `kube-up` script. You can still see the basic metrics
    for any nodes in GCE or AWS, but will need to use another OS if you want the detailed
    agent install.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Stackdriver 还提供可安装在节点上的监控和日志代理。但是，它当前不支持 GCE `kube-up` 脚本中默认使用的容器 OS。您仍然可以查看
    GCE 或 AWS 中任何节点的基本指标，但如果您想要详细的代理安装，则需要使用另一个操作
- en: Alerts
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 警报
- en: Next, we can look at the alerting policies available as part of the monitoring
    service. From the instance details page, click on the Create Alerting Policy button
    in the Incidents section at the top of the page.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以查看作为监控服务一部分提供的警报策略。从实例详细信息页面，点击页面顶部的**创建警报策略**按钮。
- en: We will click on Add Condition and select a Metric Threshold. In the Target section,
    set RESOURCE TYPE to Instance (GCE). Then, set APPLIES TO to Group and kubernetes.
    Leave CONDITION TRIGGERS IF set to Any Member Violates.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会点击**添加条件**，并选择一个指标阈值。在**目标**部分，将**资源类型**设置为实例（GCE）。然后，将**适用于**设置为组和 kubernetes。将**条件触发如果**设置为任何成员违反。
- en: In the Configuration section, leave IF METRIC as CPU Usage (GCE Monitoring)
    and CONDITION as above. Now set THRESHOLD to `80` and set the time in FOR to 5
    minutes.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在**配置**部分，将**如果指标**保持为 CPU 使用率（GCE 监控），将**条件**保持为上述。现在将**阈值**设置为`80`，并将时间设置为
    5 分钟。
- en: 'Click on Save Condition:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 点击保存条件：
- en: '![](img/B06302_08_08.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_08.png)'
- en: Google Cloud Monitoring alert policy
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 监控警报策略
- en: Next, we will add a notification. In the Notification section, leave Method
    as Email and enter your e-mail address.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加一个通知。在**通知**部分，将**方法**保持为电子邮件，并输入您的电子邮件地址。
- en: We can skip the Documentation section, but this is where we can add text and
    formatting to alert messages.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以跳过**文档**部分，但这是我们可以添加文本和格式到警报消息的地方。
- en: Finally, name the policy as `Excessive CPU Load` and click on Save Policy.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将策略命名为`过高的 CPU 负载`，然后点击**保存策略**。
- en: Now whenever the CPU from one of our instances goes above 80 percent, we will
    receive an e-mail notification. If we ever need to review our policies, we can
    find them in the Alerting dropdown and then in Policies Overview at the menu on
    the left-hand side of the screen.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每当我们的实例之一的 CPU 使用率超过 80％，我们将收到电子邮件通知。如果我们需要审查我们的策略，我们可以在**警报**下拉菜单中找到它们，然后在屏幕左侧的菜单中找到**策略概览**。
- en: Beyond system monitoring with Sysdig
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越系统监控与 Sysdig
- en: Monitoring our cloud systems is a great start, but what about visibility to
    the containers themselves? Although there are a variety of cloud monitoring and
    visibility tools, Sysdig stands out for its ability to dive deep not only into
    system operations but specifically containers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 监控我们的云系统是一个很好的开始，但是对于容器本身的可见性呢？虽然有各种各样的云监控和可见性工具，但 Sysdig 以其不仅可以深入了解系统操作而且特别是容器而脱颖而出。
- en: Sysdig is open source and is billed as *a universal system visibility tool with
    native support for containers *(you can refer to more details about this in point
    2 in the *References* section at the end of the chapter). It is a command-line
    tool, which provides insight into the areas we've looked at earlier, such as storage,
    network, and system processes. What sets it apart is the level of detail and visibility
    it offers for these process and system activities. Furthermore, it has native
    support for containers, which gives us a full picture of our container operations.
    This is a highly recommended tool for your container operations arsenal. The main
    website of Sysdig is [http://www.sysdig.org/](http://www.sysdig.org/).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Sysdig 是开源的，并被称为*具有对容器的本地支持的通用系统可见性工具*（您可以在本章末尾的*参考资料*部分的第 2 点中了解更多详情）。它是一个命令行工具，可以提供我们之前看过的领域的见解，例如存储、网络和系统进程。它的独特之处在于提供了这些进程和系统活动的详细信息和可见性水平。此外，它对容器有本地支持，这为我们提供了我们容器操作的全貌。这是您容器操作工具库中强烈推荐的工具。Sysdig
    的主要网站是 [http://www.sysdig.org/](http://www.sysdig.org/)。
- en: Sysdig Cloud
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sysdig Cloud
- en: We will take a look at the Sysdig tool and some of the useful command-line-based
    UIs in a moment. However, the team at Sysdig has also built a commercial product,
    named **Sysdig Cloud**, which provides the advanced dashboard, alerting, and notification
    services we discussed earlier in the chapter. Also, the differentiator here has
    high visibility into containers, including some nice visualizations of our application
    topology.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将马上看一下 Sysdig 工具和一些有用的基于命令行的用户界面。然而，Sysdig 团队还开发了一款商业产品，名为**Sysdig Cloud**，提供了我们在本章前面讨论过的高级仪表板、警报和通知服务。此外，这里的区别在于对容器的高可见性，包括我们应用程序拓扑的一些漂亮的可视化效果。
- en: If you'd rather skip the *Sysdig Cloud* section and just try out the command-line
    tool, simply skip to the *Sysdig command line* section later in this chapter.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您宁愿跳过*Sysdig Cloud*部分，只想尝试命令行工具，请直接跳到本章后面的*Sysdig 命令行*部分。
- en: If you have not done so already, sign up for Sysdig Cloud at [http://www.sysdigcloud.com](http://www.sysdigcloud.com).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有注册，请在 [http://www.sysdigcloud.com](http://www.sysdigcloud.com) 上注册 Sysdig
    Cloud。
- en: After activating and logging in for the first time, we'll be taken to a welcome
    page. Clicking on Next, we are shown a page with various options to install the
    `sysdig` agents. For our example environment, we will use the Kubernetes setup.
    Selecting Kubernetes will give you a page with your API key and a link to instructions.
    The instructions will walk you through how to create a Sysdig agent DaemonSet
    on your cluster. Don't forget to add the API Key from the install page.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次激活并登录后，我们将被带到一个欢迎页面。点击“下一步”，我们将看到一个页面，其中有各种选项可以安装`sysdig`代理。对于我们的示例环境，我们将使用
    Kubernetes 设置。选择 Kubernetes 将为您提供一个带有 API 密钥和指令链接的页面。该指令将指导您如何在集群上创建 Sysdig 代理
    DaemonSet。不要忘记在安装页面上添加 API 密钥。
- en: We will not be able to continue on the install page until the agents connect.
    After creating the DaemonSet and waiting a moment, the page should continue to
    the AWS integration page. You can fill this out if you like, but for this walk-through
    we will click on Skip. Then, click on Let's Get Started.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理连接之前，我们将无法继续安装页面。创建 DaemonSet 并等待片刻后，页面应继续到 AWS 集成页面。如果您愿意，您可以填写此表单，但是对于本次演练，我们将点击“跳过”。然后，点击“让我们开始吧”。
- en: As of this writing, Sysdig and Sysdig Cloud were not fully compatible with the
    latest container OS deployed by default in the GCE `kube-up` script, Container-Optimized
    OS from Google: [https://cloud.google.com/container-optimized-os/docs](https://cloud.google.com/container-optimized-os/docs).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 就目前而言，Sysdig 和 Sysdig Cloud 与 GCE `kube-up` 脚本默认部署的最新容器操作系统不完全兼容，该操作系统是谷歌的 Container-Optimized
    OS：[https://cloud.google.com/container-optimized-os/docs](https://cloud.google.com/container-optimized-os/docs)。
- en: 'We''ll be taken to the main sysdig cloud dashboard screen. We should see at
    least two minion nodes  appear under the Explore tab. We should see something
    similar to the following screenshot with our minion nodes:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将被带到主 Sysdig Cloud 仪表板屏幕。在 Explore 选项卡下，我们应该看到至少两个 minion 节点。我们应该看到类似以下带有我们的
    minion 节点的截图：
- en: '![](img/B06302_08_09.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_09.png)'
- en: Sysdig Cloud Explore page
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Sysdig Cloud 探索页面
- en: This page shows us a table view, and the links on the left let us explore some
    key metrics for CPU, memory, networking, and so on. Although this is a great start,
    the detailed views will give us a much deeper look at each node.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此页面显示了一个表格视图，左侧的链接让我们探索一些关键的 CPU、内存、网络等指标。虽然这是一个很好的开始，但详细视图将让我们更深入地了解每个节点。
- en: Detailed views
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 详细视图
- en: 'Let''s take a look at these views. Select one of the minion nodes and then
    scroll down to the detail section that appears below. By default, we should see
    the System: Overview by Process view (if it''s not selected, just click on it
    from the list on the left-hand side). If the chart is hard to read, simply use
    the maximize icon in the top-left corner of each graph for a larger view.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们来看看这些视图。选择其中一个 minion 节点，然后滚动到下方出现的详细部分。默认情况下，我们应该看到 System: Overview by
    Process 视图（如果未选中，请从左侧的列表中单击它）。如果图表难以阅读，只需点击每个图表左上角的最大化图标即可获得更大的视图。'
- en: There are a variety of interesting views to explore. Just to call out a few
    others, Services | HTTP Overview and Hosts & Containers | Overview by Container
    give us some great charts for inspection. In the later view, we can see stats
    for CPU, memory, network, and file usage by container.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种有趣的视图可供探索。仅列出其中一些，Services | HTTP Overview 和 Hosts & Containers | Overview
    by Container 为我们提供了一些很棒的图表供检查。在后一视图中，我们可以看到容器的 CPU、内存、网络和文件使用情况统计。
- en: Topology views
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拓扑视图
- en: 'In addition, there are three topology views at the bottom. These views are
    perfect for helping us understand how our application is communicating. Click
    on Topology | Network Traffic and wait a few seconds for the view to fully populate.
    It should look similar to the following screenshot:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，底部还有三个拓扑视图。这些视图非常适合帮助我们了解我们的应用程序如何通信。点击 Topology | Network Traffic，等待几秒钟让视图完全填充。它应该看起来类似以下截图：
- en: '![](img/B06302_08_10.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_10.png)'
- en: Sysdig Cloud network topology view
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Sysdig Cloud 网络拓扑视图
- en: 'We note the view maps out the flow of communication between the minion nodes
    and the master in the cluster. You may also note a + symbol in the top corner
    of the node boxes. Click on that in one of the minion nodes and use the zoom tools
    at the top of the view area to zoom into the details, as you see in the following
    screenshot:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到视图将集群中的 minion 节点与主节点之间的通信流量进行了映射。您还可以在节点方框的右上角看到一个 + 符号。点击其中一个 minion
    节点，然后使用视图区域顶部的缩放工具放大到细节，如下面的截图所示：
- en: '![](img/B06302_08_11.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_11.png)'
- en: The Sysdig Cloud network topology detailed view
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Sysdig Cloud 网络拓扑详细视图
- en: Note that we can now see all the components of Kubernetes running inside the
    master. We can see how the various components work together. We will see `kube-proxy` and
    the `kublet` process running, as well as a number of boxes with the Docker whale,
    which indicate that they are containers. If we zoom in and use the plus icon,
    we will see that these are the containers for our pods and core Kubernetes processes,
    as we saw in the services running on the master section in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to* *Kubernetes*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，现在我们可以看到运行在主节点内的 Kubernetes 的所有组件。我们可以看到各种组件是如何协同工作的。我们将看到 `kube-proxy`
    和 `kublet` 进程正在运行，以及一些带有 Docker 鲸鱼标志的方框，这表示它们是容器。如果我们放大并使用加号图标，我们将看到这些是我们的 Pod
    和核心 Kubernetes 进程的容器，就像我们在第一章[Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml)，*Introduction
    to* *Kubernetes* 中运行在主节点上的服务部分中所见到的一样。
- en: Also, if you have the master included in your monitored nodes, we can watch
    `kublet` initiate communication from a minion and follow it all the way through
    the `kube-apiserver` container in the master.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您的监控节点中包括了主节点，我们可以观察 `kublet` 从 minion 发起通信，并一直跟踪到主节点中的 `kube-apiserver`
    容器。
- en: We can even sometimes see the instance communication with GCE infrastructure
    to update metadata. This view is great in order to get a mental picture of how
    our infrastructure and underlying containers are talking to one another.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至有时可以看到实例与 GCE 基础架构进行通信以更新元数据。此视图非常适合形成我们的基础架构和底层容器之间如何通信的心理图像。
- en: Metrics
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指标
- en: Next, let's switch over to the Metrics tab in the left-hand menu next to Views.
    Here, there are also a variety of helpful views.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们切换到左侧菜单旁边的 Metrics 标签。在这里，还有各种有用的视图。
- en: 'Let''s look at capacity.estimated.request.total.count in System. This view
    shows us an estimate of how many requests a node is capable of handling when fully
    loaded. This can be really useful for infrastructure planning:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 System 中的 `capacity.estimated.request.total.count`。这个视图向我们展示了一个节点在完全加载时可以处理多少请求的估计值。这对基础设施规划非常有用：
- en: '![](img/B06302_08_12.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_12.png)'
- en: Sysdig Cloud capacity estimate view
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Sysdig Cloud 容量估算视图
- en: Alerting
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 警报
- en: Now that we have all this great information, let's create some notifications.
    Scroll back up to the top of the page and find the bell icon next to one of your
    minion entries. This will open a Create Alert dialog. Here, we can set manual
    alerts similar to what we did earlier in the chapter. However, there is also the
    option to use BASELINE and HOST COMPARISON.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有这些很棒的信息，让我们创建一些通知。滚动到页面顶部，找到一个 minion 条目旁边的铃铛图标。这将打开一个创建警报的对话框。在这里，我们可以设置类似于本章前面所做的手动警报。但是，还有使用
    **BASELINE** 和 **HOST COMPARISON** 的选项。
- en: 'Using the BASELINE option is extremely helpful as Sysdig will watch the historical
    patterns of the node and alert us whenever one of the metrics strays outside the
    expected metric thresholds. No manual settings are required, so this can really
    save time for the notification setup and help our operations team to be proactive
    before issues arise. Refer to the following image:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **BASELINE** 选项非常有帮助，因为 Sysdig 将监视节点的历史模式，并在任何一个指标偏离预期指标阈值时向我们发出警报。不需要手动设置，因此这可以真正节省通知设置的时间，并帮助我们的运维团队在问题出现之前采取主动措施。请参考以下图片：
- en: '![](img/B06302_08_13.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_13.png)'
- en: Sysdig Cloud new alert
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Sysdig Cloud 新警报
- en: The HOST COMPARISON option is also a great help as it allows us to compare metrics
    with other hosts and alert whenever one host has a metric that differs significantly
    from the group. A great use case for this is monitoring resource usage across
    minion nodes to ensure that our scheduling constraints are not creating a bottleneck
    somewhere in the cluster.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**HOST COMPARISON** 选项也是一个很好的帮助，因为它允许我们将指标与其他主机进行比较，并在一个主机具有与组不同的指标时发出警报。一个很好的用例是监视
    minion 节点之间的资源使用情况，以确保我们的调度约束没有在集群的某个地方创建瓶颈。'
- en: You can choose whichever option you like and give it a name and warning level.
    Enable the notification method. Sysdig supports e-mail, **SNS** (short for **Simple
    Notification Service**), and **PagerDuty** as notification methods. You can optionally
    enable **Sysdig Capture** to gain deeper insight into issues. Once you have everything
    set, just click on Create and you will start to receive alerts as issues come
    up.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择任何您喜欢的选项并给它一个名称和警告级别。启用通知方法。Sysdig 支持电子邮件，**SNS**（简称**简单通知服务**）和 **PagerDuty**
    作为通知方法。您还可以选择启用 **Sysdig Capture** 以更深入地了解问题。一切都设置好后，只需点击创建，您就会开始收到问题警报。
- en: The sysdig command line
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: sysdig 命令行
- en: Whether you only use the open-source tool or you are trying out the full Sysdig
    Cloud package, the command-line utility is a great companion to have to track
    down issues or get a deeper understanding of your system.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您只使用开源工具还是尝试完整的 Sysdig Cloud 套装，命令行实用程序都是跟踪问题或更深入了解系统的绝佳伴侣。
- en: In the core tool, there is the main `sysdig` utility and also a command-line
    style UI named `csysdig`. Let's take a look at a few useful commands.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心工具中，有一个主要的 `sysdig` 实用程序，还有一个名为 `csysdig` 的命令行样式的用户界面。让我们看看一些有用的命令。
- en: 'Find the relevant install instructions for your OS here:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里找到您操作系统的相关安装说明：
- en: '[http://www.sysdig.org/install/](http://www.sysdig.org/install/)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.sysdig.org/install/](http://www.sysdig.org/install/)'
- en: 'Once installed, let''s first look at the process with the most network activity
    by issuing the following command:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，让我们首先查看网络活动最多的进程，发出以下命令：
- en: '[PRE5]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![](img/B06302_08_14.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_14.png)'
- en: A Sysdig top process by network activity
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 按网络活动排名的 Sysdig 高级进程
- en: 'This is an interactive view that will show us a top process in terms of network
    activity. Also, there are a plethora of commands to use with `sysdig`. A few other
    useful commands to try out include the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个交互式视图，将向我们显示网络活动最多的顶级进程。此外，还有大量可与 `sysdig` 一起使用的命令。尝试一下其他几个有用的命令，包括以下内容：
- en: '[PRE6]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: More examples can be found at [http://www.sysdig.org/wiki/sysdig-examples/](http://www.sysdig.org/wiki/sysdig-examples/).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 更多示例可以在[http://www.sysdig.org/wiki/sysdig-examples/](http://www.sysdig.org/wiki/sysdig-examples/)找到。
- en: The csysdig command-line UI
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: csysdig 命令行UI
- en: 'Because we are in a shell on one of our nodes doesn''t mean we can''t have
    a UI. Csysdig is a customizable UI to explore all the metrics and insight that
    Sysdig provides. Simply type `csysdig` at the prompt:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在一个节点的shell上并不意味着我们不能拥有一个UI。Csysdig是一个可定制的UI，用于探索Sysdig提供的所有指标和洞察力。只需在提示符下键入`csysdig`：
- en: '[PRE7]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After entering csysdig, we see a real-time listing of all processes on the machine.
    At the bottom of the screen, you'll note a menu with various options. Click on
    Views or press *F2* if you love to use your keyboard. On the left-hand menu, there
    are a variety of options, but we'll look at threads. Double-click to select Threads.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 进入csysdig后，我们看到机器上所有进程的实时列表。在屏幕底部，您会注意到一个带有各种选项的菜单。点击Views或按下*F2*（如果您喜欢使用键盘）。在左侧菜单中，有各种选项，但我们将查看线程。双击以选择线程。
- en: On some operating systems and with some SSH clients, you may have issues with
    the Function keys. Check the settings on your terminal and make sure the function
    keys are using the VT100+ sequences.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些操作系统和某些SSH客户端上，您可能会遇到功能键的问题。检查终端的设置，并确保功能键使用VT100+序列。
- en: We can see all the threads currently running on the system and some information
    about the resource usage. By default, we see a big list that is updating often.
    If we click on the Filter, *F4* for the mouse challenged, we can slim down the
    list.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到当前系统上所有正在运行的线程以及一些关于资源使用情况的信息。默认情况下，我们看到的是一个经常更新的大列表。如果我们点击过滤器，*F4*用于鼠标受挑战者，我们可以简化列表。
- en: 'Type `kube-apiserver`, if you are on the master, or `kube-proxy`, if you are
    on a node (minion), in the filter box and press *Enter*. The view now filters
    for only the threads in that command:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤框中键入`kube-apiserver`（如果您在主节点上）或`kube-proxy`（如果您在节点（minion）上），然后按*Enter*。视图现在仅过滤该命令中的线程：
- en: '![](img/B06302_08_15.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_08_15.png)'
- en: Csysdig threads
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Csysdig 线程
- en: If we want to inspect a little further, we can simply select one of the threads
    in the list and click on Dig or press *F6*. Now we see a detailed listing of system
    calls from the command in real time. This can be a really useful tool to gain
    deep insight into the containers and processing running on our cluster.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想进一步检查，我们可以简单地选择列表中的一个线程，然后点击Dig或按下*F6*。现在我们可以实时查看来自命令的系统调用的详细列表。这可以是一个非常有用的工具，可以深入了解我们集群上正在运行的容器和处理。
- en: Click on Back or press the *Backspace* key to go back to the previous screen.
    Then, go to Views once more. This time, we will look at the Containers view. Once
    again, we can filter and also use the Dig view to get more in-depth visibility
    into what is happening at a system call level.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“返回”或按下*Backspace*键返回上一个屏幕。然后，再次转到“Views”。这次，我们将查看容器视图。再次，我们可以过滤并且还可以使用Dig视图来更深入地查看发生在系统调用级别的情况。
- en: Another menu item you might note here is Actions, which is available in the
    newest release. These features allow us to go from process monitoring to action
    and response. It gives us the ability to perform a variety of actions from the
    various process views in csysdig. For example, the container view has actions
    to drop into a bash shell, kill containers, inspect logs, and more. It's worth
    getting to know the various actions and hotkeys and even add your own custom hotkeys
    for common operations.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可能注意到的另一个菜单项是Actions，在最新版本中可用。这些功能使我们能够从进程监视转到操作和响应。它使我们能够从csysdig的各种进程视图中执行各种操作。例如，容器视图具有进入bash
    shell、杀死容器、检查日志等操作。值得了解各种操作和快捷键，甚至添加您自己的常见操作的自定义快捷键。
- en: Prometheus
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Prometheus
- en: A newcomer to the monitoring scene is an open-source tool called **Prometheus**.
    Prometheus is an open-source monitoring tool that was built by a team at SoundCloud.
    You can find more about the project from [https://prometheus.io](https://prometheus.io).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 监控领域的一个新手是一个名为**Prometheus**的开源工具。Prometheus是一个由SoundCloud团队构建的开源监控工具。您可以从[https://prometheus.io](https://prometheus.io)了解更多关于该项目的信息。
- en: 'Their website offers the following features (you can refer to more details
    about this in point 3 in the *References* section at the end of the chapter):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的网站提供以下功能（您可以在本章末尾的*参考资料*中查看更多关于此的详细信息）：
- en: A multi-dimensional [data model](https://prometheus.io/docs/concepts/data_model/)
    (time series identified by metric name and key/value pairs)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个多维度的[数据模型](https://prometheus.io/docs/concepts/data_model/)（由度量名称和键/值对标识的时间序列）
- en: A [flexible query language](https://prometheus.io/docs/querying/basics/) to
    leverage this dimensionality
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[灵活的查询语言](https://prometheus.io/docs/querying/basics/)来利用这种多维性
- en: No reliance on distributed storage; single server nodes are autonomous
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不依赖于分布式存储；单服务器节点是自主的
- en: Time series collection happens via a pull model over HTTP
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列收集通过 HTTP 的拉模型实现
- en: '[pushing time series](https://prometheus.io/docs/instrumenting/pushing/) is
    supported via an intermediary gateway'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一个中间网关支持[推送时间序列](https://prometheus.io/docs/instrumenting/pushing/)
- en: Targets are discovered via service discovery or static configuration
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标通过服务发现或静态配置发现
- en: Multiple modes of graphing and dashboard support
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多种图形和仪表板支持模式
- en: 'CoreOS has a nice blog post on setting up Prometheus with Kubernetes here:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 在这里有一篇关于如何在 Kubernetes 中设置 Prometheus 的好博文：
- en: '[https://coreos.com/blog/monitoring-kubernetes-with-prometheus.html](https://coreos.com/blog/monitoring-kubernetes-with-prometheus.html)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://coreos.com/blog/monitoring-kubernetes-with-prometheus.html](https://coreos.com/blog/monitoring-kubernetes-with-prometheus.html)'
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We took a quick look at monitoring and logging with Kubernetes. You should now
    be familiar with how Kubernetes uses cAdvisor and Heapster to collect metrics
    on all the resources in a given cluster. Furthermore, we saw how Kubernetes saves
    us time by providing InfluxDB and Grafana set up and configured out of the box.
    Dashboards are easily customizable for our everyday operational needs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要了解了如何使用 Kubernetes 监控和记录。现在您应该对 Kubernetes 如何使用 cAdvisor 和 Heapster 收集给定集群中所有资源的指标有所了解。此外，我们还看到
    Kubernetes 通过提供 InfluxDB 和 Grafana 设置和配置可以为我们节省时间。仪表板可以根据我们的日常运营需求进行轻松定制。
- en: In addition, we looked at the built-in logging capabilities with FluentD and
    the Google Cloud Logging service. Also, Kubernetes gives us great time savings
    by setting up the basics for us.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还查看了使用 FluentD 和 Google 云日志服务的内置日志功能。此外，Kubernetes 通过为我们设置基础提供了极大的节约时间。
- en: Finally, you learned about the various third-party options available to monitor
    our containers and clusters. Using these tools will allow us to gain even more
    insight into the health and status of our applications. All these tools combine
    to give us a solid toolset to manage day-to-day operations.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您了解了监控我们的容器和集群的各种第三方选项。使用这些工具将使我们能够更深入地了解我们应用程序的健康和状态。所有这些工具结合在一起，为我们提供了一个扎实的工具集来管理我们的日常运营。
- en: In the next chapter, we will explore the new cluster federation capabilities.
    Still mostly in beta, this functionality will allow us to run multiple clusters
    in different datacenters and even clouds, but manage and distribute applications
    from a single control plane.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨新的集群联邦功能。尽管仍然主要处于测试阶段，但这个功能将允许我们在不同数据中心甚至云中运行多个集群，但通过单一控制平面管理和分发应用程序。
- en: References
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[http://stackdriver.com/](http://stackdriver.com/)'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://stackdriver.com/](http://stackdriver.com/)'
- en: '[http://www.sysdig.org/wiki/](http://www.sysdig.org/wiki/)'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://www.sysdig.org/wiki/](http://www.sysdig.org/wiki/)'
- en: '[https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/)'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/)'
