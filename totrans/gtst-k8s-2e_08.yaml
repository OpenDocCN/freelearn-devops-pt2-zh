- en: Monitoring and Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will cover the usage and customization of both built-in and third-party
    monitoring tools on our Kubernetes cluster. We will cover how to use the tools
    to monitor the health and performance of our cluster. In addition, we will look
    at built-in logging, the **Google Cloud Logging** service, and **Sysdig**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How Kuberentes uses cAdvisor, Heapster, InfluxDB, and Grafana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing the default Grafana dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using FluentD and Grafana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and using logging tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with popular third-party tools, such as StackDriver and Sysdig, to extend
    our monitoring capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-world monitoring goes far beyond checking whether a system is up and running.
    Although health checks, like those you learned in [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml),
    *Pods, Services, Replication Controllers, and Labels*, in the *Health checks*
    section, can help us isolate problem applications. Operation teams can best serve
    the business when they can anticipate the issues and mitigate them before a system
    goes offline.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in monitoring are to measure the performance and usage of core
    resources and watch for trends that stray from the normal baseline. Containers
    are not different here, and a key component to managing our Kubernetes cluster
    is having a clear view into performance and availability of the OS, network, system
    (CPU and memory), and storage resources across all nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will examine several options to monitor and measure the
    performance and availability of all our cluster resources. In addition, we will
    look at a few options for alerting and notifications when irregular trends start
    to emerge.
  prefs: []
  type: TYPE_NORMAL
- en: Built-in monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you recall from [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to Kubernetes*, we noted that our nodes were already running a number
    of monitoring services. We can see these once again by running the `get pods`
    command with the `kube-system` namespace specified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: System pod listing
  prefs: []
  type: TYPE_NORMAL
- en: Again, we see a variety of services, but how does this all fit together? If
    you recall the *Node (formerly minions)* section from [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml),
    *Pods, Services, Replication Controllers, and Labels*, each node is running a
    kublet. The kublet is the main interface for nodes to interact and update the
    API server. One such update is the **metrics** of the node resources. The actual
    reporting of the resource usage is performed by a program named **cAdvisor**.
  prefs: []
  type: TYPE_NORMAL
- en: cAdvisor is another open-source project from Google, which provides various
    metrics on container resource use. Metrics include CPU, memory, and network statistics.
    There is no need to tell cAdvisor about individual containers; it collects the
    metrics for all containers on a node and reports this back to the kublet, which
    in turn reports to Heapster.
  prefs: []
  type: TYPE_NORMAL
- en: '**Google''s open-source projects** Google has a variety of open-source projects
    related to Kubernetes. Check them out, use them, and even contribute your own
    code!'
  prefs: []
  type: TYPE_NORMAL
- en: 'cAdvisor and Heapster are mentioned in the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '**cAdvisor**: [https://github.com/google/cadvisor](https://github.com/google/cadvisor)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heapster**: [https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contrib** is a catch-all for a variety of components that are not part of
    core Kubernetes. It is found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kubernetes/contrib](https://github.com/kubernetes/contrib).'
  prefs: []
  type: TYPE_NORMAL
- en: '**LevelDB** is a key store library that was used in the creation of InfluxDB.
    It is found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/google/leveldb](https://github.com/google/leveldb).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Heapster** is yet another open-source project from Google; you may start
    to see a theme emerging here (see the preceding information box). Heapster runs
    in a container on one of the minion nodes and aggregates the data from kublet.
    A simple REST interface is provided to query the data.'
  prefs: []
  type: TYPE_NORMAL
- en: When using the GCE setup, a few additional packages are set up for us, which
    saves us time and gives us a complete package to monitor our container workloads.
    As we can see from the preceding *System pod listing* screenshot, there is another
    pod with `influx-grafana` in the title.
  prefs: []
  type: TYPE_NORMAL
- en: '**InfluxDB** is described on its official website as follows (you can refer
    to more details about this in point 1 in the *References* section at the end of
    the chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: An open-source distributed time series database with no external dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: InfluxDB is based on a key store package (refer to the previous *Google's open-source
    projects* information box) and is perfect to store and query event—or time-based
    statistics such as those provided by Heapster.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have **Grafana**, which provides a dashboard and graphing interface
    for the data stored in InfluxDB. Using Grafana, users can create a custom monitoring
    dashboard and get immediate visibility into the health of their Kubernetes cluster
    and therefore their entire container infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Heapster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s quickly look at the REST interface by running SSH to the node with the
    Heapster pod. First, we can list the pods to find the one running Heapster, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The name of the pod should start with `monitoring-heapster`. Run a `describe`
    command to see which node it is running on, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'From the output in the following screenshot, we can see that the pod is running
    in `kubernetes-minion-merd`. Also note the IP for the pod, a few lines down, as
    we will need that in a moment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Heapster pod details
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can SSH to this box with the familiar `gcloud ssh` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: From here, we can access the Heapster REST API directly using the pod's IP address.
    Remember that pod IPs are routable not only in the containers but also on the
    nodes themselves. The `Heapster` API is listening on port `8082`, and we can get
    a full list of metrics at `/api/v1/metric-export-schema/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the list now by issuing a `curl` command to the pod IP address we
    saved from the `describe` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see a listing that is quite long. The first section shows all the metrics
    available. The last two sections list fields by which we can filter and group.
    For your convenience, I''ve added the following tables that are a little bit easier
    to read:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Description** | **Unit** | **Type** |'
  prefs: []
  type: TYPE_TB
- en: '| uptime | The number of milliseconds since the container was started | ms
    | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| cpu/usage | The cumulative CPU usage on all cores | ns | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| cpu/limit | The CPU limit in millicores | - | gauge |'
  prefs: []
  type: TYPE_TB
- en: '| memory/usage | Total memory usage | bytes | gauge |'
  prefs: []
  type: TYPE_TB
- en: '| memory/working_set | Total working set usage; the working set is the memory
    being used and not easily dropped by the kernel | bytes | gauge |'
  prefs: []
  type: TYPE_TB
- en: '| memory/limit | The memory limit | bytes | gauge |'
  prefs: []
  type: TYPE_TB
- en: '| memory/page_faults | The number of page faults | - | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| memory/major_page_faults | The number of major page faults | - | cumulative
    |'
  prefs: []
  type: TYPE_TB
- en: '| network/rx | The cumulative number of bytes received over the network | bytes
    | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| network/rx_errors | The cumulative number of errors while receiving over
    the network | - | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| network/tx | The cumulative number of bytes sent over the network | bytes
    | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| network/tx_errors | The cumulative number of errors while sending over the
    network | - | cumulative |'
  prefs: []
  type: TYPE_TB
- en: '| filesystem/usage | The total number of bytes consumed on a filesystem | bytes
    | gauge |'
  prefs: []
  type: TYPE_TB
- en: '| filesystem/limit | The total size of filesystem in bytes | bytes | gauge
    |'
  prefs: []
  type: TYPE_TB
- en: '| filesystem/available | The number of available bytes remaining in a the filesystem
    | bytes | gauge |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1\. Available Heapster metrics
  prefs: []
  type: TYPE_NORMAL
- en: '| **Field** | **Description** | **Label type** |'
  prefs: []
  type: TYPE_TB
- en: '| `nodename` | The nodename where the container ran | Common |'
  prefs: []
  type: TYPE_TB
- en: '| `hostname` | The hostname where the container ran | Common |'
  prefs: []
  type: TYPE_TB
- en: '| `host_id` | An identifier specific to a host, which is set by the cloud provider
    or user | Common |'
  prefs: []
  type: TYPE_TB
- en: '| `container_base_image` | The user-defined image name that is run inside the
    container | Common |'
  prefs: []
  type: TYPE_TB
- en: '| `container_name` | The user-provided name of the container or full container
    name for system containers | Common |'
  prefs: []
  type: TYPE_TB
- en: '| `pod_name` | The name of the pod | Pod |'
  prefs: []
  type: TYPE_TB
- en: '| `pod_id` | The unique ID of the pod | Pod |'
  prefs: []
  type: TYPE_TB
- en: '| `pod_namespace` | The namespace of the pod | Pod |'
  prefs: []
  type: TYPE_TB
- en: '| `namespace_id` | The unique ID of the namespace of the pod | Pod |'
  prefs: []
  type: TYPE_TB
- en: '| `labels` | A comma-separated list of user-provided labels | Pod |'
  prefs: []
  type: TYPE_TB
- en: Table 6.2\. Available Heapster fields
  prefs: []
  type: TYPE_NORMAL
- en: Customizing our dashboards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the fields, we can have some fun. Recall the Grafana page
    we looked at in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml), *Introduction to Kubernetes*.
    Let''s pull that up again by going to our cluster''s monitoring URL. Note that
    you may need to log in with your cluster credentials. Refer to the following format
    of the link you need to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`https://**<your master IP>**/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana`'
  prefs: []
  type: TYPE_NORMAL
- en: We'll see the default Home dashboard. Click on the down arrow next to Home and
    select Cluster. This shows the Kubernetes cluster dashboard, and now we can add
    our own statistics to the board. Scroll all the way to the bottom and click on
    Add a Row. This should create a space for a new row and present a green tab on
    the left-hand side of the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by adding a view into the filesystem usage for each node (minion).
    Click on the *green* tab to expand and then select Add Panel and then graph. An
    empty graph should appear on the screen along with a query panel for our custom
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: The first field in this panel should show a query that starts with 'SELECT mean("value")
    FROM ...'. Click on the A character next to this field to expand it. Leave the
    first field next to FROM as default and then click on the next field with the select
    measurement value. A dropdown menu will appear with the Heapster metrics we saw
    in the previous tables. Select `filesystem/usage_bytes_gauge`. Now in the SELECT
    row, click on mean() and then on the x symbol to remove it. Next, click on the
    + symbol on the end of the row and add selectors -> max. Then, you'll see a GROUP
    BY row with time($interval) and fill(none). Carefully click on fill and not on
    the (none) portion and again on x to remove it. Then, click on the + symbol at
    the end of the row and select tag(hostname).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, at the bottom of the screen we should see a Group by time interval.
    Enter `5s` there and you should have something similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Heapster pod details
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's click on the Axes tab, so that we can set the units and legend.
    Under Left Y Axis, click on the field next to Unit and set it to data -> bytes
    and Label to Disk Space Used. Under Right Y Axis, set Unit to none -> none. Next,
    on the Legend tab, make sure to check Show in Options and Max in Values.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's quickly go to the General tab and choose a title. In my case, I named
    mine `Filesystem Disk Usage by Node (max)`.
  prefs: []
  type: TYPE_NORMAL
- en: We don't want to lose this nice new graph we've created, so let's click on the
    save icon in the top right corner. It looks like a *floppy disk* (you can do a
    Google image search if you don't know what this is).
  prefs: []
  type: TYPE_NORMAL
- en: After we click on the save icon, we will see a green dialog box that verifies
    the dashboard was saved. We can now click the x symbol above the graph details
    panel and below the graph itself.
  prefs: []
  type: TYPE_NORMAL
- en: This will return us to the dashboard page. If we scroll all the way down, we
    will see our new graph. Let's add another panel to this row. Again use the *green*
    tab and then select Add Panel -> singlestat. Once again, an empty panel will appear
    with a setting form below it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say, we want to watch a particular node and monitor network usage. We
    can easily do this by first going to the Metrics tab. Then expand the query field
    and set the second value in the FROM field to network/rx. Now we can specify the
    WHERE clause by clicking the + symbol at the end of the row and choosing hostname
    from the dropdown. After hostname = click on select tag value and choose one of
    the minion nodes from the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, leave mean() for the second SELECT field:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Singlestat options
  prefs: []
  type: TYPE_NORMAL
- en: In the Options tab, make sure that Unit format is set to data -> bytes and check
    the Show box next to Spark lines. The **sparkline** gives us a quick history view
    of the recent variation in the value. We can use Background mode to take up the
    entire background; by default, it uses the area below the value.
  prefs: []
  type: TYPE_NORMAL
- en: In Coloring, we can optionally check the Value or Background box and choose
    Thresholds and Colors. This will allow us to choose different colors for the value
    based on the threshold tier we specify. Note that an unformatted version of the
    number must be used for threshold values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s go back to the General tab and set the title as `Network bytes
    received (Node35ao)`. Use the identifier for your minion node. Once again, let''s
    save our work and return to the dashboard. We should now have a row that looks
    like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Custom dashboard panels
  prefs: []
  type: TYPE_NORMAL
- en: Grafana has a number of other panel types you can play with such as Dashboard
    list, Plugin list, Table, and Text.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, it is pretty easy to build a custom dashboard and monitor the
    health of our cluster at a glance.
  prefs: []
  type: TYPE_NORMAL
- en: FluentD and Google Cloud Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking back at the *System pod listing* screenshot at the beginning of the
    chapter, you may have noted a number of pods starting with the words `fluentd-cloud-logging-kubernetes...`
    . These pods appear when using the GCE provider for your K8s cluster. A pod like
    this exists on every node in our cluster and its sole purpose is to handle the
    processing of Kubernetes logs.
  prefs: []
  type: TYPE_NORMAL
- en: If we log in to our Google Cloud Platform account, we can see some of the logs
    processed there. Simply use the left side, under Stackdriver select Logging. This
    will take us to a log listing page with a number of drop-down menus on the top.
    If this is your first time visiting the page, the first dropdown will likely be
    set to Cloud HTTP Load Balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this drop-down menu, we''ll see a number of GCE types of entries. Select GCE
    VM Instances and then the Kubernetes master or one of the nodes. In the second
    dropdown, we can choose various log groups, including kublet. We can also filter
    by the event log level and date. Additionally, we can use the *play* button to
    watch events stream in live:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_06.png)'
  prefs: []
  type: TYPE_IMG
- en: The Google Cloud Logging filter
  prefs: []
  type: TYPE_NORMAL
- en: FluentD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we know that the `fluentd-cloud-logging-kubernetes` pods are sending the
    data to the Google Cloud, but why do we need FluentD? Simply put, **FluentD**
    is a collector. It can be configured to have multiple sources to collect and tag
    logs, which are then sent to various output points for analysis, alerting, or
    archiving. We can even transform data using plugins before it is passed on to
    its destination.
  prefs: []
  type: TYPE_NORMAL
- en: Not all provider setups have FluentD installed by default, but it is one of
    the recommended approaches to give us greater flexibility for future monitoring
    operations. The AWS Kubernetes setup also uses FluentD, but instead forwards events
    to **Elasticsearch**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploring FluentD** If you are curious about the inner workings of the FluentD
    setup or just want to customize the log collection, we can explore quite easily
    using the `kubectl exec` command and one of the pod names from the command we
    ran earlier in the chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s see if we can find the FluentD `config` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ kubectl exec fluentd-cloud-logging-kubernetes-minion-group-r4qt --namespace=kube-system
    -- ls /etc/td-agent**`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look in the `etc` folder and then `td-agent`, which is the `fluent`
    subfolder. While searching in this directory, we should see a `td-agent.conf`
    file. We can view that file with a simple `cat` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ kubectl exec fluentd-cloud-logging-kubernetes-minion-group-r4qt --namespace=kube-system
    -- cat /etc/td-agent/td-agent.conf**`'
  prefs: []
  type: TYPE_NORMAL
- en: We should see a number of sources including the various Kubernetes components,
    Docker, and some GCP elements.
  prefs: []
  type: TYPE_NORMAL
- en: While we can make changes here, remember that it is a running container and
    our changes won't be saved if the pod dies or is restarted. If we really want
    to customize, it's best to use this container as a base and build a new container
    which we can push to a repository for later use.
  prefs: []
  type: TYPE_NORMAL
- en: Maturing our monitoring operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Grafana gives us a great start to monitor our container operations, it
    is still a work in progress. In the real world of operations, having a complete
    dashboard view is great once we know there is a problem. However, in everyday
    scenarios, we'd prefer to be proactive and actually receive notifications when
    issues arise. This kind of alerting capability is a must to keep the operations
    team ahead of the curve and out of *reactive mode*.
  prefs: []
  type: TYPE_NORMAL
- en: There are many solutions available in this space, and we will take a look at
    two in particular—GCE monitoring (StackDriver) and Sysdig.
  prefs: []
  type: TYPE_NORMAL
- en: GCE (StackDriver)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**StackDriver** is a great place to start for infrastructure in the public
    cloud. It is actually owned by Google, so it''s integrated as the Google Cloud
    Platform monitoring service. Before your lock-in alarm bells start ringing, StackDriver
    also has solid integration with AWS. In addition, StackDriver has alerting capability
    with support for notification to a variety of platforms and webhooks for anything
    else.'
  prefs: []
  type: TYPE_NORMAL
- en: Sign-up for GCE monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the GCE console, in the Stackdriver section click on Monitoring. This will
    open a new window, where we can sign up for a free trial of Stackdriver. We can
    then add our GCP project and optionally an AWS account as well. This requires
    a few more steps, but instructions are included on the page. Finally, we'll be
    given instructions on how to install the agents on our cluster nodes. We can skip
    this for now, but will come back to it in a minute.
  prefs: []
  type: TYPE_NORMAL
- en: Click on Continue, set up your daily alerts, and click on Continue again.
  prefs: []
  type: TYPE_NORMAL
- en: Click on Launch Monitoring to proceed. We'll be taken to the main dashboard
    page, where we will see some basic statistics on our node in the cluster. If we
    select Resources from the side menu and then Instances, we'll be taken to a page
    with all our nodes listed. By clicking on the individual node, we can again see
    some basic information even without an agent installed.
  prefs: []
  type: TYPE_NORMAL
- en: Stackdriver also offers monitoring and logging agents that can be installed
    on the nodes. However, it currently does not support the container OS that is
    used by default in the GCE `kube-up` script. You can still see the basic metrics
    for any nodes in GCE or AWS, but will need to use another OS if you want the detailed
    agent install.
  prefs: []
  type: TYPE_NORMAL
- en: Alerts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we can look at the alerting policies available as part of the monitoring
    service. From the instance details page, click on the Create Alerting Policy button
    in the Incidents section at the top of the page.
  prefs: []
  type: TYPE_NORMAL
- en: We will click on Add Condition and select a Metric Threshold. In the Target section,
    set RESOURCE TYPE to Instance (GCE). Then, set APPLIES TO to Group and kubernetes.
    Leave CONDITION TRIGGERS IF set to Any Member Violates.
  prefs: []
  type: TYPE_NORMAL
- en: In the Configuration section, leave IF METRIC as CPU Usage (GCE Monitoring)
    and CONDITION as above. Now set THRESHOLD to `80` and set the time in FOR to 5
    minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on Save Condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Google Cloud Monitoring alert policy
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will add a notification. In the Notification section, leave Method
    as Email and enter your e-mail address.
  prefs: []
  type: TYPE_NORMAL
- en: We can skip the Documentation section, but this is where we can add text and
    formatting to alert messages.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, name the policy as `Excessive CPU Load` and click on Save Policy.
  prefs: []
  type: TYPE_NORMAL
- en: Now whenever the CPU from one of our instances goes above 80 percent, we will
    receive an e-mail notification. If we ever need to review our policies, we can
    find them in the Alerting dropdown and then in Policies Overview at the menu on
    the left-hand side of the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond system monitoring with Sysdig
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring our cloud systems is a great start, but what about visibility to
    the containers themselves? Although there are a variety of cloud monitoring and
    visibility tools, Sysdig stands out for its ability to dive deep not only into
    system operations but specifically containers.
  prefs: []
  type: TYPE_NORMAL
- en: Sysdig is open source and is billed as *a universal system visibility tool with
    native support for containers *(you can refer to more details about this in point
    2 in the *References* section at the end of the chapter). It is a command-line
    tool, which provides insight into the areas we've looked at earlier, such as storage,
    network, and system processes. What sets it apart is the level of detail and visibility
    it offers for these process and system activities. Furthermore, it has native
    support for containers, which gives us a full picture of our container operations.
    This is a highly recommended tool for your container operations arsenal. The main
    website of Sysdig is [http://www.sysdig.org/](http://www.sysdig.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Sysdig Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will take a look at the Sysdig tool and some of the useful command-line-based
    UIs in a moment. However, the team at Sysdig has also built a commercial product,
    named **Sysdig Cloud**, which provides the advanced dashboard, alerting, and notification
    services we discussed earlier in the chapter. Also, the differentiator here has
    high visibility into containers, including some nice visualizations of our application
    topology.
  prefs: []
  type: TYPE_NORMAL
- en: If you'd rather skip the *Sysdig Cloud* section and just try out the command-line
    tool, simply skip to the *Sysdig command line* section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: If you have not done so already, sign up for Sysdig Cloud at [http://www.sysdigcloud.com](http://www.sysdigcloud.com).
  prefs: []
  type: TYPE_NORMAL
- en: After activating and logging in for the first time, we'll be taken to a welcome
    page. Clicking on Next, we are shown a page with various options to install the
    `sysdig` agents. For our example environment, we will use the Kubernetes setup.
    Selecting Kubernetes will give you a page with your API key and a link to instructions.
    The instructions will walk you through how to create a Sysdig agent DaemonSet
    on your cluster. Don't forget to add the API Key from the install page.
  prefs: []
  type: TYPE_NORMAL
- en: We will not be able to continue on the install page until the agents connect.
    After creating the DaemonSet and waiting a moment, the page should continue to
    the AWS integration page. You can fill this out if you like, but for this walk-through
    we will click on Skip. Then, click on Let's Get Started.
  prefs: []
  type: TYPE_NORMAL
- en: As of this writing, Sysdig and Sysdig Cloud were not fully compatible with the
    latest container OS deployed by default in the GCE `kube-up` script, Container-Optimized
    OS from Google: [https://cloud.google.com/container-optimized-os/docs](https://cloud.google.com/container-optimized-os/docs).
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be taken to the main sysdig cloud dashboard screen. We should see at
    least two minion nodes  appear under the Explore tab. We should see something
    similar to the following screenshot with our minion nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Sysdig Cloud Explore page
  prefs: []
  type: TYPE_NORMAL
- en: This page shows us a table view, and the links on the left let us explore some
    key metrics for CPU, memory, networking, and so on. Although this is a great start,
    the detailed views will give us a much deeper look at each node.
  prefs: []
  type: TYPE_NORMAL
- en: Detailed views
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at these views. Select one of the minion nodes and then
    scroll down to the detail section that appears below. By default, we should see
    the System: Overview by Process view (if it''s not selected, just click on it
    from the list on the left-hand side). If the chart is hard to read, simply use
    the maximize icon in the top-left corner of each graph for a larger view.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of interesting views to explore. Just to call out a few
    others, Services | HTTP Overview and Hosts & Containers | Overview by Container
    give us some great charts for inspection. In the later view, we can see stats
    for CPU, memory, network, and file usage by container.
  prefs: []
  type: TYPE_NORMAL
- en: Topology views
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition, there are three topology views at the bottom. These views are
    perfect for helping us understand how our application is communicating. Click
    on Topology | Network Traffic and wait a few seconds for the view to fully populate.
    It should look similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Sysdig Cloud network topology view
  prefs: []
  type: TYPE_NORMAL
- en: 'We note the view maps out the flow of communication between the minion nodes
    and the master in the cluster. You may also note a + symbol in the top corner
    of the node boxes. Click on that in one of the minion nodes and use the zoom tools
    at the top of the view area to zoom into the details, as you see in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_11.png)'
  prefs: []
  type: TYPE_IMG
- en: The Sysdig Cloud network topology detailed view
  prefs: []
  type: TYPE_NORMAL
- en: Note that we can now see all the components of Kubernetes running inside the
    master. We can see how the various components work together. We will see `kube-proxy` and
    the `kublet` process running, as well as a number of boxes with the Docker whale,
    which indicate that they are containers. If we zoom in and use the plus icon,
    we will see that these are the containers for our pods and core Kubernetes processes,
    as we saw in the services running on the master section in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to* *Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: Also, if you have the master included in your monitored nodes, we can watch
    `kublet` initiate communication from a minion and follow it all the way through
    the `kube-apiserver` container in the master.
  prefs: []
  type: TYPE_NORMAL
- en: We can even sometimes see the instance communication with GCE infrastructure
    to update metadata. This view is great in order to get a mental picture of how
    our infrastructure and underlying containers are talking to one another.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, let's switch over to the Metrics tab in the left-hand menu next to Views.
    Here, there are also a variety of helpful views.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at capacity.estimated.request.total.count in System. This view
    shows us an estimate of how many requests a node is capable of handling when fully
    loaded. This can be really useful for infrastructure planning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Sysdig Cloud capacity estimate view
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have all this great information, let's create some notifications.
    Scroll back up to the top of the page and find the bell icon next to one of your
    minion entries. This will open a Create Alert dialog. Here, we can set manual
    alerts similar to what we did earlier in the chapter. However, there is also the
    option to use BASELINE and HOST COMPARISON.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the BASELINE option is extremely helpful as Sysdig will watch the historical
    patterns of the node and alert us whenever one of the metrics strays outside the
    expected metric thresholds. No manual settings are required, so this can really
    save time for the notification setup and help our operations team to be proactive
    before issues arise. Refer to the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_13.png)'
  prefs: []
  type: TYPE_IMG
- en: Sysdig Cloud new alert
  prefs: []
  type: TYPE_NORMAL
- en: The HOST COMPARISON option is also a great help as it allows us to compare metrics
    with other hosts and alert whenever one host has a metric that differs significantly
    from the group. A great use case for this is monitoring resource usage across
    minion nodes to ensure that our scheduling constraints are not creating a bottleneck
    somewhere in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You can choose whichever option you like and give it a name and warning level.
    Enable the notification method. Sysdig supports e-mail, **SNS** (short for **Simple
    Notification Service**), and **PagerDuty** as notification methods. You can optionally
    enable **Sysdig Capture** to gain deeper insight into issues. Once you have everything
    set, just click on Create and you will start to receive alerts as issues come
    up.
  prefs: []
  type: TYPE_NORMAL
- en: The sysdig command line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you only use the open-source tool or you are trying out the full Sysdig
    Cloud package, the command-line utility is a great companion to have to track
    down issues or get a deeper understanding of your system.
  prefs: []
  type: TYPE_NORMAL
- en: In the core tool, there is the main `sysdig` utility and also a command-line
    style UI named `csysdig`. Let's take a look at a few useful commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the relevant install instructions for your OS here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.sysdig.org/install/](http://www.sysdig.org/install/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, let''s first look at the process with the most network activity
    by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_14.png)'
  prefs: []
  type: TYPE_IMG
- en: A Sysdig top process by network activity
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an interactive view that will show us a top process in terms of network
    activity. Also, there are a plethora of commands to use with `sysdig`. A few other
    useful commands to try out include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: More examples can be found at [http://www.sysdig.org/wiki/sysdig-examples/](http://www.sysdig.org/wiki/sysdig-examples/).
  prefs: []
  type: TYPE_NORMAL
- en: The csysdig command-line UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because we are in a shell on one of our nodes doesn''t mean we can''t have
    a UI. Csysdig is a customizable UI to explore all the metrics and insight that
    Sysdig provides. Simply type `csysdig` at the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: After entering csysdig, we see a real-time listing of all processes on the machine.
    At the bottom of the screen, you'll note a menu with various options. Click on
    Views or press *F2* if you love to use your keyboard. On the left-hand menu, there
    are a variety of options, but we'll look at threads. Double-click to select Threads.
  prefs: []
  type: TYPE_NORMAL
- en: On some operating systems and with some SSH clients, you may have issues with
    the Function keys. Check the settings on your terminal and make sure the function
    keys are using the VT100+ sequences.
  prefs: []
  type: TYPE_NORMAL
- en: We can see all the threads currently running on the system and some information
    about the resource usage. By default, we see a big list that is updating often.
    If we click on the Filter, *F4* for the mouse challenged, we can slim down the
    list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type `kube-apiserver`, if you are on the master, or `kube-proxy`, if you are
    on a node (minion), in the filter box and press *Enter*. The view now filters
    for only the threads in that command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_08_15.png)'
  prefs: []
  type: TYPE_IMG
- en: Csysdig threads
  prefs: []
  type: TYPE_NORMAL
- en: If we want to inspect a little further, we can simply select one of the threads
    in the list and click on Dig or press *F6*. Now we see a detailed listing of system
    calls from the command in real time. This can be a really useful tool to gain
    deep insight into the containers and processing running on our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Click on Back or press the *Backspace* key to go back to the previous screen.
    Then, go to Views once more. This time, we will look at the Containers view. Once
    again, we can filter and also use the Dig view to get more in-depth visibility
    into what is happening at a system call level.
  prefs: []
  type: TYPE_NORMAL
- en: Another menu item you might note here is Actions, which is available in the
    newest release. These features allow us to go from process monitoring to action
    and response. It gives us the ability to perform a variety of actions from the
    various process views in csysdig. For example, the container view has actions
    to drop into a bash shell, kill containers, inspect logs, and more. It's worth
    getting to know the various actions and hotkeys and even add your own custom hotkeys
    for common operations.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A newcomer to the monitoring scene is an open-source tool called **Prometheus**.
    Prometheus is an open-source monitoring tool that was built by a team at SoundCloud.
    You can find more about the project from [https://prometheus.io](https://prometheus.io).
  prefs: []
  type: TYPE_NORMAL
- en: 'Their website offers the following features (you can refer to more details
    about this in point 3 in the *References* section at the end of the chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: A multi-dimensional [data model](https://prometheus.io/docs/concepts/data_model/)
    (time series identified by metric name and key/value pairs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A [flexible query language](https://prometheus.io/docs/querying/basics/) to
    leverage this dimensionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No reliance on distributed storage; single server nodes are autonomous
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series collection happens via a pull model over HTTP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[pushing time series](https://prometheus.io/docs/instrumenting/pushing/) is
    supported via an intermediary gateway'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Targets are discovered via service discovery or static configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple modes of graphing and dashboard support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CoreOS has a nice blog post on setting up Prometheus with Kubernetes here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://coreos.com/blog/monitoring-kubernetes-with-prometheus.html](https://coreos.com/blog/monitoring-kubernetes-with-prometheus.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We took a quick look at monitoring and logging with Kubernetes. You should now
    be familiar with how Kubernetes uses cAdvisor and Heapster to collect metrics
    on all the resources in a given cluster. Furthermore, we saw how Kubernetes saves
    us time by providing InfluxDB and Grafana set up and configured out of the box.
    Dashboards are easily customizable for our everyday operational needs.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we looked at the built-in logging capabilities with FluentD and
    the Google Cloud Logging service. Also, Kubernetes gives us great time savings
    by setting up the basics for us.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned about the various third-party options available to monitor
    our containers and clusters. Using these tools will allow us to gain even more
    insight into the health and status of our applications. All these tools combine
    to give us a solid toolset to manage day-to-day operations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the new cluster federation capabilities.
    Still mostly in beta, this functionality will allow us to run multiple clusters
    in different datacenters and even clouds, but manage and distribute applications
    from a single control plane.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[http://stackdriver.com/](http://stackdriver.com/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://www.sysdig.org/wiki/](http://www.sysdig.org/wiki/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
