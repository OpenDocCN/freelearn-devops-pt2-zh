- en: Updates, Gradual Rollouts, and Autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will expand upon the core concepts, which show you how to roll
    out updates and test new features of your application with minimal disruption
    to up-time. It will cover the basics of doing application updates, gradual rollouts,
    and A/B testing. In addition, we will look at scaling the Kubernetes cluster itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Application scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A/B testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application autoscaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling up your cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since version 1.2, Kubernetes has released a Deployments API. Deployments are
    the recommended way to deal with scaling and application updates going forward.
    However, it is still considered beta at the time of writing this book, while rolling
    updates has been stable for several versions. We will explore rolling updates
    in this chapter as an introduction to the scaling concept and then dive into the
    preferred method of using deployments in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Example set up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start exploring the various capabilities built into Kubernetes for
    scaling and updates, we will need a new example environment. We are going to use
    a variation of our previous container image with a blue background (refer to the
    *v0.1 and v0.2 (side by side)* image, later in this chapter, for a comparison).
    We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-1*: `pod-scaling-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-2*: `pod-scaling-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create these services with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The public IP address for the service may take a moment to create.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over time, as you run your applications in the Kubernetes cluster, you will
    find that some applications need more resources, whereas others can manage with
    fewer resources. Instead of removing the entire RC (and associated pods), we want
    a more seamless way to scale our application up and down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thankfully, Kubernetes includes a `scale` command, which is suited specifically
    for this purpose. The `scale` command works both with Replication Controllers
    and the new Deployments abstraction. For now, we will explore the usage with Replication
    Controllers. In our new example, we have only one replica running. You can check
    this with a `get pods` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try scaling that up to three with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If all goes well, you'll simply see the scaled word on the output of your terminal
    window.
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, you can specify the `--current-replicas` flag as a verification
    step. The scaling will only occur if the actual number of replicas currently running
    matches this count.
  prefs: []
  type: TYPE_NORMAL
- en: After listing our pods once again, we should now see three pods running with
    a name similar to `node-js-scale-**XXXXX**`, where the `X` characters are a random
    string.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the `scale` command to reduce the number of replicas. In either
    case, the `scale` command adds or removes the necessary pod replicas, and the
    service automatically updates and balances across new or remaining replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Smooth updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scaling of our application up and down as our resource demands change is
    useful for many production scenarios, but what about simple application updates?
    Any production system will have code updates, patches, and feature additions.
    These could be occurring monthly, weekly, or even daily. Making sure that we have
    a reliable way to push out these changes without interruption to our users is
    a paramount consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we benefit from the years of experience the Kubernetes system is
    built on. There is a built-in support for rolling updates with the 1.0 version.
    The `rolling-update` command allows us to update entire RCs or just the underlying
    Docker image used by each replica. We can also specify an update interval, which
    will allow us to update one pod at a time and wait until proceeding to the next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take our scaling example and perform a rolling update to the 0.2 version
    of our container image. We will use an update interval of 2 minutes, so we can
    watch the process as it happens in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You should see some text about creating a new RC named `node-js-scale-XXXXX`,
    where the `X` characters will be a random string of numbers and letters. In addition,
    you will see the beginning of a loop that is starting one replica of the new version
    and removing one from the existing RC. This process will continue until the new
    RC has the full count of replicas running.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to follow along in real time, we can open another terminal window
    and use the `get pods` command, along with a label filter, to see what''s happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This command will filter for pods with `node-js-scale` in the name. If you run
    this after issuing the `rolling-update` command, you should see several pods running
    as it creates new versions and removes the old ones one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full output of the previous `rolling-update` command should look something
    like the screenshot below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: The scaling output
  prefs: []
  type: TYPE_NORMAL
- en: As we can see here, Kubernetes is first creating a new RC named `node-js-scale-10ea08ff9a118ac6a93f85547ed28f6`.
    K8s then loops through one by one, creating a new pod in the new controller and
    removing one from the old. This continues until the new controller has the full
    replica count and the old one is at zero. After this, the old controller is deleted
    and the new one is renamed with the original controller name.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run a `get pods` command now, you''ll notice that the pods still all
    have a longer name. Alternatively, we could have specified the name of a new controller
    in the command, and Kubernetes will create a new RC and pods using that name.
    Once again, the controller of the old name simply disappears after updating is
    completed. I recommend that you specify a new name for the updated controller
    to avoid confusion in your pod naming down the line. The same `update` command
    with this method will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the static external IP address from the service we created in the first
    section, we can open the service in a browser. We should see our standard container
    information page. However, you''ll notice that the title now says Pod Scaling
    v0.2 and the background is light yellow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_04_02-1.png)'
  prefs: []
  type: TYPE_IMG
- en: v0.1 and v0.2 (side by side)
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that, during the entire update process, we've only been looking
    at pods and RCs. We didn't do anything with our service, but the service is still
    running fine and now directing to the new version of our pods. This is because
    our service is using label selectors for membership. Because both our old and
    new replicas use the same labels, the service has no problem using the new pods
    to service requests. The updates are done on the pods one by one, so it's seamless
    for the users of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Testing, releases, and cutovers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rolling update feature can work well for a simple blue-green deployment
    scenario. However, in a real-world blue-green deployment with a stack of multiple
    applications, there can be a variety of interdependencies that require in-depth
    testing. The `update-period` command allows us to add a `timeout` flag where some
    testing can be done, but this will not always be satisfactory for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you may want partial changes to persist for a longer time and all
    the way up to the load balancer or service level. For example, you may wish to
    run an A/B test on a new user interface feature with a portion of your users.
    Another example is running a canary release (a replica in this case) of your application
    on new infrastructure like a newly added cluster node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at an A/B testing example. For this example, we will need
    to create a new service that uses `sessionAffinity`. We will set the affinity
    to `ClientIP`, which will allow us to forward clients to the same backend pod.
    This is a key if we want a portion of our users to see one version while others
    see another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-3*: `pod-AB-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create this service as usual with the `create` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a service that will point to our pods running both version
    0.2 and 0.3 of the application. Next, we will create the two RCs that create two
    replicas of the application. One set will have version 0.2 of the application,
    and the other will have version 0.3, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-4*: `pod-A-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-5: *`pod-B-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we have the same service label, so these replicas will also be added
    to the service pool based on this selector. We also have `livenessProbe` and `readinessProbe`
    defined to make sure that our new version is working as expected. Again, use the
    `create` command to spin up the controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a service balancing to both versions of our app. In a true A/B test,
    we would now want to start collecting metrics on the visit to each version. Again,
    we have `sessionAffinity` set to `ClientIP`, so all requests will go to the same
    pod. Some users will see v0.2, and some will see v0.3.
  prefs: []
  type: TYPE_NORMAL
- en: Because we have `sessionAffinity` turned on, your test will likely show the
    same version every time. This is expected, and you would need to attempt a connection
    from multiple IP addresses to see both user experiences with each version.
  prefs: []
  type: TYPE_NORMAL
- en: Since the versions are each on their own pod, one can easily separate logging
    and even add a logging container to the pod definition for a sidecar logging pattern.
    For brevity, we will not cover that setup in this book, but we will look at some
    of the logging tools in [Chapter 8](1809040d-4981-427d-8de7-600c4354a872.xhtml),
    *Monitoring and Logging*.
  prefs: []
  type: TYPE_NORMAL
- en: We can start to see how this process will be useful for a canary release or
    a manual blue-green deployment. We can also see how easy it is to launch a new
    version and slowly transition over to the new release.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a basic transition quickly. It''s really as simple as a few
    `scale` commands, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Use the `get pods` command combined with the `-l` filter in between the `scale`
    commands to watch the transition as it happens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have fully transitioned over to version 0.3 (`node-js-scale-b`). All
    users will now see the version 0.3 of the site. We have four replicas of version
    0.3 and none of 0.2\. If you run a `get rc` command, you will notice that we still
    have an RC for 0.2 (`node-js-scale-a`). As a final cleanup, we can remove that
    controller completely, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Application autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A recent feature addition to Kubernetes is that of the **Horizontal Pod Autoscaler**.
    This resource type is really useful as it gives us a way to automatically set
    thresholds for scaling our application. Currently, that support is only for CPU,
    but there is alpha support for custom application metrics as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `node-js-scale` Replication Controller from the beginning of
    the chapter and add an autoscaling component. Before we start, let''s make sure
    we are scaled back down to one replica using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create a Horizontal Pod Autoscaler with the following `hpa` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-6.* `node-js-scale-hpa.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and create this with the `kubectl create -f` command. Now we can list
    the hpas and get a description as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also create autoscaling in the command line with the `kubectl autoscale`
    command. The preceding YAML will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ kubectl autoscale rc/node-js-scale --min=1 --max=3 --cpu-percent=20`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will show us an autoscaler on the `node-js-scale` Replication Controller
    with a Target CPU of 30%. Additionally, you will see that the minimum pods is
    set to 1 and maximum is 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Horizontal Pod Autoscaler with no load
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also query our pods to see how many are running right now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see only one `node-js-scale` pod because our HPA is showing 0% utilization,
    so we will need to generate some load. We will use the popular `boom` application
    common in many container demos. The following listing will help us create continuous
    load until we can hit the CPU threshold for autoscaler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 4-7.* `boomload.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `kubectl create -f` command with this listing and then be ready to start
    monitoring the `hpa`. We can do this with the `kubectl get hpa` command we used
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may take a few moments, but we should start to see the current CPU utilization
    increase. Once it goes above the 20% threshold we set the autoscaler will kick
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Horizontal Pod Autoscaler after load starts
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we see this, we can run `kubectl get pod` again and see there are now
    several `node-js-scale` pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can clean up now by killing our load generation pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now if we watch the `hpa`, we should start to see the CPU usage drop. It may
    take a few minutes, but, eventually, we will go back down to 0% CPU load.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All these techniques are great for the scaling of the application, but what
    about the cluster itself. At some point, you will pack the nodes full and need
    more resources to schedule new pods for your workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you create your cluster, you can customize the starting number of nodes
    (minions) with the `NUM_MINIONS` environment variable. By default, it is set to
    ***4***.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the Kubernetes team has started to build autoscaling capability
    into the cluster itself. Currently, this is the only support on GCE and GKE, but
    work is being done on other providers. This capability utilizes the `KUBE_AUTOSCALER_MIN_NODES`,
    `KUBE_AUTOSCALER_MAX_NODES`, and `KUBE_ENABLE_CLUSTER_AUTOSCALER` environment
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how to set the environment variables for autoscalingbefore
    running `kube-up.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Also, bear in mind that changing this after the cluster is started will have
    no effect. You would need to tear down the cluster and create it once again. Thus,
    this section will show you how to add nodes to an existing cluster without rebuilding
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Once you start a cluster with these settings, your cluster will automatically
    scale up and down with the minimum and maximum limits based on compute resource
    usage in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: GKE clusters also support autoscaling when launched, when using the alpha features.
    The preceding example will use a flag such as `--enable-autoscaling --min-nodes=2 --max-nodes=5`
    in a command-line launch.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up the cluster on GCE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you wish to scale out an existing cluster, we can do it with a few steps.
    Manually scaling up your cluster on GCE is actually quite easy. The existing plumbing
    uses managed instance groups in GCE, which allow you to easily add more machines
    of a standard configuration to the group via an instance template.
  prefs: []
  type: TYPE_NORMAL
- en: You can see this template easily in the GCE console. First, open the console;
    by default, this should open your default project console. If you are using another
    project for your Kubernetes cluster, simply select it from the project dropdown
    at the top of the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the side panel, see under Compute and then Compute Engine, and select Instance
    templates. You should see a template titled kubernetes-minion-template. Note that
    the name could vary slightly if you''ve customized your cluster naming settings.
    Click on that template to see the details. Refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_04_05-1.png)'
  prefs: []
  type: TYPE_IMG
- en: The GCE Instance template for minions
  prefs: []
  type: TYPE_NORMAL
- en: You'll see a number of settings, but the meat of the template is under the Custom
    metadata. Here, you will see a number of environment variables and also a startup
    script that is run after a new machine instance is created. These are the core
    components that allow us to create new machines and have them automatically added
    to the available cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the template for new machines is already created, it is very simple
    to scale out our cluster in GCE. Once in the Compute section of the console, simply
    go to Instance groups located right above the Instance templates link on the side
    panel. Again, you should see a group titled kubernetes-minion-group or something
    similar. Click on that group to see the details, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: The GCE Instance group for minions
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll see a page with a CPU metrics graph and three instances listed here.
    By default, the cluster creates three nodes. We can modify this group by clicking
    on the EDIT GROUP button at the top of the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_04_07-2.png)'
  prefs: []
  type: TYPE_IMG
- en: The GCE Instance group edit page
  prefs: []
  type: TYPE_NORMAL
- en: You should see kubernetes-minion-template selected in Instance template that
    we reviewed a moment ago. You'll also see an Autoscaling setting, which is Off
    by default and an instance count of `3`. Simply, increment this to `4` and click
    on Save. You'll be taken back to the group details page and you'll see a pop-up
    dialog showing the pending changes.
  prefs: []
  type: TYPE_NORMAL
- en: You'll also see some auto healing properties on the `Instance Group` edit page.
    This recreates failed instances and allows you to set health checks as well as
    an initial delay period before an action is taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a few minutes, you''ll have a new instance listed on the details page. We
    can test that this is ready using the `get nodes` command from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: A word of caution on autoscaling and scale down in generalFirst, if we repeat
    the earlier process and decrease the countdown to four, GCE will remove one node.
    However, it will not necessarily be the node you just added. The good news is
    that pods will be rescheduled on the remaining nodes. However, it can only reschedule
    where resources are available. If you are close to full capacity and shut down
    a node, there is a good chance that some pods will not have a place to be rescheduled.
    In addition, this is not a live migration, so any application state will be lost
    in the transition. The bottom line is that you should carefully consider the implications
    before scaling down or implementing an autoscaling scheme.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on general autoscaling in GCE, refer to the [https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization](https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization) link.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up the cluster on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The AWS provider code also makes it very easy to scale up your cluster. Similar
    to GCE, the AWS setup uses autoscaling groups to create the default four minion
    nodes. In the future, the autoscaling groups will hopefully be integrated into
    the Kubernetes cluster autoscaling functionality. For now, we will walk though
    a manual setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can also be easily modified using the CLI or the web console. In the console,
    from the EC2 page, simply go to the Auto Scaling Groups section at the bottom
    of the menu on the left. You should see a name similar to kubernetes-minion-group.
    Select this group and you will see details as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes minion autoscaling details
  prefs: []
  type: TYPE_NORMAL
- en: We can scale this group up easily by clicking on Edit. Then, change the Desired,
    Min, and Max values to `5` and click on Save. In a few minutes, you'll have the
    fifth node available. You can once again check this using the `get nodes` command.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling down is the same process, but remember that we discussed the same considerations
    in the previous *Scaling up the cluster on GCE *section. Workloads could get abandoned
    or, at the very least, unexpectedly restarted.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling manually
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For other providers, creating new minions may not be an automated process. Depending
    on your provider, you'll need to perform various manual steps. It can be helpful
    to look at the provider-specific scripts in the `cluster` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should now be a bit more comfortable with the basics of application scaling
    in Kubernetes. We also looked at the built-in functions in order to roll updates
    as well as a manual process for testing and slowly integrating updates. We took
    a look at how to scale the nodes of our underlying cluster and increase the overall
    capacity for our Kubernetes resources. Finally, we explored some of the new auto
    scaling concepts for both the cluster and our applications themselves.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the latest techniques for scaling and updating
    applications with the new **deployments** resource type, as well as some of the
    other types of workloads we can run on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
