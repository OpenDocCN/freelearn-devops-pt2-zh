- en: Introduction to Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we will help you learn to build and manage Kubernetes clusters.
    You will be given some of the basic container concepts and the operational context,
    wherever possible. Throughout the book, you'll be given examples that you can
    apply as you progress through the book. By the end of the book, you should have
    a solid foundation and even dabble in some of the more advance topics such as
    federation and security.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will give a brief overview of containers and how they work as well
    as why management and orchestration is important to your business and/or project
    team. The chapter will also give a brief overview of how Kubernetes orchestration
    can enhance our container management strategy and how we can get a basic Kubernetes
    cluster up, running, and ready for container deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will include the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing container operations and management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why container management is important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advantages of Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading the latest Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and starting up a new Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The components of a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief overview of containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the past three years, **containers** have grown in popularity like wildfire.
    You would be hard-pressed to attend an IT conference without finding popular sessions
    on Docker or containers in general.
  prefs: []
  type: TYPE_NORMAL
- en: Docker lies at the heart of the mass adoption and the excitement in the container
    space. As Malcom McLean revolutionized the physical shipping world in the 1950s
    by creating a standardized shipping container, which is used today for everything
    from ice cube trays to automobiles (you can refer to more details about this in
    point 1 in the *References* section at the end of the chapter), Linux containers
    are revolutionizing the software development world by making application environments
    portable and consistent across the infrastructure landscape. As an organization,
    Docker has taken the existing container technology to a new level by making it
    easy to implement and replicate across environments and providers.
  prefs: []
  type: TYPE_NORMAL
- en: What is a container?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the core of container technology are **control groups** (**cgroups**) and
    namespaces. Additionally, Docker uses union filesystems for added benefits to
    the container development process.
  prefs: []
  type: TYPE_NORMAL
- en: Cgroups work by allowing the host to share and also limit the resources each
    process or container can consume. This is important for both, resource utilization
    and security, as it prevents **denial-of-service attacks** on the host's hardware
    resources. Several containers can share CPU and memory while staying within the
    predefined constraints.
  prefs: []
  type: TYPE_NORMAL
- en: '**Namespaces** offer another form of isolation for process interaction within
    operating systems. Namespaces limit the visibility a process has on other processes,
    networking, filesystems, and user ID components. Container processes are limited
    to see only what is in the same namespace. Processes from containers or the host
    processes are not directly accessible from within this container process. Additionally,
    Docker gives each container its own networking stack that protects the sockets
    and interfaces in a similar fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Composition of a container
  prefs: []
  type: TYPE_NORMAL
- en: '**Union filesystems** are also a key advantage of using Docker containers.
    Containers run from an image. Much like an image in the VM or Cloud world, it
    represents state at a particular point in time. Container images snapshot the
    filesystem, but tend to be much smaller than a VM. The container shares the host
    kernel and generally runs a much smaller set of processes, so the filesystem and
    boot strap period tend to be much smaller. Though those constraints are not strictly
    enforced. Second, the union filesystem allows for efficient storage, download,
    and execution of these images.'
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to understand union filesystems is to think of them like a layer
    cake with each layer baked independently. The Linux kernel is our base layer;
    then, we might add an OS such as **Red Hat Linux** or **Ubuntu**. Next, we might
    add an application such as **Nginx** or **Apache**. Every change creates a new
    layer. Finally, as you make changes and new layers are added, you'll always have
    a top layer (think frosting) that is a writable layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Layered filesystem
  prefs: []
  type: TYPE_NORMAL
- en: What makes this truly efficient is that Docker caches the layers the first time
    we build them. So, let's say that we have an image with Ubuntu and then add Apache
    and build the image. Next, we build MySQL with Ubuntu as the base. The second
    build will be much faster because the Ubuntu layer is already cached. Essentially,
    our chocolate and vanilla layers, from the preceding *Layered filesystem* figure,
    are already baked. We simply need to bake the pistachio (MySQL) layer, assemble,
    and add the icing (the writable layer).
  prefs: []
  type: TYPE_NORMAL
- en: Why are containers so cool?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers on their own are not a new technology and have in fact been around
    for many years. What truly sets Docker apart is the tooling and ease of use they
    have brought to the community. Modern development practices promote the use of
    Continuous Integration and Continuous Deployment. These techniques, when done
    right, can have a profound impact on your software product quality.
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of Continuous Integration/Continuous Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ThoughtWorks defines **Continuous Integration** as a development practice that
    requires developers to integrate code into a shared repository several times a
    day. By having a continuous process of building and deploying code, organizations
    are able to instill quality control and testing as part of the everyday work cycle.
    The result is that updates and bug fixes happen much faster and the overall quality
    improves.
  prefs: []
  type: TYPE_NORMAL
- en: However, there has always been a challenge in creating development environments
    that match that of testing and production. Often inconsistencies in these environments
    make it difficult to gain the full advantage of continuous delivery.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker, developers are now able to have truly portable deployments. Containers
    that are deployed on a developer's laptop are easily deployed on an in-house staging
    server. They are then easily transferred to the production server running in the
    cloud. This is because Docker builds containers up with build files that specify
    parent layers. One advantage of this is that it becomes very easy to ensure OS,
    package, and application versions are the same across development, staging, and
    production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Because all the dependencies are packaged into the layer, the same host server
    can have multiple containers running a variety of OS or package versions. Further,
    we can have various languages and frameworks on the same host server without the
    typical dependency clashes we would get in a **virtual machine** (**VM**) with
    a single operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Resource utilization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The well-defined isolation and layer filesystem also make containers ideal for
    running systems with a very small footprint and domain-specific purposes. A streamlined
    deployment and release process means we can deploy quickly and often. As such,
    many companies have reduced their deployment time from weeks or months to days
    and hours in some cases. This development life cycle lends itself extremely well
    to small, targeted teams working on small chunks of a larger application.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices and orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we break down an application into very specific domains, we need a uniform
    way to communicate between all the various pieces and domains. Web services have
    served this purpose for years, but the added isolation and granular focus that
    containers bring have paved a way for **microservices**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition for microservices can be a bit nebulous, but a definition from
    Martin Fowler, a respected author and speaker on software development, says this
    (you can refer to more details about this in point 2 in the *References* section
    at the end of the chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: In short, the microservice architectural style is an approach to developing
    a single application as a suite of small services, each running in its own process
    and communicating with lightweight mechanisms, often an HTTP resource API. These
    services are built around business capabilities and independently deployable by
    fully automated deployment machinery. There is a bare minimum of centralized management
    of these services, which may be written in different programming languages and
    use different data storage technologies.
  prefs: []
  type: TYPE_NORMAL
- en: As the pivot to containerization and as microservices evolve in an organization,
    they will soon need a strategy to maintain many containers and microservices.
    Some organizations will have hundreds or even thousands of containers running
    in the years ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Future challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Life cycle processes alone are an important piece of operations and management.
    How will we automatically recover when a container fails? Which upstream services
    are affected by such an outage? How will we patch our applications with minimal
    downtime? How will we scale up our containers and services as our traffic grows?
  prefs: []
  type: TYPE_NORMAL
- en: Networking and processing are also important concerns. Some processes are part
    of the same service and may benefit from the proximity to the network. Databases,
    for example, may send large amounts of data to a particular microservice for processing.
    How will we place containers near each other in our cluster? Is there common data
    that needs to be accessed? How will new services be discovered and made available
    to other systems?
  prefs: []
  type: TYPE_NORMAL
- en: Resource utilization is also a key. The small footprint of containers means
    that we can optimize our infrastructure for greater utilization. Extending the
    savings started in the elastic cloud will take us even further toward minimizing
    wasted hardware. How will we schedule workloads most efficiently? How will we
    ensure that our important applications always have the right resources? How can
    we run less important workloads on spare capacity?
  prefs: []
  type: TYPE_NORMAL
- en: Finally, portability is a key factor in moving many organizations to containerization.
    Docker makes it very easy to deploy a standard container across various operating
    systems, cloud providers, and on-premise hardware or even developer laptops. However,
    we still need tooling to move containers around. How will we move containers between
    different nodes on our cluster? How will we roll out updates with minimal disruption?
    What process do we use to perform blue-green deployments or canary releases?
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are starting to build out individual microservices and separating
    concerns into isolated containers or if you simply want to take full advantage
    of the portability and immutability in your application development, the need
    for management and orchestration becomes clear. This is where orchestration tools
    such as Kubernetes offer the biggest value.
  prefs: []
  type: TYPE_NORMAL
- en: The birth of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kubernetes** (**K8s**) is an open source project that was released by Google
    in June, 2014\. Google released the project as part of an effort to share their
    own infrastructure and technology advantage with the community at large.'
  prefs: []
  type: TYPE_NORMAL
- en: Google launches 2 billion containers a week in their infrastructure and has
    been using container technology for over a decade. Originally, they were building
    a system named **Borg**, now called **Omega**, to schedule their vast quantities
    of workloads across their ever-expanding data center footprint. They took many
    of the lessons they learned over the years and rewrote their existing data center
    management tool for wide adoption by the rest of the world. The result was the
    Kubernetes open-source project (you can refer to more details about this in point
    3 in the *References* section at the end of the chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Since its initial release in 2014, K8s has undergone rapid development with
    contributions all across the open-source community, including Red Hat, VMware,
    and Canonical. The 1.0 release of Kubernetes went live in July, 2015\. Since then,
    it's been a fast-paced evolution of the project with wide support from one of
    the largest open-source communities on GitHub today. We'll be covering version
    1.5 throughout the book. K8s gives organizations a tool to deal with some of the
    major operations and management concerns. We will explore how Kubernetes helps
    deal with resource utilization, high availability, updates, patching, networking,
    service discovery, monitoring, and logging.
  prefs: []
  type: TYPE_NORMAL
- en: Our first cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is supported on a variety of platforms and OSes. For the examples
    in this book, I used an Ubuntu 16.04 Linux VirtualBox for my client and **Google
    Compute Engine** (**GCE**) with Debian for the cluster itself. We will also take
    a brief look at a cluster running on **Amazon Web Services** (**AWS**) with Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: To save some money, both GCP and AWS offer free tiers and trial offers for their
    cloud infrastructure. It's worth using these free trials for your Kubernetes learning,
    if possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the concepts and examples in this book should work on any installation
    of a Kubernetes cluster. To get more information on other platform setups, refer
    to the Kubernetes getting started page on the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://kubernetes.io/docs/getting-started-guides/](http://kubernetes.io/docs/getting-started-guides/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s make sure that our environment is properly set up before we install
    Kubernetes. Start by updating packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Install Python and curl if they are not present:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the **gcloud** SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will need to start a new shell before `gcloud` is on our path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure your **Google Cloud Platform** (**GCP**) account information. This
    should automatically open a browser from where we can log in to our Google Cloud
    account and authorize the SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If you have problems with login or want to use another browser, you can optionally
    use the `--no-launch-browser` command. Copy and paste the URL to the machine and/or
    browser of your choice. Log in with your Google Cloud credentials and click Allow
    on the permissions page. Finally, you should receive an authorization code that
    you can copy and paste back into the shell where the prompt is waiting.
  prefs: []
  type: TYPE_NORMAL
- en: 'A default project should be set, but we can verify this with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can modify this and set a new default project with this command. Make sure
    to use **project ID** and not **project name**, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can find our project ID in the console at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://console.developers.google.com/project](https://console.developers.google.com/project)
    Alternatively, we can list active projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ gcloud alpha projects list`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our environment set up, installing the latest Kubernetes version
    is done in a single step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It may take a minute or two to download Kubernetes depending on your connection
    speed. Earlier versions would automatically call the `kube-up.sh` script and start
    building our cluster. In version 1.5, we will need to call the `kube-up.sh` script
    ourselves to launch the cluster. By default, it will use the Google Cloud and
    GCE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After you run the `kube-up.sh` script, we will see quite a few lines roll past.
    Let''s take a look at them one section at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: GCE prerequisite check
  prefs: []
  type: TYPE_NORMAL
- en: If your `gcloud` components are not up to date, you may be prompted to update
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding image, *GCE prerequisite check*, shows the checks for prerequisites
    as well as making sure that all components are up to date. This is specific to
    each provider. In the case of GCE, it will verify that the SDK is installed and
    that all components are up to date. If not, you will see a prompt at this point
    to install or update:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_04.png)'
  prefs: []
  type: TYPE_IMG
- en: Upload cluster packages
  prefs: []
  type: TYPE_NORMAL
- en: Now the script is turning up the cluster. Again, this is specific to the provider.
    For GCE, it first checks to make sure that the SDK is configured for a default
    **project** and **zone**. If they are set, you'll see those in the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, it uploads the server binaries to Google Cloud storage, as seen in the
    Creating gs:... lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Master creation
  prefs: []
  type: TYPE_NORMAL
- en: 'It then checks for any pieces of a cluster already running. Then, we finally
    start creating the cluster. In the output in the preceding figure *Master creation*,
    we see it creating the **master** server, IP address, and appropriate firewall
    configurations for the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_06.png)'
  prefs: []
  type: TYPE_IMG
- en: Minion creation
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it creates the **minions** or **nodes** for our cluster. This is where
    our container workloads will actually run. It will continually loop and wait while
    all the minions start up. By default, the cluster will have four nodes (minions),
    but K8s supports having more than 1000 (and soon beyond). We will come back to
    scaling the nodes later on in the book.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster completion
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that everything is created, the cluster is initialized and started. Assuming
    that everything goes well, we will get an IP address for the master. Also, note
    that configuration along with the cluster management credentials are stored in
    `home/<Username>/.kube/config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster validation
  prefs: []
  type: TYPE_NORMAL
- en: Then, the script will validate the cluster. At this point, we are no longer
    running provider-specific code. The validation script will query the cluster via
    the `kubectl.sh` script. This is the central script for managing our cluster.
    In this case, it checks the number of minions found, registered, and in a ready
    state. It loops through giving the cluster up to 10 minutes to finish initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a successful startup, a summary of the minions and the cluster component
    health is printed on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster summary
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a `kubectl cluster-info` command is run, which outputs the URL for
    the master services including DNS, UI, and monitoring. Let's take a look at some
    of these components.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Open a browser and run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`https://<your master ip>/ui/`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The certificate is self-signed by default, so you''ll need to ignore the warnings
    in your browser before proceeding. After this, we will see a login dialog. This
    is where we use the credentials listed during the K8s installation. We can find
    them at any time by simply using the `config` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have credentials for login, use those, and we should see a dashboard
    like the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes UI dashboard
  prefs: []
  type: TYPE_NORMAL
- en: The main dashboard takes us to a page with not much display at first. There
    is a link to deploy a containerized app that will take you to a GUI for deployment.
    This GUI can be a very easy way to get started deploying apps without worrying
    about the YAML syntax for Kubernetes. However, as your use of containers matures,
    it's good practice to use the YAML definitions that are checked in to source control.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on the **Nodes** link on the left-hand side menu, you will see
    some metrics on the current cluster nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes Node Dashboard
  prefs: []
  type: TYPE_NORMAL
- en: At the top, we see an aggregate of the CPU and memory usages followed by a listing
    of our cluster nodes. Clicking on one of the nodes will take us to a page with
    detailed information about that node, its health, and various metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes UI has a lot of other views that will become more useful as we
    start launching real applications and adding configurations to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another service installed by default is **Grafana**. This tool will give us
    a dashboard to view metrics on the cluster nodes. We can access it using the following
    syntax in a browser: `https://<your master ip>/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana`
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes Grafana dashboard
  prefs: []
  type: TYPE_NORMAL
- en: From the main page, click on the Home dropdown and select Cluster. Here, Kubernetes
    is actually running a number of services. **Heapster** is used to collect the
    resource usage on the **pods** and **nodes** and stores the information in **InfluxDB**.
    The results, such as CPU and memory usage, are what we see in the Grafana UI.
    We will explore this in depth in [Chapter 8](1809040d-4981-427d-8de7-600c4354a872.xhtml),
    *Monitoring and Logging*.
  prefs: []
  type: TYPE_NORMAL
- en: Command line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `kubectl` script has commands to explore our cluster and the workloads
    running on it. You can find it in the `/kubernetes/client/bin` folder. We will
    be using this command throughout the book, so let''s take a second to set up our
    environment. We can do so by putting the binaries folder on our `PATH`, in the
    following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You may choose to download the `kubernetes` folder outside your home folder,
    so modify the preceding command as appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: It is also a good idea to make the changes permanent by adding the `export`
    command to the end of your `.bashrc` file in your home directory.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have `kubectl` on our path, we can start working with it. It has
    quite a few commands. Since we have not spun up any applications yet, most of
    these commands will not be very interesting. However, we can explore with two
    commands right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have already seen the `cluster-info` command during initialization,
    but we can run it again at any time with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Another useful command is `get`. It can be used to see currently running **services**,
    **pods**, **replication controllers**, and a lot more. Here are the three examples
    that are useful right out of the gate:'
  prefs: []
  type: TYPE_NORMAL
- en: 'List the nodes in our cluster:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'List cluster events:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can see any services that are running in the cluster, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To start with, we will only see one service, named `kubernetes`. This service
    is the core API server for the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Services running on the master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s dig a little bit deeper into our new cluster and its core services.
    By default, machines are named with the `kubernetes-` prefix. We can modify this
    using `$KUBE_GCE_INSTANCE_PREFIX` before a cluster is spun up. For the cluster
    we just started, the master should be named `kubernetes-master`. We can use the
    `gcloud` command-line utility to SSH into the machine. The following command will
    start an SSH session with the master node. Be sure to substitute your project
    ID and zone to match your environment. Also, note that you can launch SSH from
    the Google Cloud console using the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If you have trouble with SSH via the Google Cloud CLI, you can use the Console
    which has a built-in SSH client. Simply go to the VM instances page and you'll
    see an SSH option as a column in the kubernetes-master listing. Alternatively,
    the VM instance details page has the SSH option at the top.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we are logged in, we should get a standard shell prompt. Let''s run the
    `docker` command that filters for `Image` and `Status`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B06302_01_13.png)'
  prefs: []
  type: TYPE_IMG
- en: Master container listing
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though we have not deployed any applications on Kubernetes yet, we note
    that there are several containers already running. The following is a brief description
    of each container:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fluentd-gcp`: This container collects and sends the cluster logs file to the
    Google Cloud Logging service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node-problem-detector`: This container is a daemon that runs on every node
    and currently detects issues at the hardware and kernel layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescheduler`: This is another add-on container that makes sure critical components
    are always running. In cases of low resources availability, it may even remove
    less critical pods to make room.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glbc`: This is another Kubernetes add-on container that provides Google Cloud
    Layer 7 load balancing using the new *Ingress* capability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-addon-manager`: This component is core to the extension of Kubernetes
    through various add-ons. It also periodically applies any changes to  the `/etc/kubernetes/addons`
    directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd-empty-dir-cleanup`: A utility to cleanup empty keys in etcd.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager`: This is a controller manager that controls a variety
    of cluster functions, ensuring accurate and up-to-date replication is one of its
    vital roles. Additionally, it monitors, manages, and discovers new nodes. Finally,
    it manages and updates service endpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-apiserver`: This container runs the API server. As we explored in the
    Swagger interface, this RESTful API allows us to create, query, update, and remove
    various components of our Kubernetes cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler`: This scheduler takes unscheduled pods and binds them to nodes
    based on the current scheduling algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd`: This runs the **etcd** software built by CoreOS, and it is a distributed
    and consistent key-value store. This is where the Kubernetes cluster state is
    stored, updated, and retrieved by various components of K8s.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pause`: This container is often referred to as the pod infrastructure container
    and is used to set up and hold the networking namespace and resource limits for
    each pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I omitted the amd64 for many of these names to make this more generic. The purpose
    of the pods remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: To exit the SSH session, simply type `exit` at the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will also show how a few of these services work together
    in the first image, *Kubernetes core architecture*.
  prefs: []
  type: TYPE_NORMAL
- en: Services running on the minions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We could SSH to one of the minions, but since Kubernetes schedules workloads
    across the cluster, we would not see all the containers on a single minion. However,
    we can look at the pods running on all the minions using the `kubectl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have not started any applications on the cluster yet, we don''t see
    any pods. However, there are actually several system pods running pieces of the
    Kubernetes infrastructure. We can see these pods by specifying the `kube-system`
    namespace. We will explore namespaces and their significance later, but for now,
    the `--namespace=kube-system` command can be used to look at these K8s system
    resources, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The first six lines should look familiar. Some of these are the services we
    saw running on the master and will see pieces of these on the nodes. There are
    a few additional services we have not seen yet. The `kube-dns` option provides
    the DNS and service discovery plumbing, `kubernetes-dashboard-xxxx` is the user
    interface for Kubernetes, `l7-default-backend-xxxx` provides the default load
    balancing backend for the new Layer-7 load balancing capability, and `heapster-v1.2.0-xxxx` and `monitoring-influx-grafana`
    provide the **Heapster** database and user interface to monitor resource usage
    across the cluster. Finally, `kube-proxy-kubernetes-minion-group-xxxx`  is the
    proxy which directs traffic to the proper backing services and pods running on
    our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we did SSH into a random minion, we would see several containers that run
    across a few of these pods. A sample might look like this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_14.png)'
  prefs: []
  type: TYPE_IMG
- en: Minion container listing
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we saw a similar line up of services on the master. The services we
    did not see on the master include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubedns`: This container monitors the service and endpoint resources in Kubernetes
    and synchronizes any changes to DNS lookups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-dnsmasq`: This is another container that provides DNS caching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dnsmasq-metrics`: This provides metric reporting for DNS services in cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l7-defaultbackend`: This is the default backend for handling the GCE L7 load
    balancer and *Ingress*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-proxy`: This is the network and service proxy for your cluster. This
    component makes sure service traffic is directed to wherever your workloads are
    running on the cluster. We will explore this in more depth later in the book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`heapster`: This container is for monitoring and analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`addon-resizer`: This cluster utility is for scaling containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`heapster_grafana`: This does resource usage and monitoring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`heapster_influxdb`: This time-series database is for Heapster data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster-proportional-autoscaler`: This cluster utility is for scaling containers
    in proportion to the cluster size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exechealthz`: This performs health checks on the pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, I have omitted the amd64 for many of these names to make this more generic.
    The purpose of the pods remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: Tear down cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alright, this is our first cluster on GCE, but let''s explore some other providers.
    To keep things simple, we need to remove the one we just created on GCE. We can
    tear down the cluster with one simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Working with other providers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, Kubernetes uses the GCE provider for Google Cloud. We can override
    this default by setting the `KUBERNETES_PROVIDER` environment variable. The following
    providers are supported with values listed in this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Provider** | **KUBERNETES_PROVIDER value** | **Type** |'
  prefs: []
  type: TYPE_TB
- en: '| **Google Compute Engine** | `gce` | Public cloud |'
  prefs: []
  type: TYPE_TB
- en: '| **Google Container Engine** | `gke` | Public cloud |'
  prefs: []
  type: TYPE_TB
- en: '| **Amazon Web Services** | `aws` | Public cloud |'
  prefs: []
  type: TYPE_TB
- en: '| **Microsoft Azure** | `azure` | Public cloud |'
  prefs: []
  type: TYPE_TB
- en: '| **Hashicorp Vagrant** | `vagrant` | Virtual development environment |'
  prefs: []
  type: TYPE_TB
- en: '| **VMware vSphere** | `vsphere` | Private cloud/on-premise virtualization
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Libvirt running CoreOS** | `libvirt-coreos` | Virtualization management
    tool |'
  prefs: []
  type: TYPE_TB
- en: '| **Canonical Juju (folks behind Ubuntu)** | `juju` | OS service orchestration
    tool |'
  prefs: []
  type: TYPE_TB
- en: Kubernetes providers
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try setting up the cluster on AWS. As a prerequisite, we need to have
    AWS **Command Line Interface** (**CLI**) installed and configured for our account.
    The AWS CLI installation and configuration documentation can be found at the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installation documentation: [http://docs.aws.amazon.com/cli/latest/userguide/installing.html#install-bundle-other-os](http://docs.aws.amazon.com/cli/latest/userguide/installing.html#install-bundle-other-os)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configuration documentation: [http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, it is a simple environment variable setting, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we can use the `kube-up.sh` command to spin up the cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As with GCE, the setup activity will take a few minutes. It will stage files
    in **S3** and create the appropriate instances, **Virtual Private Cloud** (**VPC**),
    security groups, and so on in our AWS account. Then, the Kubernetes cluster will
    be set up and started. Once everything is finished and started, we should see
    the cluster validation at the end of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_15.png)'
  prefs: []
  type: TYPE_IMG
- en: AWS cluster validation
  prefs: []
  type: TYPE_NORMAL
- en: Note that the region where the cluster is spun up is determined by the `KUBE_AWS_ZONE` environment
    variable. By default, this is set to `us-west-2a` (the region is derived from
    this Availability Zone). Even if you have a region set in your AWS CLI, it will
    use the region defined in `KUBE_AWS_ZONE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, we will SSH into master. This time, we can use the native SSH client.
    We''ll find the key files in `/home/<username>/.ssh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll use `sudo docker ps --format ''table {{.Image}}t{{.Status}}''` to explore
    the running containers. We should see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_16.png)'
  prefs: []
  type: TYPE_IMG
- en: Master container listing (AWS)
  prefs: []
  type: TYPE_NORMAL
- en: We see some of the same containers as our GCE cluster had. However, there are
    several missing. We see the core Kubernetes components, but the `fluentd-gcp`
    service is missing as well as some of the newer utilities such as `node-problem-detector` , `rescheduler` , `glbc` , `kube-addon-manager` ,
    and `etcd-empty-dir-cleanup`. This reflects some of the subtle differences in
    the `kube-up` script between various Public Cloud providers. This is ultimately
    decided by the efforts of the large Kubernetes open-source community, but GCP
    often has many of the latest features first.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the AWS provider, **Elasticsearch** and **Kibana** are set up for us. We
    can find the Kibana UI using the following syntax as URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '`https://<your master ip>/api/v1/proxy/namespaces/kube-system/services/kibana-logging`'
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the case of the UI, you will be prompted for admin credentials, which
    can be obtained using the `config` command, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'On the first visit, you''ll need to set up your index. You can leave the defaults
    and choose @timestamp for the Time-field name. Then, click on Create and you''ll
    be taken to the index settings page. From there, click on the Discover tab at
    the top and you can explore the log dashboards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_17.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes Kibana dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Resetting the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You just had a little taste of running the cluster on AWS. For the remainder
    of the book, I will be basing my examples on a GCE cluster. For the best experience
    following along, you can get back to a GCE cluster easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simply tear down the AWS cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create a GCE cluster again using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Modifying kube-up parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's worth getting to know the parameters used for the `kube-up.sh` script.
    Each provider under the `kubernetes/cluster/` folder has its own `su`  folder
    which containers a `config-default.sh`  script.
  prefs: []
  type: TYPE_NORMAL
- en: For example, `kubernetes/cluster/aws/config-default.sh` has the default settings
    for using `kube-up.sh` with AWS. At the start of this script, you will see many
    of these values defined as well as environment variables that can be used to overrides
    the defaults.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, the `ZONE` variable is set for the script and it
    uses the value from the environment variable named `KUBE_AWS_ZONE`. If this variable
    is not set, it will use the default `us-west-2a`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Understanding these parameters will help you get a lot more mileage out of your
    `kube-up.sh` script.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatives to kube-up.sh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `kube-up.sh` script is still a pretty handy way to get started using Kubernetes
    on your platform of choice. However, it's not without flaws and can sometimes
    run aground when conditions are not just so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, since K8''s inception, a number of alternative methods for creating
    clusters have emerged. Two such GitHub projects are *KOPs* and *kube-aws*. While
    the later is tied to AWS, they both provide an alternative method to easily spinning
    up your new cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[https://github.com/kubernetes/kops](https://github.com/kubernetes/kops)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[https://github.com/coreos/kube-aws](https://github.com/coreos/kube-aws)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, a number of managed services have arisen including **Google Container
    Engine** (**GKE**) and Microsoft **Azure Container Service** (**ACS**), which
    provide an automated install and some managed cluster operations. We will look
    at a brief demo of these in [Chapter 12](0ebbb579-01ba-4e51-b801-b1656d0558d7.xhtml),
    *Towards Production Ready*.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, there is the option to start from scratch. Luckily, starting in 1.4,
    the Kubernetes team has put a major focus in easing the cluster setup process.
    To that end they have introduced kubeadm for Ubuntu 16.04, CentOS 7, and HypriotOS
    v1.0.1+.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a quick look at spinning up a cluster on AWS from scratch using the
    kubeadm tool.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will need to provision our cluster master and nodes beforehand. For the moment,
    we are limited to the operating systems and version listed earlier. Additionally,
    it is recommended that you have at least 1 GB of RAM and all the nodes must have
    network connectivity to one another.
  prefs: []
  type: TYPE_NORMAL
- en: For this walk through, we will need one t2.medium (master node) and three t2.mirco
    (nodes) sized instances on AWS. These instance have burstable CPU and come with
    the minimum 1 GB of RAM needed. We will need to create one master and three worker
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also need to create some security groups for the cluster. The following
    ports are needed for the master:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Protocol** | **Port range** | **Source** |'
  prefs: []
  type: TYPE_TB
- en: '| All Traffic | All | All | {This SG ID (Master SG)} |'
  prefs: []
  type: TYPE_TB
- en: '| All Traffic | All | All | {Node SG ID} |'
  prefs: []
  type: TYPE_TB
- en: '| SSH | TCP | 22 | {Your Local Machine''s IP} |'
  prefs: []
  type: TYPE_TB
- en: '| HTTTPS | TCP | 443 | {Range allowed to access K8s API and UI} |'
  prefs: []
  type: TYPE_TB
- en: Master Security Group Rules
  prefs: []
  type: TYPE_NORMAL
- en: 'The next table shows the ports node security groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Protocol** | **Port range** | **Source** |'
  prefs: []
  type: TYPE_TB
- en: '| All Traffic | All | All | {Master SG ID} |'
  prefs: []
  type: TYPE_TB
- en: '| All Traffic | All | All | {This SG ID (Node SG)} |'
  prefs: []
  type: TYPE_TB
- en: '| SSH | TCP | 22 | {Your Local Machine''s IP} |'
  prefs: []
  type: TYPE_TB
- en: Node Security Group Rules
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have these SGs, go ahead and spin up four instances (one t2.medium
    and three t2.mircos) using Ubuntu 16.04\. If you are new to AWS, refer to the
    documentation on spinning up EC2 instances at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html)**'
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to identify the t2.medium instance as the master and associate the master
    security group. Name the other three as nodes and associate the node security
    group with those.
  prefs: []
  type: TYPE_NORMAL
- en: These steps are adapted from the walk-through in the manual. For more information
    or to work with an alternative to Ubuntu refer to [https://kubernetes.io/docs/getting-started-guides/kubeadm/](https://kubernetes.io/docs/getting-started-guides/kubeadm/).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Kubernetes components (kubelet and kubeadm)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next we will need to SSH into all four of the instances and install the Kubernetes
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'As root, perform the following steps on all four instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Update packages and install the `apt-transport-https` package so we can
    download from sources   that use HTTPS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Install the Google Cloud public key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Next, create a source list for the Kubernetes package downloads with your
    favorite editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '4\. Use the following as contents for this file and save:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 1-1.* `/etc/apt/sources.list.d/kubernetes.list`'
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Update your sources once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Install Docker and the core Kubernetes components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Setting up a Master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On the instance you have previously chosen as *master*, we will run master
    initialization. Again, as root run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note that initialization can only be run once, so if you run into problems you'll
    `kubeadm reset`.
  prefs: []
  type: TYPE_NORMAL
- en: Joining nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After a successful initialization, you will get a join command that can be
    used by the nodes. Copy this down for the join process later on. It should look
    similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The token is used to authenticate cluster nodes, so make sure to store it somewhere
    securely for future use.
  prefs: []
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our cluster will need a networking layer for the pods to communicate on. Note
    that kubeadm requires a CNI compatible network fabric. The list of plugins currently
    available can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[http://kubernetes.io/docs/admin/addons/](http://kubernetes.io/docs/admin/addons/)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we will use calico. We will need to create the calico components
    on our cluster using the following `yaml`. For convenience you can download it
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[http://docs.projectcalico.org/v1.6/getting-started/kubernetes/installation/hosted/kubeadm/calico.yaml](http://docs.projectcalico.org/v1.6/getting-started/kubernetes/installation/hosted/kubeadm/calico.yaml)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have this file on your *master*, create the components with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Give this a minute to run setup and then list the `kube-system` nodes to check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get a listing similar to the following one with three new calico
    pods and one completed job that is not shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_18.png)'
  prefs: []
  type: TYPE_IMG
- en: Calico setup
  prefs: []
  type: TYPE_NORMAL
- en: Joining the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we need to run the `join` command we copied earlier, on each of our node
    instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve finished that, you should be able to see all nodes from the master
    by running this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If all went well, this will show three nodes and one master, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_01_19.png)'
  prefs: []
  type: TYPE_IMG
- en: Calico setup
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We took a very brief look at how containers work and how they lend themselves
    to the new architecture patterns in microservices. You should now have a better
    understanding of how these two forces will require a variety of operations and
    management tasks and how Kubernetes offers strong features to address these challenges.
    We created two different clusters on both GCE and AWS and explored the startup
    script as well as some of the built-in features of Kubernetes. Finally, we looked
    at the alternatives to the `kube-up` script and tried the new kubeadm tool on
    AWS with Ubuntu 16.04.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the core concept and abstractions K8s provides
    to manage containers and full application stacks. We will also look at basic scheduling,
    service discovery, and health checking.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Malcom McLean entry on PBS website: [https://www.pbs.org/wgbh/theymadeamerica/whomade/mclean_hi.html](https://www.pbs.org/wgbh/theymadeamerica/whomade/mclean_hi.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Martin Fowler on microservices: [http://martinfowler.com/articles/microservices.html](http://martinfowler.com/articles/microservices.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kubernetes GitHub project page: [https://github.com/kubernetes/kubernetes](https://github.com/kubernetes/kubernetes)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.thoughtworks.com/continuous-integration](https://www.thoughtworks.com/continuous-integration)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/](https://en.wikipedia.org/wiki/Continuous_integration%20https:/docs.docker.com/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://kubernetes.io/docs/getting-started-guides/kubeadm/](http://kubernetes.io/docs/getting-started-guides/kubeadm/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
