- en: Pods, Services, Replication Controllers, and Labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will cover the core Kubernetes constructs, namely **pods**, **services**,
    **replication controllers**, **replica sets**, and **labels**. A few simple application
    examples will be included to demonstrate each construct. The chapter will also
    cover basic operations for your cluster. Finally, **health checks** and **scheduling**
    will be introduced with a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes overall architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to core Kubernetes constructs, namely pods, services, replication
    controllers, replica sets, and labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how labels can ease management of a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to monitor services and container health
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to set up scheduling constraints based on available cluster
    resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although, **Docker** brings a helpful layer of abstraction and tooling around
    container management, Kubernetes brings similar assistance to orchestrating containers
    at scale and managing full application stacks.
  prefs: []
  type: TYPE_NORMAL
- en: '**K8s** moves up the stack giving us constructs to deal with management at
    the application or service level. This gives us automation and tooling to ensure
    high availability, application stack, and service-wide portability. K8s also allows
    finer control of resource usage, such as CPU, memory, and disk space across our
    infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes provides this higher level of orchestration management by giving
    us key constructs to combine multiple containers, endpoints, and data into full
    application stacks and services. K8s also provides the tooling to manage the when,
    where, and how many of the stack and its components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes core architecture
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we see the core architecture of Kubernetes. Most administrative
    interactions are done via the `kubectl` script and/or RESTful service calls to
    the API.
  prefs: []
  type: TYPE_NORMAL
- en: Note the ideas of the desired state and actual state carefully. This is the
    key to how Kubernetes manages the cluster and its workloads. All the pieces of
    K8s are constantly working to monitor the current actual state and synchronize
    it with the desired state defined by the administrators via the API server or
    `kubectl` script. There will be times when these states do not match up, but the
    system is always working to reconcile the two.
  prefs: []
  type: TYPE_NORMAL
- en: Master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essentially, **master** is the brain of our cluster. Here, we have the core
    API server, which maintains RESTful web services for querying and defining our
    desired cluster and workload state. It's important to note that the control pane
    only accesses the master to initiate changes and not the nodes directly.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the master includes the **scheduler**, which works with the API
    server to schedule workloads in the form of pods on the actual minion nodes. These
    pods include the various containers that make up our application stacks. By default,
    the basic Kubernetes scheduler spreads pods across the cluster and uses different
    nodes for matching pod replicas. Kubernetes also allows specifying necessary resources
    for each container, so scheduling can be altered by these additional factors.
  prefs: []
  type: TYPE_NORMAL
- en: The replication controller/replica set works with the API server to ensure that
    the correct number of pod replicas are running at any given time. This is exemplary
    of the desired state concept. If our replication controller/replica set is defining
    three replicas and our actual state is two copies of the pod running, then the
    scheduler will be invoked to add a third pod somewhere on our cluster. The same
    is true if there are too many pods running in the cluster at any given time. In
    this way, K8s is always pushing toward that desired state.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have **etcd** running as a distributed configuration store. The
    Kubernetes state is stored here and etcd allows values to be watched for changes.
    Think of this as the brain's shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Node (formerly minions)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In each node, we have a couple of components. The **kubelet** interacts with
    the API server to update the state and to start new workloads that have been invoked
    by the scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kube-proxy** provides basic load balancing and directs the traffic destined
    for specific services to the proper pod on the backend. Refer to the *Services*
    section later in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have some default pods, which run various infrastructure services
    for the node. As we explored briefly in the previous chapter, the pods include
    services for **Domain Name System** (**DNS**), logging, and pod health checks.
    The default pod will run alongside our scheduled pods on every node.
  prefs: []
  type: TYPE_NORMAL
- en: In v1.0, **minion** was renamed to **node**, but there are still remnants of
    the term minion in some of the machine naming scripts and documentation that exists
    on the Web. For clarity, I've added the term minion in addition to node in a few
    places throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Core constructs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's dive a little deeper and explore some of the core abstractions Kubernetes
    provides. These abstractions will make it easier to think about our applications
    and ease the burden of life cycle management, high availability, and scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pods allow you to keep related containers close in terms of the network and
    hardware infrastructure. Data can live near the application, so processing can
    be done without incurring a high latency from network traversal. Similarly, common
    data can be stored on volumes that are shared between a number of containers.
    Pods essentially allow you to logically group containers and pieces of our application
    stacks together.
  prefs: []
  type: TYPE_NORMAL
- en: While pods may run one or more containers inside, the pod itself may be one
    of many that is running on a Kubernetes node (minion). As we'll see, pods give
    us a logical group of containers that we can then replicate, schedule, and balance
    service endpoints across.
  prefs: []
  type: TYPE_NORMAL
- en: Pod example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a quick look at a pod in action. We will spin up a **Node.js** application
    on the cluster. You'll need a GCE cluster running for this; if you don't already
    have one started, refer to the *Our first cluster* section in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml)*, Introduction to
    Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s make a directory for our definitions. In this example, I will create
    a folder in the `/book-examples` subfolder under our home directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Downloading the example code
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use your favorite editor to create the following file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-1*: `nodejs-pod.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This file creates a pod named `node-js-pod` with the latest `bitnami/apache`
    container running on port `80`. We can check this using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a pod running the specified container. We can see more information
    on the pod by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll see a good deal of information, such as the pod''s status, IP address,
    and even relevant log events. You''ll note the pod IP address is a private IP
    address, so we cannot access it directly from our local machine. Not to worry,
    as the `kubectl exec` command mirrors Docker''s `exec` functionality. Once the
    pod shows to be in a running state, we can use this feature to run a command inside
    a pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: By default, this runs a command in the first container it finds, but you can
    select a specific one using the `-c` argument.
  prefs: []
  type: TYPE_NORMAL
- en: After running the command, you should see some HTML code. We'll have a prettier
    view later in the chapter, but for now, we can see that our pod is indeed running
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labels give us another level of categorization, which becomes very helpful in
    terms of everyday operations and management. Similar to tags, labels can be used
    as the basis of service discovery as well as a useful grouping tool for day-to-day
    operations and management tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Labels are just simple key-value pairs. You will see them on pods, replication
    controllers, replica sets, services, and so on. The label acts as a selector and
    tells Kubernetes which resources to work with for a variety of operations. Think
    of it as a filtering option.
  prefs: []
  type: TYPE_NORMAL
- en: We will take a look at labels more in depth later in this chapter, but first,
    we will explore the remaining three constructs—services, replication controllers,
    and replica sets.
  prefs: []
  type: TYPE_NORMAL
- en: The container's afterlife
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As Werner Vogels, CTO of AWS, famously said *everything fails all the time;* containers
    and pods can and will crash, become corrupted, or maybe even just get accidentally
    shut off by a clumsy admin poking around on one of the nodes. Strong policy and
    security practices like enforcing least privilege curtail some of these incidents,
    but involuntary workload slaughter happens and is simply a fact of operations.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, Kubernetes provides two very valuable constructs to keep this somber
    affair all tidied up behind the curtains. Services and replication controllers/replica
    sets give us the ability to keep our applications running with little interruption
    and graceful recovery.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Services allow us to abstract access away from the consumers of our applications.
    Using a reliable endpoint, users and other programs can access pods running on
    your cluster seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: K8s achieves this by making sure that every node in the cluster runs a proxy
    named **kube-proxy**. As the name suggests, the job of **kube-proxy** is to proxy
    communication from a service endpoint back to the corresponding pod that is running
    the actual application.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_002.png)'
  prefs: []
  type: TYPE_IMG
- en: The kube-proxy architecture
  prefs: []
  type: TYPE_NORMAL
- en: Membership of the service load balancing pool is determined by the use of selectors
    and labels. Pods with matching labels are added to the list of candidates where
    the service forwards traffic. A virtual IP address and port are used as the entry
    points for the service, and the traffic is then forwarded to a random pod on a
    target port defined by either K8s or your definition file.
  prefs: []
  type: TYPE_NORMAL
- en: Updates to service definitions are monitored and coordinated from the K8s cluster
    master and propagated to the **kube-proxy daemons** running on each node.
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, kube-proxy is running on the node host itself. There are plans
    to containerize this and the kubelet by default in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Replication controllers and replica sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Replication controllers** (**RCs**), as the name suggests, manage the number
    of nodes that a pod and included container images run on. They ensure that an
    instance of an image is being run with the specific number of copies.'
  prefs: []
  type: TYPE_NORMAL
- en: As you start to operationalize your containers and pods, you'll need a way to
    roll out updates, scale the number of copies running (both up and down), or simply
    ensure that at least one instance of your stack is always running. RCs create
    a high-level mechanism to make sure that things are operating correctly across
    the entire application and cluster.
  prefs: []
  type: TYPE_NORMAL
- en: RCs are simply charged with ensuring that you have the desired scale for your
    application. You define the number of pod replicas you want running and give it
    a template for how to create new pods. Just like services, we will use selectors
    and labels to define a pod's membership in a replication controller.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes doesn't require the strict behavior of the replication controller,
    which is ideal for long-running processes. In fact, **job controllers** can be
    used for short lived workloads which allow jobs to be run to a completion state
    and are well suited for batch work.
  prefs: []
  type: TYPE_NORMAL
- en: '**Replica sets**, are a new type, currently in Beta, that represent an improved
    version of replication controllers. Currently, the main difference consists of
    being able to use the new set-based label selectors as we will see in the following
    examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Our first Kubernetes application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we move on, let's take a look at these three concepts in action. Kubernetes
    ships with a number of examples installed, but we will create a new example from
    scratch to illustrate some of the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: We already created a pod definition file, but as you learned, there are many
    advantages to running our pods via replication controllers. Again, using the `book-examples/02_example`
    folder we made earlier, we will create some definition files and start a cluster
    of Node.js servers using a replication controller approach. Additionally, we'll
    add a public face to it with a load-balanced service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use your favorite editor to create the following file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-2*: `nodejs-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the first resource definition file for our cluster, so let''s take
    a closer look. You''ll note that it has four first-level elements (`kind`, `apiVersion`,
    `metadata`, and `spec`). These are common among all top-level Kubernetes resource
    definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Kind`: This tells K8s the type of resource we are creating. In this case,
    the type is `ReplicationController`. The `kubectl` script uses a single `create`
    command for all types of resources. The benefit here is that you can easily create
    a number of resources of various types without the need for specifying individual
    parameters for each type. However, it requires that the definition files can identify
    what it is they are specifying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`apiVersion`: This simply tells Kubernetes which version of the schema we are
    using.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Metadata`: This is where we will give the resource a name and also specify
    labels that will be used to search and select resources for a given operation.
    The metadata element also allows you to create annotations, which are for the
    non-identifying information that might be useful for client tools and libraries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we have `spec`, which will vary based on the `kind` or type of resource
    we are creating. In this case, it's `ReplicationController`, which ensures the
    desired number of pods are running. The `replicas` element defines the desired
    number of pods, the `selector` element tells the controller which pods to watch,
    and finally, the `template` element defines a template to launch a new pod. The
    `template` section contains the same pieces we saw in our pod definition earlier.
    An important thing to note is that the `selector` values need to match the `labels`
    values specified in the pod template. Remember that this matching is used to select
    the pods being managed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at the service definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-3*: `nodejs-rc-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: The YAML here is similar to `ReplicationController`. The main difference is
    seen in the service `spec` element. Here, we define the `Service` type, listening
    `port`, and `selector`, which tell the `Service` proxy which pods can answer the
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes supports both YAML and JSON formats for definition files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the Node.js express replication controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a replication controller that ensures that three copies of the
    container are always running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: On GCE, this will create an external load balancer and forwarding rules, but
    you may need to add additional firewall rules. In my case, the firewall was already
    open for port `80`. However, you may need to open this port, especially if you
    deploy a service with ports other than `80` and `443`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alright, now we have a running service, which means that we can access the
    Node.js servers from a reliable URL. Let''s take a look at our running services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6302_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Services listing
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding image (*Services listing*), we should note that the `node-js`
    service is running, and in the IP(S) column, we should have both a private and
    a public (`130.211.186.84` in the screenshot) IP address. If you don''t see the
    external IP, you may need to wait a minute for the IP to be allocated from GCE.
    Let''s see if we can connect by opening up the public address in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Container info application
  prefs: []
  type: TYPE_NORMAL
- en: You should see something like the figure *Container info application* . If we
    visit multiple times, you should note that the container name changes. Essentially,
    the service load balancer is rotating between available pods on the backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Browsers usually cache web pages, so to really see the container name change,
    you may need to clear your cache or use a proxy like this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hide.me/en/proxy](https://hide.me/en/proxy)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try playing chaos monkey a bit and kill off a few containers to see
    what Kubernetes does. In order to do this, we need to see where the pods are actually
    running. First, let''s list our pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Currently running pods
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s get some more details on one of the pods running a `node-js` container.
    You can do this with the `describe` command with one of the pod names listed in
    the last command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_02_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Pod description
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the preceding output. The information we need is the `Node:`
    section. Let''s use the node name to **SSH** (short for **Secure Shell**) into
    the (minion) node running this workload:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once SSHed into the node, if we run a `sudo docker ps` command, we should see
    at least two containers: one running the `pause` image and one running the actual
    `node-express-info` image. You may see more if the K8s scheduled more than one
    replica on this node. Let''s grab the container ID of the `jonbaier/node-express-info`
    image (not `gcr.io/google_containers/pause`) and kill it off to see what happens.
    Save this container ID somewhere for later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Unless you are really quick you'll probably note that there is still a `node-express-info`
    container running, but look closely and you'll note that the `container id` is
    different and the creation time stamp shows only a few seconds ago. If you go
    back to the service URL, it is functioning as normal. Go ahead and exit the SSH
    session for now.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are already seeing Kubernetes playing the role of on-call operations
    ensuring that our application is always running.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see if we can find any evidence of the outage. Go to the Events page in
    the Kubernetes UI. You can find it by navigating to the Nodes page on the main
    K8s dashboard. Select a node from the list (the same one that we SSHed into) and
    scroll down to Events on the node details page.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see a screen similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes UI event page
  prefs: []
  type: TYPE_NORMAL
- en: You should see three recent events. First, Kubernetes pulls the image. Second,
    it creates a new container with the pulled image. Finally, it starts that container
    again. You'll note that, from the time stamps, this all happens in less than a
    second. Time taken may vary based on the cluster size and image pulls, but the
    recovery is very quick.
  prefs: []
  type: TYPE_NORMAL
- en: More on labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned previously, labels are just simple key-value pairs. They are available
    on pods, replication controllers, replica sets, services, and more. If you recall
    our service YAML, in *Listing 2-3*: `nodejs-rc-service.yaml`, there was a `selector`
    attribute. The `selector` attribute tells Kubernetes which labels to use in finding
    pods to forward traffic for that service.'
  prefs: []
  type: TYPE_NORMAL
- en: 'K8s allows users to work with labels directly on replication controllers, replica
    sets, and services. Let''s modify our replicas and services to include a few more
    labels. Once again, use your favorite editor and create these two files, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-4*: `nodejs-labels-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-5*: `nodejs-labels-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the replication controller and service as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at how we can use labels in everyday management. The following
    table shows us the options to select labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operators** | **Description** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| `=` or `==` | You can use either style to select keys with values equal to
    the string on the right | `name = apache` |'
  prefs: []
  type: TYPE_TB
- en: '| `!=` | Select keys with values that do not equal the string on the right
    | `Environment != test` |'
  prefs: []
  type: TYPE_TB
- en: '| `in` | Select resources whose labels have keys with values in this set |
    `tier in (web, app)` |'
  prefs: []
  type: TYPE_TB
- en: '| `notin` | Select resources whose labels have keys with values not in this
    set | `tier notin (lb, app)` |'
  prefs: []
  type: TYPE_TB
- en: '| `<Key name>` | Use a key name only to select resources whose labels contain
    this key | `tier` |'
  prefs: []
  type: TYPE_TB
- en: Label selectors
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try looking for replicas with `test` deployments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_02_08-updated.png)'
  prefs: []
  type: TYPE_IMG
- en: Replication controller listing
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll notice that it only returns the replication controller we just started.
    How about services with a label named `component`? Use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing of services with a label named component
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we see the core Kubernetes service only. Finally, let''s just get the
    `node-js` servers we started in this chapter. See the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_02_10-updated.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing of services with a label name and a value of node-js or node-js-labels
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we can perform management tasks across a number of pods and services.
    For example, we can kill all replication controllers that are part of the `demo`
    deployment (if we had any running), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, kill all services that are part of a `production` or `test` deployment
    (again, if we had any running), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: It's important to note that, while label selection is quite helpful in day-to-day
    management tasks, it does require proper deployment hygiene on our part. We need
    to make sure that we have a tagging standard and that it is actively followed
    in the resource definition files for everything we run on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we used service definition YAML files to create our services thus far,
    you can actually create them using a `kubectl` command only. To try this out,
    first run the `get pods` command and get one of the `node-js` pod names. Next,
    use the following `expose` command to create a service endpoint for just that
    pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ kubectl expose pods node-js-gxkix --port=80 --name=testing-vip --create-external-load-balancer=true**`
    This will create a service named `testing-vip` and also a public `vip` (load balancer
    IP) that can be used to access this pod over port `80`. There are number of other
    optional parameters that can be used. These can be found with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`kubectl expose --help`**'
  prefs: []
  type: TYPE_NORMAL
- en: Replica sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier, replica sets are the new and improved version of replication
    controllers. They take advantage of set-based label selection, but they are still
    considered beta at time of this writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a `ReplicaSet` based on and similar to the `ReplicationController`
    in *listing 2-4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-6*: `nodejs-labels-replicaset.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: Health checks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes provides two layers of health checking. First, in the form of HTTP
    or TCP checks, K8s can attempt to connect to a particular endpoint and give a
    status of healthy on a successful connection. Second, application-specific health
    checks can be performed using command-line scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a few health checks in action. First, we''ll create a
    new controller with a health check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-7*: `nodejs-health-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: Note the addition of the `livenessprobe` element. This is our core health check
    element. From here, we can specify `httpGet`, `tcpScoket`, or `exec`. In this
    example, we use `httpGet` to perform a simple check for a URI on our container.
    The probe will check the path and port specified and restart the pod if it doesn't
    successfully return.
  prefs: []
  type: TYPE_NORMAL
- en: Status codes between `200` and `399` are all considered healthy by the probe.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `initialDelaySeconds` gives us the flexibility to delay health checks
    until the pod has finished initializing. The `timeoutSeconds` value is simply
    the timeout value for the probe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use our new health check-enabled controller to replace the old `node-js`
    RC. We can do this using the `replace` command, which will replace the replication
    controller definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Replacing the RC on its own won''t replace our containers because it still
    has three healthy pods from our first run. Let''s kill off those pods and let
    the updated `ReplicationController` replace them with containers that have health
    checks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, after waiting a minute or two, we can list the pods in an RC and grab
    one of the pod IDs to inspect a bit deeper with the `describe` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_02_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Description of node-js replication controller
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, use the following command for one of the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_02_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Description of node-js-1m3cs pod
  prefs: []
  type: TYPE_NORMAL
- en: At the top, we will see the overall pod details. Depending on your timing, under
    `State`, it will either show `Running` or `Waiting` with a `CrashLoopBackOff`
    reason and some error information. A bit below that we can see information on
    our `Liveness` probe and we will likely see a failure count above `0`. Further
    down we have the pod events. Again, depending on your timing, you are likely to
    have a number of events for the pod. Within a minute or two, you'll note a pattern
    of killing, started, and created events repeating over and over again. You should
    also see a note in the `Killing` entry that the container is unhealthy. This is
    our health check failing because we don't have a page responding at `/status`.
  prefs: []
  type: TYPE_NORMAL
- en: You may note that if you open a browser to the service load balancer address,
    it still responds with a page. You can find the load balancer IP with a `kubectl
    get services` command.
  prefs: []
  type: TYPE_NORMAL
- en: This is happening for a number of reasons. First, the health check is simply
    failing because `/status` doesn't exist, but the page where the service is pointed
    is still functioning normally in between restarts. Second, the `livenessProbe`
    is only charged with restarting the container on a health check fail. There is
    a separate `readinessProbe` that will remove a container from the pool of pods
    answering service endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s modify the health check for a page that does exist in our container,
    so we have a proper health check. We''ll also add a readiness check and point
    it to the nonexistent status page. Open the `nodejs-health-controller.yaml` file
    and modify the `spec` section to match *Listing 2-8* and save it as `nodejs-health-controller-2.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-8*: `nodejs-health-controller-2.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we will delete the old RC, which will kill the pods with it, and
    create a new RC with our updated YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we describe one of the pods, we only see the creation of the pod
    and the container. However, you''ll note that the service load balancer IP no
    longer works. If we run the `describe` command on one of the new nodes we''ll
    note a `Readiness probe failed` error message, but the pod itself continues running.
    If we change the readiness probe path to `path: /`, we will again be able to fulfill
    requests from the main service. Open up `nodejs-health-controller-2.yaml` in an
    editor and make that update now. Then, once again remove and recreate the replication
    controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now the load balancer IP should work once again. Keep these pods around as we
    will use them again in [Chapter 3](112f7a80-e9ed-43e9-a165-ea41fa523250.xhtml),
    *Networking, Load Balancers, and Ingress*.
  prefs: []
  type: TYPE_NORMAL
- en: TCP checks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes also supports health checks via simple TCP socket checks and also
    with custom command-line scripts. The following snippets are examples of what
    both use cases look like in the YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-9*: *Health check using command-line script*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-10*: *Health check using simple TCP Socket connection*'
  prefs: []
  type: TYPE_NORMAL
- en: Life cycle hooks or graceful shutdown
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you run into failures in real-life scenarios, you may find that you want
    to take additional action before containers are shutdown or right after they are
    started. Kubernetes actually provides life cycle hooks for just this kind of use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example controller definition defines both a `postStart` action
    and a `preStop` action to take place before Kubernetes moves the container into
    the next stage of its life cycle (you can refer to more details about this in
    point 1 in the *References* section at the end of the chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-11*: `apache-hooks-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: You'll note for the `postStart` hook, we define an `httpGet` action, but for
    the `preStop` hook, I define an `exec` action. Just as with our health checks,
    the `httpGet` action attempts to make an HTTP call to the specific endpoint and
    port combination, while the `exec` action runs a local command in the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `httpGet` and `exec` actions are both supported for the `postStart` and
    `preStop` hooks. In the case of `preStop`, a parameter named `reason` will be
    sent to the handler as a parameter. See the following table for valid values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Reason parameter** | **Failure Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Delete | Delete command issued via `kubectl` or the API |'
  prefs: []
  type: TYPE_TB
- en: '| Health | Health check fails |'
  prefs: []
  type: TYPE_TB
- en: '| Dependency | Dependency failure such as a disk mount failure or a default
    infrastructure pod crash |'
  prefs: []
  type: TYPE_TB
- en: Valid preStop reasons (refer to point 1 in *References* section)
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that hook calls are delivered at least once. Therefore,
    any logic in the action should gracefully handle multiple calls. Another important
    note is that `postStart` runs before a pod enters its ready state. If the hook
    itself fails, the pod will be considered unhealthy.
  prefs: []
  type: TYPE_NORMAL
- en: Application scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand how to run containers in pods and even recover from failure,
    it may be useful to understand how new containers are scheduled on our cluster
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, the default behavior for the Kubernetes scheduler is to
    spread container replicas across the nodes in our cluster. In the absence of all
    other constraints, the scheduler will place new pods on nodes with the least number
    of other pods belonging to matching services or replication controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the scheduler provides the ability to add constraints based on
    resources available to the node. Today, this includes minimum CPU and memory allocations.
    In terms of Docker, these use the **CPU-shares** and **memory limit flags** under
    the covers.
  prefs: []
  type: TYPE_NORMAL
- en: When additional constraints are defined, Kubernetes will check a node for available
    resources. If a node does not meet all the constraints, it will move to the next.
    If no nodes can be found that meet the criteria, then we will see a scheduling
    error in the logs.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes roadmap also has plans to support networking and storage. Because
    scheduling is such an important piece of overall operations and management for
    containers, we should expect to see many additions in this area as the project
    grows.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a look at a quick example of setting some resource limits. If we
    look at our K8s dashboard, we can get a quick snapshot of the current state of
    resource usage on our cluster using `https://<your master ip>/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard` and
    clicking on Nodes on the left-hand side menu.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see a dashboard as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_02_13.png)'
  prefs: []
  type: TYPE_IMG
- en: Kube Node dashboard
  prefs: []
  type: TYPE_NORMAL
- en: This  view shows the aggregate CPU and memory across the whole cluster, nodes,
    and master. In this case, we have fairly low CPU utilization, but a decent chunk
    of memory in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what happens when I try to spin up a few more pods, but this time,
    we will request `512 Mi` for memory and `1500 m` for the CPU. We''ll use `1500
    m` to specify 1.5 CPUs; since each node only has 1 CPU, this should result in
    failure. Here''s an example of RC definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 2-12*: `nodejs-constraints-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the preceding file, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The replication controller completes successfully, but if we run a `get pods`
    command, we''ll note the `node-js-constraints` pods are stuck in a pending state.
    If we look a little closer with the `describe pods/<pod-id>` command, we''ll note
    a scheduling error (for `pod-id` use one of the pod names from the first command):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_02_14.png)'
  prefs: []
  type: TYPE_IMG
- en: Pod description
  prefs: []
  type: TYPE_NORMAL
- en: Note, in the bottom events section, that the `WarningFailedScheduling pod` error
    listed in `Events` is accompanied by `fit failure on node....Insufficient cpu` after
    the error. As you can see, Kubernetes could not find a fit in the cluster that
    met all the constraints we defined.
  prefs: []
  type: TYPE_NORMAL
- en: If we now modify our CPU constraint down to `500 m`, and then recreate our replication
    controller, we should have all three pods running within a few moments.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We took a look at the overall architecture for Kubernetes, as well as the core
    constructs, provided to build your services and application stacks. You should
    have a better understanding of how these abstractions make it easier to manage
    the life cycle of your stack and/or services as a whole and not just the individual
    components. Additionally, we took a first-hand look at how to manage some simple
    day-to-day tasks using pods, services, and replication controllers. We also looked
    at how to use Kubernetes to automatically respond to outages via health checks.
    Finally, we explored the Kubernetes scheduler and some of the constraints users
    can specify to influence scheduling placement.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into the networking layer of Kubernetes. We'll
    see how networking is done and also look at the core Kubernetes proxy that is
    used for traffic routing. We will also look at service discovery and the logical
    namespace groupings.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://github.com/GoogleCloudPlatform/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks](https://github.com/GoogleCloudPlatform/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
