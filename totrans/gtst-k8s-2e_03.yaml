- en: Networking, Load Balancers, and Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be covering how the Kubernetes cluster handles networking
    and how it differs from other approaches. We will be describing the three requirements
    for Kubernetes networking solutions and exploring why these are key to ease of
    operations. Further, we will take a deeper dive into services and how the Kubernetes
    proxy works on each node. Finishing up, we will see a brief overview of some higher
    level isolation features for multitenancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will discuss the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced services concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namespace limits and quotas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Networking is a vital concern for production-level operations. At a service
    level, we need a reliable way for our application components to find and communicate
    with each other. Introducing containers and clustering into the mix makes things
    more complex as we now have multiple networking namespaces to bear in mind. Communication
    and discovery now becomes a feat that must traverse container IP space, host networking,
    and sometimes even multiple data center network topologies.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes benefits here from getting its ancestry from the clustering tools
    used by Google for the past decade. Networking is one area where Google has outpaced
    the competition with one of the largest networks on the planet. Earlier, Google
    built its own hardware switches and **Software-defined Networking** (**SDN**)
    to give them more control, redundancy, and efficiency in their day-to-day network
    operations (you can refer to more details about this in point 1 in the *References*
    section at the end of the chapter). Many of the lessons learned from running and
    networking two billion containers per week have been distilled into Kubernetes
    and informed how K8s networking is done.
  prefs: []
  type: TYPE_NORMAL
- en: Networking in Kubernetes requires that each pod has its own IP address. Implementation
    details may vary based on the underlying infrastructure provider. However, all
    implementations must adhere to some basic rules. First and second, Kubernetes
    does not allow the use of **Network Address Translation** (**NAT**) for container-to-container
    or for container-to-node (minion) traffic. Further, the internal container IP
    address must match the IP address that is used to communicate with it.
  prefs: []
  type: TYPE_NORMAL
- en: These rules keep much of the complexity out of our networking stack and ease
    the design of the applications. Further, they eliminate the need to redesign network
    communication in legacy applications that are migrated from existing infrastructure.
    Finally, in greenfield applications, they allows for greater scale in handling
    hundreds, or even thousands of services and application communication.
  prefs: []
  type: TYPE_NORMAL
- en: K8s achieves this pod-wide IP magic using a **placeholder**. Remember that the `pause`
    container, we saw in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to* *Kubernetes*, under the *Services running on the master* section,
    is often referred to as a **pod infrastructure container**, and it has the important
    job of reserving the network resources for our application containers that will
    be started later on. Essentially, the `pause` container holds the networking namespace
    and IP address for the entire pod and can be used by all the containers running
    within. The `pause` container joins first and holds the namespace while the subsequent
    containers in the pod join it when they start up.
  prefs: []
  type: TYPE_NORMAL
- en: Networking options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes provides a wide range of networking options. There are solutions
    that work with the native networking layers in AWS and GCP. There are various
    overlay plugins, some of which are discussed in the next section. Finally, there
    is support for **Container Networking Interface** (**CNI**) plugins. CNI is meant
    to be a common plugin architecture for containers. It''s currently supported by
    several orchestration tools such as Kubernetes, Mesos, and CloudFoundry. Find
    out more information at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni).'
  prefs: []
  type: TYPE_NORMAL
- en: Always refer to the Kubernetes documentation for the latest and full list of
    supported networking options.
  prefs: []
  type: TYPE_NORMAL
- en: Networking comparisons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get a better understanding of networking in containers, it can be instructive
    to look at other approaches to container networking. The following approaches do
    not make an exhaustive list, but should give a taste of the options available.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Docker Engine** creates three types of networks by default. These are
    **bridged**, **host**, and **none**.
  prefs: []
  type: TYPE_NORMAL
- en: The bridged network is the default choice unless otherwise specified. In this
    mode, the container has its own networking namespace and is then bridged via virtual
    interfaces to the host (or node in the case of K8s) network. In the bridged network,
    two containers can use the same IP range because they are completely isolated.
    Therefore, service communication requires some additional port mapping through
    the host side of network interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Docker also supports a host network, which allows the containers to use the
    host network stack. Performance is greatly benefited since it removes a level
    of network virtualization; however, you lose the security of having an isolated
    network namespace. Additionally, port usage must be managed more carefully since
    all containers share an IP.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Docker supports a none network, which creates a container with no external
    interface. Only a loopback device is shown if you inspect the network interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: In all these scenarios, we are still on a single machine, and outside of a host
    mode, the container IP space is not available, outside that machine. Connecting
    containers across two machines then requires **NAT** and **port mapping** for
    communication.
  prefs: []
  type: TYPE_NORMAL
- en: Docker user-defined networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to address the cross-machine communication issue and allow greater
    flexibility, Docker also supports user-defined networks via network plugins. These
    networks exist independent of the containers themselves. In this way, containers
    can join the same existing **networks**. Through the new plugin architecture,
    various drivers can be provided for different network use cases.
  prefs: []
  type: TYPE_NORMAL
- en: The first of these is the **bridge** driver, which allows creation of networks
    somewhat similar to the default bridge network.
  prefs: []
  type: TYPE_NORMAL
- en: The second is the **overlay** driver. In order to coordinate across multiple
    hosts, they must all agree on the available networks and their topologies. The
    overlay driver uses a distributed key-value store to synchronize the network creation
    across multiple hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Docker also supports a **Macvlan** driver, which uses the interface and sub-interfaces on
    the host. Macvlan offers a more efficient network virtualization and isolation
    as it bypasses the Linux bridge.
  prefs: []
  type: TYPE_NORMAL
- en: The plugin mechanism will allow a wide range of networking possibilities in
    Docker. In fact, many of the third-party options such as Weave have already created
    their own Docker network plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Weave
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Weave** provides an overlay network for Docker containers. It can be used
    as a plugin with the new Docker network plugin interface, and it is also compatible
    with Kubernetes through a CNI plugin. Like many overlay networks, many criticize
    the performance impact of the encapsulation overhead. Note that they have recently
    added a preview release with **Virtual Extensible LAN** (**VXLAN**) encapsulation
    support, which greatly improves performance. For more information, visit [http://blog.weave.works/2015/06/12/weave-fast-datapath/](http://blog.weave.works/2015/06/12/weave-fast-datapath/).
    [](http://blog.weave.works/2015/06/12/weave-fast-datapath/)'
  prefs: []
  type: TYPE_NORMAL
- en: Flannel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Flannel** comes from CoreOS and is an etcd-backed overlay. Flannel gives
    a full subnet to each host/node enabling a similar pattern to the Kubernetes practice
    of a routable IP per pod or group of containers. Flannel includes an in-kernel
    VXLAN encapsulation mode for better performance and has an experimental multi-network
    mode similar to the overlay Docker plugin. For more information, visit [https://github.com/coreos/flannel](https://github.com/coreos/flannel).
    [](https://github.com/coreos/flannel)'
  prefs: []
  type: TYPE_NORMAL
- en: Project Calico
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Project Calico** is a layer 3-based networking model that uses the built-in
    routing functions of the Linux kernel. Routes are propagated to virtual routers
    on each host via **Border Gateway Protocol** (**BGP**). Calico can be used for
    anything from small-scale deploys to large Internet-scale installations. Because
    it works at a lower level on the network stack, there is no need for additional
    NAT, tunneling, or overlays. It can interact directly with the underlying network
    infrastructure. Additionally, it has a support for network-level ACLs to provide
    additional isolation and security. For more information visit the following URL:
    [http://www.projectcalico.org/](http://www.projectcalico.org/). [](http://www.projectcalico.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: Canal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Canal** merges both Calico for network policy and Flannel for overlay into
    one solution. It supports both Calico and Flannel type overlays and uses the Calico
    policy enforcement logic. Users can choose from overlay and non-overlay options
    with this setup as it combines the features of the preceding two projects. For
    more information visit the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/tigera/canal](https://github.com/tigera/canal)'
  prefs: []
  type: TYPE_NORMAL
- en: Balanced design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's important to point out the balance Kubernetes is trying to achieve by placing
    the IP at the pod level. Using unique IP addresses at the host level is problematic
    as the number of containers grow. Ports must be used to expose services on specific
    containers and allow external communication. In addition to this, the complexity
    of running multiple services that may or may not know about each other (and their
    custom ports) and managing the port space becomes a big issue.
  prefs: []
  type: TYPE_NORMAL
- en: However, assigning an IP address to each container can be overkill. In cases
    of sizable scale, overlay networks and NATs are needed in order to address each
    container. Overlay networks add latency, and IP addresses would be taken up by
    backend services as well since they need to communicate with their frontend counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we really see an advantage in the abstractions that Kubernetes provides
    at the application and service level. If I have a web server and a database, we
    can keep them on the same pod and use a single IP address. The web server and
    database can use the local interface and standard ports to communicate, and no
    custom setup is required. Further, services on the backend are not needlessly
    exposed to other application stacks running elsewhere in the cluster (but possibly
    on the same host). Since the pod sees the same IP address that the applications
    running within it see, service discovery does not require any additional translation.
  prefs: []
  type: TYPE_NORMAL
- en: If you need the flexibility of an overlay network, you can still use an overlay
    at the pod level. Weave, Flannel, and Project Calico can be used with Kubernetes
    as well as a plethora of other plugins and overlays available now.
  prefs: []
  type: TYPE_NORMAL
- en: This is also very helpful in the context of scheduling the workloads. It is
    a key to have a simple and standard structure for the scheduler to match constraints
    and understand where space exists on the cluster's network at any given time.
    This is a dynamic environment with a variety of applications and tasks running,
    so any additional complexity here will have rippling effects.
  prefs: []
  type: TYPE_NORMAL
- en: There are also implications for service discovery. New services coming online
    must determine and register an IP address on which the rest of the world, or at
    least cluster, can reach them. If NAT is used, the services will need an additional
    mechanism to learn their externally facing IP.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's explore the IP strategy as it relates to services and communication between
    containers. If you recall, in the *Services* section, [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml),
    *Pods, Services, Replication Controllers, and Labels*, you learned that Kubernetes
    is using kube-proxy to determine the proper pod IP address and port serving each
    request. Behind the scenes, kube-proxy is actually using virtual IPs and iptables
    to make all this magic work.
  prefs: []
  type: TYPE_NORMAL
- en: Kube-proxy now has two modes—*userspace* and *iptables*. As of now, 1.2 iptables
    is the default mode. In both modes, kube-proxy is running on every host. Its first
    duty is to monitor the API from the Kubernetes master. Any updates to services
    will trigger an update to iptables from kube-proxy. For example, when a new service
    is created, a virtual IP address is chosen and a rule in iptables is set, which
    will direct its traffic to kube-proxy via a random port. Thus, we now have a way
    to capture service-destined traffic on this node. Since kube-proxy is running
    on all nodes, we have cluster-wide resolution for the service **VIP** (short for
    **virtual IP**). Additionally, DNS records can point to this VIP as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the userspace mode,we have a hook created in iptables, but the proxying
    of traffic is still handled by kube-proxy. The iptables rule is only sending traffic
    to the service entry in kube-proxy at this point. Once kube-proxy receives the
    traffic for a particular service, it must then forward it to a pod in the service''s
    pool of candidates. It does this using a random port that was selected during
    service creation. Refer to the following figure for an overview of the flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Kube-proxy communication
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to always forward traffic from the same client IP to the
    same backend pod/container using the `sessionAffinity` element in your service
    definition.
  prefs: []
  type: TYPE_NORMAL
- en: In the iptables mode, the pods are coded directly in the iptable rules. This
    removes the dependency on kube-proxy for actually proxying the traffic. The request
    will go straight to iptables and then on to the pod. This is faster and removes
    a possible point of failure. Readiness probe, as we discussed in the *Health Check *section,
    [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml), *Pods, Services, Replication
    Controllers, and Labels*, is your friend here as this mode also loses the ability
    to retry pods.
  prefs: []
  type: TYPE_NORMAL
- en: External services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we saw a few service examples. For testing and demonstration
    purposes, we wanted all the services to be externally accessible. This was configured
    by the `type: LoadBalancer` element in our service definition. The `LoadBalancer`
    type creates an external load balancer on the cloud provider. We should note that
    support for external load balancers varies by provider, as does the implementation.
    In our case, we are using GCE, so integration is pretty smooth. The only additional
    setup needed is to open firewall rules for the external service ports.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s dig a little deeper and do a `describe` command on one of the services
    from the *More on labels* section in [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml),
    *Pods, Services, Replication Controllers, and Labels*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Service description
  prefs: []
  type: TYPE_NORMAL
- en: In the output, in the preceding figure, you'll note several key elements. Our
    `Namespace:` is set to `default`, `Type:` is `LoadBalancer`, and we have the external
    IP listed under `LoadBalancer Ingress:`. Further, we see `Endpoints:`, which shows
    us the IPs of the pods available to answer service requests.
  prefs: []
  type: TYPE_NORMAL
- en: Internal services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s explore the other types of services we can deploy. First, by default,
    services are only internally facing. You can specify a type of `clusterIP` to
    achieve this, but, if no type is defined, `clusterIP` is the assumed type. Let''s
    take a look at an example; note the lack of the `type` element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-1*: `nodejs-service-internal.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use this listing to create the service definition file. You''ll need a healthy
    version of the `node-js` RC (*Listing 2-7*: `nodejs-health-controller-2.yaml`).
    As you can see, the selector matches on the pods named `node-js` that our RC launched
    in the previous chapter. We will create the service and then list the currently
    running services with a filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: Internal service listing
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we have a new service, but only one IP. Further, the IP address
    is not externally accessible. We won''t be able to test the service from a web
    browser this time. However, we can use the handy `kubectl exec` command and attempt
    to connect from one of the other pods. You will need `node-js-pod` (*Listing 2-1*:
    `nodejs-pod.yaml`) running. Then, you can execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to run a `docker exec` command as if we had a shell in the `node-js-pod`
    container. It then hits the internal service URL, which forwards to any pods with
    the `node-js` label.
  prefs: []
  type: TYPE_NORMAL
- en: If all is well, you should get the raw HTML output back. So, you successfully
    created an internal-only service. This can be useful for backend services that
    you want to make available to other containers running in your cluster, but not
    open to the world at large.
  prefs: []
  type: TYPE_NORMAL
- en: Custom load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A third type of service that K8s allows is the `NodePort` type. This type allows
    us to expose a service through the host or node (minion) on a specific port. In
    this way, we can use the IP address of any node (minion) and access our service
    on the assigned node port. Kubernetes will assign a node port by default in the
    range of `3000`-`32767`, but you can also specify your own custom port. In the
    example in *Listing 3-2*: `nodejs-service-nodeport.yaml`, we choose port `30001`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-2*: `nodejs-service-nodeport.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, create this YAML definition file and create your service, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should have a message like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: New GCP firewall rule
  prefs: []
  type: TYPE_NORMAL
- en: You'll note a message about opening firewall ports. Similar to the external
    load balancer type, `NodePort` is exposing your service externally using ports
    on the nodes. This could be useful if, for example, you want to use your own load
    balancer in front of the nodes. Let's make sure that we open those ports on GCP
    before we test our new service.
  prefs: []
  type: TYPE_NORMAL
- en: From the GCE VM instance console, click on the details for any of your nodes
    (minions). Then click on the network, which is usually default unless otherwise
    specified during creation. In Firewall rules, we can add a rule by clicking on Add
    firewall rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a rule like the one shown in the following figure (`tcp:30001` on `0.0.0.0/0`
    IP range):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Create New GCP firewall rule
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now test our new service, by opening a browser and using an IP address
    of any node (minion) in your cluster. The format to test the new service is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://<Minoion IP Address>:<NodePort>/`'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the latest version has added an `ExternalName` type, which maps a CNAME
    to the service.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-node proxy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remember that kube-proxy is running on all the nodes, so, even if the pod is
    not running there, the traffic will be given a proxy to the appropriate host.
    Refer to the *Cross-node traffic* figure for a visual on how the traffic flows.
    A user makes a request to an external IP or URL. The request is serviced by **Node** in
    this case. However, the pod does not happen to run on this node. This is not a
    problem because the pod IP addresses are routable. So, **Kube-proxy** or **iptables**
    simply passes traffic onto the pod IP for this service. The network routing then
    completes on **Node 2**, where the requested application lives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-node traffic
  prefs: []
  type: TYPE_NORMAL
- en: Custom ports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Services also allow you to map your traffic to different ports; then the containers
    and pods expose themselves. We will create a service that exposes port `90` and
    forwards traffic to port `80` on the pods. We will call the `node-js-90` pod to
    reflect the custom port number. Create the following two definition files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-3*: `nodejs-customPort-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-4*: `nodejs-customPort-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: You'll note that in the service definition, we have a `targetPort` element.
    This element tells the service the port to use for pods/containers in the pool.
    As we saw in previous examples, if you do not specify `targetPort`, it assumes
    that it's the same port as the service. This port is still used as the service
    port, but, in this case, we are going to expose the service on port `90` while
    the containers serve content on port `80`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create this RC and service and open the appropriate firewall rules, as we did
    in the last example. It may take a moment for the external load balancer IP to
    propagate to the `get service` command. Once it does, you should be able to open
    and see our familiar web application in a browser using the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://<external service IP>:90/`'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple ports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another custom port use case is that of multiple ports. Many applications expose
    multiple ports, such as HTTP on port `80` and port `8888` for web servers. The
    following example shows our app responding on both ports. Once again, we''ll also
    need to add a firewall rule for this port, as we did for *Listing 3-2*: `nodejs-service-nodeport.yaml`
    previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-5*: `nodejs-multi-controller.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-6*: `nodejs-multi-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: The application and container itself must be listening on both ports for this
    to work. In this example, port `8888` is used to represent a fake admin interface.
  prefs: []
  type: TYPE_NORMAL
- en: If, for example, you want to listen on port `443`, you would need a proper SSL
    socket listening on the server.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed previously how Kubernetes uses the service abstract as a means
    to proxy traffic to backing pod distributed throughout our cluster. While this
    is helpful in both scaling and pod recovery, there are more advanced routing scenarios
    that are not addressed by this design.
  prefs: []
  type: TYPE_NORMAL
- en: To that end, Kubernetes has added an Ingress resource, which allows for custom
    proxying and load balancing to a back service. Think of it as an extra layer or
    hop in the routing path before traffic hits our service. Just as an application
    has a service and backing pods, the Ingress resource needs both an Ingress entry
    point and an Ingress controller that perform the custom logic. The entry point
    defines the routes and the controller actually handles the routing. For our examples,
    we will use the default GCE backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some limitations to be aware of when using the Ingress API can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md](https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you may recall, in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to Kubernetes*, we saw that a GCE cluster comes with a default back
    which provides Layer 7 load balancing capability. We can see this controller running
    if we look at the `kube-system` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see an RC listed with the `l7-default-backend-v1.0` name, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: GCE Layer 7 Ingress controller
  prefs: []
  type: TYPE_NORMAL
- en: This provides the Ingress controller piece that actually routes the traffic
    defined in our Ingress entry points. Let's create some resources for an Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create a few new replication controllers with my `httpwhalesay`
    image. This is a remix of the original whalesay that was displayed in a browser.
    The following listing shows the YAML. Notice the three dashes that let us combine
    several resources into one YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-7.* `whale-rcs.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we are creating pods with the same container, but different start
    up parameters. Take note of these parameters for later. We will also create `Service` endpoints
    for each of these RCs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-8.* `whale-svcs.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again create these with the `kubectl create -f` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see messages about the RCs and Services successful creation. Next,
    we need to define the Ingress entry point. We will use `http://a.whale.hey` and
    `http://b.whale.hey` as our demo entry points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-9.* `whale-ingress.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, use `kubectl create -f` to create this Ingress. Once this is successfully
    created, we will need to wait a few moments for GCE to give the Ingress a static
    IP address. Use the following command to watch the Ingress resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the Ingress has an IP, we should see an entry in `ADDRESS` like the one
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: Ingress Description
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this is not a registered domain name, we will need to specify the resolution
    in the `curl` command, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This should display the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Whalesay A
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also try the second URL and we will get our second RC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B06302_03_10.png)'
  prefs: []
  type: TYPE_IMG
- en: Whalesay B
  prefs: []
  type: TYPE_NORMAL
- en: We notice that the images are almost the same, except that the words from each
    whale reflect the startup parameters from each RC we started earlier. Thus our
    two Ingress points are directing traffic to different backends.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we used the default GCE backend for an Ingress controller.
    Kubernetes allows us to build our own and Nginx actually has a few versions available
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Migrations, multicluster, and more
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you've seen so far, Kubernetes offers a high level of flexibility and customization
    to create a service abstraction around your containers running in the cluster.
    However, there may be times where you want to point to something outside your
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: An example of this would be working with legacy systems or even applications
    running on another cluster. In the case of the former, this is a perfectly good
    strategy in order to migrate to Kubernetes and containers in general. We can begin
    to manage the service endpoints in Kubernetes while stitching the stack together
    using the K8s orchestration concepts. Additionally, we can even start bringing
    over pieces of the stack, as the frontend, one at a time as the organization refactors
    applications for microservices and/or containerization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow access to non-pod-based applications, the services construct allows
    you to use endpoints that are outside the cluster. Kubernetes is actually creating
    an endpoint resource every time you create a service that uses selectors. The
    `endpoints` object keeps track of the pod IPs in the load balancing pool. You
    can see this by running a `get endpoints` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You'll note an entry for all the services we currently have running on our cluster.
    For most services, the endpoints are just the IP of each pod running in an RC.
    As I mentioned, Kubernetes does this automatically based on the selector. As we
    scale the replicas in a controller with matching labels, Kubernetes will update
    the endpoints automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to create a service for something that is not a pod and therefore
    has no labels to select, we can easily do this with both a service and endpoint
    definition, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-10*: `nodejs-custom-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-11*: `nodejs-custom-endpoint.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, you'll need to replace `<X.X.X.X>` with a real IP
    address, where the new service can point to. In my case, I used the public load
    balancer IP from the `node-js-multi` service we created earlier in *listing 3-6*.
    Go ahead and create these resources now.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we now run a `get endpoints` command, we will see this IP address at port
    `80` associated with the `custom-service` endpoint. Further, if we look at the
    service details, we will see the IP listed in the `Endpoints` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can test out this new service by opening the `custom-service` external IP
    from a browser.
  prefs: []
  type: TYPE_NORMAL
- en: Custom addressing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another option to customize services is with the `clusterIP` element. In our
    examples so far, we''ve not specified an IP address, which means that it chooses
    the internal address of the service for us. However, we can add this element and
    choose the IP address in advance with something like `clusterip: 10.0.125.105`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There may be times when you don''t want to load balance and would rather have
    DNS with *A* records for each pod. For example, software that needs to replicate
    data evenly to all nodes may rely on *A* records to distribute data. In this case,
    we can use an example like the following one and set `clusterip` to `None`. Kubernetes
    will not assign an IP address and instead only assign *A* records in DNS for each
    of the pods. If you are using DNS, the service should be available at `node-js-none`
    or `node-js-none.default.cluster.local` from within the cluster. We have the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-12*: `nodejs-headless-service.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Test it out after you create this service with the trusty `exec` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Service discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed earlier, the Kubernetes master keeps track of all service definitions
    and updates. Discovery can occur in one of three ways. The first two methods use
    Linux environment variables. There is support for the Docker link style of environment
    variables, but Kubernetes also has its own naming convention. Here is an example
    of what our `node-js` service example might look like using K8s environment variables
    (note IPs will vary):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-13*: *Service environment variables*'
  prefs: []
  type: TYPE_NORMAL
- en: Another option for discovery is through DNS. While environment variables can
    be useful when DNS is not available, it has drawbacks. The system only creates
    variables at creation time, so services that come online later will not be discovered
    or would require some additional tooling to update all the system environments.
  prefs: []
  type: TYPE_NORMAL
- en: DNS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DNS solves the issues seen with environment variables by allowing us to reference
    the services by their name. As services restart, scale out, or appear anew, the
    DNS entries will be updating and ensuring that the service name always points
    to the latest infrastructure. DNS is set up by default in most of the supported
    providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'If DNS is supported by your provider, but not set up, you can configure the
    following variables in your default provider config when you create your Kubernetes
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ENABLE_CLUSTER_DNS="${KUBE_ENABLE_CLUSTER_DNS:-true}"` `DNS_SERVER_IP="10.0.0.10"`'
  prefs: []
  type: TYPE_NORMAL
- en: '`DNS_DOMAIN="cluster.local"`'
  prefs: []
  type: TYPE_NORMAL
- en: '`DNS_REPLICAS=1`'
  prefs: []
  type: TYPE_NORMAL
- en: With DNS active, services can be accessed in one of two forms—either the service
    name itself, `<service-name>` or a fully qualified name that includes the namespace,
    `<service-name>.<namespace-name>.cluster.local`. In our examples, it would look
    similar to `node-js-90` or `node-js-90.default.cluster.local`.
  prefs: []
  type: TYPE_NORMAL
- en: Multitenancy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes also has an additional construct for isolation at the cluster level.
    In most cases, you can run Kubernetes and never worry about namespaces; everything
    will run in the default namespace if not specified. However, in cases where you
    run multitenancy communities or want broad-scale segregation and isolation of
    the cluster resources, namespaces can be used to this end.
  prefs: []
  type: TYPE_NORMAL
- en: To start, Kubernetes has two namespaces—`default` and `kube-system`. The `kube-system` namespace
    is used for all the system-level containers we saw in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to* *Kubernetes*, in the *Services running on the minions* section.
    The UI, logging, DNS, and so on are all run in `kube-system`. Everything else
    the user creates runs in the default namespace. However, our resource definition
    files can optionally specify a custom namespace. For the sake of experimenting,
    let's take a look at how to build a new namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll need to create a namespace definition file like the one in this
    listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-14*: `test-ns.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can go ahead and create this file with our handy `create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create resources that use the `test` namespace. The following is
    an example of a pod using this new namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-15*: `ns-pod.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the pod can still access services in other namespaces, it will need to
    use the long DNS form of `<service-name>.<namespace-name>.cluster.local`. For
    example, if you were to run a command from inside the container in *Listing 3-15*:
    `ns-pod.yaml`, you could use `node-js.default.cluster.local` to access the Node.js
    example from [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml), *Pods, Services,
    Replication Controllers, and Labels*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a note about resource utilization. At some point in this book, you
    may run out of space on your cluster to create new Kubernetes resources. The timing
    will vary based on cluster size, but it''s good to keep this in mind and do some
    clean-up from time to time. Use the following commands to remove old examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ kubectl delete pod <pod name>** **$ kubectl delete svc <service name>**
    **$ kubectl delete rc <replication controller name>** ** $ kubectl delete rs <replicaset
    name>**`'
  prefs: []
  type: TYPE_NORMAL
- en: Limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s inspect our new namespace a bit more. Run the `describe` command as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_11.png)'
  prefs: []
  type: TYPE_IMG
- en: Namespace describe
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes allows you to both limit the resources used by individual pods or
    containers and the resources used by the overall namespace using quotas. You'll
    note that there are no resource **limits** or **quotas** currently set on the
    `test` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to limit the footprint of this new namespace; we can set quotas
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-16*: `quota.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: In reality, namespaces would be for larger application communities and would
    probably never have quotas this low. I am using this in order to ease illustration
    of the capability in the example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will create a quota of `3` pods, `1` RC, and `1` service for the test
    namespace. As you probably guessed, this is executed once again by our trusty
    `create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have that in place, let''s use `describe` on the namespace, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Namespace describe after quota is set
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll note that we now have some values listed in the quota section and the
    limits section is still blank. We also have a `Used` column, which lets us know
    how close to the limits we are at the moment. Let''s try to spin up a few pods
    using the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*Listing 3-17*: `busybox-ns.yaml`'
  prefs: []
  type: TYPE_NORMAL
- en: You'll note that we are creating four replicas of this basic pod. After using
    `create` to build this RC, run the `describe` command on the `test` namespace
    once more. You'll notice that the `Used` values for pods and RCs are at their
    max. However, we asked for four replicas and only see three pods in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what''s happening with our RC. You might attempt to do that with
    the command here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: However, if you try, you'll be discouraged to see a `not found` message from
    the server. This is because we created this RC in a new namespace and `kubectl`
    assumes the default namespace if not specified. This means that we need to specify
    `--namepsace=test` with every command when we wish to access resources in the
    `test` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set the current namespace by working with the context settings.
    First, we need to find our current context, which is found with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ kubectl config view | grep current-context**`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can take that context and set the namespace variable like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ kubectl config set-context <Current Context> --namespace=test**`'
  prefs: []
  type: TYPE_NORMAL
- en: Now you can run the `kubectl` command without the need to specify the namespace.
    Just remember to switch back when you want to look at the resources running in
    your default namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the command with the namespace specified. If you''ve set your current namespace
    as demonstrated in the tip box, you can leave off the `--namespace` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06302_03_13.png)'
  prefs: []
  type: TYPE_IMG
- en: Namespace quotas
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding image, the first three pods were successfully
    created, but our final one fails with the `Limited to 3 pods` error.
  prefs: []
  type: TYPE_NORMAL
- en: This is an easy way to set limits for resources partitioned out at a community
    scale. It's worth noting that you can also set quotas for CPU, memory, persistent
    volumes, and secrets. Additionally, limits work in a similar way to quota, but
    they set the limit for each pod or container within the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: A note on resource usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As most of the examples in this book utilize GCP or AWS, it can be costly to
    keep everything running. It's also easy to run out of resources using the default
    cluster size, especially if you keep every example running. Therefore, you may
    want to delete older pods, replication controllers, replica sets, and services
    periodically. You can also destroy the cluster and recreate using [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to Kubernetes* as a way to lower your cloud provider bill.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We took a deeper look into networking and services in Kubernetes. You should
    now understand how networking communications are designed in K8s and feel comfortable
    accessing your services internally and externally. We saw how kube-proxy balances
    traffic both locally and across the cluster. Additionally, we explored the new
    Ingress resources that allow us finer control of incoming traffic. We also looked
    briefly at how DNS and service discovery is achieved in Kubernetes. We finished
    off with quick look at namespace and isolation for multitenancy.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/](http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
