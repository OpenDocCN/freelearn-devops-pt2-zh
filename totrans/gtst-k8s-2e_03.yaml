- en: Networking, Load Balancers, and Ingress
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络、负载平衡和入口控制器
- en: In this chapter, we will be covering how the Kubernetes cluster handles networking
    and how it differs from other approaches. We will be describing the three requirements
    for Kubernetes networking solutions and exploring why these are key to ease of
    operations. Further, we will take a deeper dive into services and how the Kubernetes
    proxy works on each node. Finishing up, we will see a brief overview of some higher
    level isolation features for multitenancy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节中，我们将要覆盖 Kubernetes 集群如何处理网络，以及它和其他方法的不同之处。我们将描述 Kubernetes 网络解决方案的三个要求，并探讨为什么这些对于操作的便捷性至关重要。另外，我们将深入介绍服务以及
    Kubernetes 代理在每个节点上的工作方式。最后，我们将简要概述一些用于多租户的更高级别的隔离特性。
- en: 'This chapter will discuss the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个章节将会讨论以下内容：
- en: Kubernetes networking
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 网络
- en: Advanced services concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级服务概念
- en: Service discovery
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务发现
- en: DNS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS
- en: Namespace limits and quotas
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名空间限制和配额
- en: Kubernetes networking
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 网络
- en: Networking is a vital concern for production-level operations. At a service
    level, we need a reliable way for our application components to find and communicate
    with each other. Introducing containers and clustering into the mix makes things
    more complex as we now have multiple networking namespaces to bear in mind. Communication
    and discovery now becomes a feat that must traverse container IP space, host networking,
    and sometimes even multiple data center network topologies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是生产级别运作的重要考虑因素。在服务层面上，我们需要一种可靠的方法来找到和与应用程序组件通信。引入容器和聚类使得事情更加复杂，因为现在我们必须考虑多个网络命名空间。通信和发现现在需要穿越容器
    IP 空间、主机网络，甚至是多个数据中心的网络拓扑。
- en: Kubernetes benefits here from getting its ancestry from the clustering tools
    used by Google for the past decade. Networking is one area where Google has outpaced
    the competition with one of the largest networks on the planet. Earlier, Google
    built its own hardware switches and **Software-defined Networking** (**SDN**)
    to give them more control, redundancy, and efficiency in their day-to-day network
    operations (you can refer to more details about this in point 1 in the *References*
    section at the end of the chapter). Many of the lessons learned from running and
    networking two billion containers per week have been distilled into Kubernetes
    and informed how K8s networking is done.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 受益于其祖先来自 Google 在过去十年使用的聚类工具。网络是 Google 超越竞争对手的领域之一，其拥有地球上最大的网络之一。早些时候，Google
    构建了自己的硬件交换机和软件定义网络（SDN），以在日常网络操作中获得更多的控制、冗余和效率（您可以在本章节结尾的“参考”部分中的第1点中了解更多详细信息）。从每周运行和网络化的20亿个容器中汲取的许多经验教训已经提炼成了
    Kubernetes，并指导了 K8s 网络的实现方式。
- en: Networking in Kubernetes requires that each pod has its own IP address. Implementation
    details may vary based on the underlying infrastructure provider. However, all
    implementations must adhere to some basic rules. First and second, Kubernetes
    does not allow the use of **Network Address Translation** (**NAT**) for container-to-container
    or for container-to-node (minion) traffic. Further, the internal container IP
    address must match the IP address that is used to communicate with it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中进行网络操作需要每个 Pod 有其自己的 IP 地址。基础设施提供商的实现细节可能会有所不同。但是，所有实现都必须遵守一些基本规则。首先和其次，Kubernetes
    不允许在容器与容器或容器与节点（minion）之间使用网络地址转换（NAT）。此外，内部容器 IP 地址必须与用于与其通信的 IP 地址匹配。
- en: These rules keep much of the complexity out of our networking stack and ease
    the design of the applications. Further, they eliminate the need to redesign network
    communication in legacy applications that are migrated from existing infrastructure.
    Finally, in greenfield applications, they allows for greater scale in handling
    hundreds, or even thousands of services and application communication.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则可以保持我们的网络堆栈的大部分复杂性，并简化应用程序的设计。此外，它们消除了从现有基础设施中迁移的遗留应用程序中重新设计网络通信的需要。最后，在全新的应用程序中，它们允许更大规模地处理数百个甚至数千个服务和应用程序通信。
- en: K8s achieves this pod-wide IP magic using a **placeholder**. Remember that the `pause`
    container, we saw in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to* *Kubernetes*, under the *Services running on the master* section,
    is often referred to as a **pod infrastructure container**, and it has the important
    job of reserving the network resources for our application containers that will
    be started later on. Essentially, the `pause` container holds the networking namespace
    and IP address for the entire pod and can be used by all the containers running
    within. The `pause` container joins first and holds the namespace while the subsequent
    containers in the pod join it when they start up.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: K8s通过一个**占位符**来实现这种整个pod范围的IP魔法。记住，我们在[第1章](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml)中看到的`pause`容器，在*介绍Kubernetes*的*在主节点上运行的服务*部分，通常被称为**pod基础设施容器**，它的重要工作是为稍后启动的应用容器保留网络资源。实质上，`pause`容器持有整个pod的网络命名空间和IP地址，并且可以被所有正在运行的容器使用。`pause`容器首先加入并持有命名空间，随后在pod中启动时，后续容器加入其中。
- en: Networking options
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络选项
- en: 'Kubernetes provides a wide range of networking options. There are solutions
    that work with the native networking layers in AWS and GCP. There are various
    overlay plugins, some of which are discussed in the next section. Finally, there
    is support for **Container Networking Interface** (**CNI**) plugins. CNI is meant
    to be a common plugin architecture for containers. It''s currently supported by
    several orchestration tools such as Kubernetes, Mesos, and CloudFoundry. Find
    out more information at:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了各种网络选项。有些解决方案适用于AWS和GCP中的本机网络层。还有各种覆盖插件，其中一些将在下一节中讨论。最后，还支持**容器网络接口**（**CNI**）插件。CNI旨在成为容器的通用插件架构。它目前得到了几个编排工具的支持，如Kubernetes、Mesos和CloudFoundry。更多信息请访问：
- en: '[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni).'
- en: Always refer to the Kubernetes documentation for the latest and full list of
    supported networking options.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请始终参考Kubernetes文档以获取最新和完整的支持网络选项列表。
- en: Networking comparisons
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络比较
- en: To get a better understanding of networking in containers, it can be instructive
    to look at other approaches to container networking. The following approaches do
    not make an exhaustive list, but should give a taste of the options available.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解容器中的网络，可以研究其他容器网络的方法。以下方法并非穷尽列表，但应该让你对可用选项有所了解。
- en: Docker
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker
- en: The **Docker Engine** creates three types of networks by default. These are
    **bridged**, **host**, and **none**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker引擎**默认创建三种类型的网络。这些是**桥接**、**主机**和**无**。'
- en: The bridged network is the default choice unless otherwise specified. In this
    mode, the container has its own networking namespace and is then bridged via virtual
    interfaces to the host (or node in the case of K8s) network. In the bridged network,
    two containers can use the same IP range because they are completely isolated.
    Therefore, service communication requires some additional port mapping through
    the host side of network interfaces.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接网络是默认选择，除非另有说明。在此模式下，容器有自己的网络命名空间，然后通过虚拟接口桥接到主机（或在K8s情况下是节点）网络。在桥接网络中，两个容器可以使用相同的IP范围，因为它们是完全隔离的。因此，服务通信需要通过网络接口的主机侧进行一些额外的端口映射。
- en: Docker also supports a host network, which allows the containers to use the
    host network stack. Performance is greatly benefited since it removes a level
    of network virtualization; however, you lose the security of having an isolated
    network namespace. Additionally, port usage must be managed more carefully since
    all containers share an IP.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Docker还支持主机网络，允许容器使用主机网络堆栈。性能得到了极大的改善，因为它消除了一个网络虚拟化的层级；然而，你失去了拥有独立网络命名空间的安全性。此外，必须更加谨慎地管理端口使用，因为所有容器共享一个IP。
- en: Finally, Docker supports a none network, which creates a container with no external
    interface. Only a loopback device is shown if you inspect the network interfaces.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Docker支持一个none网络，它创建一个没有外部接口的容器。如果检查网络接口，只显示一个回环设备。
- en: In all these scenarios, we are still on a single machine, and outside of a host
    mode, the container IP space is not available, outside that machine. Connecting
    containers across two machines then requires **NAT** and **port mapping** for
    communication.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些场景中，我们仍然位于单个机器上，而且在主机模式之外，容器 IP 空间对于该机器外部是不可用的。连接跨越两台机器的容器然后需要进行 **NAT**
    和 **端口映射** 以进行通信。
- en: Docker user-defined networks
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker 用户定义的网络
- en: In order to address the cross-machine communication issue and allow greater
    flexibility, Docker also supports user-defined networks via network plugins. These
    networks exist independent of the containers themselves. In this way, containers
    can join the same existing **networks**. Through the new plugin architecture,
    various drivers can be provided for different network use cases.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决跨机器通信问题并提供更大的灵活性，Docker 还通过网络插件支持用户定义的网络。这些网络独立于容器本身存在。通过这种方式，容器可以加入相同的现有
    **网络**。通过新的插件架构，可以为不同的网络用例提供各种驱动程序。
- en: The first of these is the **bridge** driver, which allows creation of networks
    somewhat similar to the default bridge network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些中的第一个是 **bridge** 驱动程序，它允许创建与默认桥接网络类似的网络。
- en: The second is the **overlay** driver. In order to coordinate across multiple
    hosts, they must all agree on the available networks and their topologies. The
    overlay driver uses a distributed key-value store to synchronize the network creation
    across multiple hosts.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个是 **overlay** 驱动程序。为了跨多个主机进行协调，它们都必须就可用网络及其拓扑达成一致。覆盖驱动程序使用分布式键值存储来在多个主机之间同步网络创建。
- en: Docker also supports a **Macvlan** driver, which uses the interface and sub-interfaces on
    the host. Macvlan offers a more efficient network virtualization and isolation
    as it bypasses the Linux bridge.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 还支持一个 **Macvlan** 驱动程序，该驱动程序使用主机上的接口和子接口。Macvlan 提供了更有效的网络虚拟化和隔离，因为它绕过了
    Linux 桥接。
- en: The plugin mechanism will allow a wide range of networking possibilities in
    Docker. In fact, many of the third-party options such as Weave have already created
    their own Docker network plugins.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 插件机制将允许 Docker 中的各种网络可能性。事实上，许多第三方选项，如 Weave，已经创建了自己的 Docker 网络插件。
- en: Weave
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Weave
- en: '**Weave** provides an overlay network for Docker containers. It can be used
    as a plugin with the new Docker network plugin interface, and it is also compatible
    with Kubernetes through a CNI plugin. Like many overlay networks, many criticize
    the performance impact of the encapsulation overhead. Note that they have recently
    added a preview release with **Virtual Extensible LAN** (**VXLAN**) encapsulation
    support, which greatly improves performance. For more information, visit [http://blog.weave.works/2015/06/12/weave-fast-datapath/](http://blog.weave.works/2015/06/12/weave-fast-datapath/).
    [](http://blog.weave.works/2015/06/12/weave-fast-datapath/)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**Weave** 为 Docker 容器提供了覆盖网络。它可以作为新的 Docker 网络插件接口的插件使用，并且还与 Kubernetes 兼容通过
    CNI 插件。像许多覆盖网络一样，许多人批评封装开销对性能的影响。请注意，他们最近添加了一个具有 **Virtual Extensible LAN** (**VXLAN**)
    封装支持的预览版本，这极大地提高了性能。欲了解更多信息，请访问 [http://blog.weave.works/2015/06/12/weave-fast-datapath/](http://blog.weave.works/2015/06/12/weave-fast-datapath/)。'
- en: Flannel
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flannel
- en: '**Flannel** comes from CoreOS and is an etcd-backed overlay. Flannel gives
    a full subnet to each host/node enabling a similar pattern to the Kubernetes practice
    of a routable IP per pod or group of containers. Flannel includes an in-kernel
    VXLAN encapsulation mode for better performance and has an experimental multi-network
    mode similar to the overlay Docker plugin. For more information, visit [https://github.com/coreos/flannel](https://github.com/coreos/flannel).
    [](https://github.com/coreos/flannel)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**Flannel** 来自 CoreOS，是一个由 etcd 支持的覆盖层。Flannel 为每个主机/节点提供了一个完整的子网，使得与 Kubernetes
    实践中每个 pod 或一组容器的可路由 IP 类似的模式成为可能。Flannel 包括一个内核中的 VXLAN 封装模式，以提高性能，并且具有类似于覆盖层
    Docker 插件的实验性多网络模式。欲了解更多信息，请访问 [https://github.com/coreos/flannel](https://github.com/coreos/flannel)。'
- en: Project Calico
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Project Calico
- en: '**Project Calico** is a layer 3-based networking model that uses the built-in
    routing functions of the Linux kernel. Routes are propagated to virtual routers
    on each host via **Border Gateway Protocol** (**BGP**). Calico can be used for
    anything from small-scale deploys to large Internet-scale installations. Because
    it works at a lower level on the network stack, there is no need for additional
    NAT, tunneling, or overlays. It can interact directly with the underlying network
    infrastructure. Additionally, it has a support for network-level ACLs to provide
    additional isolation and security. For more information visit the following URL:
    [http://www.projectcalico.org/](http://www.projectcalico.org/). [](http://www.projectcalico.org/)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Project Calico** 是一个基于层 3 的网络模型，它使用 Linux 内核的内置路由功能。路由通过 **边界网关协议** (**BGP**)
    传播到每个主机上的虚拟路由器。Calico 可用于从小规模部署到大规模互联网安装的任何用途。因为它在网络堆栈的较低级别工作，所以不需要额外的 NAT、隧道或覆盖层。它可以直接与底层网络基础设施交互。此外，它支持网络级
    ACL 以提供额外的隔离和安全性。欲了解更多信息，请访问以下网址：[http://www.projectcalico.org/](http://www.projectcalico.org/)。'
- en: Canal
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Canal
- en: '**Canal** merges both Calico for network policy and Flannel for overlay into
    one solution. It supports both Calico and Flannel type overlays and uses the Calico
    policy enforcement logic. Users can choose from overlay and non-overlay options
    with this setup as it combines the features of the preceding two projects. For
    more information visit the following URL:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**Canal** 将 Calico 的网络策略和 Flannel 的覆盖层合并为一个解决方案。它支持 Calico 和 Flannel 类型的覆盖层，并使用
    Calico 的策略执行逻辑。用户可以从这个设置中选择覆盖层和非覆盖层选项，因为它结合了前两个项目的功能。欲了解更多信息，请访问以下网址：'
- en: '[https://github.com/tigera/canal](https://github.com/tigera/canal)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/tigera/canal](https://github.com/tigera/canal)'
- en: Balanced design
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡设计
- en: It's important to point out the balance Kubernetes is trying to achieve by placing
    the IP at the pod level. Using unique IP addresses at the host level is problematic
    as the number of containers grow. Ports must be used to expose services on specific
    containers and allow external communication. In addition to this, the complexity
    of running multiple services that may or may not know about each other (and their
    custom ports) and managing the port space becomes a big issue.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 强调 Kubernetes 正在尝试通过将 IP 放置在 pod 级别来实现的平衡是很重要的。在主机级别使用唯一的 IP 地址存在问题，因为容器数量增加。必须使用端口来公开特定容器上的服务并允许外部通信。除此之外，运行可能知道或不知道彼此（及其自定义端口）的多个服务并管理端口空间的复杂性成为一个重大问题。
- en: However, assigning an IP address to each container can be overkill. In cases
    of sizable scale, overlay networks and NATs are needed in order to address each
    container. Overlay networks add latency, and IP addresses would be taken up by
    backend services as well since they need to communicate with their frontend counterparts.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为每个容器分配一个 IP 地址可能过度。在规模可观的情况下，需要使用覆盖网络和 NAT 来解决每个容器的问题。覆盖网络会增加延迟，并且 IP 地址也将被后端服务占用，因为它们需要与其前端对等体进行通信。
- en: Here, we really see an advantage in the abstractions that Kubernetes provides
    at the application and service level. If I have a web server and a database, we
    can keep them on the same pod and use a single IP address. The web server and
    database can use the local interface and standard ports to communicate, and no
    custom setup is required. Further, services on the backend are not needlessly
    exposed to other application stacks running elsewhere in the cluster (but possibly
    on the same host). Since the pod sees the same IP address that the applications
    running within it see, service discovery does not require any additional translation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们真正看到 Kubernetes 在应用程序和服务级别提供的抽象优势。如果我有一个 Web 服务器和一个数据库，我们可以将它们保留在同一个 pod
    中并使用单个 IP 地址。Web 服务器和数据库可以使用本地接口和标准端口进行通信，而不需要自定义设置。此外，后端的服务不会被不必要地暴露给在集群中其他地方运行的其他应用程序堆栈（但可能在同一主机上）。由于
    pod 看到的是应用程序在其中运行时所看到的相同 IP 地址，因此服务发现不需要任何额外的转换。
- en: If you need the flexibility of an overlay network, you can still use an overlay
    at the pod level. Weave, Flannel, and Project Calico can be used with Kubernetes
    as well as a plethora of other plugins and overlays available now.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要覆盖网络的灵活性，仍然可以在 pod 级别使用覆盖层。Weave、Flannel 和 Project Calico 以及现在可用的大量其他插件和覆盖层都可以与
    Kubernetes 一起使用。
- en: This is also very helpful in the context of scheduling the workloads. It is
    a key to have a simple and standard structure for the scheduler to match constraints
    and understand where space exists on the cluster's network at any given time.
    This is a dynamic environment with a variety of applications and tasks running,
    so any additional complexity here will have rippling effects.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这在调度工作负载的背景下也非常有帮助。对于调度器来说，拥有一个简单且标准的结构来匹配约束并了解集群网络上任何给定时间的空间是至关重要的。这是一个具有各种应用程序和任务的动态环境，因此在这里增加额外的复杂性会产生连锁效应。
- en: There are also implications for service discovery. New services coming online
    must determine and register an IP address on which the rest of the world, or at
    least cluster, can reach them. If NAT is used, the services will need an additional
    mechanism to learn their externally facing IP.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 还涉及服务发现的影响。上线的新服务必须确定并注册一个 IP 地址，其他服务或至少集群可以通过该 IP 地址访问它们。如果使用了 NAT，服务将需要另一个机制来学习其外部可访问的
    IP。
- en: Advanced services
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级服务
- en: Let's explore the IP strategy as it relates to services and communication between
    containers. If you recall, in the *Services* section, [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml),
    *Pods, Services, Replication Controllers, and Labels*, you learned that Kubernetes
    is using kube-proxy to determine the proper pod IP address and port serving each
    request. Behind the scenes, kube-proxy is actually using virtual IPs and iptables
    to make all this magic work.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨与服务和容器之间通信相关的 IP 策略。如果你还记得，在*服务*部分，[第二章](281f1b00-8685-4614-895f-df5ae1518373.xhtml)
    *Pods, Services, Replication Controllers, and Labels*，你学到 Kubernetes 使用 kube-proxy
    来确定为每个请求提供服务的正确 pod IP 地址和端口。在幕后，kube-proxy 实际上是使用虚拟 IP 和 iptables 来使所有这些魔法工作。
- en: Kube-proxy now has two modes—*userspace* and *iptables*. As of now, 1.2 iptables
    is the default mode. In both modes, kube-proxy is running on every host. Its first
    duty is to monitor the API from the Kubernetes master. Any updates to services
    will trigger an update to iptables from kube-proxy. For example, when a new service
    is created, a virtual IP address is chosen and a rule in iptables is set, which
    will direct its traffic to kube-proxy via a random port. Thus, we now have a way
    to capture service-destined traffic on this node. Since kube-proxy is running
    on all nodes, we have cluster-wide resolution for the service **VIP** (short for
    **virtual IP**). Additionally, DNS records can point to this VIP as well.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Kube-proxy 现在有两种模式——*用户空间*和*iptables*。截至目前，1.2 版本中 iptables 是默认模式。在两种模式下，kube-proxy
    都在每个主机上运行。它的首要职责是监视来自 Kubernetes 主节点的 API。对服务的任何更新都将触发从 kube-proxy 到 iptables
    的更新。例如，当创建新服务时，将选择一个虚拟 IP 地址并设置 iptables 中的规则，该规则将通过一个随机端口将其流量定向到 kube-proxy。因此，我们现在有一种方法来捕获此节点上面向服务的流量。由于
    kube-proxy 在所有节点上运行，因此我们在整个集群范围内解析服务的 VIP（**虚拟 IP**）也是可能的。此外，DNS 记录也可以指向此 VIP。
- en: 'In the userspace mode,we have a hook created in iptables, but the proxying
    of traffic is still handled by kube-proxy. The iptables rule is only sending traffic
    to the service entry in kube-proxy at this point. Once kube-proxy receives the
    traffic for a particular service, it must then forward it to a pod in the service''s
    pool of candidates. It does this using a random port that was selected during
    service creation. Refer to the following figure for an overview of the flow:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户空间模式中，我们在 iptables 中创建了一个钩子，但流量的代理仍然由 kube-proxy 处理。此时 iptables 规则只是将流量发送到
    kube-proxy 中的服务条目。一旦 kube-proxy 收到特定服务的流量，它必须将其转发到服务候选池中的一个 pod。它使用的是在服务创建过程中选择的随机端口进行此操作。请参考以下图表，了解流程概述：
- en: '![](img/B06302_03_01.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_01.png)'
- en: Kube-proxy communication
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Kube-proxy 通信
- en: It is also possible to always forward traffic from the same client IP to the
    same backend pod/container using the `sessionAffinity` element in your service
    definition.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你的服务定义中使用`sessionAffinity`元素，始终将来自相同客户端 IP 的流量转发到相同的后端 pod/container 是可能的。
- en: In the iptables mode, the pods are coded directly in the iptable rules. This
    removes the dependency on kube-proxy for actually proxying the traffic. The request
    will go straight to iptables and then on to the pod. This is faster and removes
    a possible point of failure. Readiness probe, as we discussed in the *Health Check *section,
    [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml), *Pods, Services, Replication
    Controllers, and Labels*, is your friend here as this mode also loses the ability
    to retry pods.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在iptables模式中，Pod直接编码在iptables规则中。这消除了对kube-proxy实际代理流量的依赖。请求将直接发送到iptables，然后转发到Pod。这样做更快，也消除了一个可能的故障点。如我们在*健康检查*部分中讨论的那样，就像您的朋友一样，此模式还丢失了重试Pod的能力。
- en: External services
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部服务
- en: 'In the previous chapter, we saw a few service examples. For testing and demonstration
    purposes, we wanted all the services to be externally accessible. This was configured
    by the `type: LoadBalancer` element in our service definition. The `LoadBalancer`
    type creates an external load balancer on the cloud provider. We should note that
    support for external load balancers varies by provider, as does the implementation.
    In our case, we are using GCE, so integration is pretty smooth. The only additional
    setup needed is to open firewall rules for the external service ports.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '在上一章中，我们看到了一些服务示例。出于测试和演示目的，我们希望所有服务都可以从外部访问。这是通过我们服务定义中的`type: LoadBalancer`元素进行配置的。`LoadBalancer`类型在云提供商上创建外部负载均衡器。我们应该注意，外部负载均衡器的支持因提供商而异，实现也有所不同。在我们的情况下，我们正在使用GCE，因此集成非常顺利。唯一需要的额外设置是为外部服务端口打开防火墙规则。'
- en: 'Let''s dig a little deeper and do a `describe` command on one of the services
    from the *More on labels* section in [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml),
    *Pods, Services, Replication Controllers, and Labels*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再深入一点，在[第2章](281f1b00-8685-4614-895f-df5ae1518373.xhtml)中的*标签更多内容*部分对其中一个服务进行`describe`命令：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是上述命令的结果截图：
- en: '![](img/B06302_03_02.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_02.png)'
- en: Service description
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 服务描述
- en: In the output, in the preceding figure, you'll note several key elements. Our
    `Namespace:` is set to `default`, `Type:` is `LoadBalancer`, and we have the external
    IP listed under `LoadBalancer Ingress:`. Further, we see `Endpoints:`, which shows
    us the IPs of the pods available to answer service requests.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图中的输出中，您会注意到几个关键元素。我们的`Namespace:`设置为`default`，`Type:`为`LoadBalancer`，并且我们在`LoadBalancer
    Ingress:`下列出了外部IP。此外，我们看到了`Endpoints:`，它显示了可用于响应服务请求的Pod的IP。
- en: Internal services
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内部服务
- en: 'Let''s explore the other types of services we can deploy. First, by default,
    services are only internally facing. You can specify a type of `clusterIP` to
    achieve this, but, if no type is defined, `clusterIP` is the assumed type. Let''s
    take a look at an example; note the lack of the `type` element:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解我们可以部署的其他类型的服务。首先，默认情况下，服务只面向内部。您可以指定`clusterIP`类型来实现此目的，但是，如果未定义类型，则`clusterIP`是假定的类型。让我们看一个例子；请注意缺少`type`元素：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Listing 3-1*: `nodejs-service-internal.yaml`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3-1*：`nodejs-service-internal.yaml`'
- en: 'Use this listing to create the service definition file. You''ll need a healthy
    version of the `node-js` RC (*Listing 2-7*: `nodejs-health-controller-2.yaml`).
    As you can see, the selector matches on the pods named `node-js` that our RC launched
    in the previous chapter. We will create the service and then list the currently
    running services with a filter:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此清单创建服务定义文件。您将需要一个健康的`node-js` RC版本（*清单2-7*：`nodejs-health-controller-2.yaml`）。正如您所见，选择器匹配我们在前一章中启动的名为`node-js`的Pod。我们将创建服务，然后使用过滤器列出当前运行的服务：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是上述命令的结果截图：
- en: '![](img/B06302_03_03.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_03.png)'
- en: Internal service listing
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 内部服务列表
- en: 'As you can see, we have a new service, but only one IP. Further, the IP address
    is not externally accessible. We won''t be able to test the service from a web
    browser this time. However, we can use the handy `kubectl exec` command and attempt
    to connect from one of the other pods. You will need `node-js-pod` (*Listing 2-1*:
    `nodejs-pod.yaml`) running. Then, you can execute the following command:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们有一个新的服务，但只有一个IP。此外，IP地址无法从外部访问。这次我们无法从Web浏览器测试服务。但是，我们可以使用便捷的`kubectl
    exec`命令，并尝试从其他一些Pod连接。您需要运行`node-js-pod`（*清单2-1*：`nodejs-pod.yaml`）。然后，您可以执行以下命令：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This allows us to run a `docker exec` command as if we had a shell in the `node-js-pod`
    container. It then hits the internal service URL, which forwards to any pods with
    the `node-js` label.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够像在 `node-js-pod` 容器中有一个 shell 一样运行 `docker exec` 命令。然后它命中内部服务 URL，该 URL
    转发到具有 `node-js` 标签的任何 pod。
- en: If all is well, you should get the raw HTML output back. So, you successfully
    created an internal-only service. This can be useful for backend services that
    you want to make available to other containers running in your cluster, but not
    open to the world at large.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，您应该会得到原始 HTML 输出。因此，您成功创建了一个仅内部可用的服务。这对于您希望向集群中运行的其他容器提供的后端服务可能会有用，但不对整个世界开放。
- en: Custom load balancing
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义负载均衡
- en: 'A third type of service that K8s allows is the `NodePort` type. This type allows
    us to expose a service through the host or node (minion) on a specific port. In
    this way, we can use the IP address of any node (minion) and access our service
    on the assigned node port. Kubernetes will assign a node port by default in the
    range of `3000`-`32767`, but you can also specify your own custom port. In the
    example in *Listing 3-2*: `nodejs-service-nodeport.yaml`, we choose port `30001`,
    as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 允许的第三种服务类型是 `NodePort` 类型。这种类型允许我们通过特定端口在主机或节点（minion）上暴露服务。通过这种方式，我们可以使用任何节点（minion）的
    IP 地址，并在分配的节点端口上访问我们的服务。Kubernetes 将默认在 `3000`-`32767` 范围内分配节点端口，但您也可以指定自己的自定义端口。在
    *清单 3-2* 中的示例中，我们选择端口 `30001`，如下所示：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Listing 3-2*: `nodejs-service-nodeport.yaml`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-2*：`nodejs-service-nodeport.yaml`'
- en: 'Once again, create this YAML definition file and create your service, as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，创建此 YAML 定义文件并创建您的服务，如下所示：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output should have a message like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该有类似以下的消息：
- en: '![](img/B06302_03_04.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_04.png)'
- en: New GCP firewall rule
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 新的 GCP 防火墙规则
- en: You'll note a message about opening firewall ports. Similar to the external
    load balancer type, `NodePort` is exposing your service externally using ports
    on the nodes. This could be useful if, for example, you want to use your own load
    balancer in front of the nodes. Let's make sure that we open those ports on GCP
    before we test our new service.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到有关打开防火墙端口的消息。与外部负载均衡器类型类似，`NodePort` 使用节点上的端口将您的服务外部暴露出来。例如，如果您想在节点前面使用自己的负载均衡器，则这可能很有用。在测试新服务之前，让我们确保在
    GCP 上打开这些端口。
- en: From the GCE VM instance console, click on the details for any of your nodes
    (minions). Then click on the network, which is usually default unless otherwise
    specified during creation. In Firewall rules, we can add a rule by clicking on Add
    firewall rule.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从 GCE VM 实例控制台，点击任何节点（minion）的详细信息。然后点击网络，通常是默认的，除非在创建时另有规定。在防火墙规则中，我们可以通过单击添加防火墙规则来添加规则。
- en: 'Create a rule like the one shown in the following figure (`tcp:30001` on `0.0.0.0/0`
    IP range):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个规则，如下图所示（`tcp:30001` 在 `0.0.0.0/0` IP 范围上）：
- en: '![](img/B06302_03_05.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_05.png)'
- en: Create New GCP firewall rule
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新的 GCP 防火墙规则
- en: 'We can now test our new service, by opening a browser and using an IP address
    of any node (minion) in your cluster. The format to test the new service is as
    follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过打开浏览器并使用集群中任何节点（minion）的 IP 地址来测试我们的新服务。测试新服务的格式如下：
- en: '`http://<Minoion IP Address>:<NodePort>/`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://<Minoion IP 地址>:<NodePort>/`'
- en: Finally, the latest version has added an `ExternalName` type, which maps a CNAME
    to the service.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最新版本添加了 `ExternalName` 类型，它将 CNAME 映射到服务。
- en: Cross-node proxy
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨节点代理
- en: 'Remember that kube-proxy is running on all the nodes, so, even if the pod is
    not running there, the traffic will be given a proxy to the appropriate host.
    Refer to the *Cross-node traffic* figure for a visual on how the traffic flows.
    A user makes a request to an external IP or URL. The request is serviced by **Node** in
    this case. However, the pod does not happen to run on this node. This is not a
    problem because the pod IP addresses are routable. So, **Kube-proxy** or **iptables**
    simply passes traffic onto the pod IP for this service. The network routing then
    completes on **Node 2**, where the requested application lives:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，kube-proxy 在所有节点上运行，因此，即使 pod 在那里没有运行，流量也会被代理到适当的主机。参考*跨节点流量*图以了解流量如何流动。用户向外部
    IP 或 URL 发出请求。此时请求由**节点**处理。然而，该 pod 恰好没有在此节点上运行。这并不是问题，因为 pod IP 地址是可路由的。因此，**Kube-proxy**
    或 **iptables** 简单地将流量传递到此服务的 pod IP。然后网络路由完成在 **节点 2** 上，请求的应用程序驻留在那里：
- en: '![](img/B06302_03_06.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_06.png)'
- en: Cross-node traffic
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 跨节点流量
- en: Custom ports
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义端口
- en: 'Services also allow you to map your traffic to different ports; then the containers
    and pods expose themselves. We will create a service that exposes port `90` and
    forwards traffic to port `80` on the pods. We will call the `node-js-90` pod to
    reflect the custom port number. Create the following two definition files:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 服务还允许你将流量映射到不同的端口；然后容器和 pod 将自己暴露出来。我们将创建一个服务，将流量暴露到 `90` 端口并转发到 pod 上的 `80`
    端口。我们将称这个 pod 为 `node-js-90` 来反映自定义端口号。创建以下两个定义文件：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Listing 3-3*: `nodejs-customPort-controller.yaml`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-3*: `nodejs-customPort-controller.yaml`'
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Listing 3-4*: `nodejs-customPort-service.yaml`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-4*: `nodejs-customPort-service.yaml`'
- en: You'll note that in the service definition, we have a `targetPort` element.
    This element tells the service the port to use for pods/containers in the pool.
    As we saw in previous examples, if you do not specify `targetPort`, it assumes
    that it's the same port as the service. This port is still used as the service
    port, but, in this case, we are going to expose the service on port `90` while
    the containers serve content on port `80`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在服务定义中，我们有一个 `targetPort` 元素。这个元素告诉服务使用池中的 pod/容器的端口。就像我们在之前的例子中看到的，如果你不指定
    `targetPort`，它会假定与服务相同的端口。这个端口仍然被用作服务端口，但是在这种情况下，我们将在 `90` 端口上暴露服务，而容器则在 `80`
    端口上提供内容。
- en: 'Create this RC and service and open the appropriate firewall rules, as we did
    in the last example. It may take a moment for the external load balancer IP to
    propagate to the `get service` command. Once it does, you should be able to open
    and see our familiar web application in a browser using the following format:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这个 RC 和服务并打开适当的防火墙规则，就像我们在上一个示例中所做的一样。外部负载均衡器 IP 可能需要一段时间才能传播到 `get service`
    命令。一旦传播完成，你就应该能够以以下格式在浏览器中打开并查看我们熟悉的 web 应用程序：
- en: '`http://<external service IP>:90/`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://<external service IP>:90/`'
- en: Multiple ports
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多个端口
- en: 'Another custom port use case is that of multiple ports. Many applications expose
    multiple ports, such as HTTP on port `80` and port `8888` for web servers. The
    following example shows our app responding on both ports. Once again, we''ll also
    need to add a firewall rule for this port, as we did for *Listing 3-2*: `nodejs-service-nodeport.yaml`
    previously:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个自定义端口的用例是多个端口的情况。许多应用程序会暴露多个端口，比如 `80` 端口上的 HTTP 和 `8888` 端口上的 web 服务器。下面的示例展示了我们的应用同时在这两个端口上响应。再次强调，我们还需要为这个端口添加防火墙规则，就像我们之前为
    *清单 3-2*: `nodejs-service-nodeport.yaml` 做的一样：'
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Listing 3-5*: `nodejs-multi-controller.yaml`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-5*: `nodejs-multi-controller.yaml`'
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Listing 3-6*: `nodejs-multi-service.yaml`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-6*: `nodejs-multi-service.yaml`'
- en: The application and container itself must be listening on both ports for this
    to work. In this example, port `8888` is used to represent a fake admin interface.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序和容器本身必须同时监听这两个端口才能工作。在这个例子中，端口 `8888` 被用来表示一个虚假的管理员界面。
- en: If, for example, you want to listen on port `443`, you would need a proper SSL
    socket listening on the server.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想监听 `443` 端口，你需要在服务器上使用适当的 SSL 套接字进行监听。
- en: Ingress
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内部访问
- en: We discussed previously how Kubernetes uses the service abstract as a means
    to proxy traffic to backing pod distributed throughout our cluster. While this
    is helpful in both scaling and pod recovery, there are more advanced routing scenarios
    that are not addressed by this design.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过 Kubernetes 如何使用服务抽象来代理分布在集群中的后端 pod 的流量。虽然这在扩展和 pod 恢复方面都很有帮助，但是这种设计并没有解决更高级的路由场景。
- en: To that end, Kubernetes has added an Ingress resource, which allows for custom
    proxying and load balancing to a back service. Think of it as an extra layer or
    hop in the routing path before traffic hits our service. Just as an application
    has a service and backing pods, the Ingress resource needs both an Ingress entry
    point and an Ingress controller that perform the custom logic. The entry point
    defines the routes and the controller actually handles the routing. For our examples,
    we will use the default GCE backend.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，Kubernetes 添加了一个 Ingress 资源，允许对后端服务进行自定义代理和负载均衡。可以把它想象成在流量到达我们的服务之前的路由路径中的一个额外层或跳跃。就像一个应用程序有一个服务和支持的
    pod 一样，Ingress 资源需要一个 Ingress 入口点和一个执行自定义逻辑的 Ingress 控制器。入口点定义了路由，控制器实际处理路由。在我们的示例中，我们将使用默认的
    GCE 后端。
- en: 'Some limitations to be aware of when using the Ingress API can be found here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ingress API 时需要注意的一些限制可以在这里找到：
- en: '[https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md](https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md](https://github.com/kubernetes/contrib/blob/master/ingress/controllers/gce/BETA_LIMITATIONS.md)'
- en: 'As you may recall, in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to Kubernetes*, we saw that a GCE cluster comes with a default back
    which provides Layer 7 load balancing capability. We can see this controller running
    if we look at the `kube-system` namespace:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，在[第 1 章](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml)，*Kubernetes 简介*
    中，我们看到 GCE 集群附带了一个默认的后端，提供了第 7 层负载均衡能力。如果我们查看 `kube-system` 命名空间，我们可以看到这个控制器正在运行：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We should see an RC listed with the `l7-default-backend-v1.0` name, as shown
    here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一个 RC 列出了 `l7-default-backend-v1.0` 的名称，如下所示：
- en: '![](img/B06302_03_07.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_07.png)'
- en: GCE Layer 7 Ingress controller
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: GCE Layer 7 Ingress 控制器
- en: This provides the Ingress controller piece that actually routes the traffic
    defined in our Ingress entry points. Let's create some resources for an Ingress.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了实际路由在我们 Ingress 入口点中定义的流量的 Ingress 控制器部分。让我们为 Ingress 创建一些资源。
- en: 'First, we will create a few new replication controllers with my `httpwhalesay`
    image. This is a remix of the original whalesay that was displayed in a browser.
    The following listing shows the YAML. Notice the three dashes that let us combine
    several resources into one YAML file:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用我的 `httpwhalesay` 镜像创建几个新的复制控制器。这是原始的 whalesay 的一次混音，可以在浏览器中显示。以下清单显示了
    YAML。请注意三个破折号，让我们将多个资源组合成一个 YAML 文件：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Listing 3-7.* `whale-rcs.yaml`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-7.* `whale-rcs.yaml`'
- en: 'Notice that we are creating pods with the same container, but different start
    up parameters. Take note of these parameters for later. We will also create `Service` endpoints
    for each of these RCs:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在创建具有相同容器的 pod，但具有不同的启动参数。记下这些参数以备后用。我们还将为这些 RC 的每一个创建 `Service` 端点：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Listing 3-8.* `whale-svcs.yaml`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-8.* `whale-svcs.yaml`'
- en: 'Again create these with the `kubectl create -f` command, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用 `kubectl create -f` 命令创建这些，如下所示：
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We should see messages about the RCs and Services successful creation. Next,
    we need to define the Ingress entry point. We will use `http://a.whale.hey` and
    `http://b.whale.hey` as our demo entry points:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到关于 RC 和 Service 成功创建的消息。接下来，我们需要定义 Ingress 入口点。我们将使用 `http://a.whale.hey`
    和 `http://b.whale.hey` 作为我们的演示入口点：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '*Listing 3-9.* `whale-ingress.yaml`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-9.* `whale-ingress.yaml`'
- en: 'Again, use `kubectl create -f` to create this Ingress. Once this is successfully
    created, we will need to wait a few moments for GCE to give the Ingress a static
    IP address. Use the following command to watch the Ingress resource:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用 `kubectl create -f` 来创建此 Ingress。一旦成功创建，我们需要等待几分钟让 GCE 给 Ingress 一个静态 IP
    地址。使用以下命令来观察 Ingress 资源：
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the Ingress has an IP, we should see an entry in `ADDRESS` like the one
    shown here:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Ingress 有了 IP，我们应该在 `ADDRESS` 中看到一个条目，像这样：
- en: '![](img/B06302_03_08.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_08.png)'
- en: Ingress Description
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 描述
- en: 'Since this is not a registered domain name, we will need to specify the resolution
    in the `curl` command, like this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这不是一个注册的域名，我们需要在 `curl` 命令中指定解析，就像这样：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This should display the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示如下内容：
- en: '![](img/B06302_03_09.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_09.png)'
- en: Whalesay A
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Whalesay A
- en: 'We can also try the second URL and we will get our second RC:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以尝试第二个 URL，并获得我们的第二个 RC：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/B06302_03_10.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_10.png)'
- en: Whalesay B
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Whalesay B
- en: We notice that the images are almost the same, except that the words from each
    whale reflect the startup parameters from each RC we started earlier. Thus our
    two Ingress points are directing traffic to different backends.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到图像几乎相同，只是每个鲸的字样反映了我们先前启动的每个 RC 的启动参数。因此，我们的两个 Ingress 点将流量引导到不同的后端。
- en: In this example, we used the default GCE backend for an Ingress controller.
    Kubernetes allows us to build our own and Nginx actually has a few versions available
    as well.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了默认的 GCE 后端作为 Ingress 控制器。Kubernetes 允许我们自己构建，而 Nginx 实际上也有几个版本可用。
- en: Migrations, multicluster, and more
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移、多集群等
- en: As you've seen so far, Kubernetes offers a high level of flexibility and customization
    to create a service abstraction around your containers running in the cluster.
    However, there may be times where you want to point to something outside your
    cluster.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你到目前为止所见，Kubernetes 提供了高度的灵活性和定制化，可以在集群中运行的容器周围创建服务抽象。但是，可能会有时候你想要指向集群外的某些东西。
- en: An example of this would be working with legacy systems or even applications
    running on another cluster. In the case of the former, this is a perfectly good
    strategy in order to migrate to Kubernetes and containers in general. We can begin
    to manage the service endpoints in Kubernetes while stitching the stack together
    using the K8s orchestration concepts. Additionally, we can even start bringing
    over pieces of the stack, as the frontend, one at a time as the organization refactors
    applications for microservices and/or containerization.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况的一个示例是与遗留系统或者甚至运行在另一个集群上的应用程序一起工作。就前者而言，在迁移到 Kubernetes 和容器的过程中，这是一个非常好的策略。我们可以开始在
    Kubernetes 中管理服务端点，同时使用 K8s 编排概念来组装整个堆栈。此外，随着组织对应用程序进行了微服务和/或容器化的重构，我们甚至可以逐步地将堆栈的部分（如前端）带入。
- en: 'To allow access to non-pod-based applications, the services construct allows
    you to use endpoints that are outside the cluster. Kubernetes is actually creating
    an endpoint resource every time you create a service that uses selectors. The
    `endpoints` object keeps track of the pod IPs in the load balancing pool. You
    can see this by running a `get endpoints` command, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许访问非基于 Pod 的应用程序，服务构建允许您使用在集群外的端点。实际上，每次创建使用选择器的服务时，Kubernetes 都会创建一个端点资源。`endpoints`
    对象跟踪负载平衡池中的 Pod IP。您可以通过运行 `get endpoints` 命令来查看，如下所示：
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You should see something similar to this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到类似这样的内容：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You'll note an entry for all the services we currently have running on our cluster.
    For most services, the endpoints are just the IP of each pod running in an RC.
    As I mentioned, Kubernetes does this automatically based on the selector. As we
    scale the replicas in a controller with matching labels, Kubernetes will update
    the endpoints automatically.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们当前在集群上运行的所有服务都有一个条目。对于大多数服务，端点只是运行在 RC 中的每个 Pod 的 IP。正如我之前提到的，Kubernetes
    根据选择器自动执行此操作。当我们在具有匹配标签的控制器中扩展副本时，Kubernetes 将自动更新端点。
- en: 'If we want to create a service for something that is not a pod and therefore
    has no labels to select, we can easily do this with both a service and endpoint
    definition, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想为不是 Pod 的东西创建一个服务，因此没有标签可供选择，我们可以很容易地通过服务和端点定义来实现，如下所示：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Listing 3-10*: `nodejs-custom-service.yaml`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-10*：`nodejs-custom-service.yaml`'
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*Listing 3-11*: `nodejs-custom-endpoint.yaml`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-11*：`nodejs-custom-endpoint.yaml`'
- en: In the preceding example, you'll need to replace `<X.X.X.X>` with a real IP
    address, where the new service can point to. In my case, I used the public load
    balancer IP from the `node-js-multi` service we created earlier in *listing 3-6*.
    Go ahead and create these resources now.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，您需要用实际 IP 地址替换 `<X.X.X.X>`，新服务可以指向该地址。在我的案例中，我使用了我们之前在 *清单 3-6* 中创建的
    `node-js-multi` 服务的公共负载均衡器 IP。现在就去创建这些资源吧。
- en: 'If we now run a `get endpoints` command, we will see this IP address at port
    `80` associated with the `custom-service` endpoint. Further, if we look at the
    service details, we will see the IP listed in the `Endpoints` section:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们现在运行一个 `get endpoints` 命令，我们将看到这个 IP 地址关联到 `custom-service` 端点的 `80` 端口。此外，如果我们查看服务详情，我们将在
    `Endpoints` 部分中看到列出的 IP： '
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can test out this new service by opening the `custom-service` external IP
    from a browser.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在浏览器中打开 `custom-service` 的外部 IP 来测试这项新服务。
- en: Custom addressing
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义寻址
- en: 'Another option to customize services is with the `clusterIP` element. In our
    examples so far, we''ve not specified an IP address, which means that it chooses
    the internal address of the service for us. However, we can add this element and
    choose the IP address in advance with something like `clusterip: 10.0.125.105`.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个自定义服务的选项是使用 `clusterIP` 元素。到目前为止，在我们的示例中，我们还没有指定 IP 地址，这意味着它会为我们选择服务的内部地址。然而，我们可以添加这个元素并提前选择
    IP 地址，例如使用 `clusterip: 10.0.125.105`。'
- en: 'There may be times when you don''t want to load balance and would rather have
    DNS with *A* records for each pod. For example, software that needs to replicate
    data evenly to all nodes may rely on *A* records to distribute data. In this case,
    we can use an example like the following one and set `clusterip` to `None`. Kubernetes
    will not assign an IP address and instead only assign *A* records in DNS for each
    of the pods. If you are using DNS, the service should be available at `node-js-none`
    or `node-js-none.default.cluster.local` from within the cluster. We have the following
    code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 有时您可能不想负载平衡，而是更愿意为每个 Pod 使用带有 *A* 记录的 DNS。例如，需要将数据均匀复制到所有节点的软件可能依赖于 *A* 记录来分发数据。在这种情况下，我们可以使用以下示例，并将
    `clusterip` 设置为 `None`。 Kubernetes 将不会分配 IP 地址，而是仅为每个 Pod 在 DNS 中分配 *A* 记录。如果您使用
    DNS，则服务应该可以从集群内的 `node-js-none` 或 `node-js-none.default.cluster.local` 访问。我们有以下代码：
- en: '[PRE23]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '*Listing 3-12*: `nodejs-headless-service.yaml`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-12*：`nodejs-headless-service.yaml`'
- en: 'Test it out after you create this service with the trusty `exec` command:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 创建此服务后，请使用可靠的 `exec` 命令进行测试：
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Service discovery
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务发现
- en: 'As we discussed earlier, the Kubernetes master keeps track of all service definitions
    and updates. Discovery can occur in one of three ways. The first two methods use
    Linux environment variables. There is support for the Docker link style of environment
    variables, but Kubernetes also has its own naming convention. Here is an example
    of what our `node-js` service example might look like using K8s environment variables
    (note IPs will vary):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，Kubernetes 主节点会跟踪所有服务定义和更新。发现可以通过以下三种方式之一进行。前两种方法使用 Linux 环境变量。支持
    Docker 链接样式的环境变量，但 Kubernetes 也有其自己的命名约定。这是使用 K8s 环境变量的示例，我们的 `node-js` 服务示例可能看起来像这样（注意
    IP 可能会有所不同）：
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*Listing 3-13*: *Service environment variables*'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-13*：*服务环境变量*'
- en: Another option for discovery is through DNS. While environment variables can
    be useful when DNS is not available, it has drawbacks. The system only creates
    variables at creation time, so services that come online later will not be discovered
    or would require some additional tooling to update all the system environments.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 DNS 进行发现的另一种选择。虽然环境变量在 DNS 不可用时可能很有用，但它也有缺点。系统仅在创建时创建变量，因此稍后启动的服务将无法发现，或者需要一些额外的工具来更新所有系统环境。
- en: DNS
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNS
- en: DNS solves the issues seen with environment variables by allowing us to reference
    the services by their name. As services restart, scale out, or appear anew, the
    DNS entries will be updating and ensuring that the service name always points
    to the latest infrastructure. DNS is set up by default in most of the supported
    providers.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: DNS 通过允许我们通过名称引用服务来解决使用环境变量时出现的问题。随着服务重新启动、扩展或出现新的情况，DNS 条目将被更新，确保服务名称始终指向最新的基础架构。在大多数支持的提供商中，默认设置了
    DNS。
- en: 'If DNS is supported by your provider, but not set up, you can configure the
    following variables in your default provider config when you create your Kubernetes
    cluster:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的提供商支持 DNS，但尚未设置，则在创建 Kubernetes 集群时，您可以在默认提供商配置中配置以下变量：
- en: '`ENABLE_CLUSTER_DNS="${KUBE_ENABLE_CLUSTER_DNS:-true}"` `DNS_SERVER_IP="10.0.0.10"`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`ENABLE_CLUSTER_DNS="${KUBE_ENABLE_CLUSTER_DNS:-true}"` `DNS_SERVER_IP="10.0.0.10"`'
- en: '`DNS_DOMAIN="cluster.local"`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`DNS_DOMAIN="cluster.local"`'
- en: '`DNS_REPLICAS=1`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`DNS_REPLICAS=1`'
- en: With DNS active, services can be accessed in one of two forms—either the service
    name itself, `<service-name>` or a fully qualified name that includes the namespace,
    `<service-name>.<namespace-name>.cluster.local`. In our examples, it would look
    similar to `node-js-90` or `node-js-90.default.cluster.local`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DNS 时，服务可以以两种形式之一访问-要么是服务名称本身，`<service-name>`，要么是包含命名空间的完全限定名称，`<service-name>.<namespace-name>.cluster.local`。在我们的示例中，它看起来类似于
    `node-js-90` 或 `node-js-90.default.cluster.local`。
- en: Multitenancy
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多租户
- en: Kubernetes also has an additional construct for isolation at the cluster level.
    In most cases, you can run Kubernetes and never worry about namespaces; everything
    will run in the default namespace if not specified. However, in cases where you
    run multitenancy communities or want broad-scale segregation and isolation of
    the cluster resources, namespaces can be used to this end.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 还具有在集群级别进行隔离的附加结构。在大多数情况下，您可以运行 Kubernetes 而不必担心命名空间；如果未指定，所有内容都将在默认命名空间中运行。但是，在运行多租户社区或希望对集群资源进行广泛分离和隔离的情况下，可以使用命名空间来实现此目的。
- en: To start, Kubernetes has two namespaces—`default` and `kube-system`. The `kube-system` namespace
    is used for all the system-level containers we saw in [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to* *Kubernetes*, in the *Services running on the minions* section.
    The UI, logging, DNS, and so on are all run in `kube-system`. Everything else
    the user creates runs in the default namespace. However, our resource definition
    files can optionally specify a custom namespace. For the sake of experimenting,
    let's take a look at how to build a new namespace.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Kubernetes有两个命名空间——`default`和`kube-system`。`kube-system`命名空间用于所有在[第1章](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml)中看到的系统级容器，在*运行在节点上的服务*节中。用户创建的所有其他内容都在默认命名空间中运行。但是，用户的资源定义文件可以选择指定自定义命名空间。为了进行实验，让我们看看如何构建一个新的命名空间。
- en: 'First, we''ll need to create a namespace definition file like the one in this
    listing:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个命名空间定义文件，就像这个清单中的一个：
- en: '[PRE26]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Listing 3-14*: `test-ns.yaml`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-14*：`test-ns.yaml`'
- en: 'We can go ahead and create this file with our handy `create` command:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们方便的`create`命令来创建这个文件：
- en: '[PRE27]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we can create resources that use the `test` namespace. The following is
    an example of a pod using this new namespace:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建使用`test`命名空间的资源。以下是一个使用这个新命名空间的 pod 的示例：
- en: '[PRE28]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*Listing 3-15*: `ns-pod.yaml`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-15*：`ns-pod.yaml`'
- en: 'While the pod can still access services in other namespaces, it will need to
    use the long DNS form of `<service-name>.<namespace-name>.cluster.local`. For
    example, if you were to run a command from inside the container in *Listing 3-15*:
    `ns-pod.yaml`, you could use `node-js.default.cluster.local` to access the Node.js
    example from [Chapter 2](281f1b00-8685-4614-895f-df5ae1518373.xhtml), *Pods, Services,
    Replication Controllers, and Labels*.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 pod 仍然可以访问其他命名空间中的服务，但它需要使用长 DNS 格式的`<service-name>.<namespace-name>.cluster.local`。例如，如果您要从*清单
    3-15*：`ns-pod.yaml`内的容器中运行一个命令，您可以使用`node-js.default.cluster.local`访问[第2章](281f1b00-8685-4614-895f-df5ae1518373.xhtml)中的Node.js示例，*Pods,
    Services, Replication Controllers, and Labels*。
- en: 'Here is a note about resource utilization. At some point in this book, you
    may run out of space on your cluster to create new Kubernetes resources. The timing
    will vary based on cluster size, but it''s good to keep this in mind and do some
    clean-up from time to time. Use the following commands to remove old examples:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个关于资源利用的注记。在本书的某个时候，您可能会在集群上耗尽空间以创建新的 Kubernetes 资源。这个时机会根据集群的大小而变化，但请记住定期进行一些清理是很好的。使用以下命令删除旧的示例：
- en: '`**$ kubectl delete pod <pod name>** **$ kubectl delete svc <service name>**
    **$ kubectl delete rc <replication controller name>** ** $ kubectl delete rs <replicaset
    name>**`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`**$ kubectl delete pod <pod name>** **$ kubectl delete svc <service name>**
    **$ kubectl delete rc <replication controller name>** ** $ kubectl delete rs <replicaset
    name>**`'
- en: Limits
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制
- en: 'Let''s inspect our new namespace a bit more. Run the `describe` command as
    follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地检查一下我们的新命名空间。执行如下`describe`命令：
- en: '[PRE29]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是上述命令的结果：
- en: '![](img/B06302_03_11.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_11.png)'
- en: Namespace describe
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间描述
- en: Kubernetes allows you to both limit the resources used by individual pods or
    containers and the resources used by the overall namespace using quotas. You'll
    note that there are no resource **limits** or **quotas** currently set on the
    `test` namespace.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes允许您限制单个 pod 或容器使用的资源以及整个命名空间使用的资源。请注意，`test`命名空间目前没有设置资源**限制**或**配额**。
- en: 'Suppose we want to limit the footprint of this new namespace; we can set quotas
    such as the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要限制这个新命名空间的占地面积；我们可以设置如下的配额：
- en: '[PRE30]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '*Listing 3-16*: `quota.yaml`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 3-16*：`quota.yaml`'
- en: In reality, namespaces would be for larger application communities and would
    probably never have quotas this low. I am using this in order to ease illustration
    of the capability in the example.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，命名空间将用于更大的应用程序社区，可能永远不会有这么低的配额。我之所以使用这个例子，是为了更轻松地说明示例中的功能。
- en: 'Here, we will create a quota of `3` pods, `1` RC, and `1` service for the test
    namespace. As you probably guessed, this is executed once again by our trusty
    `create` command:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将为测验命名空间创建一个`3`个 pod、`1`个 RC和`1`个服务的配额。正如你可能猜到的那样，这又一次由我们值得信赖的`create`命令执行：
- en: '[PRE31]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now that we have that in place, let''s use `describe` on the namespace, as
    follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经做好了，让我们对命名空间使用`describe`，如下所示：
- en: '[PRE32]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是上述命令的结果：
- en: '![](img/B06302_03_12.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_12.png)'
- en: Namespace describe after quota is set
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置配额后的命名空间描述
- en: 'You''ll note that we now have some values listed in the quota section and the
    limits section is still blank. We also have a `Used` column, which lets us know
    how close to the limits we are at the moment. Let''s try to spin up a few pods
    using the following definition:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到现在在配额部分列出了一些值，而限制部分仍然为空白。我们还有一个`Used`列，它让我们知道我们当前离限制有多近。让我们尝试使用以下定义来启动一些
    pod：
- en: '[PRE33]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '*Listing 3-17*: `busybox-ns.yaml`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 3-17*：`busybox-ns.yaml`'
- en: You'll note that we are creating four replicas of this basic pod. After using
    `create` to build this RC, run the `describe` command on the `test` namespace
    once more. You'll notice that the `Used` values for pods and RCs are at their
    max. However, we asked for four replicas and only see three pods in use.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到我们正在创建此基本 pod 的四个副本。在使用`create`构建此 RC 后，再次在`test`命名空间上运行`describe`命令。您会注意到
    pod 和 RC 的`Used`值已达到最大值。然而，我们要求四个副本，但只看到三个正在使用的 pod。
- en: 'Let''s see what''s happening with our RC. You might attempt to do that with
    the command here:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的 RC 正在发生什么。您可以尝试使用此处的命令来执行此操作：
- en: '[PRE34]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: However, if you try, you'll be discouraged to see a `not found` message from
    the server. This is because we created this RC in a new namespace and `kubectl`
    assumes the default namespace if not specified. This means that we need to specify
    `--namepsace=test` with every command when we wish to access resources in the
    `test` namespace.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你尝试，你会受挫于从服务器收到的`not found`消息。这是因为我们在一个新的命名空间创建了这个 RC，如果没有指定，`kubectl`会假定默认命名空间。这意味着我们在访问`test`命名空间中的资源时需要在每个命令中指定`--namepsace=test`。
- en: 'We can also set the current namespace by working with the context settings.
    First, we need to find our current context, which is found with the following
    command:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过处理上下文设置来设置当前命名空间。首先，我们需要找到我们的当前上下文，这是通过以下命令找到的：
- en: '`**$ kubectl config view | grep current-context**`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`**$ kubectl config view | grep current-context**`'
- en: 'Next, we can take that context and set the namespace variable like the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以获取该上下文并设置命名空间变量如下：
- en: '`**$ kubectl config set-context <Current Context> --namespace=test**`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`**$ kubectl config set-context <当前上下文> --namespace=test**`'
- en: Now you can run the `kubectl` command without the need to specify the namespace.
    Just remember to switch back when you want to look at the resources running in
    your default namespace.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以运行`kubectl`命令而无需指定命名空间。只需记住在想要查看运行在默认命名空间中的资源时切换回来即可。
- en: 'Run the command with the namespace specified. If you''ve set your current namespace
    as demonstrated in the tip box, you can leave off the `--namespace` argument:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 使用指定了命名空间的命令运行。如果您已按提示框中所示设置了当前命名空间，可以省略`--namespace`参数：
- en: '[PRE35]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图是前述命令的结果：
- en: '![](img/B06302_03_13.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06302_03_13.png)'
- en: Namespace quotas
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间配额
- en: As you can see in the preceding image, the first three pods were successfully
    created, but our final one fails with the `Limited to 3 pods` error.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前面的图像中所见，前三个 pod 已成功创建，但我们的最后一个失败了，出现了`Limited to 3 pods`错误。
- en: This is an easy way to set limits for resources partitioned out at a community
    scale. It's worth noting that you can also set quotas for CPU, memory, persistent
    volumes, and secrets. Additionally, limits work in a similar way to quota, but
    they set the limit for each pod or container within the namespace.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种在社区规模上设置资源限制的简单方法。值得注意的是，您还可以设置 CPU、内存、持久卷和密钥的配额。此外，限制的工作方式与配额类似，但它们为命名空间内的每个
    pod 或容器设置了限制。
- en: A note on resource usage
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于资源使用的说明
- en: As most of the examples in this book utilize GCP or AWS, it can be costly to
    keep everything running. It's also easy to run out of resources using the default
    cluster size, especially if you keep every example running. Therefore, you may
    want to delete older pods, replication controllers, replica sets, and services
    periodically. You can also destroy the cluster and recreate using [Chapter 1](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml),
    *Introduction to Kubernetes* as a way to lower your cloud provider bill.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书中的大多数示例都使用 GCP 或 AWS，保持所有内容运行可能成本很高。如果使用默认的集群大小，尤其是如果保留每个示例运行，则很容易耗尽资源。因此，您可能希望定期删除旧的
    pod、复制控制器、副本集和服务。您还可以销毁集群，并使用[第 1 章](772262b1-5b78-4a9b-bbb4-09c6fd858fdf.xhtml)——*介绍
    Kubernetes*作为降低云服务提供商账单的方法。
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We took a deeper look into networking and services in Kubernetes. You should
    now understand how networking communications are designed in K8s and feel comfortable
    accessing your services internally and externally. We saw how kube-proxy balances
    traffic both locally and across the cluster. Additionally, we explored the new
    Ingress resources that allow us finer control of incoming traffic. We also looked
    briefly at how DNS and service discovery is achieved in Kubernetes. We finished
    off with quick look at namespace and isolation for multitenancy.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深入研究了 Kubernetes 中的网络和服务。现在你应该了解了 K8s 中网络通信的设计，并且能够在内部和外部轻松访问你的服务。我们看到了 kube-proxy
    如何在本地和整个集群中平衡流量。此外，我们探讨了新的 Ingress 资源，使我们能够更精细地控制流入流量。我们还简要地了解了 Kubernetes 中如何实现
    DNS 和服务发现。最后，我们简单地看了一下多租户环境下的命名空间和隔离。
- en: References
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/](http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/)'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/](http://www.wired.com/2015/06/google-reveals-secret-gear-connects-online-empire/)'
