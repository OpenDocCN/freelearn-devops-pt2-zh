- en: Rolling Updates, Scalability, and Quotas
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滚动更新、可伸缩性和配额
- en: 'In this chapter, we will explore the automated pod scalability that Kubernetes
    provides, how it affects rolling updates, and how it interacts with quotas. We
    will touch on the important topic of provisioning and how to choose and manage
    the size of the cluster. Finally, we will go over how the Kubernetes team tests
    the limits of Kubernetes with a 5,000-node cluster. Here are the main points we
    will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨Kubernetes提供的自动Pod可伸缩性，以及它如何影响滚动更新，以及它如何与配额交互。我们将涉及重要的供应主题，以及如何选择和管理集群的大小。最后，我们将介绍Kubernetes团队如何测试5000节点集群的极限。以下是我们将涵盖的主要内容：
- en: Horizontal pod autoscaling
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平Pod自动缩放器
- en: Performing rolling updates with autoscaling
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动缩放执行滚动更新
- en: Handling scarce resources with quotas and limits
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配额和限制处理稀缺资源
- en: Pushing the envelope with Kubernetes performance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推动Kubernetes性能的边界
- en: By the end of this chapter, you will have the ability to plan a large-scale
    cluster, provision it economically, and make informed decisions about the various
    trade-offs between performance, cost, and availability. You will also understand
    how to set up horizontal pod auto-scaling and use resource quotas intelligently
    to let Kubernetes automatically handle intermittent fluctuations in volume.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够规划一个大规模的集群，经济地进行供应，并就性能、成本和可用性之间的各种权衡做出明智的决策。您还将了解如何设置水平Pod自动缩放，并聪明地使用资源配额，让Kubernetes自动处理体积的间歇性波动。
- en: Horizontal pod autoscaling
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水平Pod自动缩放
- en: Kubernetes can watch over your pods and scale them when the CPU utilization
    or some other metric crosses a threshold. The autoscaling resource specifies the
    details (percentage of CPU, how often to check) and the corresponding autoscale
    controller adjusts the number of replicas, if needed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes可以监视您的Pod，并在CPU利用率或其他指标超过阈值时对其进行扩展。自动缩放资源指定了细节（CPU百分比，检查频率），相应的自动缩放控制器会调整副本的数量，如果需要的话。
- en: 'The following diagram illustrates the different players and their relationships:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了不同参与者及其关系：
- en: '![](Images/d2b4b9da-15eb-42e9-b9da-71eb36db89b1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/d2b4b9da-15eb-42e9-b9da-71eb36db89b1.png)
- en: As you can see, the horizontal pod autoscaler doesn't create or destroy pods
    directly. It relies instead on the replication controller or deployment resources.
    This is very smart because you don't need to deal with situations where autoscaling
    conflicts with the replication controller or deployments trying to scale the number
    of pods, unaware of the autoscaler's efforts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，水平Pod自动缩放器不会直接创建或销毁Pod。相反，它依赖于复制控制器或部署资源。这非常聪明，因为您不需要处理自动缩放与复制控制器或部署尝试扩展Pod数量而不知道自动缩放器的努力之间的冲突。
- en: The autoscaler automatically does what we had to do ourselves before. Without
    the autoscaler, if we had a replication controller with replicas set to `3`, but
    we determined that based on average CPU utilization we actually needed `4`, then
    we would update the replication controller from `3` to `4` and keep monitoring
    the CPU utilization manually in all pods. The autoscaler will do it for us.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自动缩放器会自动执行我们以前必须自己执行的操作。如果没有自动缩放器，如果我们有一个副本控制器，副本设置为`3`，但我们确定基于平均CPU利用率实际上需要`4`，那么我们将把副本控制器从`3`更新到`4`，并继续手动监视所有Pod中的CPU利用率。自动缩放器会为我们完成这项工作。
- en: Declaring horizontal pod autoscaler
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声明水平Pod自动缩放器
- en: 'To declare a horizontal pod autoscaler, we need a replication controller, or
    a deployment, and an autoscaling resource. Here is a simple replication controller
    configured to maintain three `nginx` pods:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要声明水平Pod自动缩放器，我们需要一个复制控制器或部署，以及一个自动缩放资源。这是一个简单的复制控制器，配置为维护三个`nginx` Pod：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `autoscaling` resource references the NGINX replication controller in `scaleTargetRef`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`autoscaling`资源引用了`scaleTargetRef`中的NGINX复制控制器：'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`minReplicas` and `maxReplicas` specify the range of scaling. This is needed
    to avoid runaway situations that could occur because of some problem. Imagine
    that, due to some bug, every pod immediately uses 100% CPU regardless of the actual
    load. Without the `maxReplicas` limit, Kubernetes will keep creating more and
    more pods until all cluster resources are exhausted. If we are running in a cloud
    environment with autoscaling of VMs then we will incur a significant cost. The
    other side of this problem is that, if there are no `minReplicas` and there is
    a lull in activity, then all pods could be terminated, and, when new requests
    come in, all the pods will have to be created and scheduled again. If there are
    patterns of on and off activity, then this cycle can repeat multiple times. Keeping
    the minimum of replicas running can smooth this phenomenon. In the preceding example,
    `minReplicas` is set to `2` and `maxReplicas` is set to `4`. Kubernetes will ensure
    that there are always between `2` to `4` NGINX instances running.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`minReplicas`和`maxReplicas`指定了扩展的范围。这是为了避免因某些问题而发生的失控情况。想象一下，由于某个错误，每个pod立即使用100%的CPU，而不考虑实际负载。如果没有`maxReplicas`限制，Kubernetes将不断创建更多的pod，直到耗尽所有集群资源。如果我们在具有自动缩放VM的云环境中运行，那么我们将产生巨大的成本。这个问题的另一面是，如果没有`minReplicas`并且活动出现了停滞，那么所有的pod都可能被终止，当新的请求进来时，所有的pod都将被重新创建和调度。如果存在开关型活动模式，那么这个循环可能会重复多次。保持最小数量的副本运行可以平滑这种现象。在前面的例子中，`minReplicas`设置为`2`，`maxReplicas`设置为`4`。Kubernetes将确保始终有`2`到`4`个NGINX实例在运行。'
- en: 'The **target CPU** utilization percentage is a mouthful. Let''s abbreviate
    it to **TCUP**. You specify a single number like 80%. This could lead to constant
    thrashing if the average load hovers around the TCUP. Kuberentes will alternate
    frequently between adding more replicas and removing replicas. This is often not
    a desired behavior. To address this concern, you can specify a delay for either
    scaling up or scaling down. There are two flags to the `kube-controller-manager`
    to support this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标CPU**利用率百分比是一个冗长的词。让我们把它缩写为**TCUP**。您可以指定一个像80%这样的单个数字。如果平均负载在TCUP周围徘徊，这可能会导致不断的抖动。Kubernetes将频繁地在增加更多副本和删除副本之间交替。这通常不是期望的行为。为了解决这个问题，您可以为扩展或缩减指定延迟。`kube-controller-manager`有两个标志来支持这一点：'
- en: '`--horizontal-pod-autoscaler-downscale-delay`: The value for this option is
    a duration that specifies how long the autoscaler has to wait before another downscale
    operation can be performed after the current one has completed. The default value
    is 5 minutes (5m0s).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--horizontal-pod-autoscaler-downscale-delay`：此选项的值是一个持续时间，指定了在当前操作完成后，自动缩放器必须等待多长时间才能执行另一个缩减操作。默认值为5分钟（5m0s）。'
- en: '`--horizontal-pod-autoscaler-upscale-delay`: The value for this option is a
    duration that specifies how long the autoscaler has to wait before another upscale
    operation can be performed after the current one has completed. The default value
    is 3 minutes (3m0s).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--horizontal-pod-autoscaler-upscale-delay`：此选项的值是一个持续时间，指定了在当前操作完成后，自动缩放器必须等待多长时间才能执行另一个扩展操作。默认值为3分钟（3m0s）。'
- en: Custom metrics
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义指标
- en: CPU utilization is an important metric to gauge whether pods that are bombarded
    with too many requests should be scaled up, or whether they are mostly idle and
    can be scaled down. But the CPU is not the only and sometimes not even the best
    metric to keep track of. Memory may be the limiting factor, and there are more
    specialized metrics, such as the depth of a pod's internal on-disk queue, the
    average latency on a request, or the average number of service timeouts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: CPU利用率是一个重要的指标，用于判断是否应该扩展受到过多请求的Pod，或者它们是否大部分处于空闲状态并且可以缩小规模。但是CPU并不是唯一的，有时甚至不是最好的指标。内存可能是限制因素，还有更专业的指标，例如Pod内部磁盘队列的深度、请求的平均延迟或服务超时的平均次数。
- en: The horizontal pod custom metrics were added as an alpha extension in version
    1.2\. In version 1.6 they were upgraded to beta status. You can now autoscale
    your pods based on multiple custom metrics. The autoscaler will evaluate all the
    metrics and will autoscale based on the largest number of replicas required, so
    the requirements of all the metrics are respected.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 水平Pod自定义指标在1.2版本中作为alpha扩展添加。在1.6版本中，它们升级为beta状态。现在可以根据多个自定义指标自动调整Pod的规模。自动缩放器将评估所有指标，并根据所需的最大副本数量进行自动缩放，因此会尊重所有指标的要求。
- en: Using custom metrics
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义指标
- en: 'Using the horizontal pod autoscaler with custom metrics requires some configuration
    when launching your cluster. First, you need to enable the API aggregation layer.
    Then you need to register your resource metrics API and your custom metrics API.
    Heapster provides an implementation of the resource metrics API you can use. Just
    start Heapster with the `--api-server` flag set to `true`. You need to run a separate
    server that exposes the custom metrics API. A good starting point is this: [https://github.com/kubernetes-incubator/custom-metrics-apiserver](https://github.com/kubernetes-incubator/custom-metrics-apiserver).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义指标的水平Pod自动缩放器在启动集群时需要进行一些配置。首先，您需要启用API聚合层。然后，您需要注册您的资源指标API和自定义指标API。Heapster提供了一个资源指标API的实现，您可以使用。只需使用`--api-server`标志启动Heapster，并将其设置为`true`。您需要运行一个单独的服务器来公开自定义指标API。一个很好的起点是这个：[https://github.com/kubernetes-incubator/custom-metrics-apiserver](https://github.com/kubernetes-incubator/custom-metrics-apiserver)。
- en: 'The next step is to start the `kube-controller-manager` with the following
    flags:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用以下标志启动`kube-controller-manager`：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `--master` flag will override `--kubeconfig` if both are specified. These
    flags specify the location of the API aggregation layer, allowing the controller
    manager to communicate to the API server.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果同时指定了`--master`标志和`--kubeconfig`标志，则`--master`标志将覆盖`--kubeconfig`标志。这些标志指定了API聚合层的位置，允许控制器管理器与API服务器通信。
- en: 'In Kubernetes 1.7, the standard aggregation layer that Kubernetes provides
    runs in-process with the `kube-apiserver`, so the target IP address can be found
    with this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes 1.7中，Kubernetes提供的标准聚合层与`kube-apiserver`一起运行，因此可以使用以下命令找到目标IP地址：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Autoscaling with kubectl
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用kubectl进行自动缩放
- en: '`kubectl` can create an autoscale resource using the standard `create` command
    and accepting a configuration file. But `kubectl` also has a special command,
    `autoscale`, that lets you easily set an autoscaler in one command without a special
    configuration file:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl`可以使用标准的`create`命令并接受一个配置文件来创建自动缩放资源。但是`kubectl`还有一个特殊的命令`autoscale`，可以让您轻松地在一个命令中设置自动缩放器，而无需特殊的配置文件：'
- en: 'First, let''s start a replication controller that makes sure there are three
    replicas of a simple pod that just runs an infinite `bash-loop`:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们启动一个复制控制器，确保有三个简单Pod的副本，这些Pod只运行一个无限的`bash-loop`：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s create a replication controller:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个复制控制器：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the resulting replication controller:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是生成的复制控制器：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can see that the desired and current count are both three, meaning three
    pods are running. Let''s make sure:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以看到所需和当前计数都是三，意味着有三个pod正在运行。让我们确保一下：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s create an autoscaler. To make it interesting, we''ll set the minimum
    number of replicas to `4` and the maximum number to `6`:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个自动缩放器。为了使其有趣，我们将将最小副本数设置为 `4`，最大副本数设置为 `6`：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here is the resulting horizontal pod autoscaler (you can use `hpa`). It shows
    the referenced replication controller, the target and current CPU percentage,
    and the min/max pods. The name matches the referenced replication controller:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是生成的水平pod自动缩放器（您可以使用 `hpa`）。它显示了引用的复制控制器、目标和当前CPU百分比，以及最小/最大pod数。名称与引用的复制控制器匹配：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Originally, the replication controller was set to have three replicas, but
    the autoscaler has a minimum of four pods. What''s the effect on the replication
    controller? That''s right. Now the desired number of replicas is four. If the
    average CPU utilization goes above 50%, then it may climb to five, or even six:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，复制控制器被设置为具有三个副本，但自动缩放器的最小值为四个pod。这对复制控制器有什么影响？没错。现在所需的副本数是四个。如果平均CPU利用率超过50％，则可能会增加到五个，甚至六个：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Just to make sure everything works, here is another look at the pods. Note
    the new pod (17 minutes old) that was created because of the autoscaling:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保一切正常运行，让我们再看一下pod。请注意，由于自动缩放，创建了一个新的pod（17分钟前）：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When we delete the horizontal pod autoscaler, the replication controller retains
    the last desired number of replicas (four in this case). Nobody remembers that
    the replication controller was created with three replicas:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们删除水平pod自动缩放器时，复制控制器会保留最后所需的副本数（在这种情况下为四个）。没有人记得复制控制器是用三个副本创建的：
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you can see, the replication controller wasn''t reset and still maintains
    four pods even when the autoscaler is gone:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如您所看到的，即使自动缩放器消失，复制控制器也没有重置，仍然保持四个pod：
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let's try something else. What happens if we create a new horizontal pod autoscaler
    with a range of `2` to `6` and the same CPU target of `50`%?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试其他方法。如果我们创建一个新的水平pod自动缩放器，范围为 `2` 到 `6`，并且相同的CPU目标为 `50`％，会发生什么？
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Well, the replication controller still maintains its four replicas, which is
    within the range:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，复制控制器仍然保持其四个副本，这在范围内：
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: However, the actual CPU utilization is zero, or close to zero. The replica count
    should have been scaled down to two replicas, but because the horizontal pod autoscaler
    didn't receive CPU metrics from Heapster it doesn't know it needs to scale down
    the number of replicas in the replication controller.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际CPU利用率为零，或接近零。副本计数应该已经缩减到两个副本，但由于水平pod自动缩放器没有从Heapster接收到CPU指标，它不知道需要缩减复制控制器中的副本数。
- en: Performing rolling updates with autoscaling
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自动缩放进行滚动更新
- en: Rolling updates are the cornerstone of managing large clusters. Kubernetes support
    rolling updates at the replication controller level and by using deployments.
    Rolling updates using replication controllers are incompatible with the horizontal
    pod autoscaler. The reason is that, during the rolling deployment, a new replication
    controller is created and the horizontal pod autoscaler remains bound to the old
    replication controller. Unfortunately, the intuitive `kubectl rolling-update`
    command triggers a replication controller rolling update.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新是管理大型集群的基石。Kubernetes支持在复制控制器级别和使用部署进行滚动更新。使用复制控制器进行滚动更新与水平pod自动缩放器不兼容。原因是在滚动部署期间，会创建一个新的复制控制器，而水平pod自动缩放器仍然绑定在旧的复制控制器上。不幸的是，直观的
    `kubectl rolling-update` 命令会触发复制控制器的滚动更新。
- en: Since rolling updates are such an important capability, I recommend that you
    always bind horizontal pod autoscalers to a deployment object instead of a replication
    controller or a replica set. When the horizontal pod autoscaler is bound to a
    deployment, it can set the replicas in the deployment spec and let the deployment
    take care of the necessary underlying rolling update and replication.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于滚动更新是如此重要的功能，我建议您始终将水平Pod自动缩放器绑定到部署对象，而不是复制控制器或副本集。当水平Pod自动缩放器绑定到部署时，它可以设置部署规范中的副本，并让部署负责必要的底层滚动更新和复制。
- en: 'Here is a deployment configuration file we''ve used for deploying the `hue-reminders`
    service:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们用于部署`hue-reminders`服务的部署配置文件：
- en: '[PRE16]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To support it with autoscaling and ensure we always have between `10` to `15`
    instances running, we can create an `autoscaler` configuration file:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持自动缩放并确保我们始终有`10`到`15`个实例在运行，我们可以创建一个`autoscaler`配置文件：
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `kind` of the `scaleTargetRef` field is now `Deployment` instead of `ReplicationController`.
    This is important because we may have a replication controller with the same name.
    To disambiguate and ensure that the horizontal pod autoscaler is bound to the
    correct object, the `kind` and the `name` must match.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`scaleTargetRef`字段的`kind`现在是`Deployment`，而不是`ReplicationController`。这很重要，因为我们可能有一个同名的复制控制器。为了消除歧义并确保水平Pod自动缩放器绑定到正确的对象，`kind`和`name`必须匹配。'
- en: 'Alternatively, we can use the `kubectl autoscale` command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用`kubectl autoscale`命令：
- en: '[PRE18]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Handling scarce resources with limits and quotas
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理稀缺资源的限制和配额
- en: 'With the horizontal pod autoscaler creating pods on the fly, we need to think
    about managing our resources. Scheduling can easily get out of control, and inefficient
    use of resources is a real concern. There are several factors that can interact
    with each other in subtle ways:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随着水平Pod自动缩放器动态创建pod，我们需要考虑如何管理我们的资源。调度很容易失控，资源的低效使用是一个真正的问题。有几个因素可以以微妙的方式相互作用：
- en: Overall cluster capacity
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个集群的容量
- en: Resource granularity per node
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点的资源粒度
- en: Division of workloads per namespace
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按命名空间划分工作负载
- en: DaemonSets
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DaemonSets
- en: StatefulSets
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatefulSets
- en: Affinity, anti-affinity, taints, and tolerations
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亲和性、反亲和性、污点和容忍
- en: First, let's understand the core issue. The Kubernetes scheduler has to take
    into account all these factors when it schedules pods. If there are conflicts
    or a lot of overlapping requirements, then Kubernetes may have a problem finding
    room to schedule new pods. For example, a very extreme yet simple scenario is
    that a daemon set runs on every node a pod that requires 50% of the available
    memory. Now, Kubernetes can't schedule any pod that needs more than 50% memory
    because the daemon set pod gets priority. Even if you provision new nodes, the
    daemon set will immediately commandeer half of the memory.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解核心问题。Kubernetes调度器在调度pod时必须考虑所有这些因素。如果存在冲突或许多重叠的要求，那么Kubernetes可能会在安排新的pod时遇到问题。例如，一个非常极端但简单的情况是，一个守护进程集在每个节点上运行一个需要50%可用内存的pod。现在，Kubernetes无法安排任何需要超过50%内存的pod，因为守护进程集pod具有优先级。即使您提供新节点，守护进程集也会立即占用一半的内存。
- en: Stateful sets are similar to daemon sets in that they require new nodes to expand.
    The trigger to adding new members to the stateful set is growth in data, but the
    impact is taking resources from the pool available for Kubernetes to schedule
    other members. In a multi-tenant situation, the noisy neighbor problem can rear
    its head in a provisioning or resource allocation context. You may plan exact
    rations meticulously in your namespace between different pods and their resource
    requirements, but you share the actual nodes with your neighbors from other namespaces
    that you may not even have visibility into.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Stateful sets类似于守护程序集，因为它们需要新节点来扩展。向Stateful set添加新成员的触发器是数据的增长，但影响是从Kubernetes可用于调度其他成员的池中获取资源。在多租户情况下，嘈杂的邻居问题可能会在供应或资源分配上出现。您可能会在命名空间中精确地计划不同pod和它们的资源需求之间的比例，但您与来自其他命名空间的邻居共享实际节点，甚至可能无法看到。
- en: Most of these problems can be mitigated by judiciously using namespace resource
    quotas and careful management of the cluster capacity across multiple resource
    types, such as CPU, memory, and storage.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些问题可以通过谨慎使用命名空间资源配额和对跨多个资源类型（如CPU、内存和存储）的集群容量进行仔细管理来缓解。
- en: Enabling resource quotas
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启用资源配额
- en: Most Kubernetes distributions support resource quota out of the box. The API
    servers' `--admission-control` flag must have `ResourceQuota` as one of its arguments.
    You will also have to create a `ResourceQuota` object to enforce it. Note that
    there may be at most one `ResourceQuota` object per namespace to prevent potential
    conflicts. This is enforced by Kubernetes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数Kubernetes发行版都支持开箱即用的资源配额。API服务器的`--admission-control`标志必须将`ResourceQuota`作为其参数之一。您还必须创建一个`ResourceQuota`对象来强制执行它。请注意，每个命名空间最多只能有一个`ResourceQuota`对象，以防止潜在的冲突。这是由Kubernetes强制执行的。
- en: Resource quota types
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源配额类型
- en: There are different types of quota we can manage and control. The categories
    are compute, storage, and objects.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以管理和控制不同类型的配额。这些类别包括计算、存储和对象。
- en: Compute resource quota
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算资源配额
- en: 'Compute resources are CPU and memory. For each one, you can specify a limit
    or request a certain amount. Here is the list of compute related fields. Note
    that `requests.cpu` can be specified as just `cpu`, and `requests.memory` can
    be specified as just memory:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 计算资源是CPU和内存。对于每个资源，您可以指定限制或请求一定数量。以下是与计算相关的字段列表。请注意，`requests.cpu`可以简单地指定为`cpu`，`requests.memory`可以简单地指定为memory：
- en: '`limits.cpu`: Across all pods in a non-terminal state, the sum of CPU limits
    cannot exceed this value'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`limits.cpu`: 在非终端状态的所有pod中，CPU限制的总和不能超过此值'
- en: '`limits.memory`: Across all pods in a non-terminal state, the sum of memory
    limits cannot exceed this value'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`limits.memory`: 在非终端状态的所有pod中，内存限制的总和不能超过此值'
- en: '`requests.cpu`: Across all pods in a non-terminal state, the sum of CPU requests
    cannot exceed this value'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests.cpu`: 在非终端状态的所有pod中，CPU请求的总和不能超过此值'
- en: '`requests.memory`: Across all pods in a non-terminal state, the sum of memory
    requests cannot exceed this value'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests.memory`: 在非终端状态的所有pod中，内存请求的总和不能超过此值'
- en: Storage resource quota
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储资源配额
- en: 'The storage resource quota type is a little more complicated. There are two
    entities you can restrict per namespace: the amount of storage and the number
    of persistent volume claims. However, in addition to just globally setting the
    quota on total storage or total number of persistent volume claims, you can also
    do that per `storage` class. The notation for `storage` class resource quota is
    a little verbose, but it gets the job done:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 存储资源配额类型有点复杂。您可以限制每个命名空间的两个实体：存储量和持久卷索赔的数量。但是，除了全局设置总存储配额或持久卷索赔总数之外，您还可以按`storage`类别设置。`storage`类别资源配额的表示法有点冗长，但它可以完成工作：
- en: '`requests.storage`: Across all persistent volume claims, the sum of storage
    requests cannot exceed this value'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests.storage`: 在所有持久卷索赔中，存储请求的总和不能超过此值'
- en: '`persistentvolumeclaims`: The total number of persistent volume claims that
    can exist in the namespace'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`persistentvolumeclaims`: 可以存在于命名空间中的持久卷索赔的总数'
- en: '`<storage-class>.storageclass.storage.k8s.io/requests.storage`: Across all
    persistent volume claims associated with the `storage-class-name`, the sum of
    storage requests cannot exceed this value'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<storage-class>.storageclass.storage.k8s.io/requests.storage`: 与`storage-class-name`相关联的所有持久卷索赔中，存储请求的总和不能超过此值'
- en: '`<storage-class>.storageclass.storage.k8s.io/persistentvolumeclaims`: Across
    all persistent volume claims associated with the `storage-class-name`, this is
    the total number of persistent volume claims that can exist in the namespace'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<storage-class>.storageclass.storage.k8s.io/persistentvolumeclaims`: 与`storage-class-name`相关联的所有持久卷索赔中，可以存在于命名空间中的持久卷索赔的总数'
- en: 'Kubernetes 1.8 added alpha support for ephemeral storage quotas too:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 1.8还增加了对临时存储配额的alpha支持：
- en: '`requests.ephemeral-storage`: Across all pods in the namespace, the sum of
    local ephemeral storage requests cannot exceed this value'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests.ephemeral-storage`: 在命名空间中的所有Pod中，本地临时存储请求的总和不能超过此值'
- en: '`limits.ephemeral-storage`: Across all pods in the namespace, the sum of local
    ephemeral storage limits cannot exceed this value'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`limits.ephemeral-storage`: 在命名空间中的所有Pod中，本地临时存储限制的总和不能超过此值'
- en: Object count quota
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对象计数配额
- en: Kubernetes has another category of resource quotas, which is API objects. My
    guess is that the goal is to protect the Kubernetes API server from having to
    manage too many objects. Remember that Kubernetes does a lot of work under the
    hood. It often has to query multiple objects to authenticate, authorize, and ensure
    that an operation doesn't violate any of the many policies that may be in place.
    A simple example is pod scheduling based on replication controllers. Imagine that
    you have 1 billion replication controller objects. Maybe you just have three pods
    and most of the replication controllers have zero replicas. Still, Kubernetes
    will spend all its time just verifying that indeed all those billion replication
    controllers have no replicas of their pod template and that they don't need to
    kill any pods. This is an extreme example, but the concept applies. Too many API
    objects means a lot of work for Kubernetes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还有另一类资源配额，即API对象。我猜想目标是保护Kubernetes API服务器免受管理太多对象的影响。请记住，Kubernetes在幕后做了很多工作。它经常需要查询多个对象来进行身份验证、授权，并确保操作不违反可能存在的许多策略。一个简单的例子是基于复制控制器的Pod调度。想象一下，您有10亿个复制控制器对象。也许您只有三个Pod，大多数复制控制器都没有副本。但是，Kubernetes将花费大量时间来验证这10亿个复制控制器确实没有其Pod模板的副本，并且它们不需要终止任何Pod。这是一个极端的例子，但这个概念适用。太多的API对象意味着Kubernetes需要做很多工作。
- en: The overage of objects that can be restricted is a little spotty. For example,
    you can limit the number of replication controllers, but not replica sets, which
    are almost an improved version of replication controller that can do exactly the
    same damage if too many of them are around.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 可以限制的对象的超额有点零散。例如，可以限制复制控制器的数量，但不能限制副本集的数量，副本集几乎是复制控制器的改进版本，如果有太多副本集存在，它们可能会造成完全相同的破坏。
- en: The most glaring omission is namespaces. There is no limit to the number of
    namespaces. Since all limits are per namespace, you can easily overwhelm Kubernetes
    by creating too many namespaces, as each namespace has only a small number of
    API objects.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的遗漏是命名空间。对命名空间的数量没有限制。由于所有限制都是针对命名空间的，因此通过创建太多的命名空间，可以轻松地压倒Kubernetes，因为每个命名空间只有少量的API对象。
- en: 'Here are all the supported objects:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是所有支持的对象：
- en: '`ConfigMaps`: The total number of configuration maps that can exist in the
    namespace.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置映射：可以存在于命名空间中的配置映射的总数。
- en: '`PersistentVolumeClaims`: The total number of persistent volume claims that
    can exist in the namespace.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久卷索赔：可以存在于命名空间中的持久卷索赔的总数。
- en: '`Pods`: The total number of pods in a non-terminal state that can exist in
    the namespace. A pod is in a terminal state if `status.phase` in (`Failed`, `Succeeded`)
    is `true`.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods：可以存在于命名空间中的非终端状态的Pod的总数。如果`status.phase`在（`Failed`，`Succeeded`）中为`true`，则Pod处于终端状态。
- en: '`ReplicationControllers`: The total number of replication controllers that
    can exist in the namespace.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制控制器：可以存在于命名空间中的复制控制器的总数。
- en: '`ResourceQuotas`: The total number of resource quotas that can exist in the
    namespace.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源配额：可以存在于命名空间中的资源配额的总数。
- en: '`Services`: The total number of services that can exist in the namespace.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务：可以存在于命名空间中的服务的总数。
- en: '`Services.LoadBalancers`: The total number of load balancer services that can
    exist in the namespace.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务负载均衡器：可以存在于命名空间中的负载均衡器服务的总数。
- en: '`Services.NodePorts`: The total number of node port services that can exist
    in the namespace.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务节点端口：可以存在于命名空间中的节点端口服务的总数。
- en: '`Secrets`: The total number of secrets that can exist in the namespace.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 秘密：可以存在于命名空间中的秘密的总数。
- en: Quota scopes
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配额范围
- en: 'Some resources, such as pods, may be in different states, and it is useful
    to have different quotas for these different states. For example, if there are
    many pods that are terminating (this happens a lot during rolling updates) then
    it is OK to create more pods even if the total number exceeds the quota. This
    can be achieved by only applying a `pod` object `count quota` to `non-terminating`
    pods. Here are the existing scopes:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一些资源，如Pod，可能处于不同的状态，为这些不同的状态设置不同的配额是有用的。例如，如果有许多正在终止的Pod（这在滚动更新期间经常发生），即使总数超过配额，也可以创建更多的Pod。这可以通过仅将`pod`对象`计数配额`应用于`非终止`的Pod来实现。以下是现有的范围：
- en: '`Terminating`: Match pods where `spec.activeDeadlineSeconds >= 0`'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 终止：匹配`spec.activeDeadlineSeconds >= 0`的Pod。
- en: '`NotTerminating`: Match pods where `spec.activeDeadlineSeconds` is nil'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非终止：匹配`spec.activeDeadlineSeconds`为空的Pod。
- en: '`BestEffort`: Match pods that have best effort quality of service'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳努力：匹配具有最佳努力的服务质量的Pod
- en: '`NotBestEffort`: Match pods that do not have best effort quality of service'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非最佳努力：匹配没有最佳努力服务质量的Pod
- en: 'While the `BestEffort` scope applies only to pods, the `Terminating`, `NotTerminating`,
    and `NotBestEffort` scopes apply to CPU and memory, too. This is interesting because
    a resource quota limit can prevent a pod from terminating. Here are the supported
    objects:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`BestEffort`范围仅适用于Pod，但`Terminating`，`NotTerminating`和`NotBestEffort`范围也适用于CPU和内存。这很有趣，因为资源配额限制可以阻止Pod终止。以下是支持的对象：
- en: '`cpu`'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU
- en: '`limits.cpu`'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制CPU
- en: '`limits.memory`'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制内存
- en: '`memory`'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存
- en: '`pods`'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pods`'
- en: '`requests.cpu`'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests.cpu`'
- en: '`requests.memory`'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests.memory`'
- en: Requests and limits
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 请求和限制
- en: The meaning of requests and limits in the context of resource quotas is that
    it requires the containers to explicitly specify the target attribute. This way,
    Kubernetes can manage the total quota because it knows exactly what range of resources
    is allocated to each container.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在资源配额的背景下，请求和限制的含义是它要求容器明确指定目标属性。这样，Kubernetes可以管理总配额，因为它确切地知道为每个容器分配了什么范围的资源。
- en: Working with quotas
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用配额
- en: 'Let''s create a `namespace` first:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们创建一个`namespace`：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Using namespace-specific context
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特定于命名空间的上下文
- en: 'When working with namespaces other than default, I prefer to use a `context`,
    so I don''t have to keep typing `--namespace=ns` for every command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在与默认值不同的命名空间中工作时，我更喜欢使用`context`，这样我就不必为每个命令不断输入`--namespace=ns`：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Creating quotas
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建配额
- en: 'Create a `compute quota` object:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`compute quota`对象：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, let''s add a `count quota` object:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们添加一个`count quota`对象：
- en: '[PRE22]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can observe all the quotas:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以观察所有的配额：
- en: '[PRE23]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And we can even get all the information using `describe`:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们甚至可以使用`describe`获取所有信息：
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This view gives us an instant understanding of the global resource usage of
    important resources across the cluster without diving into too many separate objects.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个视图让我们立即了解集群中重要资源的全局资源使用情况，而无需深入研究太多单独的对象。
- en: 'Let''s add an NGINX server to our namespace:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们向我们的命名空间添加一个NGINX服务器：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Uh-oh. No resources found. But there was no error when the `deployment` was
    created. Let''s check out the `deployment` resource:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哦哦。没有找到资源。但是在创建`deployment`时没有错误。让我们检查一下`deployment`资源：
- en: '[PRE26]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'There it is, in the `conditions` section. The `ReplicaFailure` status is `True`
    and the reason is `FailedCreate`. You can see that the deployment created a new
    replica set called `nginx-8586cf59`, but it couldn''t create the pod it was supposed
    to create. We still don''t know why. Let''s check out the replica set:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在`conditions`部分就在那里。`ReplicaFailure`状态是`True`，原因是`FailedCreate`。您可以看到部署创建了一个名为`nginx-8586cf59`的新副本集，但它无法创建它应该创建的pod。我们仍然不知道原因。让我们检查一下副本集：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The output is very wide, so it overlaps several lines, but the message is crystal
    clear. Since there is a compute quota in the namespace, every container must specify
    its CPU, memory requests, and limit. The quota controller must account for every
    container compute resources usage to ensure the total namespace quota is respected.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输出非常宽，所以它跨越了几行，但是消息非常清晰。由于命名空间中有计算配额，因此每个容器必须指定其CPU、内存请求和限制。配额控制器必须考虑每个容器的计算资源使用情况，以确保总命名空间配额得到尊重。
- en: OK. We understand the problem, but how to resolve it? One way is to create a
    dedicated `deployment` object for each pod type we want to use and carefully set
    the CPU and memory requests and limit. But what if we're not sure? What if there
    are many pod types and we don't want to manage a bunch of `deployment` configuration
    files?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我们理解了问题，但如何解决呢？一种方法是为我们想要使用的每种pod类型创建一个专用的`deployment`对象，并仔细设置CPU和内存请求和限制。但如果我们不确定呢？如果有很多pod类型，我们不想管理一堆`deployment`配置文件呢？
- en: 'Another solution is to specify the limit on the command line when we run the
    `deployment`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个解决方案是在运行`deployment`时在命令行上指定限制：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'That works, but creating deployments on the fly with lots of arguments is a
    very fragile way to manage your cluster:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是有效的，但是通过大量参数动态创建部署是管理集群的一种非常脆弱的方式：
- en: '[PRE29]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Using limit ranges for default compute quotas
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用默认计算配额的限制范围
- en: 'A better way is to specify default compute limits. Enter limit ranges. Here
    is a configuration file that sets some defaults for containers:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更好的方法是指定默认的计算限制。输入限制范围。这是一个设置一些容器默认值的配置文件：
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here are the current default `limits`:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是当前默认的`limits`：
- en: '[PRE31]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, let''s run NGINX again without specifying any CPU or memory requests and
    limits. But first, let''s delete the current NGINX deployment:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们再次运行NGINX，而不指定任何CPU或内存请求和限制。但首先，让我们删除当前的NGINX部署：
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s see if the pod was created. Yes, it was:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看Pod是否已创建。是的，它已经创建了：
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Choosing and managing the cluster capacity
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择和管理集群容量
- en: With Kubernetes' horizontal pod autoscaling, daemon sets, stateful sets, and
    quotas, we can scale and control our pods, storage, and other objects. However,
    in the end, we're limited by the physical (virtual) resources available to our
    Kubernetes cluster. If all your nodes are running at 100% capacity, you need to
    add more nodes to your cluster. There is no way around it. Kubernetes will just
    fail to scale. On the other hand, if you have very dynamic workloads then Kubernetes
    can scale down your pods, but if you don't scale down your nodes correspondingly,
    you will still pay for the excess capacity. In the cloud, you can stop and start
    instances.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Kubernetes的水平Pod自动缩放、守护进程集、有状态集和配额，我们可以扩展和控制我们的Pod、存储和其他对象。然而，最终，我们受限于Kubernetes集群可用的物理（虚拟）资源。如果所有节点的容量都达到100%，您需要向集群添加更多节点。没有其他办法。Kubernetes将无法扩展。另一方面，如果您的工作负载非常动态，那么Kubernetes可以缩小您的Pod，但如果您不相应地缩小节点，您仍然需要支付额外的容量费用。在云中，您可以停止和启动实例。
- en: Choosing your node types
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择您的节点类型
- en: The simplest solution is to choose a single node type with a known quantity
    of CPU, memory, and local storage. But that is typically not the most efficient
    and cost-effective solution. It makes capacity planning simple because the only
    question is how many nodes are needed. Whenever you add a node, you add a known
    quantity of CPU and memory to your cluster, but most Kubernetes clusters and components
    within the cluster handle different workloads. We may have a stream processing
    pipeline where many pods receive some data and process it in one place. This workload
    is CPU-heavy and may or may not need a lot of memory. Other components, such as
    a distributed memory cache, need a lot of memory, but very little CPU. Other components,
    such as a Cassandra cluster, need multiple SSD disks attached to each node.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解决方案是选择一个已知数量的CPU、内存和本地存储的单一节点类型。但这通常不是最有效和成本效益的解决方案。这使得容量规划变得简单，因为唯一的问题是需要多少个节点。每当添加一个节点，就会向集群添加已知数量的CPU和内存，但大多数Kubernetes集群和集群内的组件处理不同的工作负载。我们可能有一个流处理管道，许多Pod在一个地方接收一些数据并对其进行处理。这种工作负载需要大量CPU，可能需要大量内存，也可能不需要。其他组件，如分布式内存缓存，需要大量内存，但几乎不需要CPU。其他组件，如Cassandra集群，需要每个节点连接多个SSD磁盘。
- en: For each type of node, you should consider proper labeling and making sure that
    Kubernetes schedules the pods that are designed to run on that node type.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种类型的节点，您应考虑适当的标记和确保Kubernetes调度设计为在该节点类型上运行的Pod。
- en: Choosing your storage solutions
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择您的存储解决方案
- en: 'Storage is a huge factor in scaling a cluster. There are three categories of
    scalable storage solution:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 存储是扩展集群的重要因素。有三种可扩展的存储解决方案：
- en: Roll your own
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义解决方案
- en: Use your cloud platform storage solution
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用您的云平台存储解决方案
- en: Use an out-of-cluster solution
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集群外解决方案
- en: When you use roll your own, you install some type of storage solution in your
    Kubernetes cluster. The benefits are flexibility and full control, but you have
    to manage and scale it yourself.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用自定义解决方案时，在Kubernetes集群中安装某种存储解决方案。优点是灵活性和完全控制，但您必须自行管理和扩展。
- en: When you use your cloud platform storage solution, you get a lot out of the
    box, but you lose control, you typically pay more, and, depending on the service,
    you may be locked in to that provider.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用云平台存储解决方案时，您可以获得很多开箱即用的功能，但您失去了控制，通常需要支付更多费用，并且根据服务的不同，您可能会被锁定在该提供商那里。
- en: When you use an out-of-cluster solution, the performance and cost of data transfer
    may be much greater. You typically use this option if you need to integrate with
    an existing system.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用集群外的解决方案时，数据传输的性能和成本可能会更大。通常情况下，如果你需要与现有系统集成，你会选择这个选项。
- en: Of course, large clusters may have multiple data stores from all categories.
    This is one of the most critical decisions you have to make, and your storage
    needs may change and evolve over time.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，大型集群可能会有来自所有类别的多个数据存储。这是你必须做出的最关键的决定之一，你的存储需求可能会随着时间的推移而发生变化和演变。
- en: Trading off cost and response time
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权衡成本和响应时间
- en: If money is not an issue you can just over-provision your cluster. Every node
    will have the best hardware configuration available, you'll have way more nodes
    than are needed to process your workloads, and you'll have copious amounts of
    available storage. Guess what? Money is always an issue!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果金钱不是问题，你可以过度配置你的集群。每个节点都将拥有最佳的硬件配置，你将拥有比处理工作负载所需更多的节点，以及大量可用的存储空间。猜猜？金钱总是一个问题！
- en: You may get by with over-provisioning when you're just starting and your cluster
    doesn't handle a lot of traffic. You may just run five nodes, even if two nodes
    are enough most of the time. Multiply everything by 1,000 and someone will come
    asking questions if you have thousands of idle machines and petabytes of empty
    storage.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当你刚开始并且你的集群处理的流量不多时，你可能会通过过度配置来解决问题。即使大部分时间只需要两个节点，你可能只运行五个节点。将一切乘以1,000，如果你有成千上万台空闲机器和宠字节的空闲存储，有人会来问问题。
- en: OK. So, you measure and optimize carefully and you get 99.99999% utilization
    of every resource. Congratulations, you just created a system that can't handle
    an iota of extra load or the failure of a single node without dropping requests
    on the floor or delaying responses.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。所以，你仔细测量和优化，你得到了每个资源的99.99999%利用率。恭喜，你刚创造了一个系统，它无法处理额外的负载或单个节点的故障，而不会丢弃请求或延迟响应。
- en: You need to find the middle ground. Understand the typical fluctuations of your
    workloads and consider the cost/benefit ratio of having excess capacity versus
    having reduced response time or processing ability.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要找到一个折中的方法。了解你的工作负载的典型波动，并考虑过剩容量与减少响应时间或处理能力之间的成本效益比。
- en: Sometimes, if you have strict availability and reliability requirements, you
    can build redundancy into the system and then you over-provision by design. For
    example, you want to be able to hot swap a failed component with no downtime and
    no noticeable effects. Maybe you can't lose even a single transaction. In this
    case, you'll have a live backup for all critical components, and that extra capacity
    can be used to mitigate temporary fluctuations without any special actions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，如果你有严格的可用性和可靠性要求，你可以通过设计在系统中构建冗余来过度配置。例如，你希望能够在没有停机和没有明显影响的情况下热插拔失败的组件。也许你甚至不能失去一笔交易。在这种情况下，你将为所有关键组件提供实时备份，这种额外的容量可以用来缓解临时波动，而无需任何特殊操作。
- en: Using effectively multiple node configurations
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效地使用多个节点配置
- en: Effective capacity planning requires you to understand the usage patterns of
    your system and the load each component can handle. That may include a lot of
    data streams generated inside the system. When you have a solid understanding
    of the typical workloads, you can look at workflows and which components handle
    which parts of the load. Then you can compute the number of pods and their resource
    requirements. In my experience, there are some relatively fixed workloads, some
    workloads that vary predictably (such as office hours versus non-office hours),
    and then you have your completely crazy workloads that behave erratically. You
    have to plan according to each workload, and you can design several families of
    node configurations that can be used to schedule pods that match a particular
    workload.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的容量规划需要你了解系统的使用模式以及每个组件可以处理的负载。这可能包括系统内部产生的大量数据流。当你对典型的工作负载有很好的理解时，你可以查看工作流程以及哪些组件处理负载的哪些部分。然后你可以计算Pod的数量和它们的资源需求。根据我的经验，有一些相对固定的工作负载，一些可以可预测变化的工作负载（比如办公时间与非办公时间），然后你有一些完全疯狂的工作负载，表现得不稳定。你必须根据每个工作负载进行规划，并且你可以设计几个节点配置系列，用于安排与特定工作负载匹配的Pod。
- en: Benefiting from elastic cloud resources
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 受益于弹性云资源
- en: Most cloud providers let you scale instances automatically, which is a perfect
    complement to Kubernetes' horizontal pod autoscaling. If you use cloud storage,
    it also grows magically without you having to do anything. However, there are
    some gotchas that you need to be aware of.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数云提供商都可以让你自动扩展实例，这是对Kubernetes水平Pod自动缩放的完美补充。如果你使用云存储，它也会在你无需做任何事情的情况下神奇地增长。然而，有一些需要注意的地方。
- en: Autoscaling instances
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动缩放实例
- en: All the big cloud providers have instance autoscaling in place. There are some
    differences, but scaling up and down based on CPU utilization is always available,
    and sometimes custom metrics are available too. Sometimes, load balancing is offered
    as well. As you can see, there is some overlap with Kubernetes here. If your cloud
    provider doesn't have adequate autoscaling with proper control, it is relatively
    easy to roll your own, so that you monitor your cluster resource usage and invoke
    cloud APIs to add or remove instances. You can extract the metrics from Kubernetes.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 所有大型云提供商都已经实现了实例自动缩放。虽然有一些差异，但基于CPU利用率的扩展和缩减始终可用，有时也可以使用自定义指标。有时也提供负载均衡。你可以看到，这里与Kubernetes有一些重叠。如果你的云提供商没有适当的自动缩放和适当的控制，相对容易自己实现，这样你就可以监控集群资源使用情况并调用云API来添加或删除实例。你可以从Kubernetes中提取指标。
- en: 'Here is a diagram that shows how two new instances are added based on a CPU
    load monitor:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个图表，显示了基于CPU负载监视器添加了两个新实例的情况。
- en: '![](Images/997c6242-6b95-4225-b1b3-992e7f12863d.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/997c6242-6b95-4225-b1b3-992e7f12863d.png)'
- en: Mind your cloud quotas
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意你的云配额
- en: When working with cloud providers, some of the most annoying things are quotas.
    I've worked with four different cloud providers (AWS, GCP, Azure, and Alibaba
    Cloud) and I was always bitten by quotas at some point. The quotas exist to let
    the cloud providers do their own capacity planning (and also to protect you from
    inadvertently starting 1 million instances that you won't be able to pay for),
    but from your point of view it is yet one more thing that can trip you up. Imagine
    that you set up a beautiful autoscaling system that works like magic, and suddenly
    the system doesn't scale when you hit 100 nodes. You quickly discover that you
    are limited to 100 nodes and you open a support request to increase the quota.
    However, a human must approve quota requests, and that can take a day or two.
    In the meantime, your system is unable to handle the load.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在与云提供商合作时，一些最让人讨厌的事情是配额。我曾与四个不同的云提供商合作过（AWS，GCP，Azure和阿里云），总会在某个时候受到配额的限制。配额的存在是为了让云提供商进行自己的容量规划（也是为了保护您免受意外启动100万个无法支付的实例），但从您的角度来看，这又是一个可能让您遇到麻烦的事情。想象一下，您设置了一个像魔术一样工作的美丽的自动扩展系统，突然当您达到100个节点时，系统不再扩展。您很快发现自己被限制在100个节点，并且打开了一个支持请求来增加配额。然而，配额请求必须由人员批准，这可能需要一两天的时间。与此同时，您的系统无法处理负载。
- en: Manage regions carefully
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谨慎管理区域
- en: Cloud platforms are organized in regions and availability zones. Some services
    and machine configurations are available only in some regions. Cloud quotas are
    also managed at the regional level. The performance and cost of data transfers
    within regions is much lower (often free) than across regions. When planning your
    cluster, you should consider your geo-distribution strategy carefully. If you
    need to run your cluster across multiple regions, you may have some tough decisions
    to make regarding redundancy, availability, performance, and cost.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 云平台按区域和可用性区域组织。某些服务和机器配置仅在某些区域可用。云配额也是在区域级别管理的。区域内数据传输的性能和成本要比跨区域低得多（通常是免费）。在规划您的集群时，您应该仔细考虑您的地理分布策略。如果您需要在多个区域运行您的集群，您可能需要做出一些关于冗余、可用性、性能和成本的艰难决定。
- en: Considering Hyper.sh (and AWS Fargate)
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考虑Hyper.sh（和AWS Fargate）
- en: '`Hyper.sh` is a container-aware hosting service. You just start containers.
    The service takes care of allocating the hardware. Containers start within seconds.
    You never need to wait minutes for a new VM. Hypernetes is Kubernetes on Hyper.sh,
    and it completely eliminates the need to scale the nodes because there are no
    nodes as far as you''re concerned. There are only containers (or pods).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hyper.sh`是一个容器感知的托管服务。您只需启动容器。该服务负责分配硬件。容器在几秒钟内启动。您永远不需要等待几分钟来获取新的虚拟机。Hypernetes是在Hyper.sh上的Kubernetes，它完全消除了扩展节点的需要，因为在您看来根本没有节点。只有容器（或Pod）。'
- en: 'In the following diagram, you can see on the right how **Hyper Containers**
    run directly on a multi-tenant bare-metal container cloud:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您可以看到右侧的**Hyper容器**直接在多租户裸金属容器云上运行：
- en: '![](Images/061b3cae-185a-4856-8612-506f76e3365c.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/061b3cae-185a-4856-8612-506f76e3365c.png)'
- en: AWS recently released Fargate, which similarly abstracts away the underlying
    instances and just let you schedule containers in the cloud. In combination with
    EKS, it may become the most popular way to deploy Kubernetes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: AWS最近发布了Fargate，类似地将底层实例抽象化，并允许您在云中安排容器。与EKS结合使用，可能成为部署Kubernetes的最流行方式。
- en: Pushing the envelope with Kubernetes
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kubernetes推动信封
- en: 'In this section, we will see how the Kubernetes team pushes Kubernetes to its
    limit. The numbers are quite telling, but some of the tools and techniques, such
    as Kubemark, are ingenious, and you may even use them to test your clusters. In
    the wild, there are some Kubernetes clusters with 3,000 nodes. At CERN, the OpenStack
    team achieved 2 million requests per second:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到Kubernetes团队如何将Kubernetes推向极限。这些数字相当说明问题，但一些工具和技术，如Kubemark，是巧妙的，您甚至可以使用它们来测试您的集群。在野外，有一些拥有3,000个节点的Kubernetes集群。在CERN，OpenStack团队实现了每秒2百万次请求：
- en: '[http://superuser.openstack.org/articles/scaling-magnum-and-kubernetes-2-million-requests-per-second/](http://superuser.openstack.org/articles/scaling-magnum-and-kubernetes-2-million-requests-per-second/).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://superuser.openstack.org/articles/scaling-magnum-and-kubernetes-2-million-requests-per-second/](http://superuser.openstack.org/articles/scaling-magnum-and-kubernetes-2-million-requests-per-second/)。'
- en: Mirantis conducted a performance and scaling test in their scaling lab where
    they deployed 5,000 Kubernetes nodes (in VMs) on 500 physical servers.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Mirantis在其扩展实验室进行了性能和扩展测试，部署了5,000个Kubernetes节点（在虚拟机中）在500台物理服务器上。
- en: 'For more detail on Mirantis, please refer to: [http://bit.ly/2oijqQY](http://bit.ly/2oijqQY).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Mirantis的更多详细信息，请参阅：[http://bit.ly/2oijqQY](http://bit.ly/2oijqQY)。
- en: 'OpenAI scaled their machine learning Kubernetes cluster to 2,500 nodes and
    learned some valuable lessons, such as minding the query load of logging agents
    and storing events in a separate `etcd` cluster:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI将其机器学习Kubernetes集群扩展到2,500个节点，并学到了一些宝贵的经验教训，比如注意日志代理的查询负载，并将事件存储在单独的`etcd`集群中：
- en: '[https://blog.openai.com/scaling-kubernetes-to-2500-nodes/](https://blog.openai.com/scaling-kubernetes-to-2500-nodes/)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://blog.openai.com/scaling-kubernetes-to-2500-nodes/](https://blog.openai.com/scaling-kubernetes-to-2500-nodes/)'
- en: At the end of this section, you'll appreciate the effort and creativity that
    goes into improving Kubernetes on a large scale, you will know how far you can
    push a single Kubernetes cluster and what performance to expect, and you'll get
    an inside look at some of the tools and techniques that can help you evaluate
    the performance of your own Kubernetes clusters.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节结束时，您将欣赏到改进大规模Kubernetes所需的努力和创造力，您将了解单个Kubernetes集群的极限以及预期的性能，您将深入了解一些工具和技术，可以帮助您评估自己的Kubernetes集群的性能。
- en: Improving the performance and scalability of Kubernetes
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进Kubernetes的性能和可扩展性
- en: The Kubernetes team focused heavily on performance and scalability in Kubernetes
    1.6\. When Kubernetes 1.2 was released, it supported clusters of up to 1,000 nodes
    within the Kubernetes service-level objectives. Kubernetes 1.3 doubled the number
    to 2,000 nodes, and Kubernetes 1.6 brought it to a staggering 5,000 nodes per
    cluster. We will get into the numbers later, but first let's look under the hood
    and see how Kubernetes achieved these impressive improvements.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes团队在Kubernetes 1.6中大力专注于性能和可扩展性。当Kubernetes 1.2发布时，它支持Kubernetes服务水平目标内的最多1,000个节点的集群。Kubernetes
    1.3将该数字增加到2,000个节点，而Kubernetes 1.6将其提高到惊人的5,000个节点每个集群。我们稍后会详细介绍这些数字，但首先让我们来看看Kubernetes是如何实现这些令人印象深刻的改进的。
- en: Caching reads in the API server
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在API服务器中缓存读取
- en: Kubernetes keeps the state of the system in etcd, which is very reliable, though
    not superfast (although etcd3 delivered a massive improvement specifically in
    order to enable larger Kubernetes clusters). The various Kubernetes components
    operate on snapshots of that state and don't rely on real-time updates. That fact
    allows the trading of some latency for throughput. All the snapshots used to be
    updated by etcd watches. Now, the API server has an in-memory read cache that
    is used for updating state snapshots. The in-memory read cache is updated by etcd
    watches. These schemes significantly reduce the load on etcd and increase the
    overall throughput of the API server.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes将系统状态保存在etcd中，这是非常可靠的，尽管不是超级快速的（尽管etcd3专门提供了巨大的改进，以便实现更大的Kubernetes集群）。各种Kubernetes组件在该状态的快照上操作，并不依赖于实时更新。这一事实允许在一定程度上以一些延迟换取吞吐量。所有快照都曾由etcd监视更新。现在，API服务器具有用于更新状态快照的内存读取缓存。内存读取缓存由etcd监视更新。这些方案显著减少了etcd的负载，并增加了API服务器的整体吞吐量。
- en: The pod life cycle event generator
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod生命周期事件生成器
- en: Increasing the number of nodes in a cluster is key for horizontal scalability,
    but pod density is crucial too. Pod density is the number of pods that the Kubelet
    can manage efficiently on one node. If pod density is low, then you can't run
    too many pods on one node. That means that you might not benefit from more powerful
    nodes (more CPU and memory per node) because the Kubelet will not be able to manage
    more pods. The other alternative is to force the developers to compromise their
    design and create coarse-grained pods that do more work per pod. Ideally, Kubernetes
    should not force your hand when it comes to pod granularity. The Kubernetes team
    understands this very well and has invested a lot of work in improving pod density.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 增加集群中节点的数量对于水平扩展至关重要，但Pod密度也至关重要。Pod密度是Kubelet在一个节点上能够有效管理的Pod数量。如果Pod密度低，那么你就不能在一个节点上运行太多的Pod。这意味着你可能无法从更强大的节点（每个节点的CPU和内存更多）中受益，因为Kubelet将无法管理更多的Pod。另一种选择是强迫开发人员妥协他们的设计，并创建粗粒度的Pod，每个Pod执行更多的工作。理想情况下，Kubernetes在Pod粒度方面不应该强迫你的决定。Kubernetes团队非常了解这一点，并投入了大量工作来改善Pod密度。
- en: In Kubernetes 1.1, the official (tested and advertised) number was 30 pods per
    node. I actually ran 40 pods per node on Kubernetes 1.1, but I paid for it with
    an excessive kubelet overhead that stole CPU from the worker pods. In Kubernetes
    1.2, the number jumped to 100 pods per node.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes 1.1中，官方（经过测试和宣传）的数量是每个节点30个Pod。我实际上在Kubernetes 1.1上每个节点运行了40个Pod，但我付出了过多的kubelet开销，这从工作Pod中窃取了CPU。在Kubernetes
    1.2中，这个数字跳升到每个节点100个Pod。
- en: The kubelet used to poll the container runtime constantly for each pod in its
    own go routine. That put a lot of pressure on the container runtime so that, during
    performance peaks, there were reliability issues, particularly with CPU utilization.
    The solution was the **Pod Lifecycle Event Generator** (**PLEG**). The way the
    PLEG works is that it lists the state of all the pods and containers and compares
    it to the previous state. This is done once for all the pods and containers. Then,
    by comparing the state to the previous state, the PLEG knows which pods need to
    sync again and invokes only those pods. That change resulted in a significant
    four times lower CPU usage by the Kubelet and the container runtime. It also reduced
    the polling period, which improves responsiveness.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Kubelet以自己的go例程不断轮询容器运行时，以获取每个pod的状态。这给容器运行时带来了很大的压力，因此在性能高峰期会出现可靠性问题，特别是CPU利用率方面。解决方案是**Pod生命周期事件生成器**（**PLEG**）。PLEG的工作方式是列出所有pod和容器的状态，并将其与先前的状态进行比较。这只需要一次，就可以对所有的pod和容器进行比较。然后，通过将状态与先前的状态进行比较，PLEG知道哪些pod需要再次同步，并只调用这些pod。这一变化导致Kubelet和容器运行时的CPU使用率显著降低了四倍。它还减少了轮询周期，提高了响应性。
- en: 'The following diagram shows the **CPU utilization for 120 pods** on Kubernetes
    1.1 versus Kubernetes 1.2\. You can see the 4x factor very clearly:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Kubernetes 1.1和Kubernetes 1.2上**120个pod的CPU利用率**。您可以清楚地看到4倍的因素：
- en: '![](Images/233f183e-7dd1-4856-9050-2862a9e5e591.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/233f183e-7dd1-4856-9050-2862a9e5e591.png)'
- en: Serializing API objects with protocol buffers
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用协议缓冲区对API对象进行序列化
- en: The API server has a REST API. REST APIs typically use JSON as their serialization
    format, and the Kubernetes API server was no different. However, JSON serialization
    implies marshaling and unmarshaling JSON to native data structures. This is an
    expensive operation. In a large-scale Kubernetes cluster, a lot of components
    need to query or update the API server frequently. The cost of all that JSON parsing
    and composition adds up quickly. In Kubernetes 1.3, the Kubernetes team added
    an efficient protocol buffers serialization format. The JSON format is still there,
    but all internal communication between Kubernetes components uses the protocol
    buffers serialization format.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: API服务器具有REST API。REST API通常使用JSON作为其序列化格式，Kubernetes API服务器也不例外。然而，JSON序列化意味着将JSON编组和解组为本机数据结构。这是一个昂贵的操作。在大规模的Kubernetes集群中，许多组件需要频繁查询或更新API服务器。所有这些JSON解析和组合的成本很快就会累积起来。在Kubernetes
    1.3中，Kubernetes团队添加了一个高效的协议缓冲区序列化格式。JSON格式仍然存在，但Kubernetes组件之间的所有内部通信都使用协议缓冲区序列化格式。
- en: etcd3
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: etcd3
- en: 'Kubernetes switched from etcd2 to etcd3 in Kubernetes 1.6\. This was a big
    deal. Scaling Kubernetes to 5,000 nodes wasn''t possible due to limitations of
    etcd2, especially related to the watch implementation. The scalability needs of
    Kubernetes drove many of the improvements of etcd3, as CoreOS used Kubernetes
    as a measuring stick. Some of the big ticket items are as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在Kubernetes 1.6中从etcd2切换到etcd3。这是一件大事。由于etcd2的限制，尤其是与watch实现相关的限制，将Kubernetes扩展到5000个节点是不可能的。Kubernetes的可扩展性需求推动了etcd3的许多改进，因为CoreOS将Kubernetes作为一个衡量标准。一些重要的项目如下：
- en: GRPC instead of REST-etcd2 has a REST API, etcd3 has a gRPC API (and a REST
    API via gRPC gateway). The http/2 protocol at the base of gRPC can use a single
    TCP connection for multiple streams of requests and responses.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRPC而不是REST-etcd2具有REST API，etcd3具有gRPC API（以及通过gRPC网关的REST API）。在gRPC基础上的http/2协议可以使用单个TCP连接来处理多个请求和响应流。
- en: Leases instead of TTLs-etcd2 uses **time to live** (**TTL**) per key as the
    mechanism to expire keys, and etcd3 uses leases with TTLs, where multiple keys
    can share the same key. This reduces significantly keep alive traffic.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 租约而不是 TTL-etcd2 使用**生存时间**（**TTL**）来过期键，而 etcd3 使用带有 TTL 的租约，多个键可以共享同一个键。这显著减少了保持活动的流量。
- en: The watch implementation of etcd3 takes advantage of GRPC bi-directional streams
    and maintains a single TCP connection to send multiple events, which reduced the
    memory footprint by at least an order of magnitude.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd3 的 watch 实现利用了 GRPC 双向流，并维护单个 TCP 连接以发送多个事件，这至少减少了一个数量级的内存占用。
- en: With etcd3, Kubernetes started storing all the state as protobug, which eliminated
    a lot of wasteful JSON serialization overhead.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 etcd3，Kubernetes 开始将所有状态存储为 protobug，这消除了许多浪费的 JSON 序列化开销。
- en: Other optimizations
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他优化
- en: 'The Kubernetes team made many other optimizations:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 团队进行了许多其他优化：
- en: Optimizing the scheduler (which resulted in 5-10x higher scheduling throughput)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化调度程序（导致调度吞吐量提高了 5-10 倍）
- en: Switching all controllers to a new recommended design using shared informers,
    which reduced resource consumption of controller-manager-for reference see this
    document at [https://github.com/kubernetes/community/blob/master/contributors/devel/controllers.md](https://github.com/kubernetes/community/blob/master/contributors/devel/controllers.md)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有控制器切换到新的推荐设计，使用共享通知器，这减少了控制器管理器的资源消耗-有关详细信息，请参阅此文档[https://github.com/kubernetes/community/blob/master/contributors/devel/controllers.md](https://github.com/kubernetes/community/blob/master/contributors/devel/controllers.md)
- en: Optimizing individual operations in the API server (conversions, deep-copies,
    patch)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化 API 服务器中的单个操作（转换、深拷贝、补丁）
- en: Reducing memory allocation in the API server (which significantly impacts the
    latency of API calls)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少 API 服务器中的内存分配（这对 API 调用的延迟有显著影响）
- en: Measuring the performance and scalability of Kubernetes
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量 Kubernetes 的性能和可伸缩性
- en: In order to improve performance and scalability, you need a sound idea of what
    you want to improve and how you're going to measure the improvements. You must
    also make sure that you don't violate basic properties and guarantees in the quest
    for improved performance and scalability. What I love about performance improvements
    is that they often buy you scalability improvements for free. For example, if
    a pod needs 50% of the CPU of a node to do its job and you improve performance
    so that the pod can do the same work using 33% of the CPU, then you can suddenly
    run three pods instead of two on that node, and you've improved the scalability
    of your cluster by 50% overall (or reduced your cost by 33%).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能和可伸缩性，您需要清楚地知道您想要改进什么，以及如何去衡量这些改进。您还必须确保在追求性能和可伸缩性的过程中不违反基本属性和保证。我喜欢性能改进的地方在于它们通常可以免费为您带来可伸缩性的改进。例如，如果一个
    pod 需要节点的 50% CPU 来完成其工作，而您改进了性能，使得该 pod 只需要 33% 的 CPU 就能完成相同的工作，那么您可以在该节点上突然运行三个
    pod 而不是两个，从而将集群的可伸缩性整体提高了 50%（或者将成本降低了 33%）。
- en: The Kubernetes SLOs
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 的 SLO
- en: Kubernetes has **Service Level Objectives** (**SLOs**). These guarantees must
    be respected when trying to improve performance and scalability. Kubernetes has
    a one-second response time for API calls. That's 1,000 milliseconds. It actually
    achieves an order of magnitude faster response time most of the time.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有**服务水平目标**（**SLOs**）。在尝试改进性能和可伸缩性时，必须遵守这些保证。Kubernetes 对 API 调用有一秒的响应时间。那是
    1,000 毫秒。它实际上在大多数情况下实现了一个数量级的更快响应时间。
- en: Measuring API responsiveness
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量 API 的响应速度
- en: The API has many different endpoints. There is no simple API responsiveness
    number. Each call has to be measured separately. In addition, due to the complexity
    and the distributed nature of the system, not to mention networking issues, there
    can be a lot of volatility to the results. A solid methodology is to break the
    API measurements into separate endpoints, then run a lot of tests over time and
    look at percentiles (which is standard practice).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: API有许多不同的端点。没有简单的API响应性数字。每个调用都必须单独测量。此外，由于系统的复杂性和分布式特性，更不用说网络问题，结果可能会有很大的波动。一个可靠的方法是将API测量分成单独的端点，然后随着时间的推移进行大量测试，并查看百分位数（这是标准做法）。
- en: It's also important to use enough hardware to manage a large number of objects.
    The Kubernetes team used a 32-core VM with 120 GB for the master in this test.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 使用足够的硬件来管理大量对象也很重要。Kubernetes团队在这次测试中使用了一个32核心、120GB的虚拟机作为主节点。
- en: 'The following diagram describes the 50th, 90th, and 99th percentile of various
    important API call latencies for Kubernetes 1.3\. You can see that the 90th percentile
    is very low, below 20 milliseconds. Even the 99th percentile is less than 125
    milliseconds for the `DELETE` pods operation, and less than 100 milliseconds for
    all other operations:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了Kubernetes 1.3各种重要API调用延迟的50th、90th和99th百分位数。你可以看到，90th百分位数非常低，低于20毫秒。甚至对于`DELETE`
    pods操作，99th百分位数也低于125毫秒，对于其他所有操作也低于100毫秒：
- en: '![](Images/ebd83246-25cd-4544-9354-0137881e81e3.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ebd83246-25cd-4544-9354-0137881e81e3.jpg)'
- en: 'Another category of API calls is LIST operations. Those calls are more expansive
    because they need to collect a lot of information in a large cluster, compose
    the response, and send a potential large response. This is where performance improvements
    such as the in-memory read-cache and the protocol buffers serialization really
    shine. The response time is understandably greater than the single API calls,
    but it is still way below the SLO of one second (1,000 milliseconds):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类API调用是LIST操作。这些调用更加昂贵，因为它们需要在大型集群中收集大量信息，组成响应，并发送可能很大的响应。这就是性能改进，比如内存读取缓存和协议缓冲区序列化真正发挥作用的地方。响应时间理所当然地大于单个API调用，但仍远低于一秒（1000毫秒）的SLO。
- en: '![](Images/8dbffc43-0ad7-4818-9d70-18f9b77b6f24.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/8dbffc43-0ad7-4818-9d70-18f9b77b6f24.jpg)'
- en: 'This is excellent, but check out the API call latencies with Kubernetes 1.6
    on a 5,000 nodes cluster:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，但是看看Kubernetes 1.6在一个5000个节点的集群上的API调用延迟：
- en: '![](Images/a2a907bc-9ab2-4c00-9ecf-f6d7a1135793.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/a2a907bc-9ab2-4c00-9ecf-f6d7a1135793.png)'
- en: Measuring end-to-end pod startup time
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衡量端到端的Pod启动时间
- en: One of the most important performance characteristics of a large dynamic cluster
    is end-to-end pod startup time. Kubernetes creates, destroys, and shuffles pods
    around all the time. You could say that the primary function of Kubernetes is
    to schedule pods.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 大型动态集群最重要的性能特征之一是端到端的Pod启动时间。Kubernetes一直在创建、销毁和调度Pod。可以说，Kubernetes的主要功能是调度Pod。
- en: 'In the following diagram, you can see that pod startup time is less volatile
    than API calls. This makes sense since there is a lot of work that needs to be
    done, such as launching a new instance of a runtime that doesn''t depend on cluster
    size. With Kubernetes 1.2 on a 1,000-node cluster, the 99th percentile end-to-end
    time to launch a pod was less than 3 seconds. With Kubernetes 1.3, the 99th percentile
    end-to-end time to launch a pod was a little over 2.5 seconds. It''s remarkable
    that the time is very close, but a little better with Kubernetes 1.3 on a 2,000-node
    cluster versus a 1,000-node cluster:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您可以看到Pod启动时间比API调用不太波动。这是有道理的，因为有很多工作需要做，比如启动一个不依赖于集群大小的运行时的新实例。在拥有1,000个节点的Kubernetes
    1.2上，启动Pod的99th百分位端到端时间不到3秒。在Kubernetes 1.3上，启动Pod的99th百分位端到端时间略高于2.5秒。值得注意的是，时间非常接近，但在拥有2,000个节点的Kubernetes
    1.3上，比拥有1,000个节点的集群稍微好一点：
- en: '![](Images/13d475e6-8cf8-4e11-ac2f-e81dd4a16300.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/13d475e6-8cf8-4e11-ac2f-e81dd4a16300.jpg)'
- en: 'Kubernetes 1.6 takes it to the next level and does even better on a larger
    cluster:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 1.6将其提升到了下一个级别，并在更大的集群上表现得更好：
- en: '![](Images/ef3e1fed-94c5-41e5-8e4a-fbe72aa0a21f.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ef3e1fed-94c5-41e5-8e4a-fbe72aa0a21f.png)'
- en: Testing Kubernetes at scale
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在规模上测试Kubernetes
- en: Clusters with thousands of nodes are expensive. Even a project such as Kubernetes
    that enjoys the support of Google and other industry giants still needs to come
    up with reasonable ways to test without breaking the bank.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有数千个节点的集群是昂贵的。即使像Kubernetes这样得到Google和其他行业巨头支持的项目，仍然需要找到合理的测试方法，而不会让自己破产。
- en: The Kubernetes team runs a full-fledged test on a real cluster at least once
    per release to collect real-world performance and scalability data. However, there
    is also a need for a lightweight and cheaper way to experiment with potential
    improvements and to detect regressions. Enter the Kubemark.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes团队每次发布都会在真实集群上运行全面的测试，以收集真实世界的性能和可伸缩性数据。然而，还需要一种轻量级和更便宜的方法来尝试潜在的改进，并检测回归。这就是Kubemark。
- en: Introducing the Kubemark tool
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Kubemark工具
- en: The Kubemark is a Kubernetes cluster that runs mock nodes called hollow nodes
    used for running lightweight benchmarks against large-scale (hollow) clusters.
    Some of the Kubernetes components that are available on a real node such as the
    kubelet is replaced with a hollow kubelet. The hollow kubelet fakes a lot of the
    functionality of a real kubelet. A hollow kubelet doesn't actually start any containers,
    and it doesn't mount any volumes. But from the Kubernetes cluster point of view
    -the state stored in etcd- all those objects exist and you can query the API server.
    The hollow kubelet is actually the real kubelet with an injected mock Docker client
    that doesn't do anything.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Kubemark是一个运行模拟节点（称为空心节点）的Kubernetes集群，用于针对大规模（空心）集群运行轻量级基准测试。一些在真实节点上可用的Kubernetes组件，如kubelet，被替换为空心kubelet。空心kubelet模拟了真实kubelet的许多功能。空心kubelet实际上不会启动任何容器，也不会挂载任何卷。但从Kubernetes集群的角度来看
    - 存储在etcd中的状态 - 所有这些对象都存在，您可以查询API服务器。空心kubelet实际上是带有注入的模拟Docker客户端的真实kubelet，该客户端不执行任何操作。
- en: Another important hollow component is the `hollow-proxy`, which mocks the Kubeproxy
    component. It again uses the real Kubeproxy code with a mock proxier interface
    that does nothing and avoids touching iptables.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的空心组件是`hollow-proxy`，它模拟了Kubeproxy组件。它再次使用真实的Kubeproxy代码，具有一个不执行任何操作并避免触及iptables的模拟proxier接口。
- en: Setting up a Kubemark cluster
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Kubemark集群
- en: 'A Kubemark cluster uses the power of Kubernetes. To set up a Kubemark cluster,
    perform the following steps:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Kubemark集群利用了Kubernetes的强大功能。要设置Kubemark集群，请执行以下步骤：
- en: Create a regular Kubernetes cluster where we can run `N hollow-nodes`.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个常规的Kubernetes集群，我们可以在其中运行`N hollow-nodes`。
- en: Create a dedicated VM to start all master components for the Kubemark cluster.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个专用的VM来启动Kubemark集群的所有主要组件。
- en: Schedule `N hollow-node` pods on the base Kubernetes cluster. Those hollow-nodes
    are configured to talk to the Kubemark API server running on the dedicated VM.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基本Kubernetes集群上安排`N个空节点` pods。这些空节点被配置为与运行在专用VM上的Kubemark API服务器进行通信。
- en: Create add-on pods by scheduling them on the base cluster and configuring them
    to talk to the Kubemark API server.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在基本集群上安排并配置它们与Kubemark API服务器进行通信来创建附加的pods。
- en: A full-fledged guide on GCP is available at [http://bit.ly/2nPMkwc](http://bit.ly/2nPMkwc).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: GCP上提供了完整的指南，网址为[http://bit.ly/2nPMkwc](http://bit.ly/2nPMkwc)。
- en: Comparing a Kubemark cluster to a real-world cluster
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Kubemark集群与真实世界集群进行比较
- en: 'The performance of Kubemark clusters is pretty similar to the performance of
    real clusters. For the pod startup end-to-end latency, the difference is negligible.
    For the API-responsiveness, the differences are higher, though generally less
    than a factor of two. However, trends are exactly the same: an improvement/regression
    in a real cluster is visible as a similar percentage drop/increase in metrics
    in Kubemark.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Kubemark集群的性能与真实集群的性能非常相似。对于pod启动的端到端延迟，差异可以忽略不计。对于API的响应性，差异较大，尽管通常不到两倍。然而，趋势完全相同：真实集群中的改进/退化在Kubemark中表现为类似百分比的下降/增加。
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've covered many topics relating to scaling Kubernetes clusters.
    We discussed how the horizontal pod autoscaler can automatically manage the number
    of running pods-based CPU utilization or other metrics, how to perform rolling
    updates correctly and safely in the context of auto-scaling, and how to handle
    scarce resources via resource quotas. Then we moved on to overall capacity planning
    and management of the cluster's physical or virtual resources. Finally, we delved
    into a real-world example of scaling a single Kubernetes cluster to handle 5,000
    nodes.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了许多与扩展Kubernetes集群相关的主题。我们讨论了水平pod自动缩放器如何根据CPU利用率或其他指标自动管理运行的pod数量，如何在自动缩放的情况下正确安全地执行滚动更新，以及如何通过资源配额处理稀缺资源。然后，我们转向了整体容量规划和管理集群的物理或虚拟资源。最后，我们深入探讨了将单个Kubernetes集群扩展到处理5,000个节点的真实示例。
- en: At this point, you have a good understanding of all the factors that come into
    play when a Kubernetes cluster is facing dynamic and growing workloads. You have
    multiple tools to choose from for planning and designing your own scaling strategy.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经对Kubernetes集群面对动态和不断增长的工作负载时涉及的所有因素有了很好的理解。您有多种工具可供选择，用于规划和设计自己的扩展策略。
- en: In the next chapter, we will dive into advanced Kubernetes networking. Kubernetes
    has a networking model based on the **Common Networking Interface** (**CNI**)
    and supports multiple providers.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨高级Kubernetes网络。Kubernetes具有基于**通用网络接口**（**CNI**）的网络模型，并支持多个提供程序。
