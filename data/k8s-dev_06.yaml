- en: Background Processing in Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的后台处理
- en: Kubernetes includes support for one-off (also known as batch) computation work,
    as well as supporting common use cases for asynchronous background work. In this
    chapter, we look at the Kubernetes concept of job, and its neighbor, CronJob.
    We also look at how Kubernetes handles and supports persistence, and some of the
    options that are available within Kubernetes. We then look at how Kubernetes can
    support asynchronous background tasks and the ways those can be represented, operated,
    and tracked by Kubernetes. We also go over how to set up worker codes operating
    from a message queue.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes包括对一次性（也称为批处理）计算工作的支持，以及支持异步后台工作的常见用例。在本章中，我们将介绍Kubernetes的作业概念及其邻居CronJob。我们还将介绍Kubernetes如何处理和支持持久性，以及Kubernetes中可用的一些选项。然后，我们将介绍Kubernetes如何支持异步后台任务以及Kubernetes可以如何表示、操作和跟踪这些任务的方式。我们还将介绍如何设置从消息队列操作的工作代码。
- en: 'Topics covered in this chapter include:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题包括：
- en: Job
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作
- en: CronJob
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CronJob
- en: A worker queue example with Python and Celery
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python和Celery的工作队列示例
- en: Persistence with Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes中的持久性
- en: Stateful Sets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有状态集
- en: '**Custom Resource Definitions** (**CRDs**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义资源定义**（**CRD**）'
- en: Job
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作
- en: Most of what we have covered so far has been focused on continuous, long-running
    processes. Kubernetes also has support for shorter, discrete runs of software.
    A job in Kubernetes is focused on a discrete run that is expected to end within
    some reasonably-known timeframe, and report a success or failure. Jobs use and
    build upon the same construct as the long-running software, so they use the pod
    specification at their heart, and add the concept of tracking the number of successful
    completions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所涵盖的大部分内容都集中在持续运行的长期进程上。Kubernetes还支持较短的、离散的软件运行。Kubernetes中的作业专注于在一定时间内结束并报告成功或失败的离散运行，并构建在与长期运行软件相同的构造之上，因此它们在其核心使用pod规范，并添加了跟踪成功完成数量的概念。
- en: The simplest use case is to run a single pod to completion, letting Kubernetes
    handle any failures due to a node failure or reboot. The two optional settings
    you can use with jobs are parallelism and completion. Without specifying parallelism,
    the default is `1` and only one job will be scheduled at a time. You can specify
    both values as integers to run a number of jobs in parallel to achieve multiple
    completions, and you can leave completions unset if the job is working from a
    work queue of some form.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的用例是运行一个单独的pod，让Kubernetes处理由于节点故障或重启而导致的任何故障。您可以在作业中使用的两个可选设置是并行性和完成。如果不指定并行性，默认值为`1`，一次只会安排一个作业。您可以将这两个值都指定为整数，以并行运行多个作业以实现多个完成，并且如果作业是从某种工作队列中工作，则可以不设置完成。
- en: It is important to know that the settings for completions and parallelism aren't
    guarantees – so the code within your pods needs to be tolerant of multiple instances
    running. Likewise, the job needs to be tolerant of a container restarting in the
    event of a container failure (for example, when using the `restartPolicy OnFailure`),
    as well as handling any initialization or setup that it needs if on restart it
    finds itself running on a new pod (which can happen in the event of node failure).
    If your job is using temporary files, locks, or working off a local file to do
    its work, it should verify on startup what the state is and not presume the files
    will always be there, in the event of a failure during processing.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要知道，完成和并行设置并不是保证 - 因此Pod内的代码需要能够容忍多个实例运行。同样，作业需要能够容忍容器在容器失败时重新启动（例如，在使用`restartPolicy
    OnFailure`时），以及处理任何初始化或设置，如果在重新启动时发现自己在新的Pod上运行（这可能发生在节点故障的情况下）。如果作业使用临时文件、锁定或从本地文件进行工作，它应该在启动时验证状态，并且不应该假定文件始终存在，以防在处理过程中发生故障。
- en: When a job runs to completion, the system does not create any more pods, but
    does not delete the pod either. This lets you interrogate the pod state for success
    or failure, and look at any logs from the containers within the pod. Pods that
    have run to completion will not show up in a simple run of `kubectl get pods`,
    but will appear if you use the `-a` option. It is up to you to delete completed
    jobs, and when you use `kubectl delete` to remove the job, the associated pod
    will be removed and cleaned up as well.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当作业运行完成时，系统不会创建更多的Pod，但也不会删除Pod。这样可以让您查询成功或失败的Pod状态，并查看Pod内容器的任何日志。已经完成的Pod不会出现在`kubectl
    get pods`的简单运行中，但如果您使用`-a`选项，它们将会出现。您需要删除已完成的作业，当您使用`kubectl delete`删除作业时，相关的Pod也将被删除和清理。
- en: 'As an example, let''s run an example job to look at how this works. A simple
    job that simply prints `hello world` can be specified with the following YAML:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们运行一个示例作业来看看它是如何工作的。一个简单的作业，只需打印`hello world`，可以使用以下YAML指定：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And then you can run this job using either `kubectl create` or `kubectl apply`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用`kubectl create`或`kubectl apply`来运行此作业：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The expected command of `kubectl get jobs` will show you the jobs that exist
    and their current state. Since this job is so simple, it will likely complete
    before you can run a command to see its current state:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的`kubectl get jobs`命令将显示存在的作业及其当前状态。由于这个作业非常简单，它可能会在您运行命令查看其当前状态之前完成：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And like pods, you can use the `kubectl describe` command to get more detailed
    state and output:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与Pod一样，您可以使用`kubectl describe`命令获取更详细的状态和输出：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you run the command `kubectl get pods`, you won''t see the pod `helloworld-2b2xt`
    in the list of pods, but running `kubectl get pods -a` will show the pods, including
    completed or failed pods that still exist:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行`kubectl get pods`命令，您将看不到Pod`helloworld-2b2xt`在Pod列表中，但运行`kubectl get
    pods -a`将显示Pod，包括仍然存在的已完成或失败的Pod：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you just want to see for yourself what the state of the pod was, using `kubectl
    describe` to get the details will show you the information in human, readable
    form:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只是想亲自查看Pod的状态，可以使用`kubectl describe`获取详细信息，以人类可读的形式显示信息：
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'An example of this is the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例：
- en: '![](assets/a7c7f878-dbeb-4b92-8ccb-9e4d10436dc8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/a7c7f878-dbeb-4b92-8ccb-9e4d10436dc8.png)'
- en: If you are making a simple job with a shell script like in this example, it's
    easy to make a mistake. In those cases, the default will be for Kubernetes to
    retry running the pod repeatedly, leaving pods in a failed state in the system
    for you to see. Having a backoff limit, in this case, can limit the number of
    times the system retries your job. If you don't specify this value, it uses the
    default value of six tries.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您像在此示例中一样使用shell脚本创建一个简单的作业，很容易出错。在这些情况下，默认情况是Kubernetes会重试运行pod，使得pod在系统中处于失败状态供您查看。在这种情况下，设置一个退避限制可以限制系统重试作业的次数。如果您不指定此值，它将使用默认值为六次。
- en: 'A job with a simple mistake in the command might look like the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个命令中的简单错误可能看起来像下面这样：
- en: '[PRE9]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And looking at the `pods`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 并查看`pods`：
- en: '[PRE11]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The logs for each pod will be available, so you can diagnose what went wrong.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 每个pod的日志都将可用，因此您可以诊断出了什么问题。
- en: If you do make a mistake, then you may be tempted to make a quick modification
    to the job specification and use `kubectl apply` to fix the error. Jobs are considered
    immutable by the system, so you will get an error if you try to make a quick fix
    and apply it. When you are working with jobs, it is better to delete the job and
    create a new one.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您犯了一个错误，那么您可能会想要快速修改作业规范，并使用`kubectl apply`来修复错误。系统认为作业是不可变的，因此如果您尝试快速修复并应用它，将会收到错误。在处理作业时，最好删除作业并创建一个新的。
- en: Jobs are not tied to the life cycle of other objects in Kubernetes, so if you
    are thinking about using one to initialize data in a persistence store, remember
    that you will need to coordinate running that job. In cases where you want some
    logic checked every time before a service starts to preload data, you may be better
    off using an initialization container, as we explored in the last chapter.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作业与Kubernetes中其他对象的生命周期无关，因此，如果您考虑使用作业来初始化持久性存储中的数据，请记住您需要协调运行该作业。在您希望在服务启动之前每次检查一些逻辑以预加载数据的情况下，最好使用初始化容器，就像我们在上一章中探讨的那样。
- en: Some common cases that work well for jobs are loading a backup into a database,
    making a backup, doing some deeper system introspection or diagnostics, or running
    out-of-band cleanup logic. In all these cases, you want to know that the function
    you wrote ran to completion, and that it succeeded. And in the case of failure,
    you may want to retry, or simply know what happened via the logs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的适合作业的情况包括将备份加载到数据库中、创建备份、进行一些更深入的系统内省或诊断，或者运行超出带宽清理逻辑。在所有这些情况下，您希望知道您编写的函数已经完成，并且成功运行。在失败的情况下，您可能希望重试，或者仅仅通过日志了解发生了什么。
- en: CronJob
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CronJob
- en: CronJobs are an extension that build on jobs to allow you to specify a recurring
    schedule for when they run. The name pulls from a common Linux utility for scheduling
    recurring scripts called `cron`. CronJobs were alpha as of Kubernetes version
    1.7, and moved to beta in version 1.8, and remain in beta as of version 1.9\.
    Remember that Kubernetes specifications may change, but tend to be fairly solid
    and have expected utility with beta, so the v1 release of CronJobs may be different,
    but you can likely expect it to be pretty close to what's available as of this
    writing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: CronJobs是建立在作业基础上的扩展，允许您指定它们运行的重复计划。该名称源自一个用于调度重复脚本的常见Linux实用程序`cron`。CronJobs在Kubernetes版本1.7中是alpha版本，在版本1.8中转为beta版本，并且在版本1.9中仍然是beta版本。请记住，Kubernetes规范可能会发生变化，但往往相当稳定，并且具有预期的beta实用性，因此CronJobs的v1版本可能会有所不同，但您可以期望它与本文提供的内容非常接近。
- en: The specification is highly related to a job, with the primary difference being
    the kind is CronJob and there is a required field schedule that takes a string
    representing the timing for running this job.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 规范与作业密切相关，主要区别在于种类是CronJob，并且有一个必需的字段schedule，它接受一个表示运行此作业的时间的字符串。
- en: 'The format for this string is five numbers, and wildcards can be used. The
    fields represent:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此字符串的格式是五个数字，可以使用通配符。这些字段表示：
- en: Minute (0–59)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分钟（0-59）
- en: Hour (0–23)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小时（0-23）
- en: Day of Month (1–31)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份的日期（1-31）
- en: Month (1–12)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份（1-12）
- en: Day of Week (0–6)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 星期几（0-6）
- en: 'A `*` or? a character can be used in any of these fields to represent that
    any value is acceptable. A field can also include a `*/` and a number, which indicates
    a recurring instance at some interval, specified by the associated number. Some
    examples of this format are:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`*`或？字符可以在这些字段中的任何一个中使用，表示任何值都可以接受。字段还可以包括`*/`和一个数字，这表示在一些间隔内定期发生的实例，由相关数字指定。这种格式的一些例子是：'
- en: '`12 * * * *`: Run every hour at 12 minutes past the hour'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`12 * * * *`：每小时在整点后12分钟运行'
- en: '`*/5 * * * *`: Run every 5 minutes'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*/5 * * * *`：每5分钟运行'
- en: '`0 0 * * 0`: Run every Saturday at midnight'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每周六午夜运行
- en: 'There are also a few special strings that can be used for common occurrences
    that are a bit more human readable:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些特殊的字符串可以用于一些更容易阅读的常见事件：
- en: '`@yearly`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@yearly`'
- en: '`@monthly`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@monthly`'
- en: '`@weekly`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@weekly`'
- en: '`@daily`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@daily`'
- en: '``@hourly``'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '``@hourly``'
- en: A CronJob has five additional fields that you can specify, but which are not
    required. Unlike jobs, CronJobs are mutable (just like pods, deployments, and
    so on), so these values can be changed or updated after the CronJob is created.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: CronJob有五个额外的字段，可以指定，但不是必需的。与作业不同，CronJobs是可变的（就像pod、部署等一样），因此这些值可以在创建CronJob后更改或更新。
- en: The first is `startingDeadlineSeconds`, which, if specified, will put a limit
    on when a job can be started if Kubernetes doesn't meet its specified deadline
    of when to start the job. If the time exceeds `startingDeadlineSeconds`, that
    iteration will be marked as a failure.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是`startingDeadlineSeconds`，如果指定，将限制作业在Kubernetes未满足其指定的启动作业时间限制时可以启动的时间。如果时间超过`startingDeadlineSeconds`，该迭代将标记为失败。
- en: The second is `concurrencyPolicy`, which controls whether Kubernetes allows
    multiple instances of the same job to run concurrently. The default for this is
    `Allow`, which will let multiple jobs run at the same time, with alternate values
    of `Forbid` and `Replace`. `Forbid` will mark the following job as a failure if
    the first is still running, and `Replace` will cancel the first job and attempt
    to run that same code again.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个是`concurrencyPolicy`，它控制Kubernetes是否允许多个相同作业的实例同时运行。默认值为`Allow`，这将允许多个作业同时运行，备用值为`Forbid`和`Replace`。`Forbid`将在第一个作业仍在运行时将以下作业标记为失败，而`Replace`将取消第一个作业并尝试再次运行相同的代码。
- en: The third field is `suspended`, which defaults to `False`, and can be used to
    suspend any further invocations of jobs on the schedule. If a job is already running
    and `suspend` is added to the CronJob specification, that current job will run
    to completion, but any further jobs won't be scheduled.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个字段是`suspended`，默认值为`False`，可以用于暂停计划中作业的任何进一步调用。如果作业已经在运行，并且将`suspend`添加到CronJob规范中，那么当前作业将运行到完成，但不会安排任何进一步的作业。
- en: The fourth and fifth fields are `successfulJobsHistoryLimit` and `failedJobsHistoryLimit`,
    which default to values of `3` and `1` respectively. By default, Kubernetes will
    clean up old jobs beyond these values, but retain the recent success and failures,
    including the logs, so they can be inspected as you wish.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第四和第五个字段是`successfulJobsHistoryLimit`和`failedJobsHistoryLimit`，默认值分别为`3`和`1`。默认情况下，Kubernetes将清理超出这些值的旧作业，但保留最近的成功和失败，包括日志，以便根据需要进行检查。
- en: When you create a CronJob, you will also want to choose (and define in your
    specification) a `restartPolicy`. A CronJob doesn't allow for the default value
    of `Always`, so you will want to choose between `OnFailure` and `Never`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 创建CronJob时，您还需要选择（并在规范中定义）`restartPolicy`。CronJob不允许`Always`的默认值，因此您需要在`OnFailure`和`Never`之间进行选择。
- en: 'A simple CronJob that prints `hello world` every minute might look like the
    following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每分钟打印`hello world`的简单CronJob可能如下所示：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After creating this job with `kubectl apply -f cronjob.yaml`, you can see the
    summary output with `kubectl get cronjob`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl apply -f cronjob.yaml`创建此作业后，您可以使用`kubectl get cronjob`查看摘要输出：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Or, see more detailed output with `kubectl describe cronjob helloworld`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，使用`kubectl describe cronjob helloworld`查看更详细的输出：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you might guess from this output, the CronJob is actually creating jobs
    to the schedule you define, and from the template in the specification. Each job
    gets its own name based on the name of the CronJob, and can be viewed independently:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从此输出中，您可能会猜到CronJob实际上是根据您定义的时间表和规范中的模板创建作业。每个作业都基于CronJob的名称获得自己的名称，并且可以独立查看：
- en: '![](assets/1b610135-65a4-4cd3-8991-e02851bc1d2a.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1b610135-65a4-4cd3-8991-e02851bc1d2a.png)'
- en: 'You can see the jobs that are created on the schedule defined from the preceding
    CronJob, by using the `kubectl get jobs` command:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`kubectl get jobs`命令查看从前面的CronJob定义的时间表创建的作业：
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And you can view the pods that were created and run to completion from each
    of those jobs, leveraging the `-a` option with `kubectl get pods`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`kubectl get pods`的`-a`选项查看从这些作业中创建并运行到完成的Pod：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: A worker queue example with Python and Celery
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python和Celery的工作队列示例
- en: Where the CronJob is well positioned to run repeated tasks at a specific schedule,
    another common need is to process a series of work items more or less constantly.
    A job is well oriented to running a single task until it is complete, but if the
    volume of things you need to process is large enough, it may be far more effective
    to maintain a constant process to work on those items.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: CronJob很适合在特定时间表上运行重复任务，但另一个常见的需求是更多或更少地不断处理一系列工作项。作业很适合运行单个任务直到完成，但如果需要处理的事务量足够大，保持不断的处理过程可能更有效。
- en: 'A common pattern to accommodate this kind of work uses a message queue, as
    shown here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 适应这种工作的常见模式使用消息队列，如下所示：
- en: '![](assets/587495c0-7d7c-4060-9796-3c2d5e40607f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/587495c0-7d7c-4060-9796-3c2d5e40607f.png)'
- en: With a message queue, you can have an API frontend that creates the work to
    be run asynchronously, move that into a queue, and then have a number of worker
    processes pull from the queue to do the relevant work. Amazon has a web-based
    service supporting exactly this pattern of processing called **Simple Queue Service**
    (**SQS**). A huge benefit of this pattern is decoupling the workers from the request,
    so you can scale each of those pieces independently, as required.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过消息队列，您可以拥有一个API前端，用于异步创建要运行的工作，将其移动到队列中，然后有多个工作进程从队列中拉取相关工作。亚马逊有一个支持这种处理模式的基于Web的服务，称为**简单队列服务**（**SQS**）。这种模式的巨大好处是将工作人员与请求解耦，因此您可以根据需要独立扩展每个部分。
- en: You can do exactly the same within Kubernetes, running the queue as a service
    and workers connecting to that queue as a deployment. Python has a popular framework,
    Celery, that carries out background processing from a message, supporting a number
    of queue mechanisms. We will look at how you can set up an example queue and worker
    process, and how to leverage a framework such as Celery within Kubernetes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Kubernetes中做完全相同的事情，将队列作为服务运行，并将连接到该队列的工作程序作为部署。Python有一个流行的框架Celery，它可以从消息中进行后台处理，支持多种队列机制。我们将看看如何设置一个示例队列和工作进程，以及如何在Kubernetes中利用Celery这样的框架。
- en: Celery worker example
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Celery worker example
- en: Celery has been in development and use since 2009, long before Kubernetes existed.
    It was written expecting to be deployed on multiple machines. This translates
    reasonably well to containers, which our example will illustrate. You can get
    more details about Celery at: [http://docs.celeryproject.org/en/latest/](http://docs.celeryproject.org/en/latest/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Celery自2009年以来一直在开发和使用，早于Kubernetes存在。它是为了部署在多台机器上而编写的。这在我们的示例中可以很好地转化为容器。您可以在[http://docs.celeryproject.org/en/latest/](http://docs.celeryproject.org/en/latest/)获取有关Celery的更多详细信息。
- en: 'In this example, we will set up a service with a deployment of RabbitMQ and
    a separate deployment of our own container, **celery-worker**, to process jobs
    from that queue:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将设置一个包含RabbitMQ部署和我们自己的容器**celery-worker**的部署，用来处理来自该队列的作业：
- en: '![](assets/28e529ac-48cc-40f5-9217-60753631314a.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/28e529ac-48cc-40f5-9217-60753631314a.png)'
- en: 'The deployments and source code for this example are available on GitHub at
    [https://github.com/kubernetes-for-developers/kfd-celery/](https://github.com/kubernetes-for-developers/kfd-celery/).
    You can get this code by using the following commands:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的部署和源代码可在GitHub上找到[https://github.com/kubernetes-for-developers/kfd-celery/](https://github.com/kubernetes-for-developers/kfd-celery/)。您可以使用以下命令获取此代码：
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: RabbitMQ and configuration
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RabbitMQ和配置
- en: This example uses a container that includes RabbitMQ from Bitnami. The source
    for that image is available at [https://github.com/bitnami/bitnami-docker-rabbitmq](https://github.com/bitnami/bitnami-docker-rabbitmq),
    and the container images are hosted publicly on DockerHub at [https://hub.docker.com/r/bitnami/rabbitmq/](https://hub.docker.com/r/bitnami/rabbitmq/).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例使用了一个包含Bitnami的RabbitMQ的容器。该镜像的源代码可在[https://github.com/bitnami/bitnami-docker-rabbitmq](https://github.com/bitnami/bitnami-docker-rabbitmq)找到，并且容器镜像公开托管在DockerHub上[https://hub.docker.com/r/bitnami/rabbitmq/](https://hub.docker.com/r/bitnami/rabbitmq/)。
- en: RabbitMQ has a large number of options that can be used with it, and a variety
    of ways to deploy it, including in clusters to support HA. For this example, we
    are using a single RabbitMQ replica within a deployment to back a service called
    `message-queue`. We also set up a `ConfigMap` with some of the variables that
    we might want to adjust for a local setting, although in this example, the values
    are the same as the defaults within the container. The deployment does use a persistent
    volume to enable persistence for the queue in the event of a failure. We will
    go into persistent volumes and how to use them later in this chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ有大量可用的选项，以及多种部署方式，包括集群支持HA。在这个例子中，我们使用了一个单一的RabbitMQ副本在部署中支持名为`message-queue`的服务。我们还设置了一个`ConfigMap`，其中包含一些我们可能想要调整的变量，尽管在这个例子中，这些值与容器内的默认值相同。该部署确实使用了持久卷，以便在发生故障时为队列启用持久性。我们将在本章后面详细介绍持久卷以及如何使用它们。
- en: 'The `ConfigMap` we create will be used by both the RabbitMQ container and our
    worker deployment. The `ConfigMap` is called `queue-config.yaml` and reads as
    follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的`ConfigMap`将被RabbitMQ容器和我们的工作程序部署使用。`ConfigMap`名为`queue-config.yaml`，内容如下：
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To deploy it, you can use this command:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署它，您可以使用以下命令：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `ConfigMap` was created based on the documentation for the Bitnami RabbitMQ
    container, which supports setting a number of configuration items through environment
    variables. You can see all the details that the container could take at the Docker
    Hub web page, or at the source in GitHub. In our case, we set some of the most
    common values.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConfigMap`是基于Bitnami RabbitMQ容器的文档创建的，该容器支持通过环境变量设置许多配置项。您可以在Docker Hub网页或GitHub源中查看容器可以接受的所有细节。在我们的情况下，我们设置了一些最常见的值。'
- en: '**Note**: You would probably want to set username and password more correctly
    with secrets instead of including the values in a `ConfigMap`.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：您可能希望使用密钥而不是在`ConfigMap`中包含值来更正确地设置用户名和密码。'
- en: 'You can see the specification for the deployment:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看部署的规范：
- en: '![](assets/acdf2ee8-f989-4935-8ff0-abefc68f18ee.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/acdf2ee8-f989-4935-8ff0-abefc68f18ee.png)'
- en: 'And this is how to deploy the instance:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何部署实例的：
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Celery worker
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Celery worker
- en: To create a worker, we made our own container image very similar to the Flask
    container. The Dockerfile uses Alpine Linux and explicitly loads Python 3 onto
    that image, then installs the requirements from a `requirements.txt` file and
    adds in two Python files. The first, `celery_conf.py`, is the Python definition
    for a couple of tasks taken directly from the Celery documentation. The second, `submit_tasks.py`,
    is a short example that is meant to be run interactively to create work and send
    it over the queue. The container also includes two shell scripts: `run.sh` and
    `celery_status.sh`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个worker，我们制作了一个非常类似于Flask容器的自己的容器镜像。Dockerfile使用Alpine Linux，并明确将Python
    3加载到该镜像中，然后从`requirements.txt`文件安装要求，并添加了两个Python文件。第一个`celery_conf.py`是直接从Celery文档中获取的一些任务的Python定义。第二个`submit_tasks.py`是一个简短的示例，旨在交互式运行以创建工作并将其发送到队列中。容器还包括两个shell脚本：`run.sh`和`celery_status.sh`。
- en: In all of these cases, we used environment variables that we source from the
    preceding `ConfigMap` to set up the logging output from the worker, as well as
    the host, username, and password for communicating with RabbitMQ within Kubernetes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，我们使用了从前面的`ConfigMap`中获取的环境变量来设置worker的日志输出，以及与Kubernetes内的RabbitMQ通信的主机、用户名和密码。
- en: 'The Dockerfile uses the `run.sh` script as its command, so that we can use
    this shell script to set up any environment variables and invoke Celery. Because
    Celery was originally written as a command-line tool, using a shell script to
    set up and invoke what you want is very convenient. Here is a closer look at `run.sh`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Dockerfile使用`run.sh`脚本作为其命令，因此我们可以使用此shell脚本设置任何环境变量并调用Celery。因为Celery最初是作为一个命令行工具编写的，所以使用shell脚本来设置和调用您想要的内容非常方便。以下是对`run.sh`的更详细介绍：
- en: '![](assets/02e28d34-3a15-4bf3-9030-3533a0120941.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/02e28d34-3a15-4bf3-9030-3533a0120941.png)'
- en: The script sets two shell script options, `-e` and `-x`. The first, (`-e`),
    is to make sure that if we make a typo or a command in the script returned an
    error, the script itself will return an error. The second, (`-x`), echoes the
    commands invoked in the script to `STDOUT`, so we can see that in the container
    log output.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本设置了两个shell脚本选项，`-e`和`-x`。第一个（`-e`）是为了确保如果我们在脚本中犯了拼写错误或命令返回错误，脚本本身将返回错误。第二个（`-x`）将在`STDOUT`中回显脚本中调用的命令，因此我们可以在容器日志输出中看到它。
- en: 'The next line with `DEBUG_LEVEL` uses the shell to look for a default environment
    variable: `WORKER_DEBUG_LEVEL`. If it''s set, it will use it, and `WORKER_DEBUG_LEVEL`
    was added earlier to the `ConfigMap`. If the value isn''t set, this will use a
    default of `info` in its place, so if the value is missing from the `ConfigMap`,
    we will still have a reasonable value here.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行中的`DEBUG_LEVEL`使用shell查找默认环境变量：`WORKER_DEBUG_LEVEL`。如果设置了，它将使用它，而`WORKER_DEBUG_LEVEL`是早期添加到`ConfigMap`中的。如果值未设置，它将使用默认值`info`，因此如果`ConfigMap`中缺少该值，我们仍将在此处有一个合理的值。
- en: As mentioned earlier, Celery was written as a command-line utility and takes
    advantage of Python's module loading to do its work. Python module loading includes
    working from the current directory, so we explicitly change to the directory containing
    the Python code. Finally, the script invokes the command to start a Celery worker.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Celery是作为命令行实用程序编写的，并利用Python的模块加载来完成其工作。 Python模块加载包括从当前目录工作，因此我们明确更改为包含Python代码的目录。最后，脚本调用命令启动Celery
    worker。
- en: We use a similar structure in the script, `celery_status.sh`, which is used
    to provide an exec command used in both liveness and readiness probes for the
    worker container, with the key idea being if the command `celery status` returns
    without an error, the container is communicating effectively with RabbitMQ and
    should be fully able to process tasks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在脚本`celery_status.sh`中使用类似的结构，该脚本用于提供用于worker容器的活动性和可用性探针的exec命令，其关键思想是如果命令`celery
    status`返回而不出现错误，则容器正在有效地与RabbitMQ通信，并且应完全能够处理任务。
- en: The code that contains the logic that will be invoked is all in `celery_conf.py:`
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 包含将被调用的逻辑的代码都在`celery_conf.py`中：
- en: '![](assets/e8492df5-f86d-4f03-8d02-cb9e5a2a3834.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e8492df5-f86d-4f03-8d02-cb9e5a2a3834.png)'
- en: You can see that we again make use of environment variables to get the values
    needed to communicate with RabbitMQ (a hostname, username, password, and `vhost`)
    and assemble these from environment variables with defaults, if they're not provided.
    The hostname default (`message-queue`) also matches the service name in our service
    definition that fronts RabbitMQ, giving us a stable default. The remainder of
    the code comes from the Celery documentation, supplying two sample tasks that
    we can import and use separately as well.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，我们再次利用环境变量来获取与RabbitMQ通信所需的值（主机名、用户名、密码和`vhost`），并从环境变量中组装这些默认值，如果未提供。主机名默认值（`message-queue`）也与我们的服务定义中的服务名称匹配，该服务定义了RabbitMQ的前端，为我们提供了一个稳定的默认值。代码的其余部分来自Celery文档，提供了两个示例任务，我们也可以分别导入和使用。
- en: 'You can deploy the worker using this command:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令部署worker：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This should report the deployment created, such as:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该报告已创建的部署，例如：
- en: '[PRE26]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And now, you should have two deployments running together. You can verify this
    with `kubectl get pods`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该有两个部署一起运行。您可以使用`kubectl get pods`来验证这一点：
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To watch the system a bit more interactively, run this command:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要更加交互地观察系统，请运行此命令：
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will stream the logs from the `celery-worker`, for example:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从`celery-worker`流式传输日志，例如：
- en: '![](assets/17a0f4de-79ed-45b2-8d96-f703686865a8.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/17a0f4de-79ed-45b2-8d96-f703686865a8.png)'
- en: 'This will display the logs from the `celery-worker` deployment, as they happen.
    Open a second Terminal window and invoke the following command to run a temporary
    pod with an interactive shell:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示`celery-worker`部署的日志，因为它们发生。打开第二个终端窗口并调用以下命令以运行一个临时pod并获得交互式shell：
- en: '[PRE29]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Within the shell, you can now run the script to generate some tasks for the
    worker to process:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在shell中，您现在可以运行脚本来生成一些任务供worker处理：
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'An example of this script is:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本的一个例子是：
- en: '![](assets/a99ef267-0e69-4f89-b00e-69f57f3a17b0.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/a99ef267-0e69-4f89-b00e-69f57f3a17b0.png)'
- en: 'This script will run indefinitely, invoking the two sample tasks in the worker,
    roughly every five seconds, and in the window that is showing you the logs, you
    should see the output update with logged results from the Celery worker:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本将无限期地运行，大约每五秒调用一次worker中的两个示例任务，在显示日志的窗口中，您应该看到输出更新，显示来自Celery worker的记录结果：
- en: '![](assets/15f20ba0-d087-4589-8b67-98d7c1aec03b.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/15f20ba0-d087-4589-8b67-98d7c1aec03b.png)'
- en: Persistence with Kubernetes
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes的持久性
- en: So far, all our examples, and even code, have been essentially stateless. In
    the last chapter, we introduced a container using Redis, but didn't specify anything
    special for it. By default, Kubernetes will assume any resources associated with
    a pod are ephemeral, and if the node fails, or a deployment is deleted, all the
    associated resources can and will be deleted with it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所有的例子，甚至代码，都基本上是无状态的。在上一章中，我们介绍了使用Redis的容器，但没有为它指定任何特殊的东西。默认情况下，Kubernetes将假定与pod关联的任何资源都是临时的，如果节点失败或部署被删除，所有关联的资源都可以并且将被删除。
- en: That said, almost all the work we do requires storing and maintaining state
    somewhere—a database, an object store, or even a persistent, in-memory queue.
    Kubernetes includes support for persistence, and as of this writing, it's still
    changing and evolving fairly rapidly.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们所做的几乎所有工作都需要在某个地方存储和维护状态——数据库、对象存储，甚至是持久的内存队列。Kubernetes包括对持久性的支持，截至目前为止，它仍在快速变化和发展。
- en: Volumes
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷
- en: The earliest support in Kubernetes was for volumes, which can be defined by
    the cluster administrator, and we've already seen some variations of this construct
    with the configuration being exposed into a container using the Downward API back
    in [Chapter 4](a210420d-4d80-43c1-9acb-531bc6b19b75.xhtml), *Declarative Infrastructure*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes最早的支持是卷，可以由集群管理员定义，并且我们已经看到了一些这种构造的变体，配置被暴露到容器中使用Downward API在[第4章](a210420d-4d80-43c1-9acb-531bc6b19b75.xhtml)中，*声明式基础设施*。
- en: Another kind of volume that can be easily used is `emptyDir`, which you can
    use in a pod specification to create an empty directory, and mount it to one or
    more of the containers in your pod. This is typically created on whatever storage
    the local node has available, but includes an option to specify a medium of *memory*,
    which you can use to make a temporary memory-backed filesystem. This takes up
    more memory on the node, but creates a very fast, ephemeral file system for use
    by your pods. If your code wants to use some scratch space on disk, maintain a
    periodic checkpoint, or load ephemeral content, this can be a very good way of
    managing that space.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可以轻松使用的卷是`emptyDir`，您可以在pod规范中使用它来创建一个空目录，并将其挂载到一个或多个容器中。这通常在本地节点可用的存储上创建，但包括一个选项来指定*memory*的介质，您可以使用它来创建一个临时的内存支持文件系统。这会占用节点上更多的内存，但为您的pod创建一个非常快速的临时文件系统。如果您的代码想要在磁盘上使用一些临时空间，保持定期的检查点，或者加载临时内容，这可能是一个非常好的管理空间的方法。
- en: As we specified in the configuration, when you use a volume, you specify it
    under volumes and make a related entry under `volumeMounts` that indicates where
    you're using it on each container.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在配置中指定的那样，当您使用卷时，您将其指定在卷下，并在`volumeMounts`下进行相关条目，指示您在每个容器上使用它的位置。
- en: 'We can modify our Flask example application to have a cache space that is a
    memory-backed temporary space:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以修改我们的Flask示例应用程序，使其具有一个内存支持的临时空间：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If we deploy this version of the specification and open an interactive shell
    in the container, you can see `/opt/cache` listed as a volume of type `tmpfs`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们部署规范的这个版本并在容器中打开一个交互式shell，你可以看到`/opt/cache`被列为`tmpfs`类型的卷：
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If we had specified the volume without the medium of type `Memory`, the directory
    would show up on the local disk instead:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有指定类型为“Memory”的介质，那么该目录将显示在本地磁盘上：
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: If you are using volumes on a cloud provider, then you can use one of their
    persistent volumes. In these cases, you need to have created a persistent disk
    at the cloud provider that is accessible to the nodes where you have your Kubernetes
    cluster, but this allows you to have data exist beyond the lifetime of any pod
    or node. The volumes for each cloud provider are specific to each provider, for
    example, `awsElasticBlockStore`, `azureDisk`, or `gcePersistentDisk`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在云服务提供商上使用卷，那么您可以使用他们的持久卷之一。在这些情况下，您需要在云服务提供商那里创建一个持久磁盘，该磁盘对您的Kubernetes集群中的节点是可访问的，但这样可以使数据存在于任何pod或节点的生命周期之外。每个云提供商的卷都是特定于该提供商的，例如`awsElasticBlockStore`、`azureDisk`或`gcePersistentDisk`。
- en: There are a number of other volume types available, and most of these depend
    on how your cluster was set up and what might be available in that setup. You
    can get a sense of all the supported volumes from the formal documentation for
    volumes at [https://kubernetes.io/docs/concepts/storage/volumes/.](https://kubernetes.io/docs/concepts/storage/volumes/)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他类型的卷可用，大多数取决于您的集群是如何设置的以及该设置中可能有什么可用的。您可以从卷的正式文档[https://kubernetes.io/docs/concepts/storage/volumes/](https://kubernetes.io/docs/concepts/storage/volumes/)中了解所有支持的卷。
- en: PersistentVolume and PersistentVolumeClaim
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久卷和持久卷索赔
- en: 'If you want to use persistent volumes, independent of the specific location
    where you have built your cluster, you probably want to take advantage of two
    newer Kubernetes resources: `PersistentVolume` and `PersistentVolumeClaim`. These
    separate the specifics of how volumes are provided and allow you to specify more
    of how you expect those volumes to be used, with both falling under the idea of
    dynamic volume provisioning—meaning that when you deploy your code into Kubernetes,
    the system should make any persistent volumes available from disks that have already
    been identified. The Kubernetes administrator will need to specify at least one,
    and possibly more, storage classes, which are used to define the general behavior
    and backing stores for persistent volumes available to the cluster. If you''re
    using Kubernetes on Amazon Web Services, Google Compute Engine, or Microsoft''s
    Azure, the public offerings all have storage classes predefined and available
    for use. You can see the default storage classes and how they are defined in the
    documentation at [https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/).
    If you are using Minikube locally to try things out, it also comes with a default
    storage class defined, which uses the volume type of `HostPast`.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要使用持久卷，而不受构建集群的特定位置的限制，您可能想要利用两个较新的Kubernetes资源：`PersistentVolume`和`PersistentVolumeClaim`。这些资源将提供卷的具体细节与您期望使用这些卷的方式分开，两者都属于动态卷分配的概念，这意味着当您将代码部署到Kubernetes时，系统应该从已经识别的磁盘中提供任何持久卷。Kubernetes管理员将需要指定至少一个，可能更多的存储类，用于定义可供集群使用的持久卷的一般行为和后备存储。如果您在亚马逊网络服务、谷歌计算引擎或微软的Azure上使用Kubernetes，这些公共服务都预定义了存储类可供使用。您可以在文档[https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/)中查看默认的存储类及其定义。如果您在本地使用Minikube进行尝试，它也有一个默认的存储类定义，使用的卷类型是`HostPath`。
- en: Defining a `PersistentVolumeClaim` to use with your code in a deployment is
    very much like defining the configuration volume or cache with `EmptyDir`, with
    the exception that you will need to make a `persistentVolumeClaim` resource before
    you reference it in your pod specification.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`PersistentVolumeClaim`以与部署中的代码一起使用非常类似于使用`EmptyDir`定义配置卷或缓存，唯一的区别是您需要在引用它之前创建`persistentVolumeClaim`资源。
- en: 'An example `persistentVolumeClaim` that we might use for our Redis storage
    might be:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能用于Redis存储的`persistentVolumeClaim`示例可能是：
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will create a 1 GB volume available for our container to use. We can add
    this onto the Redis container to give it persistent storage by referencing this
    `persistentVolumeClaim` by name:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个可供我们容器使用的1GB卷。我们可以将其添加到Redis容器中，通过名称引用这个`persistentVolumeClaim`来给它提供持久存储：
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The `mountPath` of `/data` was chosen to match how the Redis container was built.
    If we look at the documentation for that container (from [https://hub.docker.com/_/redis/](https://hub.docker.com/_/redis/)),
    we can see that the built-in configuration expects all data to be used from the `/data` path,
    so we can override that path with our own `persistentVolumeClaim` in order to
    back that space with something that will live beyond the life cycle of the deployment.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 选择`/data`的`mountPath`是为了与Redis容器的构建方式相匹配。如果我们查看该容器的文档（来自[https://hub.docker.com/_/redis/](https://hub.docker.com/_/redis/)），我们可以看到内置配置期望所有数据都从`/data`路径使用，因此我们可以用我们自己的`persistentVolumeClaim`覆盖该路径，以便用一些能够在部署的生命周期之外存在的东西来支持该空间。
- en: 'If you deployed these changes to Minikube, you can see the resulting resources
    reflected within the cluster:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将这些更改部署到Minikube，您可以看到集群中反映出的结果资源：
- en: '[PRE36]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can also open an interactive Terminal into the Redis instance to see how
    it was set up:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打开一个交互式终端进入Redis实例，看看它是如何设置的：
- en: '[PRE37]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Stateful Sets
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态集
- en: Following dynamic provisioning, as you think about persistence systems – whether
    they are classic databases, key-value data stores, memory caches, or document-based
    datastores – it is common to want to have some manner of redundancy and failover.
    ReplicaSets and deployments go a fairly significant way to supporting some of
    that capability, especially with persistent volumes, but it would be greatly beneficial
    to these systems to have them more fully integrated with Kubernetes, so that we
    can leverage Kubernetes to handle the life cycle and coordination of these systems.
    A starting point for this effort is Stateful Sets, which act similarly to a deployment
    and ReplicaSet in that they manage a group of pods.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态配置之后，当您考虑持久性系统时，无论它们是经典数据库、键值数据存储、内存缓存还是基于文档的数据存储，通常希望具有某种冗余和故障转移的方式。 ReplicaSets和部署在支持某些功能方面走得相当远，特别是对于持久卷，但是将它们更完全地集成到Kubernetes中将极大地有利于这些系统，以便我们可以利用Kubernetes来处理这些系统的生命周期和协调。这项工作的起点是Stateful
    Sets，它们的工作方式类似于部署和ReplicaSet，因为它们管理一组pod。
- en: Stateful Sets differ from the other systems as they also support each pod having
    a stable, unique identity and specific ordered scaling, both up and down. Stateful
    Sets are relatively new in Kubernetes, first appearing in Kubernetes 1.5, and
    moving into beta in version 1.9\. Stateful Sets also work closely with a specific
    service we touched upon earlier, a Headless Service, which needs to be created
    prior to the Stateful Set, and is responsible for the network identify of the
    pods.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Stateful Sets与其他系统不同，它们还支持每个pod具有稳定的、唯一的标识和特定的有序扩展，无论是向上还是向下。 Stateful Sets在Kubernetes中相对较新，首次出现在Kubernetes
    1.5中，并在1.9版本中进入beta。 Stateful Sets还与我们之前提到的特定服务密切合作，即无头服务，需要在Stateful Set之前创建，并负责pod的网络标识。
- en: 'As a reminder, a Headless Service is a service that does not have a specific
    cluster IP, and instead provides a unique service identity to all the pods associated
    with it as individual endpoints. This means that any system consuming the service
    will need to know that the service has identity-specific endpoints, and needs
    to be able to communicate with the one it wants. When a Stateful Set is created
    along with a headless service to match it, the pods will get an identity based
    on the name of the Stateful Set and then an ordinal number. For example, if we
    created a Stateful Set called datastore and requested three replicas, then the
    pods would be created as `datastore-0`, `datastore-1`, and `datastore-2`. Stateful
    Sets also have a `serviceName` field that gets included in the domain name of
    the service. To complete this example, if we set the `serviceName` to `db`, the
    associated DNS entries created for the pods would be:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，无头服务是一种没有特定集群 IP 的服务，而是为与其关联的所有 pods 提供独特的服务标识作为单独的端点。这意味着任何使用该服务的系统都需要知道该服务具有特定标识的端点，并且需要能够与其想要通信的端点进行通信。当创建一个与无头服务相匹配的
    Stateful Set 时，pods 将根据 Stateful Set 的名称和序号获得一个标识。例如，如果我们创建了一个名为 datastore 的 Stateful
    Set 并请求了三个副本，那么 pods 将被创建为 `datastore-0`、`datastore-1` 和 `datastore-2`。Stateful
    Sets 还有一个 `serviceName` 字段，该字段包含在服务的域名中。以完成这个示例，如果我们将 `serviceName` 设置为 `db`，那么为
    pods 创建的关联 DNS 条目将是：
- en: '`datastore-0.db.[namespace].svc.cluster.local`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datastore-0.db.[namespace].svc.cluster.local`'
- en: '`datastore-1.db.[namespace].svc.cluster.local`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datastore-1.db.[namespace].svc.cluster.local`'
- en: '`datastore-2.db.[namespace].svc.cluster.local`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datastore-2.db.[namespace].svc.cluster.local`'
- en: As the number of replicas is changed, a Stateful Set will also explicitly and
    carefully add and remove pods. It terminates the pods sequentially from the highest
    number first, and will not terminate a higher-numbered pod until lower-numbered
    pods are reporting `Ready` and `Running`. As of Kubernetes 1.7, Stateful Sets
    introduced the ability to vary this with an optional field, `podManagementPolicy`.
    The default, `OrderedReady`, operates as described, with the alternative, `Parallel`,
    which does not operate sequentially nor require lower-numbered pods to be `Running`
    or `Ready` prior to terminating a pod.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随着副本数量的变化，Stateful Set 也会明确而谨慎地添加和删除 pods。它会按照从最高编号开始顺序终止 pods，并且不会在较低编号的 pods
    报告“Ready”和“Running”之前终止较高编号的 pods。从 Kubernetes 1.7 开始，Stateful Sets 引入了一个可选字段
    `podManagementPolicy` 来改变这一行为。默认值 `OrderedReady` 的操作如上所述，另一个选项 `Parallel` 则不会按顺序操作，也不需要较低编号的
    pods 处于“Running”或“Ready”状态才能终止一个 pod。
- en: Rolling updates, akin to a deployment, are also slightly different on Stateful
    Sets. It is defined by the `updateStrategy` optional field, and if not explicitly
    set, uses the `OnDelete` setting. With this setting, Kubernetes will not delete
    older pods, even after a spec update, requiring you to manually delete those pods.
    When you do, the system will automatically recreate the pods, according to the
    updated spec. The other value is `RollingUpdate`, which acts more akin to a deployment
    in terminating and recreating the pods automatically, but following the ordering
    explicitly, and verifying that the pods are *ready and running* prior to continuing
    to update the next pod. `RollingUpdate` also has an additional (optional) field, `partition`,
    which if specified with a number, will have the `RollingUpdate` operate automatically
    on only a subset of the pods. For example, if partition was set to `3` and you
    had `6` replicas, then only pods `3`, `4`, and `5` would be updated automatically
    as the spec was updated. Pods `0`, `1`, and `2` would be left alone, and even
    if they were manually deleted, they will be recreated at the previous version.
    The partition capability can be useful to stage an update or to perform a phased
    rollout.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新，类似于部署，对于Stateful Sets也略有不同。它由`updateStrategy`可选字段定义，如果未明确设置，则使用`OnDelete`设置。使用此设置，Kubernetes不会删除旧的pod，即使在规范更新后，需要您手动删除这些pod。当您这样做时，系统将根据更新后的规范自动重新创建pod。另一个值是`RollingUpdate`，它更类似于部署，会自动终止和重新创建pod，但会明确遵循顺序，并验证pod在继续更新下一个pod之前是否*准备就绪和运行*。`RollingUpdate`还有一个额外的（可选）字段，`partition`，如果指定了一个数字，将使`RollingUpdate`自动在一部分pod上操作。例如，如果分区设置为`3`，并且有`6`个副本，则只有pod
    `3`，`4`和`5`会在规范更新时自动更新。Pod `0`，`1`和`2`将被保留，即使它们被手动删除，它们也将以先前的版本重新创建。分区功能可用于分阶段更新或执行分阶段部署。
- en: A Node.js example using Stateful Set
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Stateful Set的Node.js示例
- en: The code within the application doesn't have any need for the Stateful Set mechanics,
    but let's use it as an easy-to-understand update to show how you might use a Stateful
    Set and how to watch it operate.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序内的代码不需要Stateful Set机制，但让我们将其用作易于理解的更新，以展示您可能如何使用Stateful Set以及如何观察其运行。
- en: 'The code for this update is available on GitHub with the project at: [https://github.com/kubernetes-for-developers/kfd-nodejs](https://github.com/kubernetes-for-developers/kfd-nodejs)
    on the branch 0.4.0\. The project''s code wasn''t changed, except that the deployment
    specification was changed to a Stateful Set. You can use the following commands
    to get the code for this version:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此更新的代码可在GitHub上的项目中找到：[https://github.com/kubernetes-for-developers/kfd-nodejs](https://github.com/kubernetes-for-developers/kfd-nodejs)，分支为0.4.0。该项目的代码没有更改，只是将部署规范更改为Stateful
    Set。您可以使用以下命令获取此版本的代码：
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The service definition changed, removing the `Nodeport` type and setting `clusterIP`
    to `None`. The new definition for `nodejs-service` now reads:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 服务定义已更改，删除了`Nodeport`类型，并将`clusterIP`设置为`None`。现在`nodejs-service`的新定义如下：
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This sets up a headless service to use with the Stateful Set. The change from
    deployment to Stateful Set is equally simple, replacing type `Deployment` with
    type `StatefulSet` and adding values for `serviceName`, replicas, and setting
    a selector. I went ahead and added a datastore mount with a persistent volume
    claim as well, to show how that can integrate with your existing specification.
    The existing `ConfigMap`, `livenessProbe`, and `readinessProbe` settings were
    all maintained. The resulting `StatefulSet` specification now reads:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这将设置一个无头服务，用于与Stateful Set一起使用。从部署到Stateful Set的更改同样简单，将类型`Deployment`替换为类型`StatefulSet`，并添加`serviceName`、副本和设置选择器的值。我还添加了一个带有持久卷索赔的数据存储挂载，以展示它如何与您现有的规范集成。现有的`ConfigMap`、`livenessProbe`和`readinessProbe`设置都得到了保留。最终的`StatefulSet`规范现在如下所示：
- en: '[PRE40]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Since we updated the code in the last chapter to use Redis with its readiness
    probes, we will want to make sure we have Redis up and running for this Stateful
    Set to advance. You can deploy the updated Redis service definition set with this
    command:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 自上一章中更新了代码以使用带有就绪探针的Redis后，我们将希望确保我们的Redis已经运行起来，以便这个Stateful Set能够继续进行。您可以使用以下命令部署更新后的Redis服务定义集：
- en: '[PRE41]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, we can take advantage of the watch option (`-w`) with `kubectl get` to
    watch how Kubernetes sets up the Stateful Set and carefully progresses. Open an
    additional terminal window and run this command:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以利用`kubectl get`的观察选项（`-w`）来观察Kubernetes如何设置Stateful Set并进行仔细的进展。打开一个额外的终端窗口并运行以下命令：
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: At first, you shouldn't see any output, but updates will appear as Kubernetes
    advances through the Stateful Set.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，您不应该看到任何输出，但随着Kubernetes在Stateful Set中的进展，更新将会出现。
- en: 'In your original Terminal window, deploy the specification we''ve updated to
    be a `StatefulSet` with this command:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在您原始的终端窗口中，使用以下命令部署我们已更新为`StatefulSet`的规范：
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You should see a response that the `service`, `configmap`, and `statefulset`
    objects were all created:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到`service`、`configmap`和`statefulset`对象都已创建的响应：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In the window where you''re watching the pods, you should see output start
    to appear as the first container comes online. A line will appear in the output,
    every time the watch triggers that there''s an update to one of the pods matching
    the descriptor we set (`-l app=nodejs`):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在您观看pod的窗口中，当第一个容器上线时，您应该会看到输出开始出现。每当观察触发器发现我们设置的描述符（`-l app=nodejs`）中的一个pod有更新时，输出中会出现一行：
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The definition we set has five replicas, so five pods will be generated in
    all. You can see the status of that rollout with this command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置的定义有五个副本，因此总共会生成五个pod。您可以使用以下命令查看该部署的状态：
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'In the preceding command, `sts` is a shortcut for `statefulset`. You can also
    get a more detailed view on the current state in a human-readable form with this
    command:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，`sts`是`statefulset`的缩写。您还可以使用以下命令以人类可读的形式获得当前状态的更详细视图：
- en: '[PRE48]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](assets/c12e0f06-ca4c-40e6-af6e-b80d2b8854b0.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/c12e0f06-ca4c-40e6-af6e-b80d2b8854b0.png)'
- en: 'If you edit the specification, change the replicas to two, and then apply the
    changes, you will see the pods get torn down in the reverse order to how they
    were set up—highest ordinal number first. The following command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您编辑规范，将副本更改为两个，然后应用更改，您将看到pod按照设置的相反顺序被拆除，最高序数号先。以下命令：
- en: '[PRE49]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Should report back:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 应该报告：
- en: '[PRE50]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: And in the window where you're watching the pods, you will see `nodejs-4` start
    terminating, and it will continue until `nodejs-3` and then `nodejs-2` terminates.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在观看pod的窗口中，您将看到`nodejs-4`开始终止，并且会一直持续到`nodejs-3`，然后是`nodejs-2`终止。
- en: 'If you were to run a temporary `pod` to look at DNS:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行一个临时的`pod`来查看DNS：
- en: '[PRE51]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You can use the `nslookup` command to verify the DNS values for the `pods`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`nslookup`命令验证`pods`的DNS值：
- en: '[PRE52]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Custom Resource Definition
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义资源定义
- en: Stateful Sets don't automatically match all the persistent stores that are available,
    and some of them have even more complex logic requirements for managing the life
    cycle of the application. As Kubernetes looked at how to support extending its
    controllers to support more complex logic, the project started with the idea of
    Operators, external code that could be included in the Kubernetes project, and
    has evolved as of Kubernetes 1.8 to make this more explicit with `CustomResourceDefinitions`.
    A custom resource extends the Kubernetes API, and allows for custom API objects
    to be created and matched with a custom controller that you can also load into
    Kubernetes to handle the life cycle of those objects.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Stateful Sets并不自动匹配所有可用的持久存储，其中一些甚至对管理应用程序的生命周期有更复杂的逻辑要求。随着Kubernetes考虑如何支持扩展其控制器以支持更复杂的逻辑，该项目从Operators的概念开始，即可以包含在Kubernetes项目中的外部代码，并且截至Kubernetes
    1.8版本已经发展为更明确地使用`CustomResourceDefinitions`。自定义资源扩展了Kubernetes API，并允许创建自定义API对象，并与自定义控制器匹配，您还可以将其加载到Kubernetes中以处理这些对象的生命周期。
- en: 'Custom Resource Definitions go beyond the scope of what we will cover in this
    book, although you should be aware that they exist. You can get more details about
    Custom Resource Definitions and how to extend Kubernetes at the project''s documentation
    site: [https://kubernetes.io/docs/concepts/api-extension/custom-resources/.](https://kubernetes.io/docs/concepts/api-extension/custom-resources/)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源定义超出了我们在本书中将涵盖的范围，尽管您应该意识到它们的存在。您可以在项目的文档站点上获取有关自定义资源定义以及如何扩展Kubernetes的更多详细信息：[https://kubernetes.io/docs/concepts/api-extension/custom-resources/.](https://kubernetes.io/docs/concepts/api-extension/custom-resources/)
- en: There are a number of Operators available via open source projects that utilize
    Custom Resource Definitions to manage specific applications within Kubernetes.
    The CoreOS team supports operators and custom resources for managing Prometheus
    and `etcd`. There is also an open source storage resource and associated technology
    called Rook, which functions using Custom Resource Definitions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多通过开源项目提供的Operators可利用自定义资源定义来管理Kubernetes中特定的应用程序。CoreOS团队支持用于管理Prometheus和`etcd`的Operators和自定义资源。还有一个名为Rook的开源存储资源和相关技术，它使用自定义资源定义来运行。
- en: The broad set of how to best run persistent stores with Kubernetes is still
    evolving, as of this writing. There are numerous examples of how you can run a
    database or NoSQL data store of your choice in Kubernetes that also supports redundancy
    and failover. Most of these systems were created with a variety of mechanisms
    to support managing them, and few of them have much support for automated scaling
    and redundancy. There are a number of techniques supporting a wide variety of
    data stores that are available as examples. Some of the more complex systems use
    Operators and these Custom Resource Definitions; others use sets of pods and containers
    with simpler Replica Sets to achieve their goals.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，如何最好地在Kubernetes中运行持久存储的广泛集合仍在不断发展。有许多示例可以演示如何在Kubernetes中运行您选择的数据库或NoSQL数据存储，同时还支持冗余和故障转移。这些系统大多是通过各种机制来支持其管理，其中很少有系统对自动扩展和冗余提供了很多支持。有许多支持各种数据存储的技术可作为示例。一些更复杂的系统使用Operators和这些自定义资源定义；其他系统使用一组Pod和容器以更简单的复制集来实现其目标。
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we reviewed jobs and CronJobs, which Kubernetes provides to
    support batch and scheduled batch processing, respectively. We also looked through
    a Python example of how to set up a Celery worker queue with RabbitMQ and configure
    the two deployments to work together. We then looked at how Kubernetes can provide
    persistence with volumes, `PersistentVolume`, and its concept of `PersistentVolumeClaims`
    for automatically creating volumes for deployments as needed. Kubernetes also
    supports Stateful Sets for a variation of deployment that requires stable identity
    and persistent volumes, and we looked at a simple Node.js example converting our
    previous example of a deployment into a Stateful Set. We finished the chapter
    with a look at Custom Resource Definitions, used to extend Kubernetes.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了Kubernetes提供的作业和CronJobs，以支持批处理和定期批处理处理。我们还通过一个Python示例来了解如何使用RabbitMQ设置Celery工作队列，并配置这两个部署以共同工作。然后，我们看了一下Kubernetes如何通过卷、`PersistentVolume`和其自动创建部署所需卷的`PersistentVolumeClaims`的概念来提供持久性。Kubernetes还支持Stateful
    Sets，用于需要稳定标识和持久卷的部署变体，我们看了一个简单的Node.js示例，将我们之前的部署示例转换为Stateful Set。最后，我们通过查看用于扩展Kubernetes的自定义资源定义来结束本章。
- en: In the next chapter, we start to look at how to leverage Kubernetes to get information
    about all these structures. We review how to capture and view metrics, leveraging
    Kubernetes and additional open source projects, as well as examples of collating
    logs from the horizontally-scaled systems that Kubernetes encourages.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们开始研究如何利用Kubernetes获取所有这些结构的信息。我们将回顾如何捕获和查看指标，利用Kubernetes和其他开源项目，以及从Kubernetes鼓励的水平扩展系统中整理日志的示例。
