- en: Scaling and Upgrading Applications
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展和升级应用程序
- en: TERNAIn this chapter, we will discuss the methods and strategies that we can
    use to dynamically scale containerized services running on Kubernetes to handle
    the changing traffic needs of our service. After following the recipes in this
    chapter, you will have the skills needed to create load balancers to distribute
    traffic to multiple workers and increase bandwidth. You will also know how to
    handle upgrades in production with minimum downtime.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论可以使用的方法和策略，以动态地扩展在Kubernetes上运行的容器化服务，以处理我们服务的不断变化的流量需求。在本章的配方中，您将掌握创建负载均衡器以将流量分发到多个工作节点并增加带宽所需的技能。您还将了解如何在生产环境中处理升级以最小化停机时间。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下配方：
- en: Scaling applications on Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes上扩展应用程序
- en: Assigning applications to nodes with priority
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为节点分配应用程序的优先级
- en: Creating an external load balancer
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建外部负载均衡器
- en: Creating an ingress service and service mesh using Istio
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Istio创建入口服务和服务网格
- en: Creating an ingress service and service mesh using Linkerd
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Linkerd创建入口服务和服务网格
- en: Auto-healing pods in Kubernetes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes中自动修复Pod
- en: Managing upgrades through blue/green deployments
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过蓝/绿部署管理升级
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The recipes in this chapter assume that you have a functional Kubernetes cluster
    deployed by following one of the recommended methods described in [Chapter 1](a8580410-3e1c-4e28-8d18-aaf9d38d011f.xhtml),
    *Building Production-Ready Kubernetes Clusters*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的配方假定您已通过[第1章](a8580410-3e1c-4e28-8d18-aaf9d38d011f.xhtml)中描述的推荐方法之一部署了一个功能齐全的Kubernetes集群，*构建生产就绪的Kubernetes集群*。
- en: The Kubernetes command-line tool, `kubectl`, will be used for the rest of the
    recipes in this chapter since it's the main command-line interface for running
    commands against Kubernetes clusters. We will also use helm where Helm charts
    are available to deploy solutions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes命令行工具`kubectl`将在本章的其余部分中用于配方，因为它是针对Kubernetes集群运行命令的主要命令行界面。我们还将在Helm图表可用的情况下使用helm来部署解决方案。
- en: Scaling applications on Kubernetes
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上扩展应用程序
- en: In this section, we will perform application and cluster scaling tasks. You
    will learn how to manually and also automatically scale your service capacity
    up or down in Kubernetes to support dynamic traffic.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将执行应用程序和集群扩展任务。您将学习如何在Kubernetes中手动和自动地扩展服务容量，以支持动态流量。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Clone the `k8sdevopscookbook/src` repository to your workstation to use the
    manifest files in the `chapter7` directory, as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 将`k8sdevopscookbook/src`存储库克隆到您的工作站，以便使用`chapter7`目录中的清单文件，如下所示：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已准备好一个Kubernetes集群，并配置了`kubectl`和`helm`来管理集群资源。
- en: How to do it…
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤：
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分进一步分为以下子部分，以使此过程更加简单：
- en: Validating the installation of Metrics Server
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证Metrics Server的安装
- en: Manually scaling an application
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动扩展应用程序
- en: Autoscaling applications using Horizontal Pod Autoscaler
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用水平Pod自动缩放器自动缩放应用程序
- en: Validating the installation of Metrics Server
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证Metrics Server的安装
- en: 'The *Autoscaling applications using the Horizontal Pod Autoscaler* recipe in
    this section also requires Metrics Server to be installed on your cluster. Metrics
    Server is a cluster-wide aggregator for core resource usage data. Follow these
    steps to validate the installation of Metrics Server:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的*使用水平Pod自动缩放器自动缩放应用程序*配方还需要在您的集群上安装Metrics Server。Metrics Server是用于核心资源使用数据的集群范围聚合器。按照以下步骤验证Metrics
    Server的安装：
- en: 'Confirm if you need to install Metrics Server by running the following command:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令确认是否需要安装Metrics Server：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If it''s been installed correctly, you should see the following node metrics:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果安装正确，您应该看到以下节点指标：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you get an error message stating `metrics not available yet`, then you need
    to follow the steps provided in the next chapter in the *Adding metrics using
    the Kubernetes Metrics Server* recipe to install Metrics Server.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果收到错误消息，指出`尚未提供指标`，则需要按照*使用Kubernetes Metrics Server添加指标*配方中提供的步骤安装Metrics
    Server。
- en: Manually scaling an application
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动扩展应用程序
- en: When the usage of your application increases, it becomes necessary to scale
    the application up. Kubernetes is built to handle the orchestration of high-scale
    workloads.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的应用程序的使用量增加时，有必要将应用程序扩展。Kubernetes被设计用来处理高规模工作负载的编排。
- en: 'Let''s perform the following steps to understand how to manually scale an application:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤，了解如何手动扩展应用程序：
- en: 'Change directories to `/src/chapter7/charts/node`, which is where the local
    clone of the example repository that you created in the *Getting ready* section
    can be found:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改目录到`/src/chapter7/charts/node`，这是您在*准备就绪*部分创建的示例存储库的本地克隆所在的位置：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Install the To-Do application example using the following command. This Helm
    chart will deploy two pods, including a Node.js service and a MongoDB service:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装待办事项应用程序示例。这个Helm图表将部署两个pod，包括一个Node.js服务和一个MongoDB服务：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Get the service IP of `my-ch7-app-node` to connect to the application. The
    following command will return an external address for the application:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取`my-ch7-app-node`的服务IP以连接到应用程序。以下命令将返回应用程序的外部地址：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Open the address from *Step 3* in a web browser. You will get a fully functional
    To-Do application:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Web浏览器中打开*步骤3*中的地址。您将获得一个完全功能的待办事项应用程序：
- en: '![](assets/1e02a226-7f1d-4804-98f7-6176144d9845.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1e02a226-7f1d-4804-98f7-6176144d9845.png)'
- en: 'Check the status of the application using `helm status`. You will see the number
    of pods that have been deployed as part of the deployment in the `Available` column:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`helm status`检查应用程序的状态。您将看到已部署的pod数量在`Available`列中：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Scale the node pod to `3` replicas from the current scale of a single replica:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将节点pod的规模从当前的单个副本扩展到`3`个副本：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Check the status of the application again and confirm that, this time, the
    number of available replicas is `3` and that the number of `my-ch7-app-node` pods
    in the `v1/Pod` section has increased to `3`:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次检查应用程序的状态，并确认，这次可用副本的数量为`3`，`v1/Pod`部分中的`my-ch7-app-node` pod数量已增加到`3`：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To scale down your application, repeat *Step 5*, but this time with `2` replicas:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要缩小应用程序的规模，请重复*步骤5*，但这次使用`2`个副本：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With that, you've learned how to scale your application when needed. Of course,
    your Kubernetes cluster resources should be able to support growing workload capacities
    as well. You will use this knowledge to test the service healing functionality
    in the *Auto-healing pods in Kubernetes* recipe.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，您学会了如何在需要时扩展您的应用程序。当然，您的Kubernetes集群资源也应该能够支持不断增长的工作负载能力。
- en: The next recipe will show you how to autoscale workloads based on actual resource
    consumption instead of manual steps.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个配方将向您展示如何根据实际资源消耗而不是手动步骤来自动缩放工作负载。
- en: Autoscaling applications using a Horizontal Pod Autoscaler
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用水平Pod自动缩放器自动缩放应用程序
- en: 'In this recipe, you will learn how to create a **Horizontal Pod Autoscaler**
    (**HPA**) to automate the process of scaling the application we created in the
    previous recipe. We will also test the HPA with a load generator to simulate a
    scenario of increased traffic hitting our services. Follow these steps:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您将学习如何创建**水平Pod自动缩放器**（**HPA**）来自动化我们在上一个教程中创建的应用程序的扩展过程。我们还将使用负载生成器测试HPA，模拟增加流量击中我们的服务的情景。请按照以下步骤操作：
- en: 'First, make sure you have the sample To-Do application deployed from the *Manually
    scaling an application* recipe. When you run the following command, you should
    get both MongoDB and Node pods listed:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，确保您已经部署了*手动扩展应用程序*中的示例待办事项应用程序。当您运行以下命令时，您应该会看到MongoDB和Node pods的列表：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Create an HPA declaratively using the following command. This will automate
    the process of scaling the application between `1` to `5` replicas when the `targetCPUUtilizationPercentage`
    threshold is reached. In our example, the mean of the pods'' CPU utilization target
    is set to `50` percent usage. When the utilization goes over this threshold, your
    replicas will be increased:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令声明性地创建HPA。这将自动化在达到`targetCPUUtilizationPercentage`阈值时在`1`到`5`个副本之间扩展应用程序的过程。在我们的示例中，pod的CPU利用率目标的平均值设置为`50`％。当利用率超过此阈值时，您的副本将增加：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Although the results may be the same most of the time, a declarative configuration
    requires an understanding of the Kubernetes object configuration specs and file
    format. As an alternative, `kubectl` can be used for the imperative management
    of Kubernetes objects.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果大多数情况下可能是相同的，但声明性配置需要理解Kubernetes对象配置规范和文件格式。作为替代方案，`kubectl`可以用于对Kubernetes对象进行命令式管理。
- en: Note that you must have a request set in your deployment to use autoscaling.
    If you do not have a request for CPU in your deployment, the HPA will deploy but
    will not work correctly.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您必须在部署中设置CPU请求才能使用自动缩放。如果您的部署中没有CPU请求，HPA将部署但不会正常工作。
- en: You can also create the same `HorizontalPodAutoscaler` imperatively by running
    the `$ kubectl autoscale deployment my-ch7-app-node --cpu-percent=50 --min=1 --max=5`
    command.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过运行`$ kubectl autoscale deployment my-ch7-app-node --cpu-percent=50 --min=1
    --max=5`命令来命令式地创建相同的`HorizontalPodAutoscaler`。
- en: 'Confirm the number of current replicas and the status of the HPA. When you
    run the following command, the number of replicas should be `1`:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认当前副本的数量和HPA的状态。当您运行以下命令时，副本的数量应为`1`：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Get the service IP of `my-ch7-app-node` so that you can use it in the next
    step:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取`my-ch7-app-node`的服务IP，以便在下一步中使用：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Start a new Terminal window and create a load generator to test the HPA. Make
    sure that you replace `YOUR_SERVICE_IP` in the following code with the actual
    service IP from the output of *Step 4*. This command will generate traffic to
    your To-Do application:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的终端窗口并创建一个负载生成器来测试HPA。确保您在以下代码中用实际服务IP替换`YOUR_SERVICE_IP`。此命令将向您的待办事项应用程序生成流量：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Wait a few minutes for the Autoscaler to respond to increasing traffic. While
    the load generator is running on one Terminal, run the following command on a
    separate Terminal window to monitor the increased CPU utilization. In our example,
    this is set to `210%`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待几分钟，使自动缩放器对不断增加的流量做出响应。在一个终端上运行负载生成器的同时，在另一个终端窗口上运行以下命令，以监视增加的CPU利用率。在我们的示例中，这被设置为`210％`：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, check the deployment size and confirm that the deployment has been resized
    to `5` replicas as a result of the increased workload:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，检查部署大小并确认部署已调整为`5`个副本，以应对工作负载的增加：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: On the Terminal screen where you run the load generator, press *Ctrl* + *C* to
    terminate the load generator. This will stop the traffic coming to your application.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行负载生成器的终端屏幕上，按下*Ctrl* + *C*来终止负载生成器。这将停止发送到您的应用程序的流量。
- en: 'Wait a few minutes for the Autoscaler to adjust and then verify the HPA status
    by running the following command. The current CPU utilization should be lower.
    In our example, it shows that it went down to `0%`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待几分钟，让自动缩放器进行调整，然后通过运行以下命令来验证HPA状态。当前的CPU利用率应该更低。在我们的示例中，它显示下降到`0%`：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Check the deployment size and confirm that the deployment has been scaled down
    to `1` replica as the result of stopping the traffic generator:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查部署大小，并确认部署已经因停止流量生成器而缩减到`1`个副本：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this recipe, you learned how to automate how an application is scaled dynamically
    based on changing metrics. When applications are scaled up, they are dynamically
    scheduled on existing worker nodes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，您学会了如何根据不断变化的指标动态地自动化应用程序的扩展。当应用程序被扩展时，它们会动态地调度到现有的工作节点上。
- en: How it works...
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: This recipe showed you how to manually and automatically scale the number of
    pods in a deployment dynamically based on the Kubernetes metric.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程向您展示了如何根据Kubernetes指标动态地手动和自动地扩展部署中的Pod数量。
- en: 'In this recipe, in *Step 2*,we created an Autoscaler that adjusts the number
    of replicas between the defined minimum using `minReplicas: 1` and `maxReplicas:
    5`. As shown in the following example, the adjustment criteria are triggered by
    the `targetCPUUtilizationPercentage: 50` metric:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个教程中，在*步骤2*中，我们创建了一个自动缩放器，它会在`minReplicas: 1`和`maxReplicas: 5`之间调整副本的数量。如下例所示，调整标准由`targetCPUUtilizationPercentage:
    50`指标触发：'
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`targetCPUUtilizationPercentage` was used with the `autoscaling/v1` APIs. You
    will soon see that `targetCPUUtilizationPercentage` will be replaced with an array
    called metrics.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`targetCPUUtilizationPercentage`是与`autoscaling/v1`API一起使用的。您很快将看到`targetCPUUtilizationPercentage`将被一个名为metrics的数组所取代。'
- en: 'To understand the new metrics and custom metrics, run the following command.
    This will return the manifest we created with V1 APIs into a new manifest using
    V2 APIs:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解新的指标和自定义指标，运行以下命令。这将返回我们使用V1 API创建的清单到使用V2 API的新清单：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This enables you to specify additional resource metrics. By default, CPU and
    memory are the only supported resource metrics. In addition to these resource
    metrics, v2 APIs enable two other types of metrics, both of which are considered
    custom metrics: per-pod custom metrics and object metrics. You can read more about
    this by going to the *Kubernetes HPA documentation* link mentioned in the *See
    also *section.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这使您能够指定额外的资源指标。默认情况下，CPU和内存是唯一支持的资源指标。除了这些资源指标，v2 API还支持另外两种类型的指标，这两种指标都被视为自定义指标：每个Pod的自定义指标和对象指标。您可以通过转到*参见*部分中提到的*Kubernetes
    HPA文档*链接来了解更多信息。
- en: See also
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: Kubernetes pod Autoscaler using custom metrics: [https://sysdig.com/blog/kubernetes-autoscaler/](https://sysdig.com/blog/kubernetes-autoscaler/)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义指标的Kubernetes Pod自动缩放器：[https://sysdig.com/blog/kubernetes-autoscaler/](https://sysdig.com/blog/kubernetes-autoscaler/)
- en: Kubernetes HPA documentation: [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes HPA文档：[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- en: Declarative Management of Kubernetes Objects Using Configuration Files: [https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配置文件声明式管理Kubernetes对象：[https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/)
- en: Imperative Management of Kubernetes Objects Using Configuration Files: [https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配置文件命令式管理Kubernetes对象：[https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-config/)
- en: Assigning applications to nodes
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将应用程序分配给节点
- en: In this section, we will make sure that pods are not scheduled onto inappropriate
    nodes. You will learn how to schedule pods into Kubernetes nodes using node selectors,
    taints, toleration and by setting priorities.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将确保pod不会被调度到不合适的节点上。您将学习如何使用节点选择器、污点、容忍和设置优先级将pod调度到Kubernetes节点上。
- en: Getting ready
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已准备好一个Kubernetes集群，并配置了`kubectl`和`helm`来管理集群资源。
- en: How to do it…
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分进一步分为以下子部分，以使此过程更容易：
- en: Labeling nodes
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给节点贴标签
- en: Assigning pods to nodes using nodeSelector
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用nodeSelector将pod分配给节点
- en: Assigning pods to nodes using node and inter-pod affinity
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用节点和pod亲和性将pod分配给节点
- en: Labeling nodes
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 给节点贴标签
- en: Kubernetes labels are used for specifying the important attributes of resources
    that can be used to apply organizational structures onto system objects. In this
    recipe, we will learn about the common labels that are used for Kubernetes nodes
    and apply a custom label to be used when scheduling pods into nodes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes标签用于指定资源的重要属性，这些属性可用于将组织结构应用到系统对象上。在这个配方中，我们将学习用于Kubernetes节点的常见标签，并应用一个自定义标签，以便在调度pod到节点时使用。
- en: 'Let''s perform the following steps to list some of the default labels that
    have been assigned to your nodes:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤，列出已分配给您的节点的一些默认标签：
- en: 'List the labels that have been assigned to your nodes. In our example, we will
    use a kops cluster that''s been deployed on AWS EC2, so you will also see the
    relevant AWS labels, such as availability zones:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出已分配给您的节点的标签。在我们的示例中，我们将使用部署在AWS EC2上的kops集群，因此您还将看到相关的AWS标签，例如可用区：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Get the list of the nodes in your cluster. We will use node names to assign
    labels in the next step:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取您的集群中的节点列表。我们将使用节点名称在下一步中分配标签：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Label two nodes as `production` and `development`. Run the following command
    using your worker node names from the output of *Step 2*:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个节点标记为`production`和`development`。使用*步骤2*的输出中的工作节点名称运行以下命令：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Verify that the new labels have been assigned to the nodes. This time, you
    should see `environment` labels on all the nodes except the node labeled `role=master`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证新标签是否已分配给节点。这次，除了标记为`role=master`的节点外，您应该在所有节点上看到`environment`标签：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It is recommended to document labels for other people who will use your clusters.
    While they don't directly imply semantics to the core system, make sure they are still
    meaningful and relevant to all users.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 建议为将使用您的集群的其他人记录标签。虽然它们不直接暗示核心系统的语义，但确保它们对所有用户仍然有意义和相关。
- en: Assigning pods to nodes using nodeSelector
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用nodeSelector将pod分配给节点
- en: 'In this recipe, we will learn how to schedule a pod onto a selected node using
    the nodeSelector primitive:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将学习如何使用nodeSelector原语将pod调度到选定的节点：
- en: 'Create a copy of the Helm chart we used in the *Manually scaling an application*
    recipe in a new directory called `todo-dev`. We will edit the templates later
    in order to specify `nodeSelector`:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在名为`todo-dev`的新目录中创建我们在*手动扩展应用程序*配方中使用的Helm图表的副本。稍后我们将编辑模板，以指定`nodeSelector`：
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Edit the `deployment.yaml` file in the `templates` directory:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`templates`目录中的`deployment.yaml`文件：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Add `nodeSelector:` and `environment: "{{ .Values.environment }}"` right before
    the `containers:` parameter. This should look as follows:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在`containers:`参数之前添加`nodeSelector:`和`environment: "{{ .Values.environment }}"`。这应该如下所示：'
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The Helm installation uses templates to generate configuration files. As shown
    in the preceding example, to simplify how you customize the provided values, `{{expr}}`
    is used, and these values come from the `values.yaml` file names. The `values.yaml`
    file contains the default values for a chart.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Helm安装使用模板生成配置文件。如前面的示例所示，为了简化您自定义提供的值的方式，使用`{{expr}}`，这些值来自`values.yaml`文件名称。`values.yaml`文件包含图表的默认值。
- en: 'Although it may not be practical on large clusters, instead of using `nodeSelector`
    and labels, you can also schedule a pod on one specific node using the `nodeName`
    setting. In that case, instead of the `nodeSelector` setting, you add `nodeName:
    yournodename` to your deployment manifest.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管在大型集群上可能不太实用，但是除了使用`nodeSelector`和标签之外，您还可以使用`nodeName`设置在一个特定的节点上安排Pod。在这种情况下，您可以将`nodeName:
    yournodename`添加到部署清单中，而不是`nodeSelector`设置。'
- en: 'Now that we''ve added the variable, edit the `values.yaml` file. This is where
    we will set the environment to the `development` label:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经添加了变量，编辑`values.yaml`文件。这是我们将环境设置为`development`标签的地方：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Add the `environment: development` line to the end of the files. It should
    look as follows:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在文件末尾添加`environment: development`行。它应该如下所示：'
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Edit the `Chart.yaml` file and change the chart name to its folder name. In
    this recipe, it''s called `todo-dev`. After these changes, the first two lines
    should look as follows:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`Chart.yaml`文件，并将图表名称更改为其文件夹名称。在这个配方中，它被称为`todo-dev`。在这些更改之后，前两行应该如下所示：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Update the Helm dependencies and build them. The following commands will pull
    all the dependencies and build the Helm chart:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新Helm依赖项并构建它们。以下命令将拉取所有依赖项并构建Helm图表：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Examine the chart for issues. If there are any issues with the chart''s files,
    the linting process will bring them up; otherwise, no failures should be found:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查图表是否存在问题。如果图表文件有任何问题，linting过程将提出问题；否则，不应该发现任何失败：
- en: '[PRE32]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Install the To-Do application example using the following command. This Helm
    chart will deploy two pods, including a Node.js service and a MongoDB service,
    except this time the nodes are labeled as `environment: development`:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '使用以下命令安装示例待办应用程序。这个Helm图表将部署两个Pod，包括一个Node.js服务和一个MongoDB服务，但这次节点被标记为`environment:
    development`：'
- en: '[PRE33]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Check that all the pods have been scheduled on the development nodes using
    the following command. You will find the `my-app7-dev-todo-dev` pod running on
    the node labeled `environment: development`:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '使用以下命令检查所有的Pod是否已经安排在开发节点上。您会发现`my-app7-dev-todo-dev` Pod正在带有`environment:
    development`标签的节点上运行：'
- en: '[PRE34]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: With that, you've learned how to schedule workload pods onto selected nodes
    using the `nodeSelector` primitive.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，您已经学会了如何使用`nodeSelector`原语将工作负载Pod安排到选定的节点上。
- en: Assigning pods to nodes using node and inter-pod Affinity
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用节点和Pod之间的亲和性将Pod分配给节点
- en: In this recipe, we will learn how to expand the constraints we expressed in
    the previous recipe, *Assigning pods to labeled nodes using nodeSelector*, using
    the affinity and anti-affinity features.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将学习如何扩展我们在上一个配方中表达的约束，即使用亲和性和反亲和性特性将Pod分配给带标签的节点。
- en: 'Let''s use a scenario-based approach to simplify this recipe for different
    affinity selector options. We will take the previous example, but this time with
    complicated requirements:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用基于场景的方法来简化不同的亲和性选择器选项的配方。我们将采用前面的示例，但这次是具有复杂要求：
- en: '`todo-prod` must be scheduled on a node with the `environment:production` label
    and should fail if it can''t.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`todo-prod`必须安排在带有`environment:production`标签的节点上，并且如果无法安排，则应该失败。'
- en: '`todo-prod` should run on a node that is labeled with `failure-domain.beta.kubernetes.io/zone=us-east-1a` or
    `us-east-1b` but can run anywhere if the label requirement is not satisfied.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`todo-prod`应该在一个被标记为`failure-domain.beta.kubernetes.io/zone=us-east-1a`或`us-east-1b`的节点上运行，但如果标签要求不满足，可以在任何地方运行。'
- en: '`todo-prod` must run on the same zone as `mongodb`, but should not run in the
    zone where `todo-dev` is running.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`todo-prod`必须在与`mongodb`相同的区域运行，但不应在`todo-dev`运行的区域运行。'
- en: The requirements listed here are only examples in order to represent the use
    of some affinity definition functionality. This is not the ideal way to configure
    this specific application. The labels may be completely different in your environment.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出的要求只是为了代表一些亲和性定义功能的使用示例。这不是配置这个特定应用程序的理想方式。在您的环境中，标签可能完全不同。
- en: 'The preceding scenario will cover both types of node affinity options (`requiredDuringSchedulingIgnoredDuringExecution`
    and `preferredDuringSchedulingIgnoredDuringExecution`). You will see these options
    later in our example. Let''s get started:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 上述情景将涵盖节点亲和性选项（`requiredDuringSchedulingIgnoredDuringExecution`和`preferredDuringSchedulingIgnoredDuringExecution`）的两种类型。您将在我们的示例中稍后看到这些选项。让我们开始吧：
- en: 'Create a copy of the Helm chart we used in the *Manually scaling an application* recipe
    to a new directory called `todo-prod`. We will edit the templates later in order
    to specify `nodeAffinity` rules:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们在*手动扩展应用程序*配方中使用的Helm图表复制到一个名为`todo-prod`的新目录中。我们稍后将编辑模板，以指定`nodeAffinity`规则：
- en: '[PRE35]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Edit the `values.yaml` file. To access it, use the following command:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`values.yaml`文件。要访问它，请使用以下命令：
- en: '[PRE36]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Replace the last line, `affinity: {}`, with the following code. This change
    will satisfy the first requirement we defined previously, meaning that a pod can
    only be placed on a node with an `environment` label and whose value is `production`:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '用以下代码替换最后一行`affinity: {}`。这个改变将满足我们之前定义的第一个要求，意味着一个pod只能放置在一个带有`environment`标签且其值为`production`的节点上：'
- en: '[PRE37]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: You can also specify more than one `matchExpressions` under the `nodeSelectorTerms`.
    In this case, the pod can only be scheduled onto a node where all `matchExpressions`
    are satisfied, which may limit your successful scheduling chances.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在`nodeSelectorTerms`下指定多个`matchExpressions`。在这种情况下，pod只能被调度到所有`matchExpressions`都满足的节点上，这可能会限制您成功调度的机会。
- en: 'Although it may not be practical on large clusters, instead of using `nodeSelector`
    and labels, you can also schedule a pod on a specific node using the `nodeName` setting.
    In this case, instead of the `nodeSelector` setting, add `nodeName: yournodename` to
    your deployment manifest.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然在大型集群上可能不太实用，但是除了使用`nodeSelector`和标签之外，您还可以使用`nodeName`设置在特定节点上调度一个pod。在这种情况下，将`nodeName:
    yournodename`添加到您的部署清单中，而不是`nodeSelector`设置。'
- en: 'Now, add the following lines right under the preceding code addition. This
    addition will satisfy the second requirement we defined, meaning that nodes with
    a label of `failure-domain.beta.kubernetes.io/zone` and whose value is `us-east-1a` or
    `us-east-1b`  will be preferred:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在上述代码添加的下面添加以下行。这个添加将满足我们之前定义的第二个要求，意味着带有`failure-domain.beta.kubernetes.io/zone`标签且其值为`us-east-1a`或`us-east-1b`的节点将被优先选择：
- en: '[PRE38]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'For the third requirement, we will use the inter-pod affinity and anti-affinity
    functionalities. They allow us to limit which nodes our pod is eligible to be
    scheduled based on the labels on pods that are already running on the node instead
    of taking labels on nodes for scheduling. The following podAffinity `requiredDuringSchedulingIgnoredDuringExecution`
    rule will look for nodes where `app: mongodb` exist and use `failure-domain.beta.kubernetes.io/zone`
    as a topology key to show us where the pod is allowed to be scheduled:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '对于第三个要求，我们将使用pod之间的亲和性和反亲和性功能。它们允许我们基于节点上已经运行的pod的标签来限制我们的pod有资格被调度到哪些节点，而不是根据节点上的标签进行调度。以下podAffinity
    `requiredDuringSchedulingIgnoredDuringExecution`规则将寻找存在`app: mongodb`的节点，并使用`failure-domain.beta.kubernetes.io/zone`作为拓扑键来显示我们的pod允许被调度到哪里：'
- en: '[PRE39]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Add the following lines to complete the requirements. This time, the `podAntiAffinity
    preferredDuringSchedulingIgnoredDuringExecution` rule will look for nodes where
    `app: todo-dev` exists and use `failure-domain.beta.kubernetes.io/zone` as a topology
    key:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '添加以下行以满足要求。这次，`podAntiAffinity preferredDuringSchedulingIgnoredDuringExecution`规则将寻找存在`app:
    todo-dev`的节点，并使用`failure-domain.beta.kubernetes.io/zone`作为拓扑键：'
- en: '[PRE40]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Edit the `Chart.yaml` file and change the chart name to its folder name. In
    this recipe, it''s called `todo-prod`. After making these changes, the first two
    lines should look as follows:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`Chart.yaml`文件，并将图表名称更改为其文件夹名称。在这个示例中，它被称为`todo-prod`。做出这些更改后，前两行应如下所示：
- en: '[PRE41]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Update the Helm dependencies and build them. The following commands will pull
    all the dependencies and build the Helm chart:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新Helm依赖项并构建它们。以下命令将拉取所有依赖项并构建Helm图表：
- en: '[PRE42]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Examine the chart for issues. If there are any issues with the chart files,
    the linting process will bring them up; otherwise, no failures should be found:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查图表是否存在问题。如果图表文件有任何问题，linting过程将指出；否则，不应该发现任何失败：
- en: '[PRE43]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Install the To-Do application example using the following command. This Helm
    chart will deploy two pods, including a Node.js service and a MongoDB service,
    this time following the detailed requirements we defined at the beginning of this
    recipe:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装待办事项应用示例。这个Helm图表将部署两个pod，包括一个Node.js服务和一个MongoDB服务，这次遵循我们在本示例开始时定义的详细要求：
- en: '[PRE44]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Check that all the pods that have been scheduled on the nodes are labeled as
    `environment: production` using the following command. You will find the `my-app7-dev-todo-dev` pod
    running on the nodes:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '使用以下命令检查已经在节点上调度的所有pod是否标记为`environment: production`。您将发现`my-app7-dev-todo-dev`
    pod正在节点上运行：'
- en: '[PRE45]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In this recipe, you learned about advanced pod scheduling practices while using
    a number of primitives in Kubernetes, including `nodeSelector`, node affinity,
    and inter-pod affinity. Now, you will be able to configure a set of applications
    that are co-located in the same defined topology or scheduled in different zones
    so that you have better **service-level agreement** (**SLA**) times.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，您学习了在Kubernetes中使用一些原语时如何进行高级pod调度实践，包括`nodeSelector`、节点亲和性和pod之间的亲和性。现在，您将能够配置一组应用程序，这些应用程序位于相同的定义拓扑中或在不同的区域中进行调度，以便您拥有更好的**服务级别协议**（**SLA**）时间。
- en: How it works...
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The recipes in this section showed you how to schedule pods on preferred locations,
    sometimes based on complex requirements.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的示例向您展示了如何根据复杂的要求在首选位置上调度pod。
- en: 'In the *Labeling nodes* recipe, in *Step 1*, you can see that some standard
    labels have been applied to your nodes already. Here is a short explanation of
    what they mean and where they are used:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在*节点标记*示例中，在*步骤1*中，您可以看到一些标准标签已经应用到您的节点上。这里是它们的简要解释以及它们的用途：
- en: '`kubernetes.io/arch`: This comes from the `runtime.GOARCH` parameter and is
    applied to nodes to identify where to run different architecture container images,
    such as x86, arm, arm64, ppc64le, and s390x, in a mixed architecture cluster.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes.io/arch`：这来自`runtime.GOARCH`参数，并应用于节点以识别在不同架构容器映像（如 x86、arm、arm64、ppc64le
    和 s390x）上运行的位置混合架构集群。'
- en: '`kubernetes.io/instance-type`: This is only useful if your cluster is deployed
    on a cloud provider. Instance types tell us a lot about the platform, especially
    for AI and machine learning workloads where you need to run some pods on instances
    with GPUs or faster storage options.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes.io/instance-type`：只有在集群部署在云提供商上时才有用。实例类型告诉我们很多关于平台的信息，特别是对于需要在具有
    GPU 或更快存储选项的实例上运行一些 Pod 的 AI 和机器学习工作负载。'
- en: '`kubernetes.io/os`: This is applied to nodes and comes from `runtime.GOOS`.
    It is probably less useful unless you have Linux and Windows nodes in the same
    cluster.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes.io/os`：这适用于节点，并来自`runtime.GOOS`。除非您在同一集群中有 Linux 和 Windows 节点，否则可能不太有用。'
- en: '`failure-domain.beta.kubernetes.io/region` and `/zone`: This is also more useful
    if your cluster is deployed on a cloud provider or your infrastructure is spread
    across a different failure-domain. In a data center, it can be used to define
    a rack solution so that you can schedule pods on separate racks for higher availability.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`failure-domain.beta.kubernetes.io/region`和`/zone`：如果您的集群部署在云提供商上或您的基础设施跨不同的故障域，这也更有用。在数据中心，它可以用于定义机架解决方案，以便您可以将
    Pod 安排在不同的机架上，以提高可用性。'
- en: '`kops.k8s.io/instancegroup=nodes`: This is the node label that''s set to the
    name of the instance group. It is only used with kops clusters.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kops.k8s.io/instancegroup=nodes`：这是设置为实例组名称的节点标签。仅在使用 kops 集群时使用。'
- en: '`kubernetes.io/hostname`: Shows the hostname of the worker.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes.io/hostname`：显示工作节点的主机名。'
- en: '`kubernetes.io/role`: This shows the role of the worker in the cluster. Some
    common values include `node` for representing worker nodes and `master`, which
    shows the node is the master node and is tainted as not schedulable for workloads
    by default.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes.io/role`：显示工作节点在集群中的角色。一些常见的值包括`node`（表示工作节点）和`master`（表示节点是主节点，并且默认情况下被标记为不可调度的工作负载）。'
- en: In the *Assigning pods to nodes using node and inter-pod affinity* recipe, in *Step
    3*, the node affinity rule says that the pod can only be placed on a node with
    a label whose key is `environment` and whose value is `production`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在*使用节点和跨 Pod 亲和力将 Pod 分配给节点*配方中，在*步骤3*中，节点亲和力规则表示 Pod 只能放置在具有键为`environment`且值为`production`的标签的节点上。
- en: 'In *Step 4*, the `affinity key: value` requirement is preferred (`preferredDuringSchedulingIgnoredDuringExecution`). The
    `weight` field here can be a value between `1` and `100`. For every node that
    meets these requirements, a Kubernetes scheduler computes a sum. The nodes with
    the highest total score are preferred.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '在*步骤4*中，首选`affinity key: value`要求（`preferredDuringSchedulingIgnoredDuringExecution`）。这里的`weight`字段可以是`1`到`100`之间的值。满足这些要求的每个节点，Kubernetes
    调度程序都会计算一个总和。总分最高的节点是首选的。'
- en: 'Another detail that''s used here is the `In` parameter. Node Affinity supports
    the following operators: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt`, and `Lt`.
    You can read more about the operators by looking at the *Scheduler affinities
    through examples* link mentioned in the *See also *section.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的另一个细节是`In`参数。节点亲和力支持以下运算符：`In`、`NotIn`、`Exists`、`DoesNotExist`、`Gt`和`Lt`。您可以通过查看“通过示例查看调度亲和力”链接来了解更多关于这些运算符的信息，该链接在*另请参阅*部分中提到。
- en: If selector and affinity rules are not well planned, they can easily block pods
    getting scheduled on your nodes. Keep in mind that if you have specified both
    `nodeSelector` and `nodeAffinity` rules, both requirements must be met for the
    pod to be scheduled on the available nodes.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择器和亲和力规则规划不当，很容易阻止pod在节点上调度。请记住，如果同时指定了`nodeSelector`和`nodeAffinity`规则，则必须同时满足这两个要求，才能将pod调度到可用节点上。
- en: 'In *Step 5*, inter-pod affinity is used (`podAffinity`) to satisfy the requirement
    in PodSpec. In this recipe, `podAffinity` is `requiredDuringSchedulingIgnoredDuringExecution`.
    Here, `matchExpressions` says that a pod can only run on nodes where `failure-domain.beta.kubernetes.io/zone`
    matches the nodes where other pods with the `app: mongodb` label are running.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '在*步骤5*中，使用`podAffinity`来满足PodSpec中的要求。在这个示例中，`podAffinity`是`requiredDuringSchedulingIgnoredDuringExecution`。在这里，`matchExpressions`表示一个pod只能在`failure-domain.beta.kubernetes.io/zone`与其他带有`app:
    mongodb`标签的pod所在的节点匹配的节点上运行。'
- en: 'In *Step 6*, the requirement is satisfied with `podAntiAffinity` using `preferredDuringSchedulingIgnoredDuringExecution`.
    Here, `matchExpressions` says that a pod can''t run on nodes where `failure-domain.beta.kubernetes.io/zone`
    matches the nodes where other pods with the `app: todo-dev` label are running.
    The weight is increased by setting it to `100`.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '在*步骤6*中，使用`podAntiAffinity`和`preferredDuringSchedulingIgnoredDuringExecution`满足了要求。在这里，`matchExpressions`表示一个pod不能在`failure-domain.beta.kubernetes.io/zone`与其他带有`app:
    todo-dev`标签的pod所在的节点匹配的节点上运行。通过将权重设置为`100`来增加权重。'
- en: See also
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'List of known labels, annotations, and taints: [https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已知标签、注释和污点列表：[https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/)
- en: 'Assigning Pods to Nodes in the Kubernetes documentation: [https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Pod分配给Kubernetes文档中的节点：[https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/)
- en: More on labels and selectors in the Kubernetes documentation: [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Kubernetes文档中标签和选择器的更多信息：[https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
- en: 'Scheduler affinities through examples: [https://banzaicloud.com/blog/k8s-affinities/](https://banzaicloud.com/blog/k8s-affinities/)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过示例了解调度器亲和力：[https://banzaicloud.com/blog/k8s-affinities/](https://banzaicloud.com/blog/k8s-affinities/)
- en: 'Node affinity and NodeSelector design document: [https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/nodeaffinity.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/nodeaffinity.md)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点亲和力和NodeSelector设计文档：[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/nodeaffinity.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/nodeaffinity.md)
- en: Interpod topological affinity and anti-affinity design document: [https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod间拓扑亲和力和反亲和力设计文档：[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md)
- en: Creating an external load balancer
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建外部负载均衡器
- en: The load balancer service type is a relatively simple service alternative to
    ingress that uses a cloud-based external load balancer. The external load balancer
    service type's support is limited to specific cloud providers but is supported
    by the most popular cloud providers, including AWS, GCP, Azure, Alibaba Cloud,
    and OpenStack.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器服务类型是一种相对简单的服务替代方案，用于使用基于云的外部负载均衡器而不是入口。外部负载均衡器服务类型的支持仅限于特定的云提供商，但受到大多数流行的云提供商的支持，包括AWS、GCP、Azure、阿里云和OpenStack。
- en: In this section, we will expose our workload ports using a load balancer. We
    will learn how to create an external GCE/AWS load balancer for clusters on public
    clouds, as well as for your private cluster using `inlet-operator`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用负载均衡器来公开我们的工作负载端口。我们将学习如何为公共云上的集群创建外部GCE/AWS负载均衡器，以及如何使用`inlet-operator`为您的私有集群创建外部负载均衡器。
- en: Getting ready
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources. In this recipe, we are using a cluster that's
    been deployed on AWS using `kops`, as described in [Chapter 1](a8580410-3e1c-4e28-8d18-aaf9d38d011f.xhtml),
    *Building Production-Ready Kubernetes Clusters*, in the* Amazon Web Services*
    recipe. The same instructions will work on all major cloud providers.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已经准备好一个Kubernetes集群，并且`kubectl`和`helm`已配置好以管理集群资源。在本教程中，我们使用了在AWS上使用`kops`部署的集群，如[第1章](a8580410-3e1c-4e28-8d18-aaf9d38d011f.xhtml)中所述，*构建生产就绪的Kubernetes集群*，在*亚马逊网络服务*的示例中。相同的说明也适用于所有主要的云提供商。
- en: 'To access the example files, clone the `k8sdevopscookbook/src` repository to
    your workstation to use the configuration files in the `src/chapter7/lb` directory,
    as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问示例文件，请将`k8sdevopscookbook/src`存储库克隆到您的工作站，以便在`src/chapter7/lb`目录中使用配置文件，方法如下：
- en: '[PRE46]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: After you've cloned the examples repository, you can move on to the recipes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在您克隆了示例存储库之后，您可以继续进行操作。
- en: How to do it…
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本节进一步分为以下小节，以使这个过程更容易：
- en: Creating an external cloud load balancer
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建外部云负载均衡器
- en: Finding the external address of the service
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找服务的外部地址
- en: Creating an external cloud load balancer
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建外部云负载均衡器
- en: When you create an application and expose it as a Kubernetes service, you usually
    need the service to be reachable externally via an IP address or URL. In this
    recipe, you will learn how to create a load balancer, also referred to as a cloud
    load balancer.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建一个应用程序并将其公开为Kubernetes服务时，通常需要使服务可以通过IP地址或URL从外部访问。在本教程中，您将学习如何创建一个负载均衡器，也称为云负载均衡器。
- en: In the previous chapters, we have seen a couple of examples that used the load
    balancer service type to expose IP addresses, including the *Configuring and managing
    S3 object storage using MinIO* and *Application backup and recovery using Kasten* recipes
    in the previous chapter, as well as the To-Do application that was provided in
    this chapter in the *Assigning applications to nodes* recipe.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们已经看到了一些示例，这些示例使用了负载均衡器服务类型来公开IP地址，包括上一章中的*使用MinIO配置和管理S3对象存储*和*使用Kasten进行应用程序备份和恢复*，以及本章中提供的To-Do应用程序在*将应用程序分配给节点*的示例中。
- en: 'Let''s use the MinIO application to learn how to create a load balancer. Follow
    these steps to create a service and expose it using an external load balancer
    service:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用MinIO应用程序来学习如何创建负载均衡器。按照以下步骤创建一个服务，并使用外部负载均衡器服务公开它：
- en: 'Review the content of the `minio.yaml` file in the examples directory in `src/chapter7/lb`
    and deploy it using the following command. This will create a StatefulSet and
    a service where the MinIO port is exposed internally to the cluster via port number
    `9000`. You can choose to apply the same steps and create a load balancer for
    your own application. In that case, skip to *Step 2*:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`src/chapter7/lb`目录中`minio.yaml`文件的内容，并使用以下命令部署它。这将创建一个StatefulSet和一个服务，其中MinIO端口通过端口号`9000`在集群内部公开。你可以选择应用相同的步骤并为你自己的应用程序创建一个负载均衡器。在这种情况下，跳到*步骤2*：
- en: '[PRE47]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'List the available services on Kubernetes. You will see that the MinIO service
    shows `ClusterIP` as the service type and `none` under the `EXTERNAL-IP` field:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出Kubernetes上可用的服务。你会看到MinIO服务的服务类型为`ClusterIP`，`EXTERNAL-IP`字段下为`none`：
- en: '[PRE48]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Create a new service with the `TYPE` set to `LoadBalancer`. The following command
    will expose `port: 9000` of our MinIO application at `targetPort: 9000` using
    the `TCP` protocol, as shown here:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '创建一个`TYPE`设置为`LoadBalancer`的新服务。以下命令将使用`TCP`协议将我们的MinIO应用程序的`port: 9000`暴露到`targetPort:
    9000`，如下所示：'
- en: '[PRE49]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The preceding command will immediately create the `Service` object, but the
    actual load balancer on the cloud provider side may take 30 seconds to a minute
    to be completely initialized. Although the object will state that it's ready,
    it will not function until the load balancer is initialized. This is one of the
    disadvantages of cloud load balancers compared to ingress controllers, which we
    will look at in the next recipe, *Creating an ingress service and service mesh
    using Istio*.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将立即创建`Service`对象，但在云服务提供商端实际的负载均衡器可能需要30秒到1分钟才能完全初始化。尽管对象将声明它已准备就绪，但在负载均衡器初始化之前它将无法正常工作。这是云负载均衡器与入口控制器相比的一个缺点，我们将在下一个教程中看到，*使用Istio创建入口服务和服务网格*。
- en: 'As an alternative to *Step 3*, you can also create the load balancer by using
    the following command:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 作为*步骤3*的替代方案，你也可以使用以下命令创建负载均衡器：
- en: '[PRE50]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Finding the external address of the service
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查找服务的外部地址
- en: 'Let''s perform the following steps to get the externally reachable address
    of the service:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤来获取服务的外部可达地址：
- en: 'List the services that use the `LoadBalancer` type. The `EXTERNAL-IP` column
    will show you the cloud vendor-provided address:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出使用`LoadBalancer`类型的服务。`EXTERNAL-IP`列将显示云供应商提供的地址：
- en: '[PRE51]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'If you are running on a cloud provider service such as AWS, you can also use
    the following command to get the exact address. You can copy and paste this into
    a web browser:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你在AWS等云服务提供商上运行，你也可以使用以下命令获取确切的地址。你可以复制并粘贴到网页浏览器中：
- en: '[PRE52]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'If you are running on a bare-metal server, then you probably won''t have a
    `hostname` entry. As an example, if you are running MetalLB ([https://metallb.universe.tf/](https://metallb.universe.tf/)),
    a load balancer for bare-metal Kubernetes clusters, or SeeSaw ([https://github.com/google/seesaw](https://github.com/google/seesaw)),
    a **Linux Virtual Server** (**LVS**)-based load balancing platform, you need to
    look for the `ip` entry instead:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你在裸机服务器上运行，那么你可能不会有`hostname`条目。例如，如果你正在运行MetalLB ([https://metallb.universe.tf/](https://metallb.universe.tf/))，一个用于裸机Kubernetes集群的负载均衡器，或者SeeSaw
    ([https://github.com/google/seesaw](https://github.com/google/seesaw))，一个基于**Linux虚拟服务器**（**LVS**）的负载均衡平台，你需要查找`ip`条目：
- en: '[PRE53]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The preceding command will return a link similar to `https://containerized.me.us-east-1.elb.amazonaws.com:9000`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将返回一个类似于`https://containerized.me.us-east-1.elb.amazonaws.com:9000`的链接。
- en: How it works...
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This recipe showed you how to quickly create a cloud load balancer to expose
    your services with an external address.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程向你展示了如何快速创建一个云负载均衡器，以便使用外部地址暴露你的服务。
- en: In the *Creating a cloud load balancer* recipe, in *Step 3*, when a load balancer
    service is created in Kubernetes, a cloud provider load balancer is created on
    your behalf without you having to go through the cloud service provider APIs separately.
    This feature helps you easily manage the creation of load balancers outside of
    your Kubernetes cluster, but at the same takes a bit of time to complete and requires
    a separate load balancer for every service, so this might be costly and not very
    flexible.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在*创建云负载均衡器*示例中，在*第3步*中，当在Kubernetes中创建负载均衡器服务时，将代表您创建一个云提供商负载均衡器，而无需单独通过云服务提供商API。这个功能可以帮助您轻松地管理负载均衡器的创建，但同时需要一些时间来完成，并且需要为每个服务单独创建一个独立的负载均衡器，因此可能成本高且不太灵活。
- en: To give load balancers flexibility and add more application-level functionality,
    you can use ingress controllers. Using ingress, traffic routing can be controlled
    by rules defined in the ingress resource. You will learn more about popular ingress
    gateways in the next two recipes, *Creating an ingress service and service mesh
    using Istio* and *Creating an ingress service and service mesh using Linkerd*.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使负载均衡器更加灵活并增加更多的应用级功能，您可以使用Ingress控制器。使用Ingress，流量路由可以由Ingress资源中定义的规则来控制。您将在接下来的两个示例中了解更多关于流行的Ingress网关，*使用Istio创建Ingress服务和服务网格*和*使用Linkerd创建Ingress服务和服务网格*。
- en: See also
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Kubernetes documentation on the load balancer service type: [https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes关于负载均衡器服务类型的文档：[https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer)
- en: Using a load balancer on Amazon EKS: [https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html](https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon EKS上使用负载均衡器：[https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html](https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html)
- en: Using a load balancer on AKS: [https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard](https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AKS上使用负载均衡器：[https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard](https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard)
- en: Using a load balancer on Alibaba Cloud: [https://www.alibabacloud.com/help/doc-detail/53759.htm](https://www.alibabacloud.com/help/doc-detail/53759.htm)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在阿里云上使用负载均衡器：[https://www.alibabacloud.com/help/doc-detail/53759.htm](https://www.alibabacloud.com/help/doc-detail/53759.htm)
- en: Load balancer for your private Kubernetes cluster: [https://blog.alexellis.io/ingress-for-your-local-kubernetes-cluster/](https://blog.alexellis.io/ingress-for-your-local-kubernetes-cluster/)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 私有Kubernetes集群的负载均衡器：[https://blog.alexellis.io/ingress-for-your-local-kubernetes-cluster/](https://blog.alexellis.io/ingress-for-your-local-kubernetes-cluster/)
- en: Creating an ingress service and service mesh using Istio
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Istio创建Ingress服务和服务网格
- en: Istio is a popular open source service mesh. In this section, we will get basic
    Istio service mesh functionality up and running. You will learn how to create
    a service mesh to secure, connect, and monitor microservices.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Istio是一个流行的开源服务网格。在本节中，我们将启动并运行基本的Istio服务网格功能。您将学习如何创建一个服务网格来保护、连接和监控微服务。
- en: Service mesh is a very detailed concept and we don't intend to explain any detailed
    use cases. Instead, we will focus on getting our service up and running.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格是一个非常详细的概念，我们不打算解释任何详细的用例。相反，我们将专注于启动和运行我们的服务。
- en: Getting ready
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已经准备好一个Kubernetes集群，并且已经配置好`kubectl`和`helm`来管理集群资源。
- en: 'Clone the `https://github.com/istio/istio` repository to your workstation,
    as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 将`https://github.com/istio/istio`存储库克隆到您的工作站，如下所示：
- en: '[PRE54]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We will use the examples in the preceding repository to install Istio on our
    Kubernetes cluster.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用前面的存储库中的示例，在我们的Kubernetes集群上安装Istio。
- en: How to do it…
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分进一步分为以下小节，以使这个过程更容易：
- en: Installing Istio using Helm
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Helm安装Istio
- en: Verifying the installation
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证安装
- en: Creating an ingress gateway
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建入口网关
- en: Installing Istio using Helm
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Helm安装Istio
- en: 'Let''s perform the following steps to install Istio:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤来安装Istio：
- en: 'Create the Istio CRDs that are required before we can deploy Istio:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在部署Istio之前，创建所需的Istio CRD：
- en: '[PRE55]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Install Istio with the default configuration. This will deploy the Istio core
    components, that is, `istio-citadel`, `istio-galley`, `istio-ingressgateway`,
    `istio-pilot`, `istio-policy`, `istio-sidecar-injector`, and `istio-telemetry`:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认配置安装Istio。这将部署Istio核心组件，即`istio-citadel`、`istio-galley`、`istio-ingressgateway`、`istio-pilot`、`istio-policy`、`istio-sidecar-injector`和`istio-telemetry`：
- en: '[PRE56]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Enable automatic sidecar injection by labeling the namespace where you will
    run your applications. In this recipe, we will be using the `default` namespace:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过为将运行应用程序的命名空间添加标签来启用自动sidecar注入。在本示例中，我们将使用`default`命名空间：
- en: '[PRE57]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: To be able to get Istio functionality for your application, the pods need to
    run an Istio sidecar proxy. The preceding command will automatically inject the
    Istio sidecar. As an alternative, you can find the instructions for manually adding
    Istio sidecars to your pods using the `istioctl` command in the *Installing the
    Istio sidecar instructions* link provided in the *See also* section.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使您的应用程序能够获得Istio功能，pod需要运行一个Istio sidecar代理。上面的命令将自动注入Istio sidecar。作为替代方案，您可以在*安装Istio
    sidecar说明*链接中找到使用`istioctl`命令手动向您的pod添加Istio sidecar的说明。
- en: Verifying the installation
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证安装
- en: 'Let''s perform the following steps to confirm that Istio has been installed
    successfully:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤来确认Istio已成功安装：
- en: 'Check the number of Istio CRDs that have been created. The following command
    should return `23`, which is the number of CRDs that have been created by Istio:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查已创建的Istio CRD的数量。以下命令应返回`23`，这是Istio创建的CRD的数量：
- en: '[PRE58]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Run the following command and confirm that the list of Istio core component
    services have been created:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令并确认已创建Istio核心组件服务的列表：
- en: '[PRE59]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Make sure that all the pods listed are in the `Running` state:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保列出的所有pod都处于`Running`状态：
- en: '[PRE60]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Confirm the Istio injection enabled namespaces. You should only see `istio-injection`
    for the `default` namespace:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认启用了Istio注入的命名空间。您应该只在`default`命名空间中看到`istio-injection`：
- en: '[PRE61]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: You can always enable injection for the other namespaces by adding the `istio-injection=enabled`
    label to a namespace.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过为命名空间添加`istio-injection=enabled`标签来始终启用其他命名空间的注入。
- en: Creating an ingress gateway
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建入口网关
- en: 'Instead of using a controller to load balance traffic, Istio uses a gateway.
    Let''s perform the following steps to create an Istio ingress gateway for our
    example application:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Istio使用网关而不是控制器来负载均衡流量。让我们执行以下步骤来为我们的示例应用程序创建一个Istio入口网关：
- en: 'Review the content of the `minio.yaml` file in the examples directory in `src/chapter7/lb` and
    deploy it using the following command. This will create a StatefulSet and a service
    where the MinIO port is exposed internally to the cluster via port number `9000`.
    You can also choose to apply the same steps and create an ingress gateway for
    your own application. In that case, skip to *Step 2*:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`src/chapter7/lb`目录中的示例目录中查看`minio.yaml`文件的内容，并使用以下命令部署它。这将创建一个StatefulSet和一个服务，其中MinIO端口通过端口号`9000`在集群内部公开。您也可以选择应用相同的步骤并为您自己的应用程序创建一个入口网关。在这种情况下，跳到*步骤2*：
- en: '[PRE62]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Get the ingress IP and ports:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取入口IP和端口：
- en: '[PRE63]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Create a new Istio gateway:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Istio网关：
- en: '[PRE64]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Create a new `VirtualService` to forward requests to the MinIO instance via
    the gateway. This helps specify routing for the gateway and binds the gateway
    to the `VirtualService`:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`VirtualService`来通过网关转发请求到MinIO实例。这有助于为网关指定路由并将网关绑定到`VirtualService`：
- en: '[PRE65]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: This configuration will expose your services to external access using Istio,
    and you will have more control over rules.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置将使用Istio将您的服务暴露给外部访问，并且您将对规则有更多的控制。
- en: How it works...
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: This recipe showed you how to quickly configure the Istio service mesh and use
    custom Istio resources such as ingress gateway to open a service to external access.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程向您展示了如何快速配置Istio服务网格，并使用自定义的Istio资源，如入口网关来向外部开放服务。
- en: For the service mesh to function correctly, each pod in the mesh needs to run
    an Envoy sidecar. In the *Installing Istio using Helm* recipe, in *Step 3*, we
    enabled automatic injection for pods in the `default` namespace so that the pods
    that are deployed in that namespace will run the Envoy sidecar.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使服务网格正常运行，网格中的每个pod都需要运行一个Envoy sidecar。在*使用Helm安装Istio*的步骤3中，我们启用了对`default`命名空间中的pod的自动注入，以便在该命名空间中部署的pod将运行Envoy
    sidecar。
- en: An ingress controller is a reverse-proxy that runs in the Kubernetes cluster
    and configures routing rules. In the *Creating an ingress gateway* recipe, in
    *Step 2*, unlike traditional Kubernetes ingress objects, we used Istio CRDs such
    as Gateway, VirtualService, and DestinationRule to create the ingress.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 入口控制器是在Kubernetes集群中运行并配置路由规则的反向代理。在*创建入口网关*的步骤2中，与传统的Kubernetes入口对象不同，我们使用了Istio
    CRD，如Gateway、VirtualService和DestinationRule来创建入口。
- en: 'We created a gateway rule for the ingress Gateway using the `istio: ingressgateway` selector
    in order to accept HTTP traffic on port number `80`.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用`istio: ingressgateway`选择器为入口网关创建了一个网关规则，以便在端口号`80`上接受HTTP流量。'
- en: In *Step 4*, we created a VirtualService for the MinIO services we wanted to
    expose. Since the gateway may be in a different namespace, we used `minio-gateway.default` to
    set the gateway name.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤4*中，我们为我们想要暴露的MinIO服务创建了一个VirtualService。由于网关可能在不同的命名空间中，我们使用`minio-gateway.default`来设置网关名称。
- en: With this, we have exposed our service using HTTP. You can read more about exposing
    the service using the HTTPS protocol by looking at the link in *See also* section.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经使用HTTP暴露了我们的服务。您可以通过查看*另请参阅*部分中的链接来了解如何使用HTTPS协议暴露服务。
- en: There's more…
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Although it is very popular, Istio is not the simplest ingress to deal with.
    We highly recommend that you look at all the options that are available for your
    use case and consider alternatives. Therefore, it is useful to know how to remove
    Istio.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它非常受欢迎，但Istio并不是最简单的入口处理方式。我们强烈建议您查看所有适用于您的用例的选项，并考虑替代方案。因此，了解如何删除Istio是很有用的。
- en: Deleting Istio
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除Istio
- en: 'You can delete Istio by using the following commands:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令删除Istio：
- en: '[PRE66]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: If you want to completely remove the deleted release records from the Helm records
    and free the release name to be used later, add the `--purge` parameter to the
    preceding commands.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要完全删除Helm记录中的已删除发布记录，并释放发布名称以供以后使用，可以在前面的命令中添加`--purge`参数。
- en: See also
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Istio documentation: [https://istio.io/docs/](https://istio.io/docs/)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio文档：[https://istio.io/docs/](https://istio.io/docs/)
- en: Istio examples: [https://istio.io/docs/examples/bookinfo/](https://istio.io/docs/examples/bookinfo/)
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio示例：[https://istio.io/docs/examples/bookinfo/](https://istio.io/docs/examples/bookinfo/)
- en: 'Installing the Istio sidecar: [https://istio.io/docs/setup/additional-setup/sidecar-injection/](https://istio.io/docs/setup/additional-setup/sidecar-injection/)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Istio sidecar：[https://istio.io/docs/setup/additional-setup/sidecar-injection/](https://istio.io/docs/setup/additional-setup/sidecar-injection/)
- en: Istio ingress tutorial from Kelsey Hightower: [https://github.com/kelseyhightower/istio-ingress-tutorial](https://github.com/kelseyhightower/istio-ingress-tutorial)
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自Kelsey Hightower的Istio入口教程：[https://github.com/kelseyhightower/istio-ingress-tutorial](https://github.com/kelseyhightower/istio-ingress-tutorial)
- en: Traffic management with Istio: [https://istio.io/docs/tasks/traffic-management/](https://istio.io/docs/tasks/traffic-management/)
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Istio进行流量管理：[https://istio.io/docs/tasks/traffic-management/](https://istio.io/docs/tasks/traffic-management/)
- en: Security with Istio: [https://istio.io/docs/tasks/security/](https://istio.io/docs/tasks/security/)
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Istio进行安全：[https://istio.io/docs/tasks/security/](https://istio.io/docs/tasks/security/)
- en: Policy enforcement with Istio: [https://istio.io/docs/tasks/policy-enforcement/](https://istio.io/docs/tasks/policy-enforcement/)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Istio进行策略强制：[https://istio.io/docs/tasks/policy-enforcement/](https://istio.io/docs/tasks/policy-enforcement/)
- en: Collecting telemetry information with Istio: [https://istio.io/docs/tasks/telemetry/](https://istio.io/docs/tasks/telemetry/)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Istio收集遥测信息：[https://istio.io/docs/tasks/telemetry/](https://istio.io/docs/tasks/telemetry/)
- en: Creating Kubernetes ingress with Cert-Manager: [https://istio.io/docs/tasks/traffic-management/ingress/ingress-certmgr/](https://istio.io/docs/tasks/traffic-management/ingress/ingress-certmgr/)
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Cert-Manager创建Kubernetes入口：[https://istio.io/docs/tasks/traffic-management/ingress/ingress-certmgr/](https://istio.io/docs/tasks/traffic-management/ingress/ingress-certmgr/)
- en: Creating an ingress service and service mesh using Linkerd
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Linkerd创建入口服务和服务网格
- en: In this section, we will get basic Linkerd service mesh up and running. You
    will learn how to create a service mesh to secure, connect, and monitor microservices.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将启动基本的Linkerd服务网格。您将学习如何创建一个服务网格来保护、连接和监视微服务。
- en: Service mesh is a very detailed concept in itself and we don't intend to explain
    any detailed use cases here. Instead, we will focus on getting our service up
    and running.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格本身是一个非常详细的概念，我们不打算在这里解释任何详细的用例。相反，我们将专注于启动和运行我们的服务。
- en: Getting ready
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已准备好Kubernetes集群，并配置了`kubectl`和`helm`来管理集群资源。
- en: 'To access the example files for this recipe, clone the `k8sdevopscookbook/src` repository
    to your workstation to use the configuration files in the `src/chapter7/linkerd` directory,
    as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此示例的文件，请将`k8sdevopscookbook/src`存储库克隆到您的工作站，以使用`src/chapter7/linkerd`目录中的配置文件，如下所示：
- en: '[PRE67]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: After you've cloned the preceding repository, you can get started with the recipes.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在克隆了上述存储库之后，您可以开始使用这些示例。
- en: How to do it…
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个过程更容易，本节进一步分为以下小节：
- en: Installing the Linkerd CLI
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Linkerd CLI
- en: Installing Linkerd
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Linkerd
- en: Verifying a Linkerd deployment
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证Linkerd部署
- en: Viewing the Linkerd metrics
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看Linkerd指标
- en: Installing the Linkerd CLI
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装Linkerd CLI
- en: 'To interact with Linkerd, you need to install the `linkerd` CLI. Follow these
    steps:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 要与Linkerd交互，您需要安装`linkerd` CLI。按照以下步骤进行：
- en: 'Install the `linkerd` CLI by running the following command:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令安装`linkerd` CLI：
- en: '[PRE68]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Add the `linkerd` CLI to your path:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`linkerd` CLI添加到您的路径：
- en: '[PRE69]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Verify that the `linkerd` CLI has been installed by running the following command.
    It should show the server as unavailable since we haven''t installed it yet:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令验证`linkerd` CLI是否已安装。由于我们尚未安装，它应显示服务器不可用：
- en: '[PRE70]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Validate that `linkerd` can be installed. This command will check the cluster
    and point to issues if they exist:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证`linkerd`是否可以安装。此命令将检查集群并指出存在的问题：
- en: '[PRE71]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: If the status checks are looking good, you can move on to the next recipe.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态检查看起来不错，您可以继续下一个示例。
- en: Installing Linkerd
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装Linkerd
- en: Compared to the alternatives, Linkerd is much easier to get started with and
    manage, so it is my preferred service mesh.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他选择相比，Linkerd更容易入门和管理，因此它是我首选的服务网格。
- en: 'Install the Linkerd control plane using the Linkerd CLI. This command will
    use the default options and install the linkerd components in the `linkerd` namespace:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Linkerd CLI安装Linkerd控制平面。此命令将使用默认选项并在`linkerd`命名空间中安装linkerd组件：
- en: '[PRE72]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Pulling all the container images may take a minute or so. After that, you can
    verify the health of the components by following the next recipe, *Verifying a
    Linkerd deployment.*
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 拉取所有容器镜像可能需要一分钟左右。之后，您可以通过以下步骤验证组件的健康状况，*验证Linkerd部署*。
- en: Verifying a Linkerd deployment
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证Linkerd部署
- en: Verifying Linkerd's deployment is as easy as the installation process.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 验证Linkerd的部署与安装过程一样简单。
- en: 'Run the following command to validate the installation. This will display a
    long summary of control plane components and APIs and will make sure you are running
    the latest version:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令验证安装。这将显示控制平面组件和API的长摘要，并确保您正在运行最新版本：
- en: '[PRE73]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: If the status checks are good, you are ready to test Linkerd with a sample application.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态检查良好，您可以准备使用示例应用程序测试Linkerd。
- en: Adding Linkerd to a service
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将Linkerd添加到服务
- en: 'Follow these steps to add Linkerd to our demo application:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤将Linkerd添加到我们的演示应用程序中：
- en: 'Change directories to the `linkerd` folder:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到`linkerd`文件夹：
- en: '[PRE74]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Deploy the demo application, which uses a mix of gRPC and HTTP calls to service
    a voting application to the user:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署使用gRPC和HTTP调用混合为用户提供投票应用程序的演示应用程序：
- en: '[PRE75]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Get the service IP of the demo application. This following command will return
    the externally accessible address of your application:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取演示应用程序的服务IP。以下命令将返回应用程序的外部可访问地址：
- en: '[PRE76]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Open the external address from *Step 3* in a web browser and confirm that the
    application is functional:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Web浏览器中打开*步骤3*中的外部地址，并确认应用程序是否正常运行：
- en: '![](assets/d32ad784-1d4e-49cb-a1d3-90a8e88004d6.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d32ad784-1d4e-49cb-a1d3-90a8e88004d6.png)'
- en: 'Enable automatic sidecar injection by labeling the namespace where you will
    run your applications. In this recipe, we''re using the `emojivoto` namespace:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过为将运行应用程序的命名空间打标签来启用自动注入sidecar。在这个示例中，我们使用了`emojivoto`命名空间：
- en: '[PRE77]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: You can also manually inject a `linkerd` sidecar by patching the pods where
    you run your applications using the
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过在运行应用程序的pod上手动注入`linkerd` sidecar来手动注入：
- en: '`kubectl get -n emojivoto deploy -o yaml | linkerd inject - | kubectl apply
    -f -` command. In this recipe, the `emojivoto` namespace is used.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl get -n emojivoto deploy -o yaml | linkerd inject - | kubectl apply
    -f -` 命令。在这个示例中，使用了`emojivoto`命名空间。'
- en: There's more…
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 本节进一步分为以下子节，以使该过程更容易：
- en: Accessing the dashboard
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问仪表板
- en: Deleting Linkerd
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除Linkerd
- en: Accessing the dashboard
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 访问仪表板
- en: 'We can either use port forwarding or use ingress to access the dashboard. Let''s
    start with the simple way of doing things, that is, by port forwarding to your
    local system:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用端口转发或使用入口来访问仪表板。让我们从简单的方法开始，也就是通过端口转发到您的本地系统：
- en: 'View the Linkerd dashboard by running the following command:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令查看Linkerd仪表板：
- en: '[PRE78]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Visit the following link in your browser to view the dashboard:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中访问以下链接以查看仪表板：
- en: '[PRE79]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The preceding commands will set up a port forward from your local system to
    the `linkerd-web` pod.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将从您的本地系统设置端口转发到`linkerd-web` pod。
- en: 'If you want to access the dashboard from an external IP, then follow these
    steps:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想从外部IP访问仪表板，请按照以下步骤操作：
- en: 'Download the sample ingress definition:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载示例的入口定义：
- en: '[PRE80]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Edit the ingress configuration in the `ingress-nginx.yaml` file in the `src/chapter7/linkerd`
    directory and change `- host`: `dashboard.example.com` on line 27 to the URL where
    you want your dashboard to be exposed. Apply the configuration using the following
    command:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在`src/chapter7/linkerd`目录中的`ingress-nginx.yaml`文件中编辑入口配置，并将第27行的`- host`: `dashboard.example.com`更改为您希望暴露仪表板的URL。使用以下命令应用配置：'
- en: '[PRE81]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The preceding example file uses `linkerddashboard.containerized.me` as the dashboard
    address. It also protects access with basic auth using `admin/admin` credentials.
    It is highly suggested that you use your own credentials by changing the base64-encoded
    key pair defined in the `auth` section of the configuration using the `username:password`
    format.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例文件使用`linkerddashboard.containerized.me`作为仪表板地址。它还使用`admin/admin`凭据对访问进行保护。强烈建议您通过更改配置中`auth`部分中定义的base64编码的密钥对来使用自己的凭据，格式为`username:password`。
- en: Deleting Linkerd
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除Linkerd
- en: 'To remove the Linkerd control plane, run the following command:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除Linkerd控制平面，请运行以下命令：
- en: '[PRE82]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: This command will pull a list of all the configuration files for the Linkerd
    control plane, including namespaces, service accounts, and CRDs, and remove them.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将拉取Linkerd控制平面的所有配置文件列表，包括命名空间、服务账户和CRD，并将它们删除。
- en: See also
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Linkerd documentation: [https://linkerd.io/2/overview/](https://linkerd.io/2/overview/)
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd文档：[https://linkerd.io/2/overview/](https://linkerd.io/2/overview/)
- en: Common tasks with Linkerd: [https://linkerd.io/2/tasks/](https://linkerd.io/2/tasks/)
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Linkerd的常见任务：[https://linkerd.io/2/tasks/](https://linkerd.io/2/tasks/)
- en: Frequently asked Linkerd questions and answers: [https://linkerd.io/2/faq/](https://linkerd.io/2/faq/)
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linkerd常见问题和答案：[https://linkerd.io/2/faq/](https://linkerd.io/2/faq/)
- en: Auto-healing pods in Kubernetes
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的自动修复Pods
- en: Kubernetes has self-healing capabilities at the cluster level. It restarts containers
    that fail, reschedules pods when nodes die, and even kills containers that don't
    respond to your user-defined health checks.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在集群级别具有自愈能力。它在容器失败时重新启动容器，在节点死机时重新调度Pod，甚至杀死不响应用户定义的健康检查的容器。
- en: In this section, we will perform application and cluster scaling tasks. You
    will learn how to use liveness and readiness probes to monitor container health
    and trigger a restart action in case of failures.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将执行应用程序和集群扩展任务。您将学习如何使用存活探针和就绪探针来监视容器健康，并在失败时触发重新启动操作。
- en: Getting ready
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您有一个准备好的Kubernetes集群，并配置`kubectl`和`helm`来管理集群资源。
- en: How to do it…
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤：
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 本节进一步分为以下子节，以使此过程更加简单：
- en: Testing self-healing pods
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试自愈Pod
- en: Adding liveness probes to pods
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向Pod添加存活探针
- en: Testing self-healing pods
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试自愈Pod
- en: 'In this recipe, we will manually remove pods in our deployment to show how
    Kubernetes replaces them. Later, we will learn how to automate this using a user-defined
    health check. Now, let''s test Kubernetes'' self-healing for destroyed pods:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将手动删除部署中的Pod，以展示Kubernetes如何替换它们。稍后，我们将学习如何使用用户定义的健康检查来自动化这个过程。现在，让我们测试Kubernetes对被销毁的Pod的自愈能力：
- en: 'Create a deployment or StatefulSet with two or more replicas. As an example,
    we will use the MinIO application we used in the previous chapter, in the *Configuring
    and managing S3 object storage using MinIO* recipe. This example has four replicas:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建具有两个或更多副本的部署或StatefulSet。例如，我们将使用在上一章中使用的MinIO应用程序，在*配置和管理S3对象存储使用MinIO*示例中。此示例有四个副本：
- en: '[PRE83]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'List the MinIO pods that were deployed as part of the StatefulSet. You will
    see four pods:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出作为StatefulSet的一部分部署的MinIO pods。您会看到四个pods：
- en: '[PRE84]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Delete a pod to test Kubernetes'' auto-healing functionality and immediately
    list the pods again. You will see that the terminated pod will be quickly rescheduled
    and deployed:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除一个pod来测试Kubernetes的自愈功能，然后立即再次列出pods。您会看到被终止的pod将被快速重新调度和部署：
- en: '[PRE85]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: With this, you have tested Kubernetes' self-healing after manually destroying
    a pod in operation. Now, we will learn how to add a health status check to pods
    to let Kubernetes automatically kill non-responsive pods so that they're restarted.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 通过手动销毁一个正在运行的pod来测试Kubernetes的自愈功能。现在，我们将学习如何向pods添加健康状态检查，以便Kubernetes自动杀死无响应的pods，然后重新启动它们。
- en: Adding liveness probes to pods
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向pods添加活跃性探测
- en: 'Kubernetes uses liveness probes to find out when to restart a container. Liveness
    can be checked by running a liveness probe command inside the container and validating
    that it returns `0` through TCP socket liveness probes or by sending an HTTP request
    to a specified path. In that case, if the path returns a success code, then kubelet
    will consider the container to be healthy. In this recipe, we will learn how to
    send an HTTP request method to the example application. Let''s perform the following
    steps to add liveness probes:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用活跃性探测来确定何时重新启动容器。可以通过在容器内运行活跃性探测命令并验证它通过TCP套接字活跃性探测返回`0`，或者通过向指定路径发送HTTP请求来检查活跃性。在这种情况下，如果路径返回成功代码，那么kubelet将认为容器是健康的。在本示例中，我们将学习如何向示例应用程序发送HTTP请求方法。让我们执行以下步骤来添加活跃性探测：
- en: 'Edit the `minio.yaml` file in the `src/chapter7/autoheal/minio` directory and
    add the following `livenessProbe` section right under the `volumeMounts` section,
    before `volumeClaimTemplates`. Your YAML manifest should look similar to the following.
    This will send an HTTP request to the `/minio/health/live` location every `20`
    seconds to validate its health:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`src/chapter7/autoheal/minio`目录中的`minio.yaml`文件，并在`volumeMounts`部分下面添加以下`livenessProbe`部分，然后再添加`volumeClaimTemplates`。您的YAML清单应该类似于以下内容。这将每`20`秒向`/minio/health/live`位置发送HTTP请求以验证其健康状况：
- en: '[PRE86]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: For liveness probes that use HTTP requests to work, an application needs to
    expose unauthenticated health check endpoints. In our example, MinIO provides
    this through the `/minio/health/live` endpoint. If your workload doesn't have
    a similar endpoint, you may want to use liveness commands inside your pods to
    verify their health.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用HTTP请求进行活跃性探测的应用程序，需要公开未经身份验证的健康检查端点。在我们的示例中，MinIO通过`/minio/health/live`端点提供此功能。如果您的工作负载没有类似的端点，您可能希望在pod内部使用活跃性命令来验证其健康状况。
- en: 'Deploy the application. It will create four pods:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署应用程序。它将创建四个pods：
- en: '[PRE87]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Confirm the liveness probe by describing one of the pods. You will see a `Liveness`
    description similar to the following:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过描述其中一个pods来确认活跃性探测。您将看到类似以下的`Liveness`描述：
- en: '[PRE88]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'To test the liveness probe, we need to edit the `minio.yaml` file again. This
    time, set the `livenessProbe` port to  `8000`, which is where the application
    will not able to respond to the HTTP request. Repeat *Steps 2* and *3*, redeploy
    the application, and check the events in the pod description. You will see a `minio
    failed liveness probe, will be restarted` message in the events:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试活动探测，我们需要再次编辑“minio.yaml”文件。这次，将“livenessProbe”端口设置为“8000”，这是应用程序无法响应HTTP请求的地方。重复*步骤2*和*3*，重新部署应用程序，并检查pod描述中的事件。您将在事件中看到一个“minio
    failed liveness probe, will be restarted”消息：
- en: '[PRE89]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'You can confirm the restarts by listing the pods. You will see that every MinIO
    pod is restarted multiple times due to it having a failing liveness status:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过列出pod来确认重启。您会看到每个MinIO pod由于具有失败的活动状态而多次重新启动：
- en: '[PRE90]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: In this recipe, you learned how to implement the auto-healing functionality
    for applications that are running in Kubernetes clusters.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，您学会了如何为在Kubernetes集群中运行的应用程序实现自动修复功能。
- en: How it works...
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: This recipe showed you how to use a liveness probe on your applications running
    on Kubernetes.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程向您展示了如何在Kubernetes上运行的应用程序中使用活动探测。
- en: In the *Adding liveness probes to pods* recipe, in *Step 1*, we added an HTTP
    request-based health check.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在*向pod添加活动探测*教程中，在*步骤1*中，我们添加了基于HTTP请求的健康检查。
- en: By adding the StatefulSet path and port, we let kubelet probe the defined endpoints.
    Here, the `initialDelaySeconds` field tells kubelet that it should wait `120`
    seconds before the first probe. If your application takes a while to get the endpoints
    ready, then make sure that you allow enough time before the first probe; otherwise,
    your pods will be restarted before the endpoints can respond to requests.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加StatefulSet路径和端口，我们让kubelet探测定义的端点。在这里，“initialDelaySeconds”字段告诉kubelet在第一次探测之前应该等待“120”秒。如果您的应用程序需要一段时间才能准备好端点，那么请确保在第一次探测之前允许足够的时间；否则，您的pod将在端点能够响应请求之前重新启动。
- en: In *Step 3*, the `periodSeconds` field specifies that kubelet should perform
    a liveness probe every `20` seconds. Again, depending on the applications' expected
    availability, you should set a period that is right for your application.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤3*中，“periodSeconds”字段指定kubelet应每“20”秒执行一次活动探测。同样，根据应用程序的预期可用性，您应该设置适合您的应用程序的周期。
- en: See also
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Configuring liveness and readiness probes: [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置活动探测和就绪探测：[https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
- en: 'Kubernetes Best Practices: Setting up health checks:  [https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes](https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes)'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes最佳实践：设置健康检查：[https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes](https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-setting-up-health-checks-with-readiness-and-liveness-probes)
- en: Managing upgrades through blue/green deployments
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过蓝/绿部署管理升级
- en: The blue-green deployment architecture is a method that's used to reduce downtime
    by running two identical production environments that can be switched between
    when needed. These two environments are identified as blue and green. In this
    section, we will perform rollover application upgrades. You will learn how to
    roll over a new version of your application with persistent storage by using blue/green
    deployment in Kubernetes.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署架构是一种方法，用于通过运行两个可以在需要时切换的相同的生产环境来减少停机时间。这两个环境被标识为蓝色和绿色。在本节中，我们将执行滚动应用程序升级。您将学习如何使用Kubernetes中的蓝绿部署来滚动应用程序的新版本并使用持久存储。
- en: Getting ready
  id: totrans-419
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Make sure you have a Kubernetes cluster ready and `kubectl` and `helm` configured
    to manage the cluster resources.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已准备好一个Kubernetes集群，并配置了`kubectl`和`helm`来管理集群资源。
- en: For this recipe, we will need a persistent storage provider to take snapshots
    from one version of the application and use clones with the other version of the
    application to keep the persistent volume content. We will use OpenEBS as a persistent
    storage provider, but you can also use any CSI-compatible storage provider.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个步骤，我们将需要一个持久存储提供程序，以从应用程序的一个版本中获取快照，并使用另一个版本的应用程序的克隆来保持持久卷内容。我们将使用OpenEBS作为持久存储提供程序，但您也可以使用任何兼容CSI的存储提供程序。
- en: Make sure OpenEBS has been configured with the cStor storage engine by following
    the instructions in [Chapter 5](22439381-89a7-4cee-8aa1-77c63cb8a014.xhtml), *Preparing
    for Stateful Workload*s, in the *Persistent storage using OpenEBS* recipe.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 确保OpenEBS已经配置了cStor存储引擎，方法是按照[第5章](22439381-89a7-4cee-8aa1-77c63cb8a014.xhtml)中的说明进行操作，即*准备有状态工作负载*，在*使用OpenEBS进行持久存储*的步骤中。
- en: How to do it…
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'This section is further divided into the following subsections to make this
    process easier:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 本节进一步分为以下子节，以使此过程更容易：
- en: Creating the blue deployment
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建蓝色部署
- en: Creating the green deployment
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建绿色部署
- en: Switching traffic from blue to green
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从蓝色切换到绿色的流量
- en: Creating the blue deployment
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建蓝色部署
- en: There are many traditional workloads that won't work with Kubernetes' way of
    rolling updates. If your workload needs to deploy a new version and cut over to
    it immediately, then you may need to perform blue/green deployment instead. Using
    the blue/green deployment approach, we will label the current production blue.
    In the next recipe, we will create an identical production environment called
    green before redirecting the services to green.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多传统工作负载无法与Kubernetes的滚动更新方式配合使用。如果您的工作负载需要部署新版本并立即切换到新版本，则可能需要执行蓝绿部署。使用蓝绿部署方法，我们将标记当前生产环境为蓝色。在下一个步骤中，我们将创建一个名为绿色的相同生产环境，然后将服务重定向到绿色。
- en: 'Let''s perform the following steps to create the first application, which we
    will call blue:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤来创建第一个应用程序，我们将其称为蓝色：
- en: 'Change directory to where the examples for this recipe are located:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到此步骤示例所在的目录：
- en: '[PRE91]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Review the content of the `blue-percona.yaml` file and use that to create the
    blue version of your application:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`blue-percona.yaml`文件的内容，并使用它来创建应用程序的蓝色版本：
- en: '[PRE92]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Review the content of the `percona-svc.yaml`  file and use that to create the
    service. You will see that `selector` in the service is set to `app: blue`. This
    service will forward all the MySQL traffic to the blue pod:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '查看`percona-svc.yaml`文件的内容，并使用它来创建服务。您将看到服务中的`selector`设置为`app: blue`。此服务将所有MySQL流量转发到蓝色pod：'
- en: '[PRE93]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Get the service IP for `percona`. In our example, the Cluster IP is `10.3.0.75`:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取`percona`的服务IP。在我们的示例中，集群IP为`10.3.0.75`：
- en: '[PRE94]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Edit the `sql-loadgen.yaml` file and replace the target IP address with your
    percona service IP. In our example, it is `10.3.0.75`:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`sql-loadgen.yaml`文件，并将目标IP地址替换为您的percona服务IP。在我们的示例中，它是`10.3.0.75`：
- en: '[PRE95]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Start the load generator by running the `sql-loadgen.yaml` job:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行`sql-loadgen.yaml`作业来启动负载生成器：
- en: '[PRE96]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: This job will generate a MySQL load targeting the IP of the service that was
    forwarded to the Percona workload (currently blue).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 此作业将生成针对Percona工作负载（当前为蓝色）转发的服务的IP的MySQL负载。
- en: Creating the green deployment
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建绿色部署
- en: 'Let''s perform the following steps to deploy the new version of the application
    as our green deployment. We will switch the service to green, take a snapshot
    of blue''s persistent volume, and deploy the green workload in a new pod:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤来部署应用程序的新版本作为我们的绿色部署。我们将把服务切换到绿色，对蓝色的持久卷进行快照，并在新的pod中部署绿色工作负载：
- en: 'Let''s create a snapshot of the data from the blue application''s PVC and use
    it to deploy the green application:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建蓝色应用程序PVC的数据快照，并使用它来部署绿色应用程序：
- en: '[PRE97]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Review the content of the `green-percona.yaml` file and use that to create
    the green version of your application:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查`green-percona.yaml`文件的内容，并使用它来创建应用的绿色版本：
- en: '[PRE98]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: This pod will use a snapshot of the PVC from the blue application as its original
    PVC.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 这个pod将使用蓝色应用程序的PVC的快照作为其原始PVC。
- en: Switching traffic from blue to green
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从蓝色切换到绿色的流量
- en: 'Let''s perform the following steps to switch traffic from blue to the new green
    deployment:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤，将流量从蓝色切换到新的绿色部署：
- en: 'Edit the service using the following command and replace `blue` with `green`.
    Service traffic will be forwarded to the pod that is labeled `green`:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令编辑服务，并将`blue`替换为`green`。服务流量将被转发到标记为`green`的pod：
- en: '[PRE99]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: In this recipe, you have learned how to upgrade your application with a stateful
    workload using the blue/green deployment strategy.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，您已经学会了如何使用蓝/绿部署策略升级具有有状态工作负载的应用程序。
- en: See also
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Zero Downtime Deployments in Kubernetes with Jenkins: [https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/](https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/)
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes中使用Jenkins进行零停机部署：[https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/](https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/)
- en: 'A Simple Guide to blue/green Deployment: [https://codefresh.io/kubernetes-tutorial/blue-green-deploy/](https://codefresh.io/kubernetes-tutorial/blue-green-deploy/)'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝/绿部署的简单指南：[https://codefresh.io/kubernetes-tutorial/blue-green-deploy/](https://codefresh.io/kubernetes-tutorial/blue-green-deploy/)
- en: 'Kubernetes blue-green Deployment Examples: [https://github.com/ContainerSolutions/k8s-deployment-strategies/tree/master/blue-green](https://github.com/ContainerSolutions/k8s-deployment-strategies/tree/master/blue-green)'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes蓝绿部署示例：[https://github.com/ContainerSolutions/k8s-deployment-strategies/tree/master/blue-green](https://github.com/ContainerSolutions/k8s-deployment-strategies/tree/master/blue-green)
