- en: Kubernetes Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes网络
- en: 'For container orchestration, there are two major challenges to be solved: managing
    container hosts (nodes) and managing the networking between containers. If you
    limited your container host cluster to only one node, networking would be fairly
    simple—for Docker on Linux, you would use the default bridge network driver, which
    creates a private network (internal to the host), allowing the containers to communicate
    with each other. External access to the containers requires exposing and mapping
    container ports as host ports. But now, if you consider a multi-node cluster,
    this solution does not scale well—you have to use NAT and track which host ports
    are used, and on top of that, the applications running in containers have to be
    aware of the networking topology.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对于容器编排，有两个主要挑战需要解决：管理容器主机（节点）和管理容器之间的网络。如果将容器主机集群限制为仅一个节点，网络将会相当简单——对于Linux上的Docker，您将使用默认的桥接网络驱动程序，它创建一个私有网络（内部到主机），允许容器相互通信。对容器的外部访问需要暴露和映射容器端口作为主机端口。但是，现在如果考虑多节点集群，这个解决方案就不太适用——您必须使用NAT并跟踪使用了哪些主机端口，而且运行在容器中的应用程序还必须了解网络拓扑。
- en: Fortunately, Kubernetes provides a solution to this challenge by providing a
    networking model that has specific, fundamental requirements—any networking solution
    that complies with the specification can be used as the networking model implementation
    in Kubernetes. The goal of this model is to provide transparent container-to-container
    communication and external access to the containers, without containerized applications requiring any
    knowledge about the underlying networking challenges. Throughout this chapter,
    we will explain the assumptions of the Kubernetes networking model and how Kubernetes
    networking issues can be solved in hybrid Linux/Windows clusters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes通过提供一个具有特定基本要求的网络模型来解决这一挑战——符合规范的任何网络解决方案都可以作为Kubernetes中的网络模型实现。该模型的目标是提供透明的容器间通信和对容器的外部访问，而无需容器化应用程序了解底层网络挑战。在本章中，我们将解释Kubernetes网络模型的假设以及如何在混合Linux/Windows集群中解决Kubernetes网络问题。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Kubernetes networking principles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes网络原则
- en: Kubernetes CNI network plugins
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes CNI网络插件
- en: Windows server networking in Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes中的Windows服务器网络
- en: Choosing Kubernetes network modes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择Kubernetes网络模式
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您将需要以下内容：
- en: Windows 10 Pro, Enterprise, or Education (version 1903 or later, 64-bit) installed
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了Windows 10 Pro、企业版或教育版（1903版本或更高版本，64位）
- en: Docker Desktop for Windows 2.0.0.3 or later installed
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了Docker Desktop for Windows 2.0.0.3或更高版本
- en: Azure CLI installed if you would like to use the AKS cluster from the previous
    chapter
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想要使用上一章中的AKS集群，则需要安装Azure CLI
- en: Installation of Docker Desktop for Windows and system requirements are covered
    in [Chapter 1](deffbcf5-3a21-4690-ad42-ae5e4cd97dea.xhtml)*, Creating Containers*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Desktop for Windows的安装和系统要求在[第1章](deffbcf5-3a21-4690-ad42-ae5e4cd97dea.xhtml)中有介绍，创建容器。
- en: For Azure CLI, you can find detailed installation instructions in [Chapter 2](43d5e48b-311c-462c-a68e-6a0b5c4224e8.xhtml)*, Managing
    State in Containers**.*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Azure CLI，您可以在[第2章](43d5e48b-311c-462c-a68e-6a0b5c4224e8.xhtml)中找到详细的安装说明，管理容器中的状态。
- en: You can download the latest code samples for this chapter from the official
    GitHub repository: [https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从官方GitHub存储库下载本章的最新代码示例：[https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05)
- en: Kubernetes networking principles
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes网络原则
- en: 'As a container orchestrator, Kubernetes provides a networking model that consists
    of a set of requirements that any given networking solution must fulfill. The
    most important requirements are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为容器编排器，Kubernetes提供了一个网络模型，其中包含任何给定网络解决方案必须满足的一组要求。最重要的要求如下：
- en: Pods running on a node must be able to communicate with all Pods on all nodes
    (including the Pod's node) without NAT and explicit port mapping.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在节点上运行的Pod必须能够与所有节点上的所有Pod进行通信（包括Pod所在的节点），而无需NAT和显式端口映射。
- en: All Kubernetes components running on a node, for example kubelet or system daemons/services,
    must be able to communicate with all Pods on that node.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在节点上运行的所有Kubernetes组件，例如kubelet或系统守护程序/服务，必须能够与该节点上的所有Pod进行通信。
- en: These requirements enforce a flat, NAT-less network model, which is one of the
    core Kubernetes concepts that make it so powerful, scalable, and easy to use.
    From this perspective, Pods are similar to VMs running in a Hyper-V cluster—each
    Pod has its own IP address assigned (IP-per-Pod model), and containers within
    a Pod share the same network namespace (like processes on a VM), which means they
    share the same localhost and need to be aware of port assignments.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些要求强制执行了一个平面、无NAT的网络模型，这是使Kubernetes如此强大、可扩展和易于使用的核心概念之一。从这个角度来看，Pod类似于在Hyper-V集群中运行的VMs——每个Pod都分配了自己的IP地址（IP-per-Pod模型），Pod内的容器共享相同的网络命名空间（就像VM上的进程），这意味着它们共享相同的本地主机并且需要知道端口分配。
- en: 'In short, networking in Kubernetes has the following challenges to overcome:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Kubernetes中的网络有以下挑战需要克服：
- en: '**Intra-Pod communication between containers**: Handled by standard localhost
    communication.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器内部的Pod间通信**：由标准的本地主机通信处理。'
- en: '**Pod-to-Pod communication**: Handled by underlying network implementation.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod间通信**：由底层网络实现处理。'
- en: '**Pod-to-Service and External-to-Service communication**: Handled by Service
    API objects, communication is dependent on the underlying network implementation.
    We will cover this later in this section.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod到Service和外部到Service的通信**：由Service API对象处理，通信取决于底层网络实现。我们将在本节后面介绍这一点。'
- en: '**Automation of networking setup by kubelet when a new Pod is created**: Handled
    by **Container Network Interface** (**CNI**) plugins. We will cover this in the
    next section.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当创建新的Pod时，kubelet自动设置网络**：由**容器网络接口**（**CNI**）插件处理。我们将在下一节中介绍这一点。'
- en: There are numerous implementations of the Kubernetes networking model, ranging
    from simple L2 networking (for example, Flannel with a host-gw backend) to complex,
    high-performance **Software-Defined Networking** (**SDN**) solutions (for example,
    Big Cloud Fabric). You can find a list of different implementations of the networking
    model in the official documentation: [https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes网络模型有许多实现，从简单的L2网络（例如，带有host-gw后端的Flannel）到复杂的高性能**软件定义网络**（**SDN**）解决方案（例如，Big
    Cloud Fabric）。您可以在官方文档中找到不同实现的网络模型的列表：[https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model)。
- en: 'In this book, we will only focus on implementations that are relevant from
    the Windows perspective:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们将只关注从Windows角度相关的实现：
- en: L2 network
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2网络
- en: Overlay network
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖网络
- en: Let's begin with the L2 network, which is the simplest network implementation
    available.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最简单的网络实现L2网络开始。
- en: L2 network
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L2网络
- en: '**Layer 2** (**L2**) refers to the data link layer, which is the second level
    in the seven-layer OSI reference model for network protocol design. This layer
    is used to transfer data between nodes in the same local area network (so, think
    of operating on MAC addresses and switch ports, not IP addresses, which belong
    to L3). For Kubernetes, an L2 network with routing tables set up on each Kubernetes
    node is the simplest network type that fulfills the Kubernetes networking model
    implementation requirements. A good example is Flannel with a host-gw backend.
    At a high level, Flannel (host-gw) provides networking for Pods in the following
    way:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二层**（**L2**）指的是数据链路层，是网络协议设计的七层OSI参考模型中的第二层。该层用于在同一局域网中的节点之间传输数据（因此，考虑在MAC地址和交换机端口上操作，而不是IP地址，IP地址属于L3）。对于Kubernetes，具有在每个Kubernetes节点上设置路由表的L2网络是满足Kubernetes网络模型实现要求的最简单的网络类型。一个很好的例子是带有host-gw后端的Flannel。在高层次上，Flannel（host-gw）以以下方式为Pod提供网络：'
- en: Each node runs a **flanneld** (or **flanneld.exe **for Windows) agent, which
    is responsible for allocating a subnet lease from a larger, preconfigured address
    space called **Pod CIDR** (**Classless Inter-Domain Routing**). In the following
    diagram, Pod CIDR is `10.244.0.0/16`, whereas Node 1 has leased subnet `10.244.1.0/24`
    and Node 2 has leased subnet `10.244.2.0/24`.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个节点都运行一个**flanneld**（或者Windows上的**flanneld.exe**）代理，负责从一个称为**Pod CIDR**（**无类别域间路由**）的较大的预配置地址空间中分配子网租约。在下图中，Pod
    CIDR是`10.244.0.0/16`，而节点1租用了子网`10.244.1.0/24`，节点2租用了子网`10.244.2.0/24`。
- en: 'In most cases, the Flannel agent is deployed as a **DaemonSet** during Pod
    network installation in the cluster. An example DaemonSet definition can be found
    here: [https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml](https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml).'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在大多数情况下，Flannel代理在集群中进行Pod网络安装时部署为**DaemonSet**。可以在这里找到一个示例DaemonSet定义：[https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml](https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml)。
- en: Flannel stores networking information and lease data using the Kubernetes API
    or **etcd** directly, depending on its configuration.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Flannel使用Kubernetes API或**etcd**直接存储网络信息和租约数据，具体取决于其配置。
- en: When a new node joins the cluster, Flannel creates a `cbr0` bridge interface
    for all Pods on a given node. Routing tablesin the operating system on nodes are
    updated, containing one entry for each node in the cluster. For example, for Node
    2 in the following diagram, the routing table has two entries, which route communication
    to `10.244.1.0/24` via the `10.0.0.2` gateway (inter-node communication to Node
    1) and communication to `10.244.2.0/24` via the local `cbr0` interface (local
    communication between Pods on Node 1).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当新节点加入集群时，Flannel为给定节点上的所有Pod创建一个`cbr0`桥接口。节点上的操作系统中的路由表会被更新，其中包含集群中每个节点的一个条目。例如，在下图中的Node
    2中，路由表有两个条目，分别通过`10.0.0.2`网关（到Node 1的节点间通信）路由到`10.244.1.0/24`，以及通过本地`cbr0`接口（Node
    1上Pod之间的本地通信）路由到`10.244.2.0/24`。
- en: When a new Pod is created, a new **veth** device pair is created. An `eth0`
    device is created in the Pod network namespace and a `vethX` device at the other
    end of the pair in the host (root) namespace. A virtual Ethernet device is used
    as a tunnel between network namespaces.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当创建一个新的Pod时，会创建一个新的**veth**设备对。在Pod网络命名空间中创建一个`eth0`设备，以及在主机（根）命名空间中对端的`vethX`设备。虚拟以太网设备用作网络命名空间之间的隧道。
- en: 'In order to trigger the preceding actions, kubelet uses CNI, which is implemented
    by the Flannel CNI plugin:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了触发上述操作，kubelet使用了由Flannel CNI插件实现的CNI：
- en: '![](assets/fb8dbafa-cca9-4a1c-a8ed-0c7cae1ac530.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/fb8dbafa-cca9-4a1c-a8ed-0c7cae1ac530.png)'
- en: All of the actions performed by Flannel could be performed manually using the
    command line but, of course, the goal of Flannel is to automate the process of
    new node registration and new Pod network creation transparently for Kubernetes
    users.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel执行的所有操作都可以通过命令行手动执行，但是，Flannel的目标当然是自动化新节点注册和新Pod网络创建的过程，对Kubernetes用户来说是透明的。
- en: 'Let''s now quickly analyze what happens when a container in Pod 1, `10.244.1.2` (on
    Node 1), would like to send a TCP packet to a container in Pod 4, `10.244.2.3` (on
    Node 2):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们快速分析一下当Pod 1中的容器`10.244.1.2`（位于Node 1上）想要向Pod 4中的容器`10.244.2.3`（位于Node
    2上）发送TCP数据包时会发生什么：
- en: Pod 1 outbound packet will be sent to the `cbr0` bridge interface as it is set
    as the default gateway for the `eth0` Pod interface.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod 1的出站数据包将被发送到`cbr0`桥接口，因为它被设置为`eth0` Pod接口的默认网关。
- en: The packet is forwarded to the `10.0.0.3` gateway due to the `10.244.2.0/24
    → 10.0.0.3` routing table entry at Node 1.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于Node 1上的`10.244.2.0/24 → 10.0.0.3`路由表条目，数据包被转发到`10.0.0.3`网关。
- en: The packet goes through the physical L2 network switch and is received by Node
    2 at the `eth0` interface.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包通过物理L2网络交换机，并在Node 2的`eth0`接口接收。
- en: Node 2's routing table contains an entry that forwards traffic to the `10.244.2.0/24`
    CIDR to the local `cbr0` bridge interface.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Node 2的路由表包含一个条目，将流量转发到本地的`cbr0`桥接口的`10.244.2.0/24` CIDR。
- en: The packet is received by Pod 2.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包被Pod 2接收。
- en: Note that the preceding example uses Linux network interface naming and terminology.
    The Windows implementation of this model is generally the same but differs on
    OS-level primitives.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述示例使用了Linux网络接口命名和术语。这个模型的Windows实现通常是相同的，但在操作系统级别的原语上有所不同。
- en: 'Using an L2 network with routing tables is efficient and simple to set up;
    however, it has some disadvantages, especially as the cluster grows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用带有路由表的L2网络是高效且简单的设置；然而，它也有一些缺点，特别是在集群规模扩大时：
- en: L2 adjacency of nodes is required. In other words, all nodes must be in the
    same local area network with no L3 routers in between.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要节点的L2邻接性。换句话说，所有节点必须在同一个本地区域网络中，中间没有L3路由器。
- en: Synchronizing routing tables between all nodes. When a new node joins, all nodes
    need to update their routing tables.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有节点之间同步路由表。当新节点加入时，所有节点都需要更新它们的路由表。
- en: Possible glitches and delays, especially for short-lived containers, due to
    the way L2 network switches set up new MAC addresses in forwarding tables.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于L2网络交换机在转发表中设置新的MAC地址的方式，可能会出现可能的故障和延迟，特别是对于短暂存在的容器。
- en: Flannel with a **host-gw** backend has stable support for Windows.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 带有host-gw后端的Flannel对Windows有稳定的支持。
- en: Generally, using Overlay networking is recommended, which allows creating a
    virtual L2 network over an existing underlay L3 network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，建议使用覆盖网络，这允许在现有的底层L3网络上创建一个虚拟的L2网络。
- en: Overlay network
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 覆盖网络
- en: As a general concept, Overlay networking uses encapsulation in order to create
    a new, tunneled, virtual network on top of an existing L2/L3 network, called an
    underlay network. This network is created without any changes to the actual physical
    network infrastructure for the underlay network. Network services in the Overlay
    network are decoupled from the underlying infrastructure by encapsulation, which
    is a process of enclosing one type of packet using another type of packet. Packets
    that are encapsulated when entering a tunnel are then de-encapsulated at the other
    end of the tunnel.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个一般概念，覆盖网络使用封装来创建一个新的、隧道化的虚拟网络，位于现有的L2/L3网络之上，称为底层网络。这个网络是在不对底层网络的实际物理网络基础设施进行任何更改的情况下创建的。覆盖网络中的网络服务通过封装与底层基础设施分离，封装是一种使用另一种类型的数据包来封装一种类型的数据包的过程。进入隧道时封装的数据包然后在隧道的另一端进行解封装。
- en: 'Overlay networking is a broad concept that has many implementations. In Kubernetes,
    one of the commonly used implementations is using the **Virtual Extensible LAN**
    (**VXLAN) **protocol for tunneling L2 Ethernet frames via UDP packets. Importantly,
    this type of Overlay network works both for Linux and Windows nodes. If you have
    a Flannel network with a VXLAN backend, the networking for Pods is provided in
    the following way:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖网络是一个广泛的概念，有许多实现。在Kubernetes中，其中一个常用的实现是使用**虚拟可扩展局域网（VXLAN）**协议通过UDP数据包进行L2以太网帧的隧道传输。重要的是，这种类型的覆盖网络对Linux和Windows节点都适用。如果你有一个带有VXLAN后端的Flannel网络，Pods的网络是以以下方式提供的：
- en: Similarly to the host-gw backend, a flanneld agent is deployed on each node
    as a DaemonSet.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于host-gw后端，每个节点上都部署了一个flanneld代理作为DaemonSet。
- en: When a new node joins the cluster, Flannel creates a `cbr0` bridge interface for
    all Pods on a given node and an additional `flannel.<vni>` VXLAN device (a VXLAN
    tunnel endpoint, or VTEP for short; VNI stands for VXLAN Network Identifier, in
    this example, `1`). This device is responsible for the encapsulation of the traffic.
    The IP routing table is updated only for the new node. Traffic to Pods running
    on the same node is forwarded to the `cbr0` interface, whereas all the remaining
    traffic to Pod CIDR is forwarded to the VTEP device. For example, for Node 2 in
    the following diagram, the routing table has two entries that route the communication
    to `10.244.0.0/16` via the `flannel.1` VTEP device (inter-node communication in
    the Overlay network) and communication to `10.244.2.0/24` is routed via the local `cbr0` interface
    (local communication between Pods on Node 1 without Overlay).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当一个新的节点加入集群时，Flannel为给定节点上的所有Pods创建一个`cbr0`桥接口和一个额外的`flannel.<vni>`VXLAN设备（一个VXLAN隧道端点，或者简称为VTEP；VNI代表VXLAN网络标识符，在这个例子中是`1`）。这个设备负责流量的封装。IP路由表只对新节点进行更新。发送到在同一节点上运行的Pod的流量被转发到`cbr0`接口，而所有剩余的发送到Pod
    CIDR的流量被转发到VTEP设备。例如，在下图中的节点2，路由表有两个条目，将通信路由到`10.244.0.0/16`通过`flannel.1` VTEP设备（覆盖网络中的节点间通信），并且将通信路由到`10.244.2.0/24`通过本地的`cbr0`接口（节点1上的Pod之间的本地通信）。
- en: 'When a new Pod is created, a newveth device pair is created, similar to the
    case of the host-gw backend:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当创建一个新的Pod时，会创建一个新的veth设备对，类似于host-gw后端的情况：
- en: '![](assets/d1e6c841-ad79-4dd0-81fb-3bef9654e9c9.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d1e6c841-ad79-4dd0-81fb-3bef9654e9c9.png)'
- en: 'Let''s now quickly analyze what happens when a container in Pod 1 `10.244.1.2` (on
    Node 1) would like to send a TCP packet to a container in Pod 4, `10.244.2.3` (on
    Node 2):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们快速分析当Pod 1中的容器`10.244.1.2`（在节点1上）想要向Pod 4中的容器`10.244.2.3`（在节点2上）发送TCP数据包时会发生什么：
- en: Pod 1 outbound packet will be sent to the `cbr0` bridge interface as it is set
    as the default gateway for the `eth0` Pod interface.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod 1的出站数据包将被发送到`cbr0`桥接口，因为它被设置为`eth0` Pod接口的默认网关。
- en: The packet is forwarded to the `flannel.1` VTEP device due to the `10.244.0.0/16
    → flannel.1` routing table entry at Node 1.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于节点1上的`10.244.0.0/16 → flannel.1`路由表条目，数据包被转发到`flannel.1` VTEP设备。
- en: '`flannel.1` uses the MAC address of Pod 4 in the `10.244.0.0/16` Overlay network as
    the inner packet destination address. This address is populated by the **flanneld**
    agent in the **forwarding database** (**FDB**).'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`flannel.1`使用`10.244.0.0/16`叠加网络中Pod 4的MAC地址作为内部数据包的目的地址。这个地址是由**flanneld**代理在**转发数据库**（**FDB**）中填充的。'
- en: '`flannel.1 ` determines the IP address of the destination VTEP device at Node
    2 using the FDB, and `10.0.0.3` is used as the destination address for the outer
    encapsulating packet.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`flannel.1`使用FDB确定节点2的目标VTEP设备的IP地址，并使用`10.0.0.3`作为外部封装数据包的目的地址。'
- en: The packet goes through the physical L2/L3 network and is received by Node 2\.
    The packet is de-encapsulated by the `flannel.1` VTEP device.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包通过物理L2/L3网络传输，并被节点2接收。数据包由`flannel.1` VTEP设备进行解封装。
- en: Node 2's routing table contains an entry that forwards traffic to the `10.244.2.0/24` CIDR
    to the local `cbr0` bridge interface.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点2的路由表包含一个条目，将流量转发到本地的`cbr0`桥接口的`10.244.2.0/24` CIDR。
- en: The packet is received by Pod 2.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod 2接收到数据包。
- en: For Windows, Flannel with an Overlay backend is currently still in the alpha
    feature stage.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Windows，Flannel与Overlay后端目前仍处于alpha功能阶段。
- en: 'Using a VXLAN backend over a host-gw backend for Flannel has several advantages:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用VXLAN后端而不是host-gw后端的Flannel具有几个优势：
- en: No need for L2 adjacency of the nodes.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点之间不需要L2邻接。
- en: L2 Overlay networks are not susceptible to spanning tree failures, which can
    happen in the case of L2 domains spanning multiple logical switches.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2叠加网络不容易受到生成树故障的影响，这种情况可能发生在跨多个逻辑交换机的L2域的情况下。
- en: The solution described earlier in this section is similar to Docker running
    in **swarm mode**. You can read more about Overlay networks for swarm mode in
    the official documentation: [https://docs.docker.com/network/Overlay/.](https://docs.docker.com/network/overlay/.)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中描述的解决方案类似于Docker在**swarm模式**下运行。您可以在官方文档中了解有关swarm模式的Overlay网络的更多信息：[https://docs.docker.com/network/overlay/.](https://docs.docker.com/network/overlay/.)
- en: The preceding two networking solutions are the most commonly used solutions
    for hybrid Linux/Windows clusters, especially when running on-premises. For other
    scenarios, it is also possible to use **Open Virtual Network** (**OVN**) and **L2
    tunneling** for Azure-specific implementations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种网络解决方案是混合Linux/Windows集群中最常用的解决方案，特别是在本地运行时。对于其他情况，也可以使用**Open Virtual Network**（**OVN**）和**L2隧道**进行Azure特定实现。
- en: Other solutions
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他解决方案
- en: 'In terms of Windows-supported networking solutions for Kubernetes, there are
    two additional implementations that can be used:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 就Kubernetes支持的Windows网络解决方案而言，还有两种额外的实现可以使用：
- en: '**Open Virtual Network** (**OVN**), for example, as part of a deployment on
    OpenStack'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，**Open Virtual Network**（**OVN**）作为OpenStack部署的一部分
- en: '**L2 tunneling** for deployments on Azure'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Azure部署中使用**L2隧道**
- en: OVN is a network virtualization platform for implementing SDN that decouples
    physical network topology from the logical one. Using OVN, users can define network
    topologies consisting of logical switches and routers. Kubernetes supports OVN
    integration using a dedicated CNI plugin **ovn-kubernetes** ([https://github.com/ovn-org/ovn-kubernetes](https://github.com/ovn-org/ovn-kubernetes)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: OVN是一个用于实现SDN的网络虚拟化平台，它将物理网络拓扑与逻辑网络拓扑解耦。使用OVN，用户可以定义由逻辑交换机和路由器组成的网络拓扑。Kubernetes支持使用专用CNI插件**ovn-kubernetes**（[https://github.com/ovn-org/ovn-kubernetes](https://github.com/ovn-org/ovn-kubernetes)）进行OVN集成。
- en: For Azure-specific scenarios, it is possible to leverage Microsoft Cloud Stack
    features directly using the **Azure-CNI** plugin, which relies on the **L2Tunnel**
    Docker network driver. In short, Pods are connected to the existing virtual network
    resources and configurations, and all Pod packets are routed directly to the virtualization
    host in order to apply Azure SDN policies. Pods get full connectivity in the virtual
    network provided by Azure, which means that each Pod can be directly reached from
    outside of the cluster. You can find more details about this solution in the official
    AKS documentation: [https://docs.microsoft.com/bs-latn-ba/azure/aks/configure-azure-cni](https://docs.microsoft.com/bs-latn-ba/azure/aks/configure-azure-cni).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定于Azure的场景，可以使用**Azure-CNI**插件，该插件依赖于**L2Tunnel** Docker网络驱动程序，直接利用Microsoft
    Cloud Stack功能。简而言之，Pod连接到现有的虚拟网络资源和配置，并且所有Pod数据包直接路由到虚拟化主机，以应用Azure SDN策略。Pod在Azure提供的虚拟网络中获得完全的连通性，这意味着每个Pod都可以直接从集群外部访问。您可以在官方AKS文档中找到有关此解决方案的更多详细信息：[https://docs.microsoft.com/bs-latn-ba/azure/aks/configure-azure-cni](https://docs.microsoft.com/bs-latn-ba/azure/aks/configure-azure-cni)。
- en: Services
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务
- en: 'In the previous chapter, we covered Services as API objects and explained how
    they can be used together with Deployments. To briefly recap, Service API objects
    enable network access to a set of Pods, based on label selectors. In terms of
    Kubernetes networking, Services are a concept that''s built on top of the standard
    networking model, which aims to do the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了服务作为API对象，并解释了它们如何与部署一起使用。简单回顾一下，服务API对象基于标签选择器，使一组Pod可以进行网络访问。在Kubernetes网络方面，服务是建立在标准网络模型之上的概念，旨在实现以下目标：
- en: Enable reliable communication to a set of Pods using **Virtual IP** (**VIP**).
    Client Pods do not need to know the current IP addresses of individual Pods, which
    can change over time. External clients also do not need to know about current
    IP addresses of Pods.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**虚拟IP**（**VIP**）可靠地与一组Pod进行通信。客户端Pod不需要知道单个Pod的当前IP地址，因为这些地址随时间可能会发生变化。外部客户端也不需要知道Pod的当前IP地址。
- en: Load-balance network traffic (internal as well as external) to a set of Pods.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将网络流量（内部和外部）负载均衡到一组Pod。
- en: Enable service discovery in the cluster. This requires the DNS service add-on
    to be running in the cluster.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群中启用服务发现。这需要在集群中运行DNS服务附加组件。
- en: 'There are four Service types available in Kubernetes, which can be specified
    in the Service object specification:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中有四种可用的服务类型，可以在服务对象规范中指定。
- en: ClusterIP
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ClusterIP
- en: NodePort
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NodePort
- en: LoadBalancer
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoadBalancer
- en: ExternalName
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExternalName
- en: 'We will go through each type separately, but let''s first take a look at what
    a **Service** looks like in the context of Deployments and Pods:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分别讨论每种类型，但首先让我们看看在部署和Pod的上下文中**服务**是什么样子的：
- en: '![](assets/81db847a-1743-436c-a593-2c40163e4895.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/81db847a-1743-436c-a593-2c40163e4895.png)'
- en: 'The preceding diagram shows how the simplest internal Service of type ClusterIP
    exposes an existing Deployment that manages three replicas of Pods labeled with
    `environment: test`. The ClusterIP Service, with the same label selector, `environment:
    test`, is responsible for monitoring the result of label selector evaluation and
    updating **endpoint** API objects with the current set of alive and ready Pod
    IPs. At the same time, kube-proxy is observing Service and endpoint objects in
    order to create iptables rules on Linux nodes or HNS policies on Windows nodes,
    which are used to implement a Virtual IP address with a value of ClusterIP specified
    in the Service specification. Finally, when a client Pod sends a request to the
    Virtual IP, it is forwarded using the rules/policies (set up by kube-proxy) to
    one of the Pods in the Deployment. As you can see, kube-proxy is the central component
    for implementing Services, and in fact it is used for all Service types, apart
    from ExternalName.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '上述图表显示了ClusterIP类型的最简单的内部Service是如何公开管理三个带有`environment: test`标签的Pod的现有部署。具有相同标签选择器`environment:
    test`的ClusterIP Service负责监视标签选择器评估的结果，并使用当前的存活和准备好的Pod IP更新**endpoint** API对象。同时，kube-proxy观察Service和endpoint对象，以在Linux节点上创建iptables规则或在Windows节点上创建HNS策略，用于实现具有Service规范中指定的ClusterIP值的虚拟IP地址。最后，当客户端Pod向虚拟IP发送请求时，它将使用kube-proxy设置的规则/策略转发到部署中的一个Pod。正如您所看到的，kube-proxy是实现服务的中心组件，实际上它用于所有服务类型，除了ExternalName。'
- en: ClusterIP
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ClusterIP
- en: 'The default type of Service in Kubernetes is ClusterIP, which exposes a service
    using an internal VIP. This means that the Service will only be reachable from
    within the cluster. Assuming that you are running the following `nginx` Deployment:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中默认的Service类型是ClusterIP，它使用内部VIP公开服务。这意味着Service只能在集群内部访问。假设您正在运行以下`nginx`部署：
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'All of the manifest files are available in the official GitHub repository for
    this book: [https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 所有清单文件都可以在本书的官方GitHub存储库中找到：[https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05)。
- en: 'You can deploy a Service of ClusterIP type using the following manifest file:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下清单文件部署ClusterIP类型的Service：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Like for every Service type, the key part is the `selector` specification,
    which has to match the Pods in the Deployment. You specify `type` as `ClusterIP`
    and assign `8080` as a port on the Service, which is mapped to `targetPort: 80` on
    the Pod. This means that client Pod would use the `nginx-deployment-example:8080`
    TCP endpoint for communicating with nginx Pods. The actual ClusterIP address is
    assigned dynamically, unless you explicitly specify one in the `spec`. The internal
    DNS Service in a Kubernetes cluster is responsible for resolving `nginx-deployment-example`
    to the actual ClusterIP address as a part of service discovery.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '与每种Service类型一样，关键部分是`selector`规范，它必须与部署中的Pod匹配。您将`type`指定为`ClusterIP`，并在Service上分配`8080`作为端口，该端口映射到Pod上的`targetPort:
    80`。这意味着客户端Pod将使用`nginx-deployment-example:8080` TCP端点与nginx Pods进行通信。实际的ClusterIP地址是动态分配的，除非您在`spec`中明确指定一个。Kubernetes集群中的内部DNS服务负责将`nginx-deployment-example`解析为实际的ClusterIP地址，作为服务发现的一部分。'
- en: The diagrams in the rest of this section represent how the Services are implemented logically.
    Under the hood, kube-proxy is responsible for managing all the forwarding rules
    and exposing ports, as in the previous diagram.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本节其余部分的图表表示了服务在逻辑上是如何实现的。在幕后，kube-proxy负责管理所有转发规则和公开端口，就像前面的图表中一样。
- en: 'This has been visualized in the following diagram:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下图表中进行了可视化：
- en: '![](assets/418727d5-dc7f-40bb-b753-5afd78678a49.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/418727d5-dc7f-40bb-b753-5afd78678a49.png)'
- en: 'ClusterIP Services are a base for the other types of Service that allow external
    communication: NodePort and LoadBalancer.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterIP服务是允许外部通信的其他服务类型的基础：NodePort和LoadBalancer。
- en: NodePort
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NodePort
- en: The first type of Service allowing external ingress communication to Pods is
    the NodePort Service. This type of Service is implemented as a ClusterIP Service
    with the additional capability of being reachable using any cluster node IP address
    and a specified port. In order to achieve that, kube-proxy exposes the same port
    on each node in the range 30000-32767 (which is configurable) and sets up forwarding
    so that any connections to this port will be forwarded to ClusterIP.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 允许对Pod进行外部入口通信的第一种服务类型是NodePort服务。这种类型的服务被实现为ClusterIP服务，并具有使用任何集群节点IP地址和指定端口可达的额外功能。为了实现这一点，kube-proxy在30000-32767范围内的每个节点上公开相同的端口（可配置），并设置转发，以便将对该端口的任何连接转发到ClusterIP。
- en: 'You can deploy a NodePort Service using the following manifest file:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下清单文件部署NodePort服务：
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you do not specify `nodePort` in the Spec, it will be allocated dynamically
    using the NodePort range. Note that the service still acts as a ClusterIP Service,
    which means that it is internally reachable at its ClusterIP endpoint.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在规范中未指定`nodePort`，则将动态分配使用NodePort范围。请注意，服务仍然充当ClusterIP服务，这意味着它在其ClusterIP端点内部可达。
- en: 'The following diagram visualizes the concept of a NodePort Service:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表可视化了NodePort服务的概念：
- en: '![](assets/f843f656-4526-48b2-a0ca-b918b742410f.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f843f656-4526-48b2-a0ca-b918b742410f.png)'
- en: Using a NodePort Service is recommended when you would like to set up your own
    load-balancing setup in front of the Service. You can also expose the NodePorts
    directly, but bear in mind that such a solution is harder to secure and may pose
    security risks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当您希望在服务前设置自己的负载均衡设置时，建议使用NodePort服务。您也可以直接暴露NodePorts，但请记住，这样的解决方案更难以保护，并可能存在安全风险。
- en: LoadBalancer
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoadBalancer
- en: The second type of Service that allows for external ingress communication is
    LoadBalancer, which is available in Kubernetes clusters that can create external
    load balancers, for example, managed Kubernetes offerings in the cloud. This type
    of Service combines the approach of NodePort with an additional external load
    balancer in front of it, which routes traffic to NodePorts.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 允许外部入口通信的第二种服务类型是LoadBalancer，在可以创建外部负载均衡器的Kubernetes集群中可用，例如云中的托管Kubernetes服务。这种类型的服务将NodePort的方法与额外的外部负载均衡器结合在一起，该负载均衡器将流量路由到NodePorts。
- en: 'You can deploy a LoadBalancer Service using the following manifest file:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下清单文件部署LoadBalancer服务：
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that in order to apply this manifest file, you need an environment that
    supports external load balancers, for example, the AKS cluster that we created
    in [Chapter 4](118e3c89-786e-4718-ba67-6c38928e2a42.xhtml), *Kubernetes Concepts
    and Windows Support*. Katacoda Kubernetes Playground is also capable of creating
    an "external" load balancer that can be accessed from the Playground terminal.
    If you attempt to create a LoadBalancer Service in an environment that does not
    support creating external load balancers, it will result in the load balancer
    ingress IP address being in the *pending* state indefinitely.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了应用此清单文件，您需要支持外部负载均衡器的环境，例如我们在[第4章](118e3c89-786e-4718-ba67-6c38928e2a42.xhtml)中创建的AKS集群，*Kubernetes概念和Windows支持*。Katacoda
    Kubernetes Playground还能够创建可以从Playground终端访问的“外部”负载均衡器。如果您尝试在不支持创建外部负载均衡器的环境中创建LoadBalancer服务，将导致负载均衡器入口IP地址无限期处于*pending*状态。
- en: 'In order to obtain an external load balancer address, execute the following
    command:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得外部负载均衡器地址，请执行以下命令：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `EXTERNAL-IP` column shows that the load balancer has the IP address `137.117.227.83`,
    and in order to access your Service you have to communicate with the `137.117.227.83:8080`
    TCP endpoint. Additionally, you can see that the Service has its own internal
    ClusterIP , `10.0.190.215`, and NodePort `30141` is exposed. A LoadBalancer Service
    running on AKS has been visualized in the following diagram:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`EXTERNAL-IP`列显示负载均衡器具有IP地址`137.117.227.83`，为了访问您的服务，您必须与`137.117.227.83:8080`
    TCP端点通信。此外，您可以看到服务有自己的内部ClusterIP，`10.0.190.215`，并且公开了NodePort `30141`。在AKS上运行的LoadBalancer服务已在以下图表中可视化：'
- en: '![](assets/3bbb81f4-46df-4681-914d-c8d192f1d55c.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/3bbb81f4-46df-4681-914d-c8d192f1d55c.png)'
- en: 'If you are interested in how the Azure Load Balancer in front of the Service
    is configured, you need to go to [https://portal.azure.com](https://portal.azure.com)
    and navigate to the load balancer resources, where you will find the Kubernetes
    load balancer instance:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对服务前面的Azure负载均衡器的配置感兴趣，您需要转到[https://portal.azure.com](https://portal.azure.com)并导航到负载均衡器资源，您将在那里找到Kubernetes负载均衡器实例：
- en: '![](assets/ac15b906-1332-4ac6-8772-a7fb5d1069b5.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ac15b906-1332-4ac6-8772-a7fb5d1069b5.png)'
- en: 'Now, let''s take a look at the last type of Service: ExternalName.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看最后一种服务类型：ExternalName。
- en: ExternalName
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ExternalName
- en: In some cases, you need to define a Service that points to an external resource
    that is not hosted in your Kubernetes cluster. This can include, for example,
    cloud-hosted database instances. Kubernetes provides a way to abstract communication
    to such resources and registers them in cluster service discovery by using an ExternalName
    Service.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您需要定义一个指向不托管在Kubernetes集群中的外部资源的服务。例如，这可能包括云托管的数据库实例。Kubernetes提供了一种将通信抽象化到这些资源并通过使用ExternalName服务在集群服务发现中注册它们的方法。
- en: 'ExternalName Services do not use selectors and are just a raw mapping of the
    Service name to an external DNS name:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalName服务不使用选择器，只是服务名称到外部DNS名称的原始映射：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: During resolution of the Service DNS name (`externalname-example-service.default.svc.cluster.local`),
    the internal cluster DNS will respond with a CNAME record with a value of `cloud.database.example.com`.
    There is no actual traffic forwarding using kube-proxy rules involved—the redirection
    happens at the DNS level.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析服务DNS名称（`externalname-example-service.default.svc.cluster.local`）期间，内部集群DNS将响应具有值`cloud.database.example.com`的CNAME记录。没有使用kube-proxy规则进行实际的流量转发-重定向发生在DNS级别。
- en: A good use case for the ExtenalName Service is providing different instances
    of an external service, for example, a database, depending on the environment
    type. From the Pods' point of view, this will not require any configuration or
    connection string changes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ExternalName服务的一个很好的用例是根据环境类型提供外部服务的不同实例，例如数据库。从Pod的角度来看，这不需要任何配置或连接字符串更改。
- en: Ingress
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ingress
- en: 'LoadBalancer Services only provide L4 load balancing capabilities. This means
    you cannot use the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: LoadBalancer服务仅提供L4负载平衡功能。这意味着您不能使用以下内容：
- en: HTTPS traffic termination and offloading
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTPS流量终止和卸载
- en: Name-based virtual hosting using the same load balancer for multiple domain
    names
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用相同的负载均衡器进行基于名称的虚拟主机托管多个域名
- en: Path-based routing to services, for example, as an API gateway
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于路径的路由到服务，例如作为API网关
- en: To solve that, Kubernetes provides the Ingress API object (which is not a type
    of Service), which can be used for L7 load balancing.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Kubernetes提供了Ingress API对象（不是服务类型），可用于L7负载平衡。
- en: Ingress deployment and configuration is a broad topic and is out of scope of
    this book. You can find more detailed information regarding Ingress and Ingress
    controllers in the official documentation: [https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress部署和配置是一个广泛的主题，超出了本书的范围。您可以在官方文档中找到关于Ingress和Ingress控制器的更详细信息：[https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/)。
- en: 'Using Ingress requires first the deployment of an Ingress controller in your
    Kubernetes cluster. An Ingress controller is a Kubernetes controller that is deployed
    manually to the cluster, most often as a DaemonSet or a deployment that runs dedicated
    Pods for handling ingress traffic load balancing and smart routing. A commonly
    used Ingress controller for Kubernetes is **ingress-nginx** ([https://www.nginx.com/products/nginx/kubernetes-ingress-controller](https://www.nginx.com/products/nginx/kubernetes-ingress-controller)),
    which is installed in the cluster as a Deployment of an nginx web host with a
    set of rules for handling Ingress API objects. The Ingress controller is exposed
    as a Service with a type that depends on the installation. For example, for an
    AKS cluster that only has Linux nodes, a basic installation of ingress-nginx,
    exposed as a LoadBalancer Service, can be performed using the following manifests:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Ingress首先需要在您的Kubernetes集群中部署一个Ingress控制器。Ingress控制器是一个Kubernetes控制器，通常作为一个DaemonSet或运行专用Pod来处理入口流量负载均衡和智能路由，手动部署到集群中。Kubernetes中常用的Ingress控制器是**ingress-nginx**（[https://www.nginx.com/products/nginx/kubernetes-ingress-controller](https://www.nginx.com/products/nginx/kubernetes-ingress-controller)），它作为一个nginx
    web主机的部署，具有一组规则来处理Ingress API对象。Ingress控制器以一种取决于安装的类型的Service暴露出来。例如，对于只有Linux节点的AKS集群，可以使用以下清单执行ingress-nginx的基本安装，将其暴露为LoadBalancer
    Service。
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In general, the installation of the Ingress controller is dependent on the Kubernetes
    cluster environment and configuration and has to be adjusted according to your
    needs. For example, for AKS with Windows nodes, you need to ensure that proper node
    selectors are used in order to schedule Ingress controller Pods properly.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Ingress控制器的安装取决于Kubernetes集群环境和配置，并且必须根据您的需求进行调整。例如，对于带有Windows节点的AKS，您需要确保适当的节点选择器被使用，以便正确调度Ingress控制器Pod。
- en: You can find the customized nginx Ingress controller definition for AKS with
    Windows nodes, together with example Services and Ingress definitions, in the
    official GitHub repository for this book: [https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05/05_ingress-example](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05/05_ingress-example).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的官方GitHub存储库中找到针对带有Windows节点的AKS的定制nginx Ingress控制器定义，以及示例服务和Ingress定义：[https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05/05_ingress-example](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter05/05_ingress-example)。
- en: 'When an Ingress controller has been installed in the cluster, the Ingress API
    Objects may be created and will be handled by the controller. For example, assuming
    that you have deployed two ClusterIP Services `example-service1` and `example-service2`,
    the Ingress definition could look as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当Ingress控制器已经安装在集群中时，Ingress API对象可以被创建并且会被控制器处理。例如，假设您已经部署了两个ClusterIP服务`example-service1`和`example-service2`，Ingress定义可能如下所示：
- en: '[PRE7]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, when you perform a HTTP request to `https://<ingressServiceLoadBalancerIp>/service1`,
    the traffic will be routed by nginx to `example-service1`. Note that you are using
    only one cloud load balancer for this operation and the actual routing to Kubernetes
    Services is performed by an Ingress controller using path-based routing.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当您对`https://<ingressServiceLoadBalancerIp>/service1`发出HTTP请求时，流量将由nginx路由到`example-service1`。请注意，您只使用一个云负载均衡器进行此操作，实际的路由到Kubernetes服务是由Ingress控制器使用基于路径的路由来执行的。
- en: 'The principle of this design has been shown in the following diagram:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计原则已在以下图表中显示：
- en: '![](assets/e56b2286-fc83-40f0-8ee7-bd8c662ff394.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/e56b2286-fc83-40f0-8ee7-bd8c662ff394.png)'
- en: For AKS, you can consider using the HTTP application routing add-on, which automates
    the management of Ingress controllers and External-DNS controllers for your cluster. More
    details can be found in the official documentation: [https://docs.microsoft.com/en-us/azure/aks/http-application-routing](https://docs.microsoft.com/en-us/azure/aks/http-application-routing).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于AKS，您可以考虑使用HTTP应用程序路由附加组件，它可以自动管理Ingress控制器和集群的External-DNS控制器。更多细节可以在官方文档中找到：[https://docs.microsoft.com/en-us/azure/aks/http-application-routing](https://docs.microsoft.com/en-us/azure/aks/http-application-routing)。
- en: A general rule of thumb for choosing whether to implement an Ingress or a Service
    is to use an Ingress for exposing HTTP (and especially HTTPS) endpoints and to
    use Services for other protocols.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 选择是否实现Ingress或Service的一个一般准则是使用Ingress来暴露HTTP（尤其是HTTPS）端点，并使用Service来处理其他协议。
- en: Kubernetes CNI network plugins
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes CNI网络插件
- en: In this chapter, we have already mentioned the terms **Container Network Interface**
    (**CNI**) and **CNI plugins **in the context of Kubernetes networking setup. In
    fact, CNI is not limited to Kubernetes—this concept originated from the Rkt container
    runtime and was adopted as a CNCF project aiming to provide a simple and clear
    interface between any container runtime and network implementation. Container
    runtimes use CNI plugins in order to connect containers to the network and remove
    them from the network when needed.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们已经提到了**容器网络接口**（**CNI**）和**CNI插件**这两个术语，这是在Kubernetes网络设置的背景下。事实上，CNI并不局限于Kubernetes——这个概念起源于Rkt容器运行时，并被采纳为CNCF项目，旨在为任何容器运行时和网络实现提供一个简单明了的接口。容器运行时使用CNI插件来连接容器到网络，并在需要时将它们从网络中移除。
- en: Understanding the CNI project
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解CNI项目
- en: 'There are three distinct parts of the CNI project:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: CNI项目有三个明确的部分：
- en: The CNI specification defines the architecture of a generic, plugin-based networking
    solution for containers and the actual interface that CNI plugins must implement.
    The specification can be found at [https://github.com/containernetworking/cni/blob/master/SPEC.md](https://github.com/containernetworking/cni/blob/master/SPEC.md).
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNI规范定义了一个通用的、基于插件的容器网络解决方案的架构，以及CNI插件必须实现的实际接口。规范可以在[https://github.com/containernetworking/cni/blob/master/SPEC.md](https://github.com/containernetworking/cni/blob/master/SPEC.md)找到。
- en: Libraries for integrating CNI into applications, which is provided in the same
    repository as the specification: [https://github.com/containernetworking/cni/tree/master/libcni](https://github.com/containernetworking/cni/tree/master/libcni).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将CNI集成到应用程序中的库可以在与规范相同的存储库中找到：[https://github.com/containernetworking/cni/tree/master/libcni](https://github.com/containernetworking/cni/tree/master/libcni)。
- en: CNI plugins reference implementation, which is in a dedicated repository: [https://github.com/containernetworking/plugins](https://github.com/containernetworking/plugins).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNI插件的参考实现可以在专用存储库中找到：[https://github.com/containernetworking/plugins](https://github.com/containernetworking/plugins)。
- en: 'The specification of CNI is fairly straightforward, and can be summarized as
    follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: CNI的规范非常简单明了，可以总结如下：
- en: CNI plugins are implemented as stand-alone executables.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNI插件是作为独立的可执行文件实现的。
- en: The container runtime is responsible for preparing a new network namespace (or
    network compartment in the case of Windows) for the container, before interacting
    with CNI plugins.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器运行时负责在与CNI插件交互之前为容器准备一个新的网络命名空间（或在Windows情况下为网络隔间）。
- en: The CNI plugin is responsible for connecting the container to the network, as
    specified by the network configuration.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNI插件负责将容器连接到由网络配置指定的网络。
- en: Network configuration is supplied to CNI plugins by the container runtime in
    JSON format, using standard input.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络配置以JSON格式由容器运行时通过标准输入提供给CNI插件。
- en: Arguments are provided to the CNI plugin using environment variables. For example,
    the `CNI_COMMAND` variable specifies the operation type that the plugin should
    perform. The set of commands is limited and consists of `ADD`, `DEL`, `CHECK`,
    and `VERSION`; the most significant are `ADD` and `DEL`, which add a container
    to the network and delete it from the network respectively.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用环境变量向CNI插件提供参数。例如，`CNI_COMMAND`变量指定插件应执行的操作类型。命令集是有限的，包括`ADD`、`DEL`、`CHECK`和`VERSION`；其中最重要的是`ADD`和`DEL`，分别用于将容器添加到网络和从网络中删除容器。
- en: 'For CNI plugins, there are three general types of plugin, which have different
    responsibilities during network configuration:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CNI插件，有三种常见类型的插件，在网络配置期间负责不同的责任：
- en: Interface-creating plugins
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接口创建插件
- en: '**IP Address Management **(**IPAM**) plugins, which are responsible for IP
    address allocation for containers'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IP地址管理（IPAM）**插件负责为容器分配IP地址。'
- en: Meta plugins, which may act as adapters for other CNI plugins or provide additional
    configuration for other CNI plugins or transform their outputs
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元插件可以作为其他CNI插件的适配器，或为其他CNI插件提供额外的配置或转换它们的输出。
- en: 'Currently, on Windows you can only use the following reference implementations:
    the host-local IPAM plugin, the win-bridge and win-Overlay interface-creating
    plugins, and the flannel meta plugin. Third-party plugins can be also used; for
    example, Microsoft provides the Azure-CNI plugin for integrating containers with
    Azure SDN ([https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md](https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md)).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，在Windows上只能使用以下参考实现：host-local IPAM插件、win-bridge和win-Overlay接口创建插件，以及flannel元插件。也可以使用第三方插件；例如，微软提供了Azure-CNI插件，用于将容器与Azure
    SDN集成（[https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md](https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md)）。
- en: In Kubernetes, CNI plugins are used by kubelet when managing a Pod's life cycle
    to ensure the connectivity and reachability of the Pod. The most basic operation
    performed by Kubelet is executing the `ADD` CNI command when a Pod is created
    and the `DELETE` CNI command when a Pod is destroyed. CNI plugins may be also
    used for adjusting the kube-proxy configuration in some cases.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，kubelet在管理Pod的生命周期时使用CNI插件，以确保Pod的连通性和可达性。Kubelet执行的最基本操作是在创建Pod时执行`ADD`
    CNI命令，在销毁Pod时执行`DELETE` CNI命令。在某些情况下，CNI插件也可以用于调整kube-proxy的配置。
- en: Choosing the CNI plugin and defining the network configuration for the CNI plugin
    is performed during the Pod network add-on installation step when deploying a
    new cluster. Most commonly, the installation is performed by the deployment of
    a dedicated DaemonSet, which performs the installation of CNI plugins using init
    containers and runs additional agent containers on each node if needed. A good
    example of such an installation is the official Kubernetes manifest for Flannel: [https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml](https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署新集群时，选择CNI插件并定义CNI插件的网络配置是在Pod网络附加组件安装步骤中执行的。最常见的安装方式是通过部署专用的DaemonSet来执行，该DaemonSet使用init容器执行CNI插件的安装，并在每个节点上运行额外的代理容器（如果需要的话）。这种安装的一个很好的例子是Flannel的官方Kubernetes清单：[https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml](https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml)。
- en: CoreOS Flannel
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CoreOS Flannel
- en: 'When working with Linux/Windows hybrid Kubernetes clusters, especially on-premises,
    you will usually install **Flannel** as a Pod network add-on ([https://github.com/coreos/flannel](https://github.com/coreos/flannel)).
    Flannel is a minimalistic L2/L3 virtual network provider for multiple nodes, specifically
    for Kubernetes and containers. There are three main components in Flannel:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Linux/Windows混合Kubernetes集群时，特别是在本地部署时，通常会将**Flannel**作为Pod网络附加组件安装（[https://github.com/coreos/flannel](https://github.com/coreos/flannel)）。Flannel是一个面向多个节点的Kubernetes和容器的最小化L2/L3虚拟网络提供程序。Flannel有三个主要组件：
- en: A **flanneld** (or `flanneld.exe` on Windows machines) agent/daemon running
    on each node in the cluster, usually deployed as a DaemonSet in Kubernetes. It
    is responsible for allocating an exclusive subnet lease for each node out of a
    larger Pod's CIDR. For example, in this chapter, we have been heavily using `10.244.0.0/16`
    as the Pod CIDR in the cluster and `10.244.1.0/24` or `10.244.2.0/24` as subnet
    leases for individual nodes. Lease information and node network configuration
    is stored by `flanneld` using the Kubernetes API or directly in `etcd`. The main
    responsibility of this agent is synchronizing subnet lease information, configuring
    the Flannel backend, and exposing the configuration (as a file in the container
    host filesystem) on the node for other components, such as the Flannel CNI plugin.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群中的每个节点上都运行一个**flanneld**（或者在Windows机器上是`flanneld.exe`）代理/守护进程，通常作为Kubernetes中的一个DaemonSet部署。它负责为每个节点分配一个较大的Pod
    CIDR中的独占子网租约。例如，在本章中，我们一直在集群中使用`10.244.0.0/16`作为Pod CIDR，而在单个节点上使用`10.244.1.0/24`或`10.244.2.0/24`作为子网租约。租约信息和节点网络配置由`flanneld`使用Kubernetes
    API或直接存储在`etcd`中。该代理的主要责任是同步子网租约信息，配置Flannel后端，并在节点上为其他组件（如Flannel CNI插件）公开配置（作为容器主机文件系统中的文件）。
- en: The Flannel **backend**, which defines how the network between Pods is created.
    Examples of backends supported on both Windows and Linux that we have already
    used in this chapter are Vxlan and host-gw. You can find more about Flannel backends
    at [https://github.com/coreos/flannel/blob/master/Documentation/backends.md](https://github.com/coreos/flannel/blob/master/Documentation/backends.md).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flannel的**后端**定义了Pod之间的网络是如何创建的。在本章中我们已经使用过的在Windows和Linux上都支持的后端的例子有Vxlan和host-gw。您可以在[https://github.com/coreos/flannel/blob/master/Documentation/backends.md](https://github.com/coreos/flannel/blob/master/Documentation/backends.md)找到更多关于Flannel后端的信息。
- en: 'The Flannel **CNI plugin**, which is executed by kubelet when adding a Pod
    to a network or removing a Pod from a network. The Flannel CNI plugin is a meta
    plugin that uses other interface-creating and IPAM plugins to perform the operations.
    Its responsibility is to read the subnet information provided by `flanneld`, generate
    JSON configuration for an appropriate CNI plugin, and execute it. The target plugins
    choice depends on which backend is used by Flannel; for example, for a vxlan backend
    on Windows nodes, the Flannel CNI plugin will invoke the host-local IPAM plugin
    and the win-Overlay plugin. You can find more about this meta plugin in the official
    documentation: [https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel](https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flannel **CNI插件**是由kubelet在将Pod添加到网络或从网络中移除Pod时执行的。Flannel CNI插件是一个元插件，它使用其他创建接口和IPAM插件来执行操作。它的责任是读取`flanneld`提供的子网信息，为适当的CNI插件生成JSON配置，并执行它。目标插件的选择取决于Flannel使用的后端；例如，在Windows节点上使用vxlan后端，Flannel
    CNI插件将调用host-local IPAM插件和win-Overlay插件。您可以在官方文档中找到有关这个元插件的更多信息：[https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel](https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel)。
- en: 'Let''s take a look at what happens step by step on a Windows node with Flannel
    running on a vxlan backend—from Flannel agent deployment to Pod creation by kubelet
    (similar steps occur for Linux nodes but with different target CNI plugins being
    executed):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步看看在运行在vxlan后端的Windows节点上发生的事情——从Flannel代理部署到kubelet创建Pod（类似的步骤也发生在Linux节点上，但执行不同的目标CNI插件）：
- en: The `flanneld.exe` agent is deployed to the node as a DaemonSet or is started
    manually (as the current documentation for Windows suggests).
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`flanneld.exe`代理作为一个DaemonSet部署到节点上，或者按照当前Windows文档的建议手动启动。'
- en: 'The agent reads the supplied `net-conf.json` file, which contains the Pod CIDR
    and the vxlan backend configuration:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理读取提供的`net-conf.json`文件，其中包含Pod CIDR和`vxlan`后端配置：
- en: '[PRE8]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The agent acquires a new subnet lease, `10.244.1.0/24`, for the node. Lease
    information is stored using the Kubernetes API. The `vxlan0` network is created,
    VTEP devices are created, and routing tables and forwarding database are updated.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理为节点获取一个新的子网租约`10.244.1.0/24`。租约信息存储在Kubernetes API中。创建`vxlan0`网络，创建VTEP设备，并更新路由表和转发数据库。
- en: 'Information about the subnet lease is written to `C:\run\flannel\subnet.env`
    in the node filesystem. Here''s an example:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 子网租约的信息被写入到节点文件系统中的`C:\run\flannel\subnet.env`。这是一个例子：
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Whenever a new node joins the cluster, the `flanneld.exe` agent performs any
    additional reconfiguration to the routing tables and forwarding database.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每当一个新节点加入集群时，`flanneld.exe`代理会对路由表和转发数据库进行任何额外的重新配置。
- en: Now, a new Pod is scheduled on this node and kubelet initializes the pod infra
    container and executes the `ADD` command on the Flannel meta CNI plugin with the
    configuration JSON, which delegates interface creation to the `win-Overlay` plugin
    and IPAM management to the `host-local` plugin. The Flannel CNI plugin generates
    the configuration JSON based on `subnet.env` and input configuration for these
    plugins.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，一个新的Pod被调度到这个节点上，kubelet初始化pod基础容器，并在Flannel meta CNI插件上执行`ADD`命令，使用配置JSON，该JSON将接口创建委托给`win-Overlay`插件，并将IPAM管理委托给`host-local`插件。Flannel
    CNI插件根据`subnet.env`和这些插件的输入配置生成配置JSON。
- en: A new IP is leased using the `host-local` IPAM plugin. Flannel is not responsible
    for managing the IPAM, it just retrieves a new free IP address from a given subnet
    on the current node.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`host-local` IPAM插件租用新的IP。Flannel不负责管理IPAM，它只是从当前节点上的给定子网中检索一个新的空闲IP地址。
- en: The `win-bridge` plugin configures the **Host Networking Service** (**HNS**)
    endpoint for the Pod and effectively connects the Pod to the Overlay network.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`win-bridge`插件配置了Pod的**主机网络服务**（HNS）端点，并有效地将Pod连接到Overlay网络。'
- en: To summarize, Flannel automates the process of L2/Overlay network creation for
    Pods and maintains the network as new Pods are created or new nodes join the cluster.
    Currently, the L2 network (the host-gw backend) is considered stable on Windows,
    whereas the Overlay network (the vxlan backend) on Windows is still in alpha—you
    will find both of these backends useful when working with on-premises Kubernetes
    clusters. For AKS and AKS-engine scenarios, the most effective way to install
    Pod networking is to use the default Azure-CNI plugin.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Flannel自动化了为Pod创建L2/Overlay网络的过程，并在创建新Pod或新节点加入集群时维护网络。目前，在Windows上，L2网络（host-gw后端）被认为是稳定的，而Overlay网络（vxlan后端）在Windows上仍处于alpha阶段——在使用本地Kubernetes集群时，这两种后端都很有用。对于AKS和AKS-engine场景，安装Pod网络的最有效方式是使用默认的Azure-CNI插件。
- en: Windows Server networking in Kubernetes
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的Windows Server网络
- en: At a high level, Kubernetes networking for Windows nodes is similar to Linux
    nodes—kubelet is decoupled from networking operations by CNI. The main differences
    are in the actual implementation of Windows container networking and in the terminology
    that is used for Windows containers.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，Windows节点的Kubernetes网络与Linux节点类似——kubelet通过CNI与网络操作解耦。主要区别在于Windows容器网络的实际实现以及用于Windows容器的术语。
- en: Windows container networking is set up similar to Hyper-V virtual machine networking,
    and in fact it shares many of the internal services, especially **Host Networking
    Service** (**HNS**), which cooperates with **Host Compute Service** (**HCS**),
    which manages the containers' life cycles. When creating a new Docker container,
    the container receives its own network namespace (compartment) and a **Virtual
    Network Interface Controller **(**vNIC** or in the case of Hyper-V, isolated containers
    or **vmNIC**) located in this namespace. The vNIC is then connected to a **Hyper-V
    Virtual Switch** (**vSwitch**), which is also connected to the host default network
    namespace using the host vNIC. You can loosely map this construct to the **container
    bridge interface **(CBR) in the Linux container world. The vSwitch utilizes Windows
    Firewall and the **Virtual Filtering Platform** (**VFP**) Hyper-V vSwitch extension
    in order to provide network security, traffic forwarding, VXLAN encapsulation,
    and load balancing. This component is crucial for kube-proxy to provide Services'
    functionalities, and you can think of VFP as *iptables* from the Linux container
    world. The vSwitch can be internal (not connected to a network adapter on the
    container host) or external (connected to a network adapter on the container host);
    it depends on the container network driver. In the case of Kubernetes, you will
    be always using network drivers (L2Bridge, Overlay, Transparent) that create an
    external vSwitch.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Windows容器网络设置类似于Hyper-V虚拟机网络，并且实际上共享许多内部服务，特别是**主机网络服务**（**HNS**），它与**主机计算服务**（**HCS**）合作，后者管理容器的生命周期。创建新的Docker容器时，容器会接收自己的网络命名空间（隔间）和位于该命名空间中的**虚拟网络接口控制器**（**vNIC**或在Hyper-V中，隔离容器或**vmNIC**）。然后将vNIC连接到**Hyper-V虚拟交换机**（**vSwitch**），该交换机还使用主机vNIC连接到主机默认网络命名空间。您可以将此结构宽松地映射到Linux容器世界中的**容器桥接口**（CBR）。vSwitch利用Windows防火墙和**虚拟过滤平台**（**VFP**）Hyper-V
    vSwitch扩展来提供网络安全、流量转发、VXLAN封装和负载平衡。这个组件对于kube-proxy提供服务功能至关重要，您可以将VFP视为Linux容器世界中的*iptables*。vSwitch可以是内部的（不连接到容器主机上的网络适配器）或外部的（连接到容器主机上的网络适配器）；这取决于容器网络驱动程序。在Kubernetes的情况下，您将始终使用创建外部vSwitch的网络驱动程序（L2Bridge、Overlay、Transparent）。
- en: VFP utilizes Windows Kernel functionalities to filter and forward network traffic.
    Until Kubernetes 1.8, VFP was not supported by kube-proxy, and the only way to
    forward the traffic was to use **userspace** mode, which does all the traffic
    management in the user space, not the kernel space.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: VFP利用Windows内核功能来过滤和转发网络流量。直到Kubernetes 1.8，kube-proxy不支持VFP，唯一的转发流量的方式是使用**用户空间**模式，该模式在用户空间而不是内核空间中进行所有流量管理。
- en: 'All of the preceding setup is performed by HNS while a container is being created. HNS
    is in general responsible for the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建容器时，所有前述设置都是由HNS执行的。HNS通常负责以下工作：
- en: Creating virtual networks and vSwitches
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建虚拟网络和vSwitches
- en: Creating network namespaces (compartments)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建网络命名空间（隔间）
- en: Creating vNICs (endpoints) and placing them inside the container network namespace
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建vNICs（端点）并将它们放置在容器网络命名空间中
- en: Creating vSwitch ports
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建vSwitch端口
- en: Managing VFP network policies (load-balancing, encapsulation)
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理VFP网络策略（负载平衡、封装）
- en: 'In the case of Kubernetes, CNI plugins are the only way to set up container
    networking (for Linux, it is possible not to use them). They perform the actual
    communication with HNS and HCS in order to set up the selected networking mode.
    Kubernetes'' networking setup has one significant difference when compared to
    the standard Docker networking setup: the container vNIC is attached to the pod
    infra container, and the network namespace is shared between all containers in
    the Pod. This is the same concept as for Linux Pods.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes的情况下，CNI插件是设置容器网络的唯一方式（对于Linux，可以选择不使用它们）。它们与HNS和HCS进行实际通信，以设置所选的网络模式。与标准的Docker网络设置相比，Kubernetes的网络设置有一个重要的区别：容器vNIC连接到pod基础设施容器，并且网络命名空间在Pod中的所有容器之间共享。这与Linux
    Pods的概念相同。
- en: 'These constructs are visualized in the following diagram:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结构在以下图表中可视化：
- en: '![](assets/f7046c96-b38c-48fd-aace-8153927c5a66.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f7046c96-b38c-48fd-aace-8153927c5a66.png)'
- en: 'The architecture of Windows container networking for Kubernetes has one more
    important concept: network drivers (modes). In the next section, we will go through
    the options and see which of them fit Kubernetes, but first, let''s take a quick
    look at the current limitations of Kubernetes networking on Windows.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Windows容器网络架构为Kubernetes有一个更重要的概念：网络驱动程序（模式）。在下一节中，我们将介绍选项，并看看它们中哪些适用于Kubernetes，但首先，让我们快速看一下Windows上Kubernetes网络的当前限制。
- en: Limitations
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制
- en: 'Windows container networking is constantly evolving, and the implementation of many features
    is still in progress. Kubernetes has currently a few networking limitations on
    the Windows platform:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Windows容器网络不断发展，许多功能的实现仍在进行中。目前，Kubernetes在Windows平台上有一些网络限制。
- en: Host networking mode is not available for Windows Pods.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Windows Pod不支持主机网络模式。
- en: Accessing NodePort from the node itself is not supported.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持从节点本身访问NodePort。
- en: The IPv6 stack is not supported for L2Bridge, L2Tunnel, and Overlay network
    drivers.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2Bridge、L2Tunnel和Overlay网络驱动不支持IPv6堆栈。
- en: ICMP for external networks is not supported. In other words, you will not be
    able to ping IP addresses outside of your Kubernetes cluster.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持外部网络的ICMP。换句话说，您将无法ping通Kubernetes集群外的IP地址。
- en: Flannel running on a vxlan backend is restricted to using VNI 4096 and UDP port
    4789.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在vxlan后端上运行的Flannel受限于使用VNI 4096和UDP端口4789。
- en: IPsec encryption for container communication is not supported.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持容器通信的IPsec加密。
- en: HTTP proxies are not supported inside containers.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器内不支持HTTP代理。
- en: For Ingress controllers running on Windows nodes, you have to choose a Deployment
    that supports both Windows and Linux nodes.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于在Windows节点上运行的Ingress控制器，您必须选择支持Windows和Linux节点的部署。
- en: You can expect this list to get shorter because new releases of Windows Server
    and Kubernetes are coming.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以期待这个列表变得更短，因为新版本的Windows Server和Kubernetes即将推出。
- en: Choosing Kubernetes network modes
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择Kubernetes网络模式
- en: Network modes (drivers) is a concept from Docker that is a part of the **Container
    Network Model** (**CNM**). This specification was proposed by Docker to solve
    container networking setup and management challenges in a modular, pluginable
    way. Docker's libnetwork is the canonical implementation of the CNM specification.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 网络模式（驱动程序）是Docker的一个概念，是**容器网络模型**（**CNM**）的一部分。这个规范是由Docker提出的，以模块化、可插拔的方式解决容器网络设置和管理的挑战。Docker的libnetwork是CNM规范的规范实现。
- en: 'At this point, you are probably wondering how CNM relates to CNI, which solves
    a similar problem. Yes, they are competing specifications for container networking!
    For Linux containers, the implementations of Docker network drivers and CNI can
    be considerably different. However, for Windows containers, network drivers implemented
    in libnetwork are a simple shim for HNS that performs all the configuration tasks.
    CNI plugins, such as win-bridge and win-Overlay, do exactly the same thing: call
    the HNS API. This means that for Windows, Docker network drivers and CNI plugins
    are in parity and fully depend on HNS and its native network configurations. If
    you are interested, you can check out the libnetwork Windows driver implementation
    and see how it interacts with HNS: [https://github.com/docker/libnetwork/blob/master/drivers/windows/windows.go](https://github.com/docker/libnetwork/blob/master/drivers/windows/windows.go).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可能想知道CNM与CNI的关系，它们解决了类似的问题。是的，它们是竞争的容器网络规范！对于Linux容器，Docker网络驱动程序和CNI的实现可能会有很大的不同。然而，对于Windows容器，libnetwork中实现的网络驱动程序只是HNS的一个简单的包装，执行所有的配置任务。CNI插件，如win-bridge和win-Overlay，也是一样的：调用HNS
    API。这意味着对于Windows来说，Docker网络驱动程序和CNI插件是平等的，并且完全依赖于HNS及其本地网络配置。如果您感兴趣，您可以查看libnetwork的Windows驱动程序实现，并了解它是如何与HNS交互的：[https://github.com/docker/libnetwork/blob/master/drivers/windows/windows.go](https://github.com/docker/libnetwork/blob/master/drivers/windows/windows.go)。
- en: CNI and CNM have a long history and some significant differences. In the early
    days of Kubernetes, a decision was made not to use Docker's libnetwork in favor
    of CNI as an abstraction for container networking management. You can read more
    about this decision in this Kubernetes blog post: [https://kubernetes.io/blog/2016/01/why-kubernetes-doesnt-use-libnetwork/](https://kubernetes.io/blog/2016/01/why-kubernetes-doesnt-use-libnetwork/).
    If you are interested in more details about CNI versus CNM, please refer to this
    article: [https://thenewstack.io/container-networking-landscape-cni-coreos-cnm-docker/](https://thenewstack.io/container-networking-landscape-cni-coreos-cnm-docker/).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: CNI和CNM有着悠久的历史和一些显著的区别。在Kubernetes早期，决定不使用Docker的libnetwork，而是选择CNI作为容器网络管理的抽象。您可以在Kubernetes的博客文章中阅读更多关于这个决定的信息：[https://kubernetes.io/blog/2016/01/why-kubernetes-doesnt-use-libnetwork/](https://kubernetes.io/blog/2016/01/why-kubernetes-doesnt-use-libnetwork/)。如果您对CNI与CNM的更多细节感兴趣，请参考这篇文章：[https://thenewstack.io/container-networking-landscape-cni-coreos-cnm-docker/](https://thenewstack.io/container-networking-landscape-cni-coreos-cnm-docker/)。
- en: In general, for Windows containers, you can use the terms Docker network driver
    and HNS network driver  interchangeably.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，对于Windows容器，您可以互换使用Docker网络驱动程序和HNS网络驱动程序这两个术语。
- en: 'There are five HNS network drivers that are currently supported by Windows
    containers:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 目前Windows容器支持五种HNS网络驱动程序：
- en: l2bridge
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: l2bridge
- en: l2tunnel
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: l2tunnel
- en: Overlay
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Overlay
- en: Transparent
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 透明
- en: NAT (not used in Kubernetes)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NAT（在Kubernetes中未使用）
- en: 'You can create a new Docker network manually using the following command:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令手动创建一个新的Docker网络：
- en: '[PRE10]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Additional parameters are required for some of the network types; you can find
    more details in the official documentation: [https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/network-drivers-topologies](https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/network-drivers-topologies).
    The Microsoft SDN repository also provides a simple PowerShell module for interacting
    with the HNS API, which you can use to analyze your network configuration: [https://github.com/microsoft/SDN/blob/master/Kubernetes/windows/hns.psm1](https://github.com/microsoft/SDN/blob/master/Kubernetes/windows/hns.psm1).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 某些网络类型需要额外的参数；您可以在官方文档中找到更多详细信息：[https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/network-drivers-topologies](https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/network-drivers-topologies)。Microsoft
    SDN 仓库还提供了一个简单的 PowerShell 模块，用于与 HNS API 进行交互，您可以使用它来分析您的网络配置：[https://github.com/microsoft/SDN/blob/master/Kubernetes/windows/hns.psm1](https://github.com/microsoft/SDN/blob/master/Kubernetes/windows/hns.psm1)。
- en: You can find the officially supported networking configurations for Windows
    containers in the support policy from Microsoft: [https://support.microsoft.com/da-dk/help/4489234/support-policy-for-windows-containers-and-docker-on-premises](https://support.microsoft.com/da-dk/help/4489234/support-policy-for-windows-containers-and-docker-on-premises).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 Microsoft 的支持政策中找到 Windows 容器的官方支持的网络配置：[https://support.microsoft.com/da-dk/help/4489234/support-policy-for-windows-containers-and-docker-on-premises](https://support.microsoft.com/da-dk/help/4489234/support-policy-for-windows-containers-and-docker-on-premises)。
- en: Let's now go through each type of HNS network to understand how they fit Kubernetes,
    when to use them, and how they relate to CNI plugins.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们逐个了解每种类型的 HNS 网络，以了解它们如何适用于 Kubernetes，何时使用它们，以及它们与 CNI 插件的关系。
- en: L2Bridge
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L2Bridge
- en: In L2Bridge network mode, containers are connected to a shared external Hyper-V
    vSwitch, which gives access to the underlay network. Containers also share the
    same IP subnet as the container host, and the container IP addresses must be statically
    assigned with the same prefix as the container host IP. MAC addresses are rewritten
    on ingress and egress to the host's address (this requires MAC spoofing to be
    enabled; remember this when testing Kubernetes clusters on local Hyper-V VMs).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在 L2Bridge 网络模式下，容器连接到共享的外部 Hyper-V vSwitch，可以访问底层网络。容器还与容器主机共享相同的 IP 子网，容器
    IP 地址必须使用与容器主机 IP 相同前缀的静态分配。MAC 地址在进入和离开时被重写为主机的地址（这需要启用 MAC 欺骗；在本地 Hyper-V VM
    上测试 Kubernetes 集群时请记住这一点）。
- en: 'The following CNI plugins use an L2Bridge network:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 CNI 插件使用 L2Bridge 网络：
- en: win-bridge
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: win-bridge
- en: Azure-CNI
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure-CNI
- en: Flannel with a host-gw backend (as a meta plugin, it invokes win-bridge)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flannel 使用 host-gw 后端（作为元插件，它调用 win-bridge）
- en: 'These are the advantages of L2Bridge:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 L2Bridge 的优点：
- en: win-bridge and Flannel (host-gw) are easy to configure
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: win-bridge 和 Flannel（host-gw）易于配置
- en: Stable support in Windows
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Windows 中有稳定的支持
- en: Best performance
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳性能
- en: 'These are the disadvantages of L2Bridge:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 L2Bridge 的缺点：
- en: L2 adjacency between nodes is required
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点之间需要 L2 邻接
- en: L2Tunnel
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L2Tunnel
- en: L2Tunnel network mode is a special case of L2Bridge in which *all* network traffic
    from containers is forwarded to the virtualization host in order to apply SDN
    policies. This network type is intended to be used in Microsoft Cloud Stack only.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: L2Tunnel 网络模式是 L2Bridge 的特例，在此模式下，*所有* 来自容器的网络流量都被转发到虚拟化主机，以应用 SDN 策略。此网络类型仅适用于
    Microsoft Cloud Stack。
- en: 'The following CNI plugins use L2Tunnel network:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 CNI 插件使用 L2Tunnel 网络：
- en: Azure-CNI
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure-CNI
- en: 'These are the advantages of L2Tunnel:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 L2Tunnel 的优点：
- en: Used in AKS and AKS-engine on Azure, and there is stable support.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Azure 上的 AKS 和 AKS-engine 中使用，并且有稳定的支持。
- en: You can leverage the features provided by Azure Virtual Network ([https://azure.microsoft.com/en-us/services/virtual-network/](https://azure.microsoft.com/en-us/services/virtual-network/)).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以利用Azure虚拟网络提供的功能（[https://azure.microsoft.com/en-us/services/virtual-network/](https://azure.microsoft.com/en-us/services/virtual-network/)）。
- en: 'These are the disadvantages of L2Tunnel:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: L2Tunnel的缺点包括：
- en: It can only be used on Azure
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它只能在Azure上使用
- en: Overlay
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 覆盖
- en: Overlay network mode creates a VXLAN Overlay network using VFP at an external
    Hyper-V vSwitch. Each Overlay network has its own IP subnet, determined by a customizable
    IP prefix.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖网络模式使用VFP在外部Hyper-V vSwitch上创建VXLAN覆盖网络。每个覆盖网络都有自己的IP子网，由可定制的IP前缀确定。
- en: 'The following CNI plugins use the Overlay network:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下CNI插件使用覆盖网络：
- en: win-Overlay
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: win-Overlay
- en: Flannel with a vxlan backend (as a meta plugin, it invokes win-Overlay)
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有vxlan后端的Flannel（作为元插件，调用win-Overlay）
- en: 'These are the advantages of Overlay:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖网络的优点包括：
- en: No limitations in subnet organization.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子网组织没有限制。
- en: No need for L2 adjacency of nodes. You can use this mode in L3 networks.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点之间不需要L2邻接。您可以在L3网络中使用此模式。
- en: Increased security and isolation from the underlay network.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强了与底层网络的安全性和隔离性。
- en: 'These are the disadvantages of Overlay:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖的缺点包括：
- en: It's currently in the alpha feature stage on Windows.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它目前处于Windows的alpha功能阶段。
- en: You are restricted to specific VNI (4096) and UDP port (4789).
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您受限于特定的VNI（4096）和UDP端口（4789）。
- en: Worse performance than L2Bridge.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能比L2Bridge差。
- en: Transparent
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transparent
- en: The last HNS network type that is supported in Kubernetes on Windows is Transparent.
    Containers attached to the Transparent network will be connected to external Hyper-V
    vSwitch with statically or dynamically assigned IP addresses from the physical
    network. In Kubernetes, this network type is used for supporting OVN where intra-Pod
    communication is enabled by logical switches and routers.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上，Kubernetes支持的最后一个HNS网络类型是Transparent。连接到透明网络的容器将连接到具有静态或动态分配的IP地址的外部Hyper-V
    vSwitch。在Kubernetes中，此网络类型用于支持OVN，其中逻辑交换机和路由器启用了Pod内部的通信。
- en: 'The following CNI plugins use the Transparent network:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下CNI插件使用透明网络：
- en: ovn-kubernetes
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ovn-kubernetes
- en: 'These are the disadvantages of the Transparent network:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 透明网络的缺点包括：
- en: If you would like to use this network type in Kubernetes hosted on-premises,
    you have to deploy OVN and Open vSwitches, which is a complex task on its own.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想在本地托管的Kubernetes中使用这种网络类型，您必须部署OVN和Open vSwitches，这本身就是一个复杂的任务。
- en: Summary
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you have learned about the principles of networking in Kubernetes.
    We have introduced the Kubernetes networking model and the requirements that any
    model implementation must fulfill. Next, we analyzed the two most important network
    model implementations from a Windows perspective: the L2 network and Overlay network.
    In the previous chapter, you were introduced to Service API objects, and in this
    chapter, you gained a deeper insight into how Services are implemented with regard
    to the networking model. And eventually, you learned about Kubernetes networking
    on Windows nodes, CNI plugins, and when to use each plugin type.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经了解了Kubernetes中网络的原则。我们介绍了Kubernetes网络模型和任何模型实现必须满足的要求。接下来，我们从Windows的角度分析了两种最重要的网络模型实现：L2网络和覆盖网络。在上一章中，您已经了解了Service
    API对象，而在本章中，您更深入地了解了服务在网络模型方面的实现。最后，您了解了Windows节点上的Kubernetes网络、CNI插件以及何时使用每种插件类型。
- en: The next chapter will focus on interacting with Kubernetes clusters from Windows
    machines using Kubernetes command-line tools, namely **kubectl**.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将重点介绍如何使用Kubernetes命令行工具（即**kubectl**）从Windows机器与Kubernetes集群进行交互。
- en: Questions
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the requirements for implementing the Kubernetes network model?
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实施Kubernetes网络模型的要求是什么？
- en: When can you use Flannel with a host-gw backend in Kubernetes?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kubernetes中何时可以使用带有host-gw后端的Flannel？
- en: What is the difference between the ClusterIP and the NodePort Service?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ClusterIP和NodePort服务之间有什么区别？
- en: What are the benefits of using an Ingress controller over the LoadBalancer Service?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Ingress控制器而不是LoadBalancer服务的好处是什么？
- en: What are CNI plugins and how are they used by Kubernetes?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNI插件是什么，它们如何被Kubernetes使用？
- en: What is the difference between internal and external Hyper-V vSwitches?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内部和外部Hyper-V vSwitch之间有什么区别？
- en: What is the difference between the CNI plugin and a Docker network driver?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNI插件和Docker网络驱动之间有什么区别？
- en: What is an Overlay network?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是覆盖网络？
- en: You can find answers to these questions in *Assessments* in the back matter
    of this book.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的*评估*部分找到这些问题的答案。
- en: Further reading
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information regarding Kubernetes concepts and networking, please refer
    to the following Packt books and resources:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Kubernetes概念和网络的更多信息，请参考以下Packt图书和资源：
- en: The Complete Kubernetes Guide ([https://www.packtpub.com/virtualization-and-cloud/complete-kubernetes-guide](https://www.packtpub.com/virtualization-and-cloud/complete-kubernetes-guide)).
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的Kubernetes指南（[https://www.packtpub.com/virtualization-and-cloud/complete-kubernetes-guide](https://www.packtpub.com/virtualization-and-cloud/complete-kubernetes-guide)）。
- en: Getting Started with Kubernetes – Third Edition ([https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition)).
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用Kubernetes-第三版（[https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition)）。
- en: Kubernetes for Developers ([https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers)).
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面向开发人员的Kubernetes（[https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers)）。
- en: Hands-On Kubernetes Networking (video) ([https://www.packtpub.com/virtualization-and-cloud/hands-kubernetes-networking-video](https://www.packtpub.com/virtualization-and-cloud/hands-kubernetes-networking-video)).
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes网络实践（视频）（[https://www.packtpub.com/virtualization-and-cloud/hands-kubernetes-networking-video](https://www.packtpub.com/virtualization-and-cloud/hands-kubernetes-networking-video)）。
- en: You can also refer to the excellent official Kubernetes documentation ([https://kubernetes.io/docs/concepts/cluster-administration/networking/](https://kubernetes.io/docs/concepts/cluster-administration/networking/)),
    which is always the most up-to-date source of knowledge about Kubernetes in general.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以参考优秀的官方Kubernetes文档（[https://kubernetes.io/docs/concepts/cluster-administration/networking/](https://kubernetes.io/docs/concepts/cluster-administration/networking/)），这始终是关于Kubernetes的最新知识来源。
- en: 'For Windows-specific networking scenarios, the official Microsoft Virtualization
    documentation is recommended: [https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/network-topologies](https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/network-topologies)
    for Kubernetes and [https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture](https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture)
    for Windows container networking in general.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于特定于Windows的网络方案，建议参考官方的Microsoft虚拟化文档：[https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/network-topologies](https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/network-topologies)
    用于Kubernetes和[https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture](https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture)
    用于Windows容器网络。
