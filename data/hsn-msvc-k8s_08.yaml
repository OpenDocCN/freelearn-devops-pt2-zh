- en: Working with Stateful Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用有状态服务
- en: So far, everything was fun and games. We built services, deployed them to Kubernetes,
    and ran commands and queries against these services. We enabled Kubernetes to
    have those services up and running by scheduling pods on deployment or if anything
    went wrong. This works great for stateless services that can just run anywhere.
    In the real world, distributed systems manage important data. If a database stores
    its data on the host filesystem and that host goes down, you (or Kubernetes) can't
    just start a fresh instance of the database on a new node because the data will
    be lost.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切都很有趣。我们构建了服务，将它们部署到Kubernetes，并对这些服务运行命令和查询。我们通过在部署时调度Pod或在出现问题时使Kubernetes能够使这些服务正常运行。这对于可以在任何地方运行的无状态服务非常有效。在现实世界中，分布式系统管理重要数据。如果数据库将其数据存储在主机文件系统上，而该主机宕机，您（或Kubernetes）不能只是在新节点上启动数据库的新实例，因为数据将丢失。
- en: In general, you keep your data from getting lost by redundancy; you keep multiple
    copies, store backups, utilize append-only logs, and more. Kubernetes assists
    by providing a whole storage model with concepts and related resources, such as
    volumes, volume claims, and StatefulSets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，通过冗余来防止数据丢失；您可以保留多个副本，存储备份，利用追加日志等。Kubernetes通过提供整个存储模型以及相关资源的概念来提供帮助，例如卷、卷索赔和StatefulSets。
- en: 'In this chapter, we will dive deeper into the Kubernetes storage model. We
    will also extend the Delinkcious news service to store its data in Redis instead
    of in memory. We will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨Kubernetes存储模型。我们还将扩展Delinkcious新闻服务，将其数据存储在Redis中，而不是内存中。我们将涵盖以下主题：
- en: Abstracting storage
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽象存储
- en: Storing data outside your Kubernetes cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据存储在Kubernetes集群之外
- en: Storing data inside your Kubernetes cluster with StatefulSets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用StatefulSets在Kubernetes集群内部存储数据
- en: Achieving high performance with local storage
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用本地存储实现高性能
- en: Using relational databases in Kubernetes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes中使用关系型数据库
- en: Using non-relational data stores in Kubernetes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes中使用非关系型数据存储
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will examine a number of Kubernetes manifests, work with
    different storage options, and extend Delinkcious to support a new data store.
    There is no need to install anything new.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将检查一些Kubernetes清单，使用不同的存储选项，并扩展Delinkcious以支持新的数据存储。无需安装任何新内容。
- en: The code
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: 'The code is split between two Git repositories, as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 代码分为两个Git存储库，如下所示：
- en: You can find the code samples at [https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter08)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter08)找到代码示例
- en: You can find the updated Delinkcious application at [https://github.com/the-gigi/delinkcious/releases/tag/v0.6](https://github.com/the-gigi/delinkcious/releases/tag/v0.6)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/the-gigi/delinkcious/releases/tag/v0.6](https://github.com/the-gigi/delinkcious/releases/tag/v0.6)找到更新后的Delinkcious应用程序
- en: Abstracting storage
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽象存储
- en: At its core, Kubernetes is an orchestration engine used for managing containerized
    workloads. Note that, here, the keyword is *containerized*. Kubernetes doesn't
    care what the workloads are as long as they are packaged in containers; it knows
    how to handle them. Initially, Kubernetes only supported Docker images, and then,
    later, it added support for other runtimes. Then, Kubernetes 1.5 introduced the
    **Container Runtime Interface** (**CRI**), and gradually pushed the explicit support
    for other runtimes out of tree. Here, Kubernetes no longer cared about which container
    runtime was actually deployed on the nodes and just needed to work with the CRI.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的核心是一个编排引擎，用于管理容器化的工作负载。请注意，这里的关键词是*容器化*。Kubernetes不关心工作负载是什么，只要它们被打包在容器中；它知道如何处理它们。最初，Kubernetes只支持Docker镜像，然后后来添加了对其他运行时的支持。然后，Kubernetes
    1.5引入了**容器运行时接口**（**CRI**），并逐渐将对其他运行时的显式支持推出了树外。在这里，Kubernetes不再关心节点上实际部署的容器运行时是什么，只需要与CRI一起工作。
- en: A similar story unfolded with networking, where the **Container Networking Interface**
    (**CNI**) was defined early. The life of Kubernetes was simple. It was left to
    different networking solutions to provide their CNI plugins. Storage, however,
    was different (until it wasn't). In the following subsections, we'll go over the
    Kubernetes storage model, understand the differences between in-tree and out-of-tree
    storage plugins, and, finally, learn about the **Container Storage Interface**
    (**CSI**), which provides a neat solution for storage in Kubernetes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的情况也发生在网络中，**容器网络接口**（**CNI**）早已定义。Kubernetes的生命周期很简单。不同的网络解决方案提供它们的CNI插件。然而，存储是不同的（直到不是）。在接下来的小节中，我们将介绍Kubernetes存储模型，了解树内和树外存储插件之间的区别，最后了解**容器存储接口**（**CSI**），它为Kubernetes中的存储提供了一个巧妙的解决方案。
- en: The Kubernetes storage model
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes存储模型
- en: 'The Kubernetes storage model consists of several concepts: storage classes,
    volumes, persistent volumes, and persistent volume claims. Let''s examine how
    these concepts interact to allow containerized workloads access to storage during
    execution.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes存储模型包括几个概念：存储类、卷、持久卷和持久卷索赔。让我们看看这些概念是如何相互作用，允许容器化工作负载在执行期间访问存储的。
- en: Storage classes
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储类
- en: 'The storage class is a way of describing the available types of storage that
    can be provisioned. Often, there is a default storage class that is used when
    provisioning a volume without specifying a particular storage class. Here is the
    standard storage class in Minikube, which stores data on the host (that is, the
    hosting node):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 存储类是描述可以供应的存储类型的一种方式。通常，在没有指定特定存储类的情况下供应卷时会使用默认存储类。这是Minikube中的标准存储类，它在主机上存储数据（即托管节点）。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Different storage classes have different parameters tied to the actual backing
    storage. Volume provisioners know how to use the parameters of their storage classes.
    The storage class metadata includes the provisioner, as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的存储类具有与实际后备存储相关的不同参数。卷供应商知道如何使用其存储类的参数。存储类元数据包括供应商，如下所示：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Volumes, persistent volumes, and provisioning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷、持久卷和供应
- en: A volume in Kubernetes has an explicit lifetime that coincides with its pod.
    When the pod goes away, so does the storage. There are many types of volumes that
    are very useful. We've already seen a few examples, such as ConfigMap and secret
    volumes. But there are other volume types that are used for reading and writing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的卷具有与其pod相一致的显式生命周期。当pod消失时，存储也会消失。有许多类型的卷非常有用。我们已经看到了一些例子，比如ConfigMap和secret卷。但还有其他用于读写的卷类型。
- en: You can take a look at the full list of volume types here: [https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里查看所有卷类型的完整列表：[https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes)。
- en: Kubernetes also supports the concept of persistent volumes. These volumes must
    be provisioned by system administrators, and they are not managed by Kubernetes
    itself. When you want to store data persistently, then you use persistent volumes.
    Administrators can statically provision persistent volumes ahead of time. The
    process involves administrators provisioning external storage and creating a `PersistentVolume`
    Kubernetes object that users can consume.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还支持持久卷的概念。这些卷必须由系统管理员进行配置，而不是由Kubernetes本身管理。当您想要持久存储数据时，就可以使用持久卷。管理员可以提前静态配置持久卷。该过程涉及管理员配置外部存储并创建用户可以使用的`PersistentVolume`
    Kubernetes对象。
- en: Dynamic provisioning is the process of creating volumes on the fly. Users request
    storage and this is created dynamically. Dynamic provisioning depends on storage
    classes. Users can specify a particular storage class, otherwise, the default
    storage class (if it exists) will be used. All Kubernetes cloud providers support
    dynamic provisioning. Minikube supports it too (the backing store is the localhost
    filesystem).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 动态配置是动态创建卷的过程。用户请求存储空间，这是动态创建的。动态配置取决于存储类。用户可以指定特定的存储类，否则将使用默认存储类（如果存在）。所有Kubernetes云提供商都支持动态配置。Minikube也支持它（后备存储是本地主机文件系统）。
- en: Persistent volume claims
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久卷索赔
- en: So, the cluster administrator either provisions some persistent volumes or,
    alternatively, the cluster supports dynamic provisioning. We can now claim some
    storage for our workload by creating a persistent volume claim. But, first, it's
    important to understand the difference between ephemeral and persistent storage.
    We'll create an ephemeral file in a pod, restart the pod, and check that the file
    vanished. Then, we'll do the same thing again, but, this time, write the file
    to the persistent storage and check that the file still exists once the pod is
    restarted.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，集群管理员要么提供一些持久卷，要么集群支持动态配置。现在，我们可以通过创建持久卷索赔来为我们的工作负载索取一些存储空间。但首先，重要的是要理解临时和持久存储之间的区别。我们将在一个pod中创建一个临时文件，重新启动pod，并检查文件是否消失。然后，我们将再次执行相同的操作，但这次将文件写入持久存储，并在重新启动pod后检查文件是否仍然存在。
- en: 'Before we start, let me share some convenient shell functions and aliases that
    I created in order to quickly launch an interactive session in specific pods.
    A Kubernetes deployment generates random pod names. For example, for the `trouble`
    deployment, the current pod name is `trouble-6785b4949b-84x22`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我分享一些方便的shell函数和别名，我创建了这些函数和别名，以便快速启动特定pod中的交互式会话。Kubernetes部署会生成随机的pod名称。例如，对于`trouble`部署，当前的pod名称是`trouble-6785b4949b-84x22`。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is not a very memorable name, and it also changes whenever the pod is
    restarted (automatically by the deployment). Unfortunately, the `kubectl exec`
    command requires an exact pod name to run commands. I created a little shell function
    called `get_pod_name_by_label()`, which returns a pod name based on a label. Since
    labels from the pod template don''t change, this is a good way to discover pod
    names. However, there may be multiple pods from the same deployment with the same
    labels. We just need any kind of pod, so we can simply pick the first. Here is
    the function, and I aliased it to `kpn` so that it''s easier to use:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个很容易记住的名字，而且每当pod被重新启动时（由部署自动完成），它也会发生变化。不幸的是，`kubectl exec`命令需要一个确切的pod名称来运行命令。我创建了一个名为`get_pod_name_by_label()`的小shell函数，它根据标签返回一个pod名称。由于pod模板中的标签不会改变，这是发现pod名称的好方法。然而，可能会有多个来自相同部署的带有相同标签的pod。我们只需要任何一种类型的pod，所以我们可以简单地选择第一个。这是函数，我将其别名为`kpn`，这样使用起来更容易：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For example, the `trouble` deployment pods can have a label called `run=trouble`.
    Here is how to find the actual pod name:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`trouble`部署的pod可以有一个名为`run=trouble`的标签。这是如何找到实际的pod名称：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Using this function, I created an alias called `trouble`, which launches an
    interactive bash session in the `trouble` pod:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数，我创建了一个名为`trouble`的别名，它在`trouble` pod中启动一个交互式的bash会话：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we can connect to the `trouble` pod and start working in it:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以连接到`trouble` pod并开始在其中工作：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This was a long digression, but it''s a very useful technique. Now, let''s
    get back to our plan and create an ephemeral file, as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很长的离题，但这是一个非常有用的技术。现在，让我们回到我们的计划，并创建一个临时文件，如下所示：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s kill the pod. The `trouble` deployment will schedule a new `trouble`
    pod, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们杀死这个pod。`trouble`部署将安排一个新的`trouble` pod，如下所示：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When we access the new pod, we discover that `life.txt` vanished as expected:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们访问新的pod时，我们发现`life.txt`如预期般消失了：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'That''s understandable because it was stored in the filesystem of the container. The
    next step is to have the `trouble` pod claim some persistent storage. Here is
    a persistent volume claim that provisions one gibibyte dynamically:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可以理解的，因为它存储在容器的文件系统中。下一步是让`trouble` pod声明一些持久存储。这里有一个动态提供一吉比特的持久卷索赔：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the YAML manifest for the entire `trouble` deployment that consumes
    this claim as a volume and mounts it to the container:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是整个`trouble`部署的YAML清单，它作为卷使用这个索赔，并将其挂载到容器中：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `keep-me` volume is based on the `some-storage` persistent volume claim:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`keep-me`卷是基于`some-storage`持久卷索赔的：'
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The volume is mounted to the `/data` directory inside the container:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 卷被挂载到容器内部的`/data`目录中：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, let''s write something to `/data`, as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们向`/data`写入一些内容，如下所示：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The final state is to delete the pod and, when a new pod is created, verify
    whether the `infinity.txt` file is still in `/data`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的状态是删除pod，并在创建新的pod时验证`infinity.txt`文件是否仍然在`/data`中：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Yay, it works! A new pod was created and the persistent storage with the `infinity.txt`
    file was mounted to the new container.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，它起作用了！一个新的pod被创建，并且带有`infinity.txt`文件的持久存储被挂载到了新的容器上。
- en: Persistent volumes can also be used to share information directly between multiple
    instances of the same image because the same persistence storage will be mounted
    to all containers using the same persistent storage claim.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 持久卷也可以用来直接在同一图像的多个实例之间共享信息，因为相同的持久存储将被挂载到使用相同持久存储索赔的所有容器中。
- en: In-tree and out-of-tree storage plugins
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树内和树外存储插件
- en: 'There are two types of storage plugins: in-tree and out-of-tree. In-tree means
    that these storage plugins are part of Kubernetes itself. In the volume clause,
    you refer to them by name. For example, here, a **Google Compute Engine** (**GCE**)
    persistent disk is configured by name. Kubernetes explicitly knows that such a
    volume has fields such as `pdName` and `fsType`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的存储插件：内部和外部。内部意味着这些存储插件是Kubernetes本身的一部分。在卷子句中，您可以按名称引用它们。例如，在这里，通过名称配置了**Google
    Compute Engine（GCE）**持久磁盘。Kubernetes明确知道这样的卷有字段，如`pdName`和`fsType`：
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Take a look at the complete list of in-tree storage plugins at: [https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下链接找到完整的内部存储插件列表：[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes)。
- en: There are several other specialized volume types, such as `emptyDir`, `local`,
    `downwardAPI`, and `hostPath`, that you can read more about. The concept of in-tree
    plugins is somewhat cumbersome. It bloats Kubernetes and requires changing Kubernetes
    itself whenever a provider wants to improve their storage plugin or introduce
    a new one.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他几种专门的卷类型，如`emptyDir`、`local`、`downwardAPI`和`hostPath`，您可以阅读更多相关信息。内部插件的概念有些繁琐。它使Kubernetes变得臃肿，并且需要在提供商想要改进其存储插件或引入新插件时改变Kubernetes本身。
- en: This is where out-of-tree plugins come into the picture. The idea is that Kubernetes
    defines a standard storage interface and a standard way of providing plugins to
    implement the interface in a running cluster. Then, it's the job of the cluster
    administrator to make sure that the proper out-of-tree plugins are available.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是外部插件出现的地方。其想法是，Kubernetes定义了一个标准的存储接口和一种提供插件以在运行集群中实现接口的标准方式。然后，集群管理员的工作就是确保适当的外部插件可用。
- en: 'There are two types of out-of-tree plugins that Kubernetes supports: FlexVolume
    and CSI. FlexVolume is old and deprecated. I will not go into detail about FlexVolume,
    except to recommend that you don''t use it.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持两种类型的外部插件：FlexVolume和CSI。FlexVolume已经过时。我不会详细介绍FlexVolume，除了建议您不要使用它。
- en: For more detail, you can refer to the following link: [https://kubernetes.io/docs/concepts/storage/volumes/#flexVolume](https://kubernetes.io/docs/concepts/storage/volumes/#flexVolume)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，您可以参考以下链接：[https://kubernetes.io/docs/concepts/storage/volumes/#flexVolume](https://kubernetes.io/docs/concepts/storage/volumes/#flexVolume)
- en: The big star of storage is the CSI. Let's drill down and understand how CSI
    works and what a huge improvement it is.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 存储的重要组成部分是CSI。让我们深入了解CSI的工作原理以及它是多么巨大的改进。
- en: Understanding CSI
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解CSI
- en: CSI was designed to address all the issues with in-tree plugins and the cumbersome
    aspects of FlexVolume plugins. What makes CSI so enticing to storage providers
    is that it is not a Kubernetes-only standard, but an industry-wide standard. It
    allows storage providers to write a single driver for their storage solution and
    become immediately compatible with a broad range of container orchestration platforms
    such as Docker, Cloud Foundry, Mesos, and, of course, Kubernetes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: CSI旨在解决内部插件的所有问题以及FlexVolume插件的繁琐方面。CSI对存储提供商如此诱人的原因在于，它不仅是Kubernetes的标准，而且是行业标准。它允许存储提供商为其存储解决方案编写单个驱动程序，并立即与Docker、Cloud
    Foundry、Mesos和当然还有Kubernetes等广泛的容器编排平台兼容。
- en: You can find the official specification at [https://github.com/container-storage-interface/spec](https://github.com/container-storage-interface/spec).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/container-storage-interface/spec](https://github.com/container-storage-interface/spec)找到官方规范。
- en: 'The Kubernetes team provides three components that are sidecar containers and
    provide generic CSI support for any CSI storage provider. These components are
    as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes团队提供了三个组件，它们是旁路容器，并为任何CSI存储提供了通用的CSI支持。这些组件如下：
- en: Driver registrar
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动注册器
- en: External provisioner
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部供应商
- en: External attacher
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部连接器
- en: Their job is to interface with the kubelet as well as the API server. The storage
    provider will typically package these sidecar containers along with their storage
    driver implementation in a single pod that can be deployed as a Kubernetes DaemonSet
    on all nodes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的工作是与kubelet和API服务器进行接口。存储供应商通常会将这些旁路容器与它们的存储驱动实现打包在一个单独的pod中，可以部署为Kubernetes
    DaemonSet在所有节点上。
- en: 'Here is a diagram that demonstrates the interaction between all the pieces:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个图表，展示了所有部件之间的交互：
- en: '![](assets/84bc28f4-4c46-4a6d-b11f-6deb77c2c413.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/84bc28f4-4c46-4a6d-b11f-6deb77c2c413.png)'
- en: It is pretty complicated, but this complication is necessary to separate concerns,
    allow the Kubernetes team to do a lot of the heavy lifting, and leave storage
    providers to focus on their storage solution. As far as users and developers are
    concerned, this is all completely transparent. They continue to interact with
    storage through the same Kubernetes storage abstractions of storage classes, volumes,
    and persistent volume claims.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当复杂，但这种复杂性是必要的，以分离关注点，允许Kubernetes团队进行大量的繁重工作，并让存储供应商专注于他们的存储解决方案。就用户和开发人员而言，这一切都是完全透明的。他们继续通过相同的Kubernetes存储抽象（存储类、卷和持久卷索赔）与存储进行交互。
- en: Standardizing on CSI
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化CSI
- en: CSI is superior to in-tree plugins (and FlexVolume plugins). However, the current
    situation of a hybrid, where you can use either in-tree plugins (or FlexVolume
    plugins) or CSI plugins is suboptimal. The Kubernetes team has a detailed plan
    to migrate in-tree plugins to CSI.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: CSI优于in-tree插件（和FlexVolume插件）。然而，目前的混合情况，您可以使用in-tree插件（或FlexVolume插件）或CSI插件，是次优的。Kubernetes团队有一个详细的计划，将in-tree插件迁移到CSI。
- en: You can find out more about this detailed plan at [https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/csi-migration.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/csi-migration.md).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/csi-migration.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/csi-migration.md)找到关于这个详细计划的更多信息。
- en: Storing data outside your Kubernetes cluster
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据存储在Kubernetes集群之外
- en: Kubernetes is not a closed system. Workloads running inside a Kubernetes cluster
    can access storage running outside the cluster. This is most appropriate when
    you migrate an existing application that is already in storage, and configured
    and operated outside of Kubernetes. In this case, it is a wise move to do it gradually.
    First, move the workloads to run as containers managed by Kubernetes. These containers
    will be configured with endpoints to data stores that live outside the cluster.
    Later, you can consider whether it is worth the effort to bring this external
    storage into the fold.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes不是一个封闭的系统。在Kubernetes集群内运行的工作负载可以访问集群外运行的存储。当您迁移一个已经存在于存储中、并在Kubernetes之外配置和操作的现有应用程序时，这是最合适的。在这种情况下，逐步进行是明智的选择。首先，将工作负载移动为由Kubernetes管理的容器运行。这些容器将配置为具有位于集群外的数据存储的端点。稍后，您可以考虑是否值得将这些外部存储引入系统。
- en: 'There are some other use cases where it makes sense to use out-of-cluster storage,
    such as the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他使用情况，使用集群外存储是有意义的，比如：
- en: Your storage cluster uses some exotic hardware, or the networking doesn't have
    a mature in-tree or CSI plugin (hopefully, as CSI becomes the gold standard, this
    will become rare).
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的存储集群使用一些奇特的硬件，或者网络没有成熟的内置或CSI插件（希望随着CSI成为黄金标准，这种情况会变得罕见）。
- en: You run Kubernetes through a cloud provider and it's going to be too expensive,
    too risky, and/or too slow to migrate all the data.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过云提供商运行Kubernetes可能会太昂贵、风险太大和/或迁移所有数据太慢。
- en: Other applications in your organization use the same storage cluster and it
    is often impractical and non-economical to migrate all the applications and systems
    in your organization to Kubernetes.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织中的其他应用程序使用相同的存储集群，将所有应用程序和系统迁移到Kubernetes通常是不切实际和不经济的。
- en: Due to regulatory requirements, you must retain control of your data.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于监管要求，您必须保留对数据的控制。
- en: 'There are several downsides to managing storage outside of Kubernetes:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes之外管理存储有几个缺点：
- en: Security (you need to provide network access from your workloads to a separate
    storage cluster).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性（您需要为您的工作负载提供对单独存储集群的网络访问）。
- en: You must implement the scaling, availability, monitoring, and configuration
    of your storage cluster.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须实现存储集群的扩展、可用性、监控和配置。
- en: When things change on the storage cluster side, you often need to make corresponding
    configuration changes on the Kubernetes side.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当存储集群端发生变化时，您通常需要在Kubernetes端进行相应的配置更改。
- en: You might suffer performance or latency overhead due to extra network hops and/or
    authentication, authorization, or encryption.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于额外的网络跳跃和/或身份验证、授权或加密，可能会遭受性能或延迟开销。
- en: Storing data inside your cluster with StatefulSets
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用StatefulSets在集群内存储数据
- en: It's best to store data within your Kubernetes cluster. This provides a uniform
    one-stop shop to manage your workloads and all the resources they depend on (excluding
    third-party external services). Additionally, you get to integrate your storage
    with your streamlined monitoring, which is very important. We will discuss monitoring
    in depth in a future chapter. However, running out of disk space is the bane of
    many system administrators. But there is a problem if you store data on a node
    and your data store pods get rescheduled to a different node, and the data it
    expects to be available is not there. The Kubernetes designers realized that the
    ephemeral pod philosophy doesn't work for storage. You could try to manage it
    yourself using pod-node affinity and other mechanisms that Kubernetes provides,
    but it's much better to use StatefulSet, which is a specific solution for managing
    storage-aware services in Kubernetes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最好将数据存储在Kubernetes集群内。这提供了一个统一的一站式管理工作负载和它们所依赖的所有资源的方式（不包括第三方外部服务）。此外，您可以将存储与流线型监控集成，这非常重要。我们将在未来的章节中深入讨论监控。然而，磁盘空间不足是许多系统管理员的苦恼。但是，如果您将数据存储在一个节点上，而您的数据存储pod被重新调度到另一个节点，它期望可用的数据却不在那里，这就会出现问题。Kubernetes的设计者意识到，短暂的pod理念对存储不起作用。您可以尝试使用pod-node亲和性和Kubernetes提供的其他机制来自行管理，但最好使用StatefulSet，这是Kubernetes中管理存储感知服务的特定解决方案。
- en: Understanding a StatefulSet
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解StatefulSet
- en: 'At its core, a StatefulSet is a controller that manages a set of pods with
    some extra properties, such as ordering and uniqueness. The StatefulSet allows
    its set of pods to be deployed and scaled, while preserving their special properties.
    StatefulSets reached **g****eneral availability** (**GA**) status in Kubernetes
    1.9\. You can think of a StatefulSet as a souped-up deployment. Let''s take a
    look at a sample StatefulSet for the user service, which uses a relational PostgresDB
    as its data store:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，StatefulSet是一个控制器，管理一组具有一些额外属性的pod，例如排序和唯一性。StatefulSet允许其一组pod被部署和扩展，同时保留它们的特殊属性。StatefulSets在Kubernetes
    1.9中达到了**一般可用性**（**GA**）状态。您可以将StatefulSet视为升级版的部署。让我们看一个用户服务的示例StatefulSet，它使用关系型PostgresDB作为其数据存储：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There is a lot going on here, but it's all a composition of familiar concepts.
    Let's break it down into its components.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多内容，但它都是由熟悉的概念组成的。让我们把它分解成组件。
- en: StatefulSet components
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StatefulSet组件
- en: 'The StatefulSet is comprised of three main parts, as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet由三个主要部分组成，如下所示：
- en: '**StatefulSet metadata and definition**: The StatefulSet metadata and definition are
    pretty similar to a deployment. You have the standard API version, kind, and metadata
    name; then, `spec`, which includes a selector for the pods (which must match the
    pod template selectors that will come next), the number of replicas (just one,
    in this case), and the major difference compared with a deployment, that is, `serviceName`:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**StatefulSet元数据和定义**：StatefulSet元数据和定义与部署非常相似。您有标准的API版本，种类和元数据名称；然后，`spec`，其中包括对pod的选择器（必须与接下来的pod模板选择器匹配），副本的数量（在这种情况下只有一个），以及与部署相比的主要区别，即`serviceName`：'
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A StatefulSet *must* have a headless service associated with the StatefulSet
    to manage the network identity of the pods. The service name is `user-db` in this
    case; here it is for completeness:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet *必须*有一个与StatefulSet关联的无头服务来管理pod的网络标识。在这种情况下，服务名称是`user-db`；这里是为了完整性：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**A pod template**: The next part is a standard pod template. The PGDATA environment
    variable (`/data/user-db`), which tells postgres where to read and write its data,
    must be the same as the mount path of the `user-db` volume (`/data/user-db`) or
    a subdirectory. This is where we wire up the data store with the underlying storage:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个pod模板**：下一部分是标准的pod模板。PGDATA环境变量（`/data/user-db`）告诉postgres从哪里读取和写入数据，必须与`user-db`卷的挂载路径（`/data/user-db`）或子目录相同。这是我们将数据存储与底层存储连接起来的地方：'
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Volume claim templates**: The last part is the volume claim templates. Note
    that this is plural; some data stores may require multiple types of volumes (for
    example, for logging or caching) that require their own persistent claims. In
    this case, one persistent claim is enough:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷索赔模板**：最后一部分是卷索赔模板。请注意，这是复数形式；一些数据存储可能需要多种类型的卷（例如，用于日志记录或缓存），这些卷需要它们自己的持久索赔。在这种情况下，一个持久索赔就足够了：'
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now is a good time to dive deeper and gain an understanding of the special properties
    of StatefulSets and why they are important.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是深入了解StatefulSets的特殊属性以及它们为什么重要的好时机。
- en: Pod identity
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod标识
- en: 'StatefulSet pods have a stable identity that includes the following triplet:
    a stable network identity, an ordinal index, and stable storage. These always
    go together; the name of each pod is `<statefulset name>-<ordinal>`.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet pod具有稳定的标识，包括以下三元组：稳定的网络标识，序数索引和稳定的存储。这些总是一起的；每个pod的名称是`<statefulset
    name>-<ordinal>`。
- en: 'The headless service associated with the StatefulSet provides the stable network
    identity. The service DNS name will be as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与StatefulSet关联的无头服务提供了稳定的网络标识。服务DNS名称将如下所示：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Each pod, *X*, will have a stable DNS name as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 每个pod，*X*，将具有如下稳定的DNS名称：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For example, the first pod of the `user-db` StatefulSet will be called the
    following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`user-db` StatefulSet的第一个pod将被称为以下内容：
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Additionally, StatefulSet pods automatically get assigned a label, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，StatefulSet的pod会自动被分配一个标签，如下所示：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Orderliness
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有序性
- en: Each pod in a StatefulSet gets an ordinal index. But, what is this for? Well,
    some data stores rely on the orderly sequence of initialization. The StatefulSet
    ensures that when the StatefulSet pods are initialized, scaled up, or scaled down,
    it is always done in order.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet中的每个pod都会获得一个序号索引。但是，这有什么作用呢？嗯，一些数据存储依赖于初始化的有序序列。StatefulSet确保当StatefulSet的pod被初始化、扩展或缩减时，总是按顺序进行。
- en: In Kubernetes 1.7, the orderliness restriction was relaxed. For data stores
    that don't require orderliness, it makes sense to allow for parallel operations
    on multiple pods in the StatefulSet. This can be specified in the `podPolicy`
    field. The values allowed are `OrderedReady` for the default orderly behavior,
    or *parallel* for the relaxed parallel mode, where pods can be launched or terminated
    while other pods are still launching or terminating.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes 1.7中，有序性限制得到了放宽。对于不需要有序性的数据存储，允许在StatefulSet中对多个pod进行并行操作是有意义的。这可以在`podPolicy`字段中指定。允许的值有`OrderedReady`用于默认的有序行为，或者*parallel*用于放宽的并行模式，其中可以在其他pod仍在启动或终止时启动或终止pod。
- en: When should you use a StatefulSet?
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时应该使用StatefulSet？
- en: You should use a StatefulSet when you manage your data store yourself in the
    cloud and require good control over the storage your data store uses. The primary
    use case is for distributed data stores, but a StatefulSet is useful even if your
    data store has just one instance or pod. The stable pod identity with the stable
    attached storage is well worth it, although orderliness, of course, is not required.
    If your data store is backed up by a shared storage layer such as NFS, then a
    StatefulSet might not be necessary.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在云中自己管理数据存储并且需要对数据存储使用的存储有良好的控制时，你应该使用StatefulSet。主要用例是分布式数据存储，但即使你的数据存储只有一个实例或pod，StatefulSet也是有用的。稳定的pod标识和稳定的附加存储是非常值得的，尽管有序性当然不是必需的。如果你的数据存储由共享存储层（如NFS）支持，那么StatefulSet可能就不是必要的。
- en: Additionally, this may be common sense, but if you don't manage the data store
    yourself, then you don't need to worry about the storage layer and you don't need
    to define your own StatefulSets. For example, if you run your system on AWS and
    use S3, RDS, DynamoDB, and Redshift, then you don't really need a StatefulSet.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这可能是常识，但如果你不自己管理数据存储，那么你就不需要担心存储层，也不需要定义自己的StatefulSets。例如，如果你在AWS上运行系统并使用S3、RDS、DynamoDB和Redshift，那么你实际上不需要StatefulSet。
- en: Comparing deployment and StatefulSets
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较部署和StatefulSets
- en: 'Deployments are designed to manage any sets of pods. They can also be used
    to manage the pods of a distributed data store. StatefulSets were specifically
    designed to support the needs of distributed data stores. However, the special
    properties of ordering and uniqueness are not always necessary. Let''s compare
    deployments to StatefulSets and see for ourselves:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 部署旨在管理任何一组pod。它们也可以用于管理分布式数据存储的pod。StatefulSets专门设计用于支持分布式数据存储的需求。然而，有序性和唯一性的特殊属性并不总是必要的。让我们将部署与StatefulSets进行比较，自己看看：
- en: Deployments don't have associated storage, whereas StatefulSets do.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署没有关联的存储，而StatefulSets有。
- en: Deployments have no associated service, whereas StatefulSets do.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署没有关联的服务，而StatefulSets有。
- en: Deployment pods have no DNS name, whereas StatefulSet pods do.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署的pod没有DNS名称，而StatefulSet的pod有。
- en: Deployments launch and terminate pods in any order, whereas StatefulSets follow
    a prescribed order (by default).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署以任意顺序启动和终止pod，而StatefulSets遵循规定的顺序（默认情况下）。
- en: I recommend that you stick to deployments unless your distributed data store
    requires the special properties of StatefulSets. If you just need a stable identity,
    and not an ordered launch and shutdown, then use `podPolicy=Parallel`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您坚持使用部署，除非您的分布式数据存储需要StatefulSets的特殊属性。如果您只需要一个稳定的标识，而不是有序的启动和关闭，那么请使用`podPolicy=Parallel`。
- en: Reviewing a large StatefulSet example
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查一个大型StatefulSet示例
- en: Cassandra ([https://cassandra.apache.org/](https://cassandra.apache.org/)) is
    an interesting distributed data store that I have a lot of experience with. It
    is very powerful, but it requires a lot of knowledge to operate properly and develop
    against. It is also a great use case for StatefulSets. Let's quickly review Cassandra
    and learn how to deploy it in Kubernetes. Note that we will not use Cassandra
    in Delinkcious.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra ([https://cassandra.apache.org/](https://cassandra.apache.org/)) 是一个我有很多经验的有趣的分布式数据存储。它非常强大，但需要大量的知识才能正确运行和开发。它也是StatefulSets的一个很好的用例。让我们快速回顾一下Cassandra，并学习如何在Kubernetes中部署它。请注意，我们将不会在Delinkcious中使用Cassandra。
- en: A quick introduction to Cassandra
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cassandra的快速介绍
- en: Cassandra is an Apache open source project. It's a columnar data store and is
    very well suited for managing time series data. I've used it to collect and manage
    data from a network of thousands of air quality sensors for more than three years.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra是一个Apache开源项目。它是一个列式数据存储，非常适合管理时间序列数据。我已经使用它来收集和管理来自数千个空气质量传感器网络的数据超过三年。
- en: Cassandra has an interesting modeling approach, but, here, we care about storage.
    Cassandra is highly available, linearly scalable, and very reliable (no SPOF)
    via redundancy. Cassandra nodes share responsibility for the data (which is partitioned
    through the **distributed hash table**, or **DHT**). Multiple copies of the data
    are spread across multiple nodes (this is typically three or five).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Cassandra有一个有趣的建模方法，但在这里，我们关心存储。Cassandra具有高可用性，线性可扩展性，并且非常可靠（没有SPOF），通过冗余。Cassandra节点共享数据的责任（通过分布式哈希表或DHT进行分区）。数据的多个副本分布在多个节点上（通常是三个或五个）。
- en: 'In this way, if a Cassandra node goes down, then there are two other nodes
    that have the same data and can respond to queries. All nodes are the same; there
    are no masters and no slaves. The nodes constantly chat with each other through
    a gossip protocol and, when new nodes join the cluster, Cassandra redistributes
    the data among all the nodes. Here is a diagram that shows how data is distributed
    across the Cassandra cluster:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，如果Cassandra节点出现故障，那么还有其他两个节点具有相同的数据并且可以响应查询。所有节点都是相同的；没有主节点和从节点。节点通过八卦协议不断地与彼此交谈，当新节点加入集群时，Cassandra会重新分配数据到所有节点。这是一个显示数据如何分布在Cassandra集群中的图表：
- en: '![](assets/82ce0658-00d3-4bb0-a37f-bcf38ec26f6a.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/82ce0658-00d3-4bb0-a37f-bcf38ec26f6a.png)'
- en: You can think of the nodes as a ring and the DHT algorithm hashes each wide
    row (the unit of work) and assigns it to the *N* nodes (depending on the replication
    factor of the cluster). With that kind of precise placement of individual rows
    in specific nodes, you can see how the stable identity and, potentially, the ordering
    properties of a StatefulSet can come in handy.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将节点视为一个环，DHT算法对每个宽行（工作单元）进行哈希处理，并将其分配给N个节点（取决于集群的复制因子）。通过这种对特定节点中的单个行的精确放置，您可以看到StatefulSet的稳定标识和潜在的排序属性如何派上用场。
- en: Let's explore what it takes to deploy a Cassandra cluster as a StatefulSet in
    Kubernetes.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨在Kubernetes中将Cassandra集群部署为StatefulSet需要做些什么。
- en: Deploying Cassandra on Kubernetes using StatefulSets
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用StatefulSets在Kubernetes上部署Cassandra
- en: Here is a truncated version that includes the parts we should focus on.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个截断版本，包括我们应该关注的部分。
- en: 'The first part includes `apiVersion`, `kind`, `metadata`, and `spec`, as we''ve
    seen before. The name is `cassandra`, and the label is `app: cassandra`. In `spec`,
    the `serviceName` name is also `cassandra`, and there are three replicas:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '第一部分包括`apiVersion`，`kind`，`metadata`和`spec`，正如我们之前所见。名称是`cassandra`，标签是`app:
    cassandra`。在`spec`中，`serviceName`名称也是`cassandra`，有三个副本：'
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The pod template has a matching label of `app: cassandra`. The container is
    named `cassandra`, too, and uses a Google sample image with the always pull policy.
    Here, `terminationGraceInSeconds` is set to 1,800 seconds (that is, 30 minutes).
    That''s the time that the StatefulSet will allow the pod to try and recover if
    it becomes unresponsive. Cassandra has a lot of redundancy built in, so it''s
    okay to let a node attempt recovery for 30 minutes. I removed a lot of ports,
    environment variables, and readiness checks (the ellipses). The volume mount is
    called `cassandra-data`, and its path is `/cassandra_data`. That''s where Cassandra
    stores its data files:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pod模板具有匹配的标签`app: cassandra`。容器也被命名为`cassandra`，并使用了一个始终拉取策略的Google示例镜像。在这里，`terminationGraceInSeconds`设置为1,800秒（即30分钟）。这是StatefulSet允许pod尝试恢复的时间。Cassandra内置了很多冗余，所以让一个节点尝试恢复30分钟是可以接受的。我删除了很多端口、环境变量和就绪检查（省略号）。卷挂载被称为`cassandra-data`，其路径为`/cassandra_data`。这就是Cassandra存储其数据文件的地方。'
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, the volume claim template defines the persistent storage that matches
    the volume mounted in the container with the name `cassandra-data`. The storage
    class, `fast`, is not shown here, but it is typically local storage on the same
    node that runs the Cassandra pod. The storage size is one gibibyte:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，卷索赔模板定义了与容器中挂载的名称为`cassandra-data`的卷匹配的持久存储。存储类`fast`在这里没有显示，但通常是运行Cassandra
    pod的同一节点上的本地存储。存储大小为1 gibibyte。
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This should all look very familiar to you at this point. However, there's more
    successful Cassandra deployment to discover. If you recall, Cassandra has no master;
    Cassandra nodes talk to each other constantly using the gossip protocol.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这一切对你来说应该都很熟悉。然而，还有更多成功的Cassandra部署要发现。如果你还记得，Cassandra没有主节点；Cassandra节点使用gossip协议不断地相互交流。
- en: But how do Cassandra nodes find each other? Enter the seed provider; whenever
    a new node is added to the cluster, it is configured with the IP addresses of
    some seed nodes (in this case, `10.0.0.1`, `10.0.0.2`, and `10.0.0.3`). It starts
    exchanging messages with these seed nodes, which inform the new node of other
    Cassandra nodes in the cluster, as well as notifying all the other existing nodes
    that a new node has joined the cluster. In this way, each node in the cluster
    can very quickly know about every other node in the cluster.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 但是Cassandra节点如何找到彼此？进入种子提供程序；每当向集群添加新节点时，它都会配置一些种子节点的IP地址（在这种情况下为`10.0.0.1`，`10.0.0.2`和`10.0.0.3`）。它开始与这些种子节点交换消息，这些种子节点通知新节点集群中的其他Cassandra节点，并通知所有其他现有节点新节点已加入集群。通过这种方式，集群中的每个节点都可以非常快速地了解集群中的每个其他节点。
- en: 'Here is a section from a typical Kubernetes config file (`cassandra.yaml`)
    that defines the seed provider. In this case, it''s just a simple list of IP addresses:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是典型Kubernetes配置文件（`cassandra.yaml`）中定义种子提供程序的部分。在这种情况下，它只是一个简单的IP地址列表。
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The seed provider can be a custom class, too. This is a very nice extensible
    design. In Kubernetes, it is necessary because the original seed nodes may be
    moved around and get new IP addresses.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 种子提供程序也可以是自定义类。这是一个非常好的可扩展设计。在Kubernetes中是必要的，因为原始种子节点可能会被移动并获得新的IP地址。
- en: To address this, there is a custom `KubernetesSeedProvider` class that talks
    to the Kubernetes API server and can always return the IP addresses of the seed
    nodes at the time of the query. Cassandra is implemented in Java, and so is the
    custom seed provider that implements the `SeedProvider` Java interface.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，有一个自定义的`KubernetesSeedProvider`类，它与Kubernetes API服务器通信，并且始终可以返回查询时种子节点的IP地址。Cassandra是用Java实现的，自定义种子提供程序也是实现了`SeedProvider`
    Java接口的Java类。
- en: 'We''re not going to dissect this code in detail. The main thing to note is
    that it interfaces with a native Go library called `cassandra-seed.so`, and then
    it uses it to get the Kubernetes endpoints of the Cassandra service:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细解析这段代码。需要注意的主要是它与一个名为`cassandra-seed.so`的本地Go库进行接口，然后使用它来获取Cassandra服务的Kubernetes端点：
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The complete source code can be found at [https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java](https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的源代码可以在[https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java](https://github.com/kubernetes/examples/blob/master/cassandra/java/src/main/java/io/k8s/cassandra/KubernetesSeedProvider.java)找到。
- en: That's the magic that connects Cassandra to Kubernetes and allows them to work
    together. Now that we've seen how a complicated distributed data store can be
    deployed in Cassandra, let's take a look at local storage, which graduated to
    GA in Kubernetes 1.14.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是将Cassandra连接到Kubernetes并使它们能够一起工作的魔力。现在我们已经看到了一个复杂的分布式数据存储如何在Cassandra中部署，让我们来看看本地存储，它在Kubernetes
    1.14中升级为GA。
- en: Achieving high performance with local storage
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用本地存储实现高性能
- en: Let's now discuss the affinity between compute and storage. There is an interesting
    relationship between speed, capacity, persistence, and cost. When your data lives
    near your processor, you can start working on it immediately, as opposed to fetching
    it over the network. That's the promise of local storage.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论计算和存储之间的关联。速度、容量、持久性和成本之间存在有趣的关系。当您的数据存储在处理器附近时，您可以立即开始处理它，而不是通过网络获取。这就是本地存储的承诺。
- en: 'There are two primary ways to store your data locally: in memory and on local
    drives. However, there are nuances; memory is the fastest, SSD drives are about
    4 times slower than memory, and spinning disks are roughly 20 times slower than
    SSD drives ([https://gist.github.com/jboner/2841832](https://gist.github.com/jboner/2841832)).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种主要的本地数据存储方式：内存和本地驱动器。然而，有细微差别；内存是最快的，SSD驱动器比内存慢大约4倍，旋转硬盘比SSD驱动器慢大约20倍（https://gist.github.com/jboner/2841832）。
- en: 'Let''s consider both of these following options:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑以下两个选项：
- en: Storing your data in memory
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据存储在内存中
- en: Storing your data on a local SSD
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据存储在本地SSD上
- en: Storing your data in memory
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据存储在内存中
- en: 'The highest performance, as far as read and write latency and throughput is
    concerned, is when you keep your data in memory. There are different memory types
    and caches, but the bottom line is that memory is super fast. However, memory
    has significant downsides too, such as the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 就读写延迟和吞吐量而言，保持数据在内存中是性能最高的。有不同的内存类型和缓存，但归根结底，内存非常快。然而，内存也有显著的缺点，例如以下：
- en: A node has much more limited memory compared to disks (that is, it requires
    more machines to store the same amount of data).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与磁盘相比，节点的内存要有限得多（也就是说，需要更多的机器来存储相同数量的数据）。
- en: Memory is very expensive.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存非常昂贵。
- en: Memory is ephemeral.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存是短暂的。
- en: 'There are some use cases where you require your entire dataset in memory. In
    these cases, either the dataset is very small, or you can split it across multiple
    machines. If the data is important and can''t be easily generated, then you can
    address the ephemeral nature of memory in the following two ways:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些用例需要将整个数据集存储在内存中。在这些情况下，数据集要么非常小，要么可以分布在多台机器上。如果数据很重要且不容易生成，那么可以通过以下两种方式解决内存的临时性：
- en: Keep a persistent copy.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持持久副本。
- en: Redundancy (that is, keep data in memory across multiple machines and potentially
    geodistributed).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冗余（即在多台机器和可能地理分布的情况下在内存中保留数据）。
- en: Storing your data on a local SSD
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据存储在本地SSD上
- en: A local SSD is not as fast as memory, but it is very fast. Of course, you can
    always combine in-memory caching too (any respectable data store will use memory
    caching to its advantage). Using an SSD is appropriate when you require fast performance,
    but your working set doesn't fit in memory or, alternatively, you don't want to
    pay the premium of large memory when you can get by with a much cheaper, yet still
    very fast, SSD. For example, Cassandra recommends using local SSD storage as the
    backing store for its data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本地SSD的速度不及内存快，但非常快。当然，您也可以始终结合内存缓存（任何体面的数据存储都会利用内存缓存）。当您需要快速性能，但工作集不适合内存，或者您不想支付大内存的高额费用时，使用SSD是合适的，因为SSD便宜得多，但仍然非常快。例如，Cassandra建议使用本地SSD存储作为其数据的后备存储。
- en: Using relational databases in Kubernetes
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中使用关系型数据库
- en: So far, we've used a relational database in all our services, but, as we will
    soon discover, we didn't have real persistence. First, we'll look at where the
    data is stored, and then we'll explore how durable it is. Finally, we'll migrate
    one of the databases to use a StatefulSet for proper persistence and durability.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在所有服务中都使用了关系型数据库，但是，正如我们很快会发现的那样，我们并没有真正的持久性。首先，我们将看看数据存储在哪里，然后我们将探讨其持久性。最后，我们将迁移其中一个数据库以使用StatefulSet来实现适当的持久性和耐久性。
- en: Understanding where the data is stored
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解数据存储的位置
- en: 'For PostgreSQL, there is a `data` directory; this directory can be set using
    the `PGDATA` environment variable. By default, it is set to `/var/lib/postgresql/data`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PostgreSQL，有一个“data”目录；可以使用“PGDATA”环境变量设置此目录。默认情况下，它设置为“/var/lib/postgresql/data”：
- en: '[PRE31]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s take a look at what this directory contains:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个目录包含什么：
- en: '[PRE32]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: However, the `data` directory can be ephemeral or persistent depending on how
    it was mounted to the container.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，“data”目录可以是临时的或持久的，这取决于它是如何挂载到容器中的。
- en: Using a deployment and service
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用部署和服务
- en: With a service fronting your database pods, you can easily access the data.
    When a database pod is killed, it will be restarted by the deployment. However,
    since the pod can be scheduled on a different node, it is up to you to make sure
    that it has access to the storage where the actual data is. Otherwise, it will
    just start empty and you'll lose all the data. This is a development-only setup,
    and how most Delinkcious services keep their data – by running a PostgresDB container
    that is only as persistent as its pod. It turns out that the data is stored in
    the Docker container itself running inside the pod.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过服务面向数据库pod，您可以轻松访问数据。当数据库pod被杀死时，它将被部署重新启动。但是，由于pod可以被调度到不同的节点上，您需要确保它可以访问实际数据所在的存储。否则，它将只是空启动，您将丢失所有数据。这是一个仅用于开发的设置，以及大多数Delinkcious服务保持其数据的方式
    - 通过运行一个只有其pod持久性的PostgresDB容器。事实证明，数据存储在运行在pod内部的Docker容器中。
- en: 'In Minikube, I can inspect the Docker container directly by first SSH-ing into
    the node, finding the ID of the postgres container, and then inspecting it (that
    is, only if the relevant information is displayed):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在Minikube中，我可以直接检查Docker容器，首先通过SSH进入节点，找到postgres容器的ID，然后检查它（也就是说，只有在显示相关信息时）：
- en: '[PRE33]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This means that, if the container goes away (for example, if we upgrade to a
    new version) and certainly if the node goes away, then all our data disappears.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果容器消失（例如，如果我们升级到新版本），并且当然如果节点消失，那么所有我们的数据都会消失。
- en: Using a StatefulSet
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用StatefulSet
- en: With a StatefulSet, the situation is different. The data directory is mounted
    to the container, but the storage itself is managed externally. As long as the
    external storage is reliable and redundant, our data is safe, regardless of what
    happens to specific containers, pods, and nodes. We've previously mentioned how
    to define a StatefulSet for the user database using a headless service. However,
    consuming the storage of the StatefulSet can be a little challenging. The headless
    service attached to a StatefulSet has no cluster IP. So, how would the user service
    connect to its database? Well, we will have to help it.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用StatefulSet，情况就不同了。数据目录被挂载到容器中，但存储本身是由外部管理的。只要外部存储可靠且冗余，我们的数据就是安全的，不管特定容器、pod和节点发生了什么。我们之前提到过如何使用无头服务为用户数据库定义StatefulSet。然而，使用StatefulSet的存储可能有点具有挑战性。附加到StatefulSet的无头服务没有集群IP。那么，用户服务如何连接到其数据库呢？好吧，我们将不得不帮助它。
- en: Helping the user service locate StatefulSet pods
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 帮助用户服务定位StatefulSet pods
- en: 'The headless `user-db` service has no cluster IP, as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 无头`user-db`服务没有集群IP，如下所示：
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'However, it does have endpoints, which are the IP addresses in the cluster
    of all the pods that back the service:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，它确实有端点，这些端点是支持服务的所有pod在集群中的IP地址：
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This is a good option; endpoints are not exposed through environment variables,
    such as a service with a cluster IP (`<service name>_SERVICE_HOST and <service
    name>_SERVICE_PORT`). So, for a service to find the endpoints of a headless service,
    they'll have to query the Kubernetes API directly. While that's possible, it adds
    unnecessary coupling between the service and Kubernetes. We won't be able to run
    the service outside of Kubernetes for testing because it relies on the Kubernetes
    API. However, we can trick the user service and populate `USER_DB_SERVICE_HOST`
    and `USER_DB_SERVICE_PORT` using a config map.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不错的选择；端点不会通过环境变量暴露，例如具有集群IP的服务（`<service name>_SERVICE_HOST`和`<service name>_SERVICE_PORT`）。因此，为了使服务找到无头服务的端点，它们将不得不直接查询Kubernetes
    API。虽然这是可能的，但它增加了服务和Kubernetes之间不必要的耦合。我们将无法在Kubernetes之外运行服务进行测试，因为它依赖于Kubernetes
    API。但是，我们可以欺骗用户服务，并使用配置映射填充`USER_DB_SERVICE_HOST`和`USER_DB_SERVICE_PORT`。
- en: 'The idea is that StatefulSet pods have a stable DNS name. For the user database,
    there is one pod whose DNS name is `user-db-0.user-db.default.svc.cluster.local`.
    Inside the troubleshooter container shell, we can verify that the DNS name indeed
    resolves to the user database endpoint, `172.17.0.25`, by running the `dig` command:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是StatefulSet pods有一个稳定的DNS名称。对于用户数据库，有一个pod，其DNS名称是`user-db-0.user-db.default.svc.cluster.local`。在故障排除容器shell中，我们可以通过运行`dig`命令来验证DNS名称确实解析为用户数据库端点`172.17.0.25`：
- en: '[PRE36]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, we can take this stable DNS name and assign it to `USER_DB_SERVICE_HOST`
    in a config map for the `user-manager` service:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这个稳定的DNS名称分配给`user-manager`服务的配置映射中的`USER_DB_SERVICE_HOST`：
- en: '[PRE37]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once this config map is applied, the user service will be able to locate the
    user database pod of the StatefulSet through the environment variables. Here is
    the code that uses these environment variables from `pkg/db_util/db_util.go`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用了此配置映射，用户服务将能够通过环境变量找到StatefulSet的用户数据库pod。以下是使用`pkg/db_util/db_util.go`中的这些环境变量的代码：
- en: '[PRE38]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The user service calls it in its `Run()` function to initialize its database
    store:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 用户服务在其`Run()`函数中调用它以初始化其数据库存储：
- en: '[PRE39]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now, let's take a look at how to address the problem of managing schema changes.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何解决管理模式更改的问题。
- en: Managing schema changes
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理模式更改
- en: One of the most challenging topics when working with relational databases is
    managing the SQL schema. When the schema changes, the change may be backward compatible
    (by adding a column) or non-backward compatible (by splitting a table into two
    separate tables). When the schema changes, we need to migrate our database, but
    also migrate the code that is affected by the schema change.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用关系数据库时，最具挑战性的话题之一是管理SQL模式。当模式发生变化时，变化可能是向后兼容的（通过添加列）或非向后兼容的（通过将一个表拆分为两个独立的表）。当模式发生变化时，我们需要迁移我们的数据库，还需要迁移受模式更改影响的代码。
- en: 'If you can afford a short downtime, then the process can be very simple, as
    follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您可以承受短暂的停机时间，那么该过程可以非常简单，如下所示：
- en: Shut down all the impacted services and perform DB migration.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭所有受影响的服务并执行DB迁移。
- en: Deploy a new code that knows how to work with the new schema.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署一个新代码，知道如何处理新模式。
- en: Everything just works.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一切都能正常工作。
- en: However, if you need to keep the system running, you'll have to go through a
    more complicated process by breaking the schema change into multiple backward-compatible
    changes, including corresponding code changes.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您需要保持系统运行，您将不得不经历一个更复杂的过程，将模式更改分解为多个向后兼容的更改，包括相应的代码更改。
- en: 'For example, when splitting a table into two tables, the following process
    can be performed:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当将一个表拆分为两个表时，可以执行以下过程：
- en: Keep the original table.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留原始表格。
- en: Add the two new tables.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加两个新表。
- en: Deploy code that writes both to the old table and the new tables and can read
    from all of the tables.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署代码，既写入旧表，也写入新表，并且可以从所有表中读取。
- en: Migrate all the data from the old table to the new tables.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有数据从旧表迁移到新表。
- en: Deploy a code change that reads only from the new tables (which have all the
    data now).
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署一个只从新表中读取数据的代码更改（现在所有数据都在新表中）。
- en: Delete the old table.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除旧表。
- en: Relational databases are very useful; however, sometimes, the correct solution
    is a non-relational data store.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 关系数据库非常有用；然而，有时正确的解决方案是非关系型数据存储。
- en: Using non-relational data stores in Kubernetes
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes中使用非关系型数据存储
- en: Kubernetes and StatefulSets are not limited or even geared toward relational
    data stores. Non-relational (also known as NoSQL) data stores are very useful
    for many use cases. One of the most versatile and popular in-memory data stores
    is Redis. Let's get to know Redis and examine how to migrate the Delinkcious news
    service to use Redis instead of storing events in ephemeral memory.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes和StatefulSets并不局限于关系型数据存储，甚至不是为其设计的。非关系型（也称为NoSQL）数据存储对许多用例非常有用。最通用和流行的内存数据存储之一是Redis。让我们了解Redis，并检查如何将Delinkcious新闻服务迁移到使用Redis，而不是将事件存储在临时内存中。
- en: An introduction to Redis
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Redis简介
- en: 'Redis is often described as a data structure server. Since it keeps the entire
    data store in memory, it can perform many advanced operations on the data efficiently.
    The price you pay, of course, is that you have to keep *all* of the data in memory.
    This is possible only for small datasets and, even then, it''s expensive. If you
    don''t access most of your data, keeping it in memory is a huge waste. Redis can
    be used as a fast, distributed cache for hot data; so, even if you can''t use
    it as a distributed cache for your entire dataset in memory, you can still use
    Redis for the hot data (which is frequently used). Redis also supports clusters
    where the data is shared across multiple nodes, so it''s able to handle very large
    datasets too. Redis has an impressive list of features, including the following:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Redis通常被描述为数据结构服务器。由于它将整个数据存储保留在内存中，因此可以高效地对数据执行许多高级操作。当然，你要付出的代价是必须将*所有*数据保留在内存中。这只对小型数据集可能，并且即使如此，也是昂贵的。如果你不访问大部分数据，将其保留在内存中是一种巨大的浪费。Redis可以用作快速的分布式缓存，用于热数据；因此，即使你不能将其用作内存中整个数据集的分布式缓存，你仍然可以将Redis用于热数据（经常使用的数据）。Redis还支持集群，其中数据在多个节点之间共享，因此它也能处理非常大的数据集。Redis具有令人印象深刻的功能列表，包括以下内容：
- en: It provides multiple data structures such as lists, hashes, sets, sorted sets,
    bitmaps, streams, and geospatial indexes.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了多种数据结构，如列表、哈希、集合、有序集合、位图、流和地理空间索引。
- en: It provides atomic operations on many data structures.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在许多数据结构上提供原子操作。
- en: It supports transactions.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持事务。
- en: It supports auto-eviction with TTL.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持带有TTL的自动驱逐。
- en: It supports LRU eviction.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持LRU驱逐。
- en: It enables pub/sub.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它启用发布/订阅。
- en: It allows optional persistence to the disk.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许可选的持久化到磁盘。
- en: It allows optional appending of operations to the journal.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许将操作可选地附加到日志中。
- en: It provides Lua scripting.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供Lua脚本。
- en: Now, let's take a look at how Delinkcious uses Redis.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看Delinkcious如何使用Redis。
- en: Persisting events in the news service
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在新闻服务中持久化事件
- en: 'The news service provisions a Redis instance as a StatefulSet, as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻服务将Redis实例作为StatefulSet进行配置，如下所示：
- en: '[PRE40]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'It is supported by a headless service:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 它由一个无头服务支持：
- en: '[PRE41]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can use the same trick of injecting the DNS name of the Redis pod through
    environment variables using a config map:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的技巧，通过使用配置映射将Redis pod的DNS名称注入到环境变量中：
- en: '[PRE42]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'With the provisioning out of the way, let''s take a look at how the code is
    accessing Redis. In the `Run()` function of the news service, if the environment
    variables for Redis are not empty, then it will create a new Redis store:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了配置，让我们来看看代码如何访问Redis。在新闻服务的`Run()`函数中，如果Redis的环境变量不为空，它将创建一个新的Redis存储：
- en: '[PRE43]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `NewRedisNewStore()` function is defined in `pkg/new_manager/redis_news_store`.
    It creates a new Redis client (from the `go-redis` library). It also calls the
    client''s `Ping()` method to ensure that Redis is up and running and is reachable:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`NewRedisNewStore()`函数在`pkg/new_manager/redis_news_store`中定义。它创建一个新的Redis客户端（来自`go-redis`库）。它还调用客户端的`Ping()`方法来确保Redis正在运行并且是可访问的：'
- en: '[PRE44]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`RedisNewsStore` stores the events in a Redis list, which is serialized to
    TOML. This is all implemented in `AddEvent()`, as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`RedisNewsStore`将事件存储在Redis列表中，并将其序列化为TOML。这一切都在`AddEvent()`中实现，如下所示：'
- en: '[PRE45]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`RedisNewsStore` implements the `GetNews()` method to fetch events in order.
    First, it calculates the start and end indexes to query the event list based on
    the starting index and the maximum page size. Then, it gets the results, which
    are serialized to TOML, unmarshals them into the `om.Event` struct, and appends
    them to the result list of events. Finally, it computes the next index to fetch
    (`-1` if there are no more events):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`RedisNewsStore` 实现了 `GetNews()` 方法来按顺序获取事件。首先，它根据起始索引和最大页面大小计算要查询事件列表的起始和结束索引。然后，它获取结果，将它们序列化为
    TOML，将它们解组为 `om.Event` 结构，并将它们附加到事件结果列表中。最后，它计算下一个要获取的索引（如果没有更多事件，则为 `-1`）：'
- en: '[PRE46]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: At this point, you should have a good grasp of a non-relational data store,
    including when to use them and how to integrate Redis as a data store for your
    services.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您应该对非关系型数据存储有很好的掌握，包括何时使用它们以及如何将Redis集成为您的服务的数据存储。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we dealt with the very important topic of storage and real-world
    data persistence. We learned about the Kubernetes storage model, the common storage
    interface, and StatefulSets. Then, we discussed how to manage relational and non-relational
    data in Kubernetes and migrated several Delinkcious services to use proper persistent
    storage through StatefulSets, including how to provide data store endpoints for
    StatefulSet pods. Finally, we implemented a non-ephemeral data store for the news
    service using Redis. At this point, you should have a clear idea of how Kubernetes
    manages storage and is able to choose the proper data stores for your system,
    as well as integrate them into your Kubernetes cluster and with your services.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们处理了存储和现实世界数据持久性的非常重要的主题。我们了解了Kubernetes存储模型、常见存储接口和StatefulSets。然后，我们讨论了如何在Kubernetes中管理关系型和非关系型数据，并迁移了几个Delinkcious服务以使用适当的持久性存储通过StatefulSets，包括如何为StatefulSet
    pods提供数据存储端点。最后，我们使用Redis为新闻服务实现了一个非短暂数据存储。在这一点上，您应该清楚地了解了Kubernetes如何管理存储，并能够为您的系统选择适当的数据存储，并将它们集成到您的Kubernetes集群和服务中。
- en: In the next chapter, we will explore the exciting domain of serverless computing.
    We'll consider when the serverless model is useful, discuss current solutions
    for Kubernetes, and extend Delinkcious with some serverless tasks.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索令人兴奋的无服务器计算领域。我们将考虑无服务器模型何时有用，讨论Kubernetes的当前解决方案，并通过一些无服务器任务扩展Delinkcious。
- en: Further reading
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can refer to the following references for more information:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下参考资料获取更多信息：
- en: '**CSI**: [https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b](https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CSI**: [https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b](https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b)'
- en: '**StatefulSet**: [https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**StatefulSet**: [https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)'
- en: '**Cassandra**: [https://cassandra.apache.org/](https://cassandra.apache.org/)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cassandra**: [https://cassandra.apache.org/](https://cassandra.apache.org/)'
- en: '**Redis**: [http://redis.io/](http://redis.io/)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Redis**: [http://redis.io/](http://redis.io/)'
- en: '**Latency numbers every programmer should know**: [https://gist.github.com/jboner/2841832](https://gist.github.com/jboner/2841832)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**程序员应该知道的延迟数字**: [https://gist.github.com/jboner/2841832](https://gist.github.com/jboner/2841832)'
