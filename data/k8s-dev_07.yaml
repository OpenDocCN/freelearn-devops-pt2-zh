- en: Monitoring and Metrics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和指标
- en: In the previous chapters, we investigated the declarative structures used in
    Kubernetes objects and resources. With the end goal of having Kubernetes help
    run software for us, in this chapter we will look at how we can get more information,
    when we're running our applications at a greater scale, and some open source tools
    that we can use for that purpose. Kubernetes is already gathering and using some
    information about how utilized the nodes of the cluster are, and there is a growing
    capability within Kubernetes to start to collect application-specific metrics,
    and even use those metrics as a control point for managing the software.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们调查了Kubernetes对象和资源中使用的声明性结构。以Kubernetes帮助我们运行软件为最终目标，在本章中，我们将看看在更大规模运行应用程序时，如何获取更多信息，以及我们可以用于此目的的一些开源工具。Kubernetes已经在收集和使用有关集群节点利用情况的一些信息，并且在Kubernetes内部有越来越多的能力开始收集特定于应用程序的指标，甚至使用这些指标作为管理软件的控制点。
- en: 'In this chapter, will we dig into these aspects of basic observability and
    walk through how you can set them up for your local development use, and how to
    leverage them to gather, aggregate, and expose details of how your software is
    running, when you scale it up. Topics within this chapter will include:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨基本可观察性的这些方面，并介绍如何为您的本地开发使用设置它们，以及如何利用它们来收集、聚合和公开软件运行的详细信息，当您扩展它时。本章的主题将包括：
- en: Built-in metrics with Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置指标与Kubernetes
- en: A Kubernetes concept—Quality of Service
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes概念-服务质量
- en: Capturing metrics with Prometheus
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Prometheus捕获指标
- en: Installing and using Grafana
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和使用Grafana
- en: Using Prometheus to view application metrics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Prometheus查看应用程序指标
- en: Built-in metrics with Kubernetes
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内置指标与Kubernetes
- en: Kubernetes comes built in with some basic instrumentation to know how much CPU
    and memory are consumed on each node in the cluster. Exactly what is captured
    and how it is captured has been evolving rapidly in recent Kubernetes releases
    (1.5 through 1.9). Many Kubernetes installations will be capturing information
    about what resources the underlying containers are using with a program called
    cAdvisor. This code was created by Google to collect, aggregate, and expose the
    metrics of how containers are operating, as a critical step of being able to know
    where best to place new containers, based on what resources a node has and where
    resources are available.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes内置了一些基本的仪表来了解集群中每个节点消耗了多少CPU和内存。确切地捕获了什么以及如何捕获它在最近的Kubernetes版本（1.5到1.9）中正在迅速发展。许多Kubernetes安装将捕获有关底层容器使用的资源的信息，使用一个名为cAdvisor的程序。这段代码是由Google创建的，用于收集、聚合和公开容器的操作指标，作为能够知道最佳放置新容器的关键步骤，基于节点的资源和资源可用性。
- en: Every node within a Kubernetes cluster will have cAdvisor running and collecting
    information, and this, in turn, is captured and used by *kubelet*, which is the
    local agent on every node that is responsible for starting, stopping, and managing
    the various resources needed to run containers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群中的每个节点都将运行并收集信息的cAdvisor，并且这反过来又被*kubelet*使用，这是每个节点上负责启动、停止和管理运行容器所需的各种资源的本地代理。
- en: cAdvisor exposes a simple web-based UI that you can use to look at the details
    for any node, manually. If you can access port `4194` of the node, that is the
    default location that exposes the cAdvisor details. Depending on your cluster
    setup, this may not be easy to get access to. In the case of using Minikube, it
    is easily and directly available.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: cAdvisor提供了一个简单的基于Web的UI，您可以手动查看任何节点的详细信息。如果您可以访问节点的端口`4194`，那么这是默认位置，可以公开cAdvisor的详细信息。根据您的集群设置，这可能不容易访问。在使用Minikube的情况下，它很容易直接可用。
- en: 'If you have Minikube installed and running, you can use this command:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已安装并运行Minikube，可以使用以下命令：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To get the IP address of the virtual machine local to your development machine
    that is running your single-node Kubernetes cluster, you can access cAdvisor running,
    thereby opening a browser and navigating to that IP address at port `4194`. For
    example, on macOS running Minikube, you could use this command:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 获取运行单节点Kubernetes集群的开发机器上虚拟机的IP地址，可以访问运行的cAdvisor，然后在浏览器中导航到该IP地址的`4194`端口。例如，在运行Minikube的macOS上，您可以使用以下命令：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And you''ll see the simple UI, showing a page that looks something like this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您将看到一个简单的UI，显示类似于这样的页面：
- en: '![](assets/07b5bff8-6ca7-4575-95f9-453e7e300d7e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/07b5bff8-6ca7-4575-95f9-453e7e300d7e.png)'
- en: 'Scroll down a bit, and you will see a number of gauges and a table of information:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 向下滚动一点，您将看到一些仪表和信息表：
- en: '![](assets/1385db01-d5e4-46dd-b882-9904088ced18.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/1385db01-d5e4-46dd-b882-9904088ced18.png)'
- en: Beneath that is a set of simple graphs showing CPU, memory, network, and filesystem
    usage. These graphs and tables will update and automatically refresh as you watch
    them, and represent the basic information that Kubernetes is capturing about your
    cluster, as it operates.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一组简单的图表，显示了CPU、内存、网络和文件系统的使用情况。这些图表和表格将在您观看时更新和自动刷新，并代表了Kubernetes正在捕获的有关集群的基本信息。
- en: 'Kubernetes also makes metrics about itself—its API server and relevant components,
    available through its own API. You can see these metrics directly using the `curl`
    command after making the API available through `kubectl` proxy:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还通过其自己的API提供有关自身（其API服务器和相关组件）的指标。在通过`kubectl`代理使API可用后，您可以使用`curl`命令直接查看这些指标：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And in a separate Terminal window:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 并在单独的终端窗口中：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Many installations of Kubernetes have used a program called Heapster to collect
    metrics from Kubernetes and from each node's instance of cAdvisor, and store them
    in a time-series database such as InfluxDB. As of Kubernetes 1.9, the open source
    project is shifting a bit further away from Heapster towards a pluggable solution,
    with a common alternative solution being Prometheus, which is frequently used
    for short-term metrics capture.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 许多Kubernetes的安装都使用一个叫做Heapster的程序来从Kubernetes和每个节点的cAdvisor实例中收集指标，并将它们存储在诸如InfluxDB之类的时间序列数据库中。从Kubernetes
    1.9开始，这个开源项目正在从Heapster进一步转向可插拔的解决方案，常见的替代方案是Prometheus，它经常用于短期指标捕获。
- en: If you are using Minikube, you can easily add Heapster to your local environment
    with a `minikube` add-on. Like the dashboard, this will run software for Kubernetes
    on its own infrastructure, in this case, Heapster, InfluxDB, and Grafana.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Minikube，可以使用`minikube`插件轻松将Heapster添加到本地环境中。与仪表板一样，这将在其自己的基础设施上运行Kubernetes的软件，这种情况下是Heapster、InfluxDB和Grafana。
- en: 'This will enable the add-on within Minikube, you can use this command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在Minikube中启用插件，您可以使用以下命令：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the background, Minikube will start up and configure Heapster, InfluxDB,
    and Grafana, creating it as a service. You can use this command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在后台，Minikube将启动并配置Heapster、InfluxDB和Grafana，并将其创建为一个服务。您可以使用以下命令：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will open a browser window to Grafana. The command will wait while the
    containers are being set up, but when the service endpoint is available, it will
    open a browser window:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开一个到Grafana的浏览器窗口。该命令将在设置容器时等待，但当服务端点可用时，它将打开一个浏览器窗口：
- en: '![](assets/c588d346-eee6-4972-84bb-a6d7559cb24d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/c588d346-eee6-4972-84bb-a6d7559cb24d.png)'
- en: 'Grafana is a single-page application used to display graphs and build dashboards
    from common data sources. In the version that is created by the Minikube Heapster
    add-on, Grafana is configured with two dashboards: Cluster and Pods. If you select
    the pull-down menu labeled Home in the default view, you can choose the other
    dashboards to view.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana是一个用于显示图表和从常见数据源构建仪表板的单页面应用程序。在Minikube Heapster附加组件创建的版本中，Grafana配置了两个仪表板：集群和Pods。如果在默认视图中选择标记为“主页”的下拉菜单，您可以选择其他仪表板进行查看。
- en: 'It will take a minute or two before Heapster, InfluxDB, and Grafana have all
    coordinated to collect and capture some of the basic metrics of the environment,
    but fairly shortly, you can go to these other dashboards to see information about
    what''s running. For example, I deployed all the sample applications from the
    prior chapters in this book and went to the Cluster dashboard, and after 10 minutes
    or so, the view looked like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在Heapster、InfluxDB和Grafana协调收集和捕获环境的一些基本指标之前，可能需要一两分钟，但相当快地，您可以转到其他仪表板查看正在运行的信息。例如，我在本书的前几章中部署了所有示例应用程序，并转到了集群仪表板，大约10分钟后，视图看起来像这样：
- en: '![](assets/08832507-6a28-453a-b701-7623903350b0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/08832507-6a28-453a-b701-7623903350b0.png)'
- en: Scroll down through this dashboard, and you will see CPU, memory, filesystem,
    and network usage by node, as well as overall cluster views for this. You may
    notice the CPU graphs have three lines being tracked—usage, limit, and request—which
    match the resources actually in use, the amount requested, and any limits set
    on the pods and containers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仪表板向下滚动，您将看到节点的CPU、内存、文件系统和网络使用情况，以及整个集群的视图。您可能会注意到CPU图表有三条线被跟踪——使用情况、限制和请求——它们与实际使用的资源、请求的数量以及对pod和容器设置的任何限制相匹配。
- en: 'If you switch to the Pods dashboard, you will see that the dashboard has selections
    for all of the pods currently running in your cluster, and provides a detailed
    view of each one. In the example shown here, I selected the pod from our `flask`
    example application that I deployed:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果切换到Pods仪表板，您将看到该仪表板中有当前在集群中运行的所有pod的选择，并提供每个pod的详细视图。在这里显示的示例中，我选择了我们部署的`flask`示例应用程序的pod：
- en: '![](assets/ee592f34-3aa0-4a4f-95e9-6573b8a6c755.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ee592f34-3aa0-4a4f-95e9-6573b8a6c755.png)'
- en: Scrolling down, you can see graphs that include memory, CPU, network, and disk
    utilization. The collection of Heapster, Grafana, and InfluxDB will automatically
    record new pods as you create them, and you can select between namespaces and
    pod names in the Pods dashboard.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 向下滚动，您可以看到包括内存、CPU、网络和磁盘利用率在内的图表。Heapster、Grafana和InfluxDB的集合将自动记录您创建的新pod，并且您可以在Pods仪表板中选择命名空间和pod名称。
- en: Kubernetes concept – Quality of Service
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes概念-服务质量
- en: 'When a pod is created in Kubernetes, it is also assigned a Quality of Service
    class, based on the data provided about the pod when it was requested. This is
    used by the scheduler to provide some upfront assurances during the scheduling
    process, and later in the management of the pods themselves. The three classes
    supported are:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Kubernetes中创建一个pod时，它也被分配了一个服务质量类，这是基于请求时提供的有关pod的数据。这在调度过程中提供了一些前期保证，并在后续管理pod本身时使用。支持的三种类别是：
- en: '`Guaranteed`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`保证`'
- en: '`Burstable`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`可突发`'
- en: '`BestEffort`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`最佳努力`'
- en: Which class is assigned to your pod is based on what resource limits and requests
    you report with the containers within your pod for CPU and memory utilization.
    In the previous examples, none of the containers were assigned a requests or limit,
    so all of those pods were classified as `BestEffort` when they were run.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给您的Pod的类别是基于您在Pod内的容器中报告的CPU和内存利用率的资源限制和请求。在先前的示例中，没有一个容器被分配请求或限制，因此当它们运行时，所有这些Pod都被分类为“BestEffort”。
- en: Resource requests and limits are defined on each container within a pod. If
    we add a request to a container, we are asking for Kubernetes to make sure that
    the cluster has sufficient resources to run our pod (memory, CPU, or both) and
    it will validate that availability as a part of the scheduling. If we add a limit,
    we are requesting Kubernetes to watch the pod and react if the container exceeds
    the limits we set. For limits, if the container tries to exceed a CPU limit, the
    container will simply be throttled to the defined CPU limit. If a memory limit
    is exceeded, the container is frequently terminated and you will likely see the
    error message `OOM killed` in the `reason` description for those terminated containers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 资源请求和限制在Pod内的每个容器上进行定义。如果我们为容器添加请求，我们要求Kubernetes确保集群具有足够的资源来运行我们的Pod（内存、CPU或两者），并且它将作为调度的一部分验证可用性。如果我们添加限制，我们要求Kubernetes监视Pod，并在容器超出我们设置的限制时做出反应。对于限制，如果容器尝试超出CPU限制，容器将被简单地限制到定义的CPU限制。如果超出了内存限制，容器将经常被终止，并且您可能会在终止的容器的“reason”描述中看到错误消息“OOM
    killed”。
- en: If a request is set, the pods are generally set to the Quality of Service class
    of `Burstable`, with the exception of when a limit is also set, and that limit
    has the same value as the request, in which case the service class of `Guaranteed`
    is assigned. As a part of scheduling, if a pod is deemed to be in the `Guaranteed`
    service class, Kubernetes will reserve resources within the cluster and if overloaded,
    it will bias towards expiring and evicting `BestEffort` containers first, and
    then `Burstable`. A cluster will generally need to expect to lose resource capacity
    (for example, one or more nodes fail). In these cases, a `Guaranteed` class pod
    will have the most longevity in the face of such failures once it has been scheduled
    into a cluster.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设置了请求，Pod通常被设置为“可突发”的服务类别，但有一个例外，即当同时设置了限制，并且该限制的值与请求相同时，将分配“保证”服务类别。作为调度的一部分，如果Pod被认为属于“保证”服务类别，Kubernetes将在集群内保留资源，并且在超载时，会倾向于首先终止和驱逐“BestEffort”容器，然后是“可突发”容器。集群通常需要预期会失去资源容量（例如，一个或多个节点失败）。在这些情况下，一旦将“保证”类别的Pod调度到集群中，它将在面对此类故障时具有最长的寿命。
- en: 'We can update our `flask` example pod so that it will operate with a `Guaranteed` Quality
    of Service, by adding a request and limit for both CPU and memory:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更新我们的“flask”示例Pod，以便它将以“保证”的服务质量运行，方法是为CPU和内存都添加请求和限制：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This places a request and limit of the same value for both CPU and memory—in
    this case, 100 MB of memory and roughly half a core of CPU utilization.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这为CPU和内存都设置了相同值的请求和限制，例如，100 MB的内存和大约半个核心的CPU利用率。
- en: It is generally considered a best practice to at least define requests, and
    ideally limits as well, for all containers and pods that you want to run in a
    production mode.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通常认为，在生产模式下运行的所有容器和Pod，至少应该定义请求，并在最理想的情况下也定义限制，这被认为是最佳实践。
- en: Choosing requests and limits for your containers
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为您的容器选择请求和限制
- en: If you are uncertain of what values to use to set a request and/or limit for
    a container, the best means of determining those values is to watch them. With
    Heapster, or Prometheus, and Grafana, you can see how many resources are being
    consumed by each pod.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定要使用哪些值来设置容器的请求和/或限制，确定这些值的最佳方法是观察它们。使用 Heapster、Prometheus 和 Grafana，您可以看到每个
    pod 消耗了多少资源。
- en: 'There is a three-step process that you can use with your code to see what it''s
    taking:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个三步过程，您可以使用您的代码来查看它所占用的资源：
- en: Run your code and review how many resources are consumed while idle
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行您的代码并查看空闲时消耗了多少资源
- en: Add load to your code and verify the resource consumption under load
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为您的代码添加负载并验证负载下的资源消耗
- en: Having set constraints, run another load test for an sustained period of time
    to see that your code fits within the defined boundaries
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置了约束条件后，再运行一个持续一段时间的负载测试，以确保您的代码符合定义的边界
- en: The first step (reviewing while idle) will give you good numbers to start with.
    Leverage Grafana, or utilize cAdvisor available at your cluster node, and simply
    deploy the pod in question. In the preceding examples, where we did that with
    the `flask` example from earlier in this book, you can see an that idle flask
    application was consuming roughly 3 millicores (.003% of a core) and roughly 35
    MB of RAM. This makes a base of what to expect for a request for both CPU and
    memory.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步（空闲时审查）将为您提供一个良好的起点。利用 Grafana，或者利用您集群节点上可用的 cAdvisor，并简单地部署相关的 pod。在前面的示例中，我们在本书的早期示例中使用
    `flask` 示例进行了这样的操作，您可以看到一个空闲的 flask 应用程序大约消耗了 3 毫核（.003% 的核心）和大约 35 MB 的 RAM。这为请求
    CPU 和内存提供了一个预期值。
- en: The second step is often best done by running an **increasing load test** (also
    known as a **ramp load test**) to review how your pod reacts under load. Generally,
    you will see your load ramp up linearly with the requests, and then make a bend,
    or knee, where it starts to become bottlenecked. You can review the same Grafana
    or cAdvisor panels to show utilization during that load.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步通常最好通过运行**逐渐增加的负载测试**（也称为**坡道负载测试**）来查看您的 pod 在负载下的反应。通常，您会看到负载随请求线性增加，然后产生一个弯曲或拐点，开始变得瓶颈。您可以查看相同的
    Grafana 或 cAdvisor 面板，以显示负载期间的利用率。
- en: 'If you wanted to generate a simple bit of load, you could generate some specific
    load points with tools such as Apache benchmark ( [https://httpd.apache.org/docs/2.4/programs/ab.html](https://httpd.apache.org/docs/2.4/programs/ab.html)).
    For example, to run an interactive container with this tool that could work against
    the Flask application, you could use the following command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想生成一些简单的负载，可以使用诸如 Apache benchmark（[https://httpd.apache.org/docs/2.4/programs/ab.html](https://httpd.apache.org/docs/2.4/programs/ab.html)）之类的工具生成一些特定的负载点。例如，要运行一个与
    Flask 应用程序配合使用的交互式容器，可以使用以下命令：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This image has both `curl` and `ab` installed, so you can verify that you can
    talk to the Flask-service that we created in our earlier example with this command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此镜像已安装了 `curl` 和 `ab`，因此您可以使用此命令验证您是否可以与我们在早期示例中创建的 Flask 服务进行通信：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This should return some verbose output, showing the connection and basic request
    as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该返回一些冗长的输出，显示连接和基本请求如下：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once you have verified that everything is operating as you expect, you can
    run some load with `ab`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您验证了一切都按您的预期运行，您可以使用 `ab` 运行一些负载：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You will see a corresponding jump in resource usage in cAdvisor, or after a
    minute or so, Grafana with Heapster. To get useful values in Heapster and Grafana,
    you will want to run extended load tests since that data is being aggregated—you
    will want to run your load test ideally for several minutes, as one minute of
    granularity is the basic level that Grafana aggregates to with Heapster.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到cAdvisor中资源使用量的相应增加，或者大约一分钟后，在Heapster中看到Grafana。为了在Heapster和Grafana中获得有用的值，您将希望运行更长时间的负载测试，因为这些数据正在被聚合——最好是在几分钟内运行负载测试，因为一分钟是Grafana与Heapster聚合的基本级别。
- en: 'cAdvisor will update more quickly, and if you''re viewing the interactive graphs,
    you will see them update as the load test progresses:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: cAdvisor将更快地更新，如果您正在查看交互式图表，您将会看到它们随着负载测试的进行而更新：
- en: '![](assets/5e3a3c74-1d06-41a8-8589-b8b18fefcdc1.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/5e3a3c74-1d06-41a8-8589-b8b18fefcdc1.png)'
- en: In this case, you see our memory usage stayed fairly consistent at around 36
    MB, and our CPU peaked (as you might expect for this application) during the load
    test.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您会看到我们的内存使用量基本保持在36 MB左右，而我们的CPU在负载测试期间达到峰值（这是您可能会预期到的应用程序行为）。
- en: If we then applied the preceding request and limit examples, and updated the
    flask deployment, then you would see the load flatten off when the CPU hit the
    roughly 1/2 core CPU limit.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们应用了前面的请求和限制示例，并更新了flask部署，那么当CPU达到大约1/2核心CPU限制时，您会看到负载趋于平稳。
- en: The third step in this process is primarily to validate your assessments for
    CPU and memory needs over a longer-running load test. Typically, you would run
    an extended load (a minimum of several minutes long) with your requests and limits
    set to validate that the container could serve the traffic expected. The most
    common flaw in this assessment is seeing memory slowly climb while an extended
    load test is being performed, resulting in the container being OOM killed (terminated
    for exceeding its memory constraints).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的第三步主要是验证您对CPU和内存需求的评估是否符合长时间运行的负载测试。通常情况下，您会运行一个较长时间的负载（至少几分钟），并设置请求和限制来验证容器是否能够提供预期的流量。这种评估中最常见的缺陷是在进行长时间负载测试时看到内存缓慢增加，导致容器被OOM杀死（因超出内存限制而被终止）。
- en: The 100 MiB of RAM that we had in the example is reserving significantly more
    memory than this container needs, so we could easily reduce it down to 40 MiB
    and do the final validation step.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在示例中使用的100 MiB RAM比这个容器实际需要的内存要多得多，因此我们可以将其减少到40 MiB并进行最终验证步骤。
- en: 'When setting requests and limits, you want to choose values that most efficiently
    characterize your needs, but don''t waste reserved resources. To run a more extended
    load test, type:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置请求和限制时，您希望选择最有效地描述您的需求的值，但不要浪费保留的资源。要运行更长时间的负载测试，请输入：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The resulting Grafana output is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana的输出如下：
- en: '![](assets/965cf39b-d505-4897-abea-4bfbac400e5d.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/965cf39b-d505-4897-abea-4bfbac400e5d.png)'
- en: Capturing metrics with Prometheus
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Prometheus捕获指标
- en: Prometheus is a prominent open source tool used for monitoring, and quite a
    bit of symbiotic work is happening between it and the Kubernetes community. Kubernetes
    application metrics are exposed in the Prometheus format. This format includes
    the data types of *counter*, *gauge*, *histogram*, and *summary*, as well as a
    means of specifying labels to be associated with specific metrics. As Prometheus
    and Kubernetes have both evolved, the metrics format from Prometheus appears to
    be emerging as a de facto standard within the project and across its various components.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus是一个用于监控的知名开源工具，它与Kubernetes社区之间正在进行相当多的共生工作。Kubernetes应用程序指标以Prometheus格式公开。该格式包括*counter*、*gauge*、*histogram*和*summary*的数据类型，以及一种指定与特定指标相关联的标签的方法。随着Prometheus和Kubernetes的发展，Prometheus的指标格式似乎正在成为该项目及其各个组件中的事实标准。
- en: 'More information about this format is available online at the Prometheus project''s
    documentation:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此格式的更多信息可在Prometheus项目的文档中在线获取：
- en: '[https://prometheus.io/docs/concepts/data_model/](https://prometheus.io/docs/concepts/data_model/)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://prometheus.io/docs/concepts/data_model/](https://prometheus.io/docs/concepts/data_model/)'
- en: '[https://prometheus.io/docs/concepts/metric_types/](https://prometheus.io/docs/concepts/metric_types/)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://prometheus.io/docs/concepts/metric_types/](https://prometheus.io/docs/concepts/metric_types/)'
- en: '[https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/)'
- en: 'Beyond the metrics format, Prometheus offers quite a variety of capabilities
    as its own open source project, and is used outside Kubernetes. The architecture
    of this project gives a reasonable sense of its primary components:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了指标格式外，Prometheus作为自己的开源项目提供了相当多样的功能，并且在Kubernetes之外也被使用。该项目的架构合理地展示了其主要组件：
- en: '![](assets/2531e4a8-92de-4873-9164-9c5c23ec7806.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2531e4a8-92de-4873-9164-9c5c23ec7806.png)'
- en: The Prometheus server itself is what we will examine in this chapter. At its
    core, it periodically sweeps through a number of remote locations, collecting
    data from those locations, storing it in a short-term time-series database, and
    providing a means to query that database. Extensions to Prometheus allow the system
    to export these time-series metrics to other systems for longer-term storage.
    In addition, Prometheus includes an alert manager that can be configured to send
    alerts, or more generally invoke actions, based on the information captured and
    derived from the time-series metrics.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus服务器本身是我们将在本章中研究的内容。在其核心，它定期地扫描多个远程位置，从这些位置收集数据，将其存储在短期时间序列数据库中，并提供一种查询该数据库的方法。Prometheus的扩展允许系统将这些时间序列指标导出到其他系统以进行长期存储。此外，Prometheus还包括一个警报管理器，可以配置为根据从时间序列指标中捕获和派生的信息发送警报，或更一般地调用操作。
- en: Prometheus is not intended to be a long-term storage for metrics, and can work
    with a variety of other systems to capture and manage the data on a longer term.
    Common Prometheus installations maintain data for 6 to 24 hours, configurable
    per installation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus并不打算成为指标的长期存储，并且可以与各种其他系统一起工作，以在长期内捕获和管理数据。常见的Prometheus安装保留数据6至24小时，可根据安装进行配置。
- en: The most minimal installation of Prometheus would include the Prometheus server
    itself and configuration for the service. But to fully leverage Prometheus, the
    installation is frequently more expansive and complex, with a separate deployment
    each for the Alertmanager and Prometheus server, optionally a deployment for the
    push gateway (to allow other systems to actively send metrics to Prometheus),
    and a DaemonSet to capture data from every node within the cluster to expose and
    export that information into Prometheus, leveraging cAdvisor.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus的最小安装包括Prometheus服务器本身和服务的配置。但是，为了充分利用Prometheus，安装通常更加广泛和复杂，为Alertmanager和Prometheus服务器分别部署，可选地为推送网关部署（允许其他系统主动向Prometheus发送指标），以及一个DaemonSet来从集群中的每个节点捕获数据，将信息暴露和导出到Prometheus中，利用cAdvisor。
- en: More complex installations of software can be done by managing a set of YAML
    files, as we have been exploring earlier in this book. There are other options
    for how to manage and install sets of deployments, services, configurations, and
    so forth. Rather than documenting all the pieces, we will leverage one of the
    more common tools for this sort of work, Helm, which is closely tied to the Kubernetes
    project and is commonly referred to as *the package manager for Kubernetes*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的软件安装可以通过管理一组YAML文件来完成，就像我们在本书中之前所探讨的那样。有其他选项可以管理和安装一组部署、服务、配置等等。我们将利用这类工作中更常见的工具之一，Helm，而不是记录所有的部分，Helm与Kubernetes项目密切相关，通常被称为*Kubernetes的包管理器*。
- en: You can find significantly more information about Helm from the project's documentation
    site at [https://helm.sh](https://helm.sh).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在项目的文档网站[https://helm.sh](https://helm.sh)上找到有关Helm的更多信息。
- en: Installing Helm
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Helm
- en: 'Helm is a two-part system: a command-line tool and software that runs within
    your Kubernetes cluster that the command-line tool interacts with. Typically,
    what you need locally is the command-line tool, and that in turn will be used
    to install the components it needs into your cluster.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Helm是一个由命令行工具和在Kubernetes集群中运行的软件组成的双重系统，命令行工具与之交互。通常，您需要的是本地的命令行工具，然后再用它来安装所需的组件到您的集群中。
- en: The documentation for installing the Helm command-line tool is available at
    the project's website: [https://github.com/kubernetes/helm/blob/master/docs/install.md.](https://github.com/kubernetes/helm/blob/master/docs/install.md)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 有关安装Helm命令行工具的文档可在项目网站上找到：[https://github.com/kubernetes/helm/blob/master/docs/install.md.](https://github.com/kubernetes/helm/blob/master/docs/install.md)
- en: 'If you are using macOS locally, it is available via Homebrew and can be installed
    with:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在本地使用macOS，则可以通过Homebrew获得，并且可以使用以下命令进行安装：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Or if you''re working from a Linux host, the Helm project offers a script you
    can use to install Helm:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您是从Linux主机工作，Helm项目提供了一个脚本，您可以使用它来安装Helm：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once Helm is installed, you use it to install the component (called Tiller)
    that runs within your cluster, using the command `helm init`. You should see output
    like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Helm后，您可以使用`helm init`命令在集群中安装运行的组件（称为Tiller）。您应该会看到如下输出：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In addition to setting up some local configuration files for its use, this
    made a deployment on your cluster within the `kube-system` namespace for its cluster-side
    component, **Tiller**. You can view that deployment, if you want to see it in
    more detail:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为其使用设置一些本地配置文件之外，这还在`kube-system`命名空间中为其集群端组件**Tiller**进行了部署。如果您想更详细地查看它，可以查看该部署：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'At this point, you have Helm installed, and you can verify the version of your
    installation (both command line and what''s on the cluster) with the command Helm
    version. This operates very much like `kubectl` version, reporting both its version
    and the version of the system it''s communicating with:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您已经安装了Helm，并且可以使用命令Helm version验证安装的版本（包括命令行和集群上的版本）。这与`kubectl` version非常相似，报告了其版本以及与之通信的系统的版本。
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we can move on to the reason we set up Helm: to install Prometheus.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续设置Helm的原因：安装Prometheus。
- en: Installing Prometheus using Helm
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Helm安装Prometheus
- en: Helm uses a set of configuration files to describe what it needs to install,
    in what order, and with what parameters. These configurations are called charts,
    and are maintained in GitHub, where the default Helm repository is maintained.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Helm使用一组配置文件来描述安装需要什么，以什么顺序以及使用什么参数。这些配置称为图表，并在GitHub中维护，其中维护了默认的Helm存储库。
- en: 'You can view the repository that Helm is using with the command `helm repo
    list`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用命令`helm repo list`查看Helm正在使用的存储库。
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This default is a wrapper around a GitHub repository, and you can view the contents
    of the repository at [https://github.com/kubernetes/charts](https://github.com/kubernetes/charts).
    Another way to see all the charts that are available for use is the command `helm
    search`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此默认值是围绕GitHub存储库的包装器，您可以在[https://github.com/kubernetes/charts](https://github.com/kubernetes/charts)上查看存储库的内容。查看所有可用于使用的图表的另一种方法是使用命令`helm
    search`。
- en: It is a good idea to make sure you have the latest cache of the repository available.
    You can update your cache to the latest, mirroring the charts in GitHub, with
    the command `helm repo update`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您拥有存储库的最新缓存是个好主意。您可以使用命令`helm repo update`将缓存更新到最新状态，以在GitHub中镜像图表。
- en: 'The resulting update should report success with output similar to:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的结果应该报告成功，并输出类似于：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We are going to use the stable/Prometheus chart (hosted at [https://github.com/kubernetes/charts/tree/master/stable/prometheus](https://github.com/kubernetes/charts/tree/master/stable/prometheus)).
    We can use Helm to pull that chart locally, to look at it in more detail:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用stable/Prometheus图表（托管在[https://github.com/kubernetes/charts/tree/master/stable/prometheus](https://github.com/kubernetes/charts/tree/master/stable/prometheus)）。我们可以使用Helm将该图表拉取到本地，以便更详细地查看它。
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This command downloads the chart from the default repository and unpacks it
    locally in a directory called Prometheus. Take a look in the directory, and you
    should see several files and a directory called `templates`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令从默认存储库下载图表并在名为Prometheus的目录中本地解压缩。查看目录，您应该会看到几个文件和一个名为`templates`的目录：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This is the common pattern for charts, where `Chart.yaml` describes the software
    that will be installed by the chart. `values.yaml` is a collection of default
    configuration values that are used throughout all the various Kubernetes resources
    that will be created, and the templates directory contains the collection of templated
    files that will get rendered out to install all the Kubernetes resources needed
    for this software in your cluster. Typically, the `README.md` will include a description
    of all the values within the `values.yaml`, what they're used for, and suggestions
    for installation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是图表的常见模式，其中`Chart.yaml`描述了将由图表安装的软件。`values.yaml`是一组默认配置值，这些值在将要创建的各种Kubernetes资源中都会使用，并且模板目录包含了将被渲染出来以安装集群中所需的所有Kubernetes资源的模板文件集合。通常，`README.md`将包括`values.yaml`中所有值的描述，它们的用途以及安装建议。
- en: 'We can now install `prometheus`, and we will do so by taking advantage of a
    couple of Helm''s options, setting a release name and using a namespace:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以安装`prometheus`，我们将利用Helm的一些选项来设置一个发布名称并使用命名空间来进行安装。
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This installs the chart included in the `prometheus` directory, installs all
    the components included into the namespace, `monitoring`, and prefixes all the
    objects with a release name of `monitor`. If we had not specified either of those
    values, Helm would have used the default namespace, and generated a random release
    name to uniquely identify the installation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装`prometheus`目录中包含的图表，将所有组件安装到命名空间`monitoring`中，并使用发布名称`monitor`为所有对象添加前缀。如果我们没有指定这些值中的任何一个，Helm将使用默认命名空间，并生成一个随机的发布名称来唯一标识安装。
- en: 'When this is invoked, you will see quite a bit of output describing what was
    created and its state at the start of the process, followed by a section of notes
    that provides information about how to access the software you just installed:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此命令时，您将看到相当多的输出，描述了在过程开始时创建的内容及其状态，然后是提供有关如何访问刚刚安装的软件的信息的注释部分：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`helm list` will show you the current releases that you have installed:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`helm list`将显示您已安装的当前发布：'
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And you can use the `helm status` command, along with the name of the release,
    to get the current state of all the Kubernetes resources created by the chart:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`helm status`命令，以及发布的名称，来获取图表创建的所有Kubernetes资源的当前状态：
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The notes section is included in the templates and rendered again on every status
    call, and is generally written to include notes on how to access the software.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注释部分包含在模板中，并在每次状态调用时重新呈现，通常编写以包括有关如何访问软件的说明。
- en: 'You can install a chart without having explicitly retrieved it first. Helm
    will use any local charts first, but fall back to searching through its available
    repositories, so we could have installed this same chart with just this command:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以安装图表而无需显式先检索它。Helm首先使用任何本地图表，但会回退到搜索其可用存储库，因此我们可以只使用以下命令安装相同的图表：
- en: '`**helm install stable/prometheus -n monitor --namespace monitoring**`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`**helm install stable/prometheus -n monitor --namespace monitoring**`'
- en: 'You can also have Helm mix together `values.yaml` and its templates, to render
    out all the objects it will create and simply display them, which can be useful
    to see how all the pieces will come together. The command to do this is `helm
    template`, and to render the YAML that was used to create the Kubernetes resources,
    the command would be:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以让Helm混合`values.yaml`和其模板，以呈现出它将创建的所有对象并简单显示它们，这对于查看所有部件将如何组合在一起很有用。执行此操作的命令是`helm
    template`，要呈现用于创建Kubernetes资源的YAML，命令将是：
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `helm template` command does require the chart to be available on the local
    filesystem, so while `helm install` can work from a remote repository, you will
    need to use `helm fetch` to have the chart locally, in order to take advantage
    of the `helm template` command.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`helm template`命令确实需要图表在本地文件系统上可用，因此，虽然`helm install`可以从远程存储库中工作，但您需要使用`helm
    fetch`将图表本地化，以便利用`helm template`命令。'
- en: Viewing metrics with Prometheus
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Prometheus查看指标
- en: 'Using the details available in the notes, you can set up a port-forward, as
    we have done earlier in this book and access Prometheus directly. The information
    that was displayed from the notes is shown here:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用注释中提供的详细信息，您可以设置端口转发，就像我们在本书中之前所做的那样，并直接访问Prometheus。从注释中显示的信息如下：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This will allow you to directly access the Prometheus server with a browser.
    Run these commands in a terminal, and then open a browser and navigate to `http://localhost:9090/`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这将允许您直接使用浏览器访问Prometheus服务器。在终端中运行这些命令，然后打开浏览器并导航到`http://localhost:9090/`。
- en: 'You can view the current state of what Prometheus is monitoring by looking
    at the list of targets at `http://localhost:9090/targets`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看`http://localhost:9090/targets`上的目标列表来查看Prometheus正在监视的当前状态：
- en: '![](assets/207a582a-322b-4fbf-9b78-88134b726460.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/207a582a-322b-4fbf-9b78-88134b726460.png)'
- en: Switch to the Prometheus query/browser at `http://localhost:9090/graph` to be
    able to view the metrics that have been collected by Prometheus. There are a large
    number of metrics that are collected, and we are specifically interested in the
    ones that match the information we saw earlier with cAdvisor and Heapster. In
    Kubernetes clusters of version 1.7 and later, these metrics moved and are specifically
    collected by the `kubernetes-nodes-cadvisor` job that we see in the targets in
    the screenshot.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到 Prometheus 查询/浏览器，网址为`http://localhost:9090/graph`，以查看 Prometheus 收集的指标。收集了大量的指标，我们特别感兴趣的是与之前在
    cAdvisor 和 Heapster 中看到的信息匹配的指标。在 Kubernetes 1.7 版本及更高版本的集群中，这些指标已经移动，并且是由我们在屏幕截图中看到的`kubernetes-nodes-cadvisor`作业专门收集的。
- en: In the query browser, you can start typing metric names and it will attempt
    to autocomplete, or you can use the pull-down menu to see the list of all possible
    metrics. Type in the metric name, `container_memory_usage_bytes`, and hit *Enter*
    to see a list of those metrics in table form.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询浏览器中，您可以开始输入指标名称，它将尝试自动完成，或者您可以使用下拉菜单查看所有可能的指标列表。输入指标名称`container_memory_usage_bytes`，然后按*Enter*以以表格形式查看这些指标的列表。
- en: The general form of good metrics will have some identifiers of what the metric
    is and usually end with a unit identifier, in this case, bytes. Looking at the
    table, you can see the metrics that are collected along with a fairly dense set
    of key-value pairs for each metric.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 良好指标的一般形式将具有一些指标的标识符，并且通常以单位标识符结尾，在本例中为字节。查看表格，您可以看到收集的指标以及每个指标的相当密集的键值对。
- en: These key-value pairs are labels on the metrics, and function similarly to the
    way labels and selectors do, within Kubernetes in general.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这些键值对是指标上的标签，并且在整体上类似于 Kubernetes 中的标签和选择器的工作方式。
- en: 'An example metric, reformatted to make it easier to read, is shown here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例指标，重新格式化以便更容易阅读，如下所示：
- en: '[PRE27]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the query, we can filter just the metrics we are interested in by including
    matches in the query to those labels. For example, all of the metrics that are
    associated with specific containers will have an `image` tag, so we can filter
    to just those metrics with:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询中，我们可以通过在查询中包含与这些标签匹配的内容来过滤我们感兴趣的指标。例如，与特定容器相关联的所有指标都将具有`image`标签，因此我们可以仅过滤这些指标：
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You may have noticed that the namespaces and pod names are also included, and
    we can match on those as well. For example, to just view the metrics related to
    the default namespace where we have been deploying our sample applications, we
    could add `namespace="default"`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，命名空间和 pod 名称也包括在内，我们也可以进行匹配。例如，只查看与我们部署示例应用程序的默认命名空间相关的指标，我们可以添加`namespace="default"`：
- en: '[PRE29]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is starting to get down to a more reasonable number of metrics. While
    the table will show you the most recent value, the history of these values is
    what we''re interested in seeing. If you select the Graph button on the current
    query, it will attempt to render out a single graph of the metrics that you have
    selected, for example:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这开始变得更合理了。虽然表格将向您显示最近的值，但我们感兴趣的是这些值的历史记录。如果您选择当前查询上的图形按钮，它将尝试呈现出您选择的指标的单个图形，例如：
- en: '![](assets/fbccdafc-027d-4fae-ac33-27b56dde19fa.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/fbccdafc-027d-4fae-ac33-27b56dde19fa.png)'
- en: 'Since the metrics also include `container_name` to match the deployment, you
    can tune this down to a single container. For example, to see the memory usage
    associated with our `flask` deployment:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于指标还包括`container_name`以匹配部署，您可以将其调整为单个容器。例如，查看与我们的`flask`部署相关的内存使用情况：
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If we scale up the number of replicas in our `flask` deployment, it will create
    new metrics for each container, so in order to view not just single containers
    but also sets at a time, we can take advantage of aggregating operators in the
    Prometheus query language. Some of the most useful operators include `sum`, `count`,
    `count_values`, and `topk`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们增加`flask`部署中副本的数量，它将为每个容器创建新的指标，因此为了不仅查看单个容器而且一次查看多个集合，我们可以利用Prometheus查询语言中的聚合运算符。一些最有用的运算符包括`sum`、`count`、`count_values`和`topk`。
- en: 'We can also use these same aggregation operators to group metrics together
    where the aggregated set has different tag values within it. For example, after
    increasing the replicas on the `flask` deployment to three, we can view the total
    memory usage of the deployment with:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用这些相同的聚合运算符将指标分组在一起，其中聚合集合具有不同的标签值。例如，在将`flask`部署的副本增加到三个后，我们可以查看部署的总内存使用情况：
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And then break it out into each container again by the pod name:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再次按照Pod名称将其分解为每个容器：
- en: '[PRE32]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The graph function can give you a nice visual overview, including stacking
    the values, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图形功能可以为您提供一个良好的视觉概览，包括堆叠值，如下所示：
- en: '![](assets/2b9ea59d-0862-40c8-a304-1fd6932379de.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2b9ea59d-0862-40c8-a304-1fd6932379de.png)'
- en: With the graphs getting more complex, you may want to start to collect the queries
    you find most interesting, as well as put together dashboards of these graphs
    to be able to use them. This leads us to another open source project, Grafana,
    which can be easily hosted on Kubernetes, providing the dashboards and graphs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图形变得更加复杂，您可能希望开始收集您认为最有趣的查询，以及组合这些图形的仪表板，以便能够使用它们。这将引导我们进入另一个开源项目Grafana，它可以很容易地在Kubernetes上托管，提供仪表板和图形。
- en: Installing Grafana
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Grafana
- en: Grafana isn't by itself a complex installation, but configuring it can be. Grafana
    can plug into a number of different backend systems and provide dashboarding and
    graphing for them. In our example, we would like to have it provide dashboards
    from Prometheus. We will set up an installation and then configure it through
    its user interface.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana本身并不是一个复杂的安装，但配置它可能会很复杂。Grafana可以插入多种不同的后端系统，并为它们提供仪表板和图形。在我们的示例中，我们希望它从Prometheus提供仪表板。我们将设置一个安装，然后通过其用户界面进行配置。
- en: 'We can use Helm again to install Grafana, and since we have put Prometheus
    in the namespace monitoring, we will do the same with Grafana. We could do `helm
    fetch` and install to look at the charts. In this case, we will just install them
    directly:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次使用Helm来安装Grafana，由于我们已经将Prometheus放在监控命名空间中，我们将用相同的方式处理Grafana。我们可以使用`helm
    fetch`并安装来查看图表。在这种情况下，我们将直接安装它们：
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In the resulting output, you will see a secret, ConfigMap, and deployment among
    the resources created, and in the notes, something like:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的输出中，您将看到一个秘密、ConfigMap和部署等资源被创建，并且在注释中会有类似以下内容：
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The notes first include information about retrieving the secret. This highlights
    a feature that you will see used in several charts: where it requires a confidential
    password, it will generate a unique one and save it as a secret. This secret is
    directly available to people with access to the namespace and `kubectl`.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注释首先包括有关检索秘密的信息。这突出了一个功能，您将看到在几个图表中使用：当它需要一个机密密码时，它将生成一个唯一的密码并将其保存为秘密。这个秘密直接可供访问命名空间和`kubectl`的人使用。
- en: 'Use the provided command to retrieve the password for your Grafana interface:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提供的命令检索Grafana界面的密码：
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And then open a Terminal and run these commands to get access to the dashboard:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后打开终端并运行这些命令以访问仪表板：
- en: '[PRE36]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, open a browser window and navigate to `https://localhost:3000/`, which
    should show you the Grafana login window:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，打开浏览器窗口，导航至`https://localhost:3000/`，这将显示Grafana登录窗口：
- en: '![](assets/56c2c32b-f460-4561-b192-3e2a24c4a6b8.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/56c2c32b-f460-4561-b192-3e2a24c4a6b8.png)'
- en: 'Now, log in with the username `admin`; the password is the secret you retrieved
    earlier. This will bring you to a Home dashboard in Grafana, where you can configure
    data sources and assemble graphs into dashboards:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用用户名`admin`登录；密码是您之前检索到的秘密。这将带您进入Grafana中的主页仪表板，您可以在那里配置数据源并将图形组合成仪表板：
- en: '![](assets/229ace04-bb99-4ac2-b282-cdc03ef23b21.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/229ace04-bb99-4ac2-b282-cdc03ef23b21.png)'
- en: 'Click on Add data source and you will see a window with two tabs: Config allows
    you to set the location to a data source, and Dashboards allows you to import
    dashboard configurations.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“添加数据源”，您将看到一个带有两个选项卡的窗口：配置允许您设置数据源的位置，仪表板允许您导入仪表板配置。
- en: 'Under Config, set the type of the data source to Prometheus, and where it says
    Name, you can enter `prometheus`. Naming the data source after the type is a bit
    redundant, and if you had multiple Prometheus instances on your cluster, you would
    want to name them separately and specifically for their purpose. Add in the DNS
    name of our Prometheus instance to the URL so that Grafana can access it: `http://monitor-prometheus-server.monitoring.svc.cluster.local`.
    This same name was listed in the notes when we installed Prometheus using Helm.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置下，将数据源的类型设置为Prometheus，在名称处，您可以输入`prometheus`。在类型之后命名数据源有点多余，如果您的集群上有多个Prometheus实例，您会希望为它们分别命名，并且特定于它们的目的。在URL中添加我们的Prometheus实例的DNS名称，以便Grafana可以访问它：`http://monitor-prometheus-server.monitoring.svc.cluster.local`。在使用Helm安装Prometheus时，这个相同的名称在注释中列出了。
- en: 'Click on the Dashboards tab and import Prometheus stats and Grafana metrics,
    which will provide built-in dashboards for Prometheus and Grafana itself. Click
    back to the Config tab, scroll down, and click on the Add button to set up the
    Prometheus data source. You should see Data source is working when you add it:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“仪表板”选项卡，并导入Prometheus统计信息和Grafana指标，这将为Prometheus和Grafana本身提供内置仪表板。点击返回到“配置”选项卡，向下滚动，并点击“添加”按钮设置Prometheus数据源。当您添加时，您应该会看到“数据源正在工作”。
- en: '![](assets/9c2bdd4a-b03e-4fdb-a19f-4cd7946cef4b.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/9c2bdd4a-b03e-4fdb-a19f-4cd7946cef4b.png)'
- en: 'Now, you can navigate to the built-in dashboards and see some of the information.
    The top of the web page user interface consists of pull-downs, the top left leading
    to overall Grafana configuration and the next listing the dashboards you have
    set up, which generally starts with a Home dashboard. Select the Prometheus Stats
    dashboard that we just imported, and you should see some initial information about
    Prometheus:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以导航到内置仪表板并查看一些信息。网页用户界面的顶部由下拉菜单组成，左上角导航到整体Grafana配置，下一个列出了您设置的仪表板，通常从主页仪表板开始。选择我们刚刚导入的Prometheus
    Stats仪表板，您应该会看到有关Prometheus的一些初始信息：
- en: '![](assets/8fc15a8e-3e4b-4a7b-ac73-78c26e24e863.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/8fc15a8e-3e4b-4a7b-ac73-78c26e24e863.png)'
- en: The Grafana project maintains a collection of dashboards that you can search
    through and either use directly, or use for inspiration to modify and create your
    own. You can search through the dashboards that have been shared—for example,
    limit it to dashboards sourced from Prometheus and related to Kubernetes. You'll
    see a variety of dashboards to browse through, some including screenshots, at [https://grafana.com/dashboards?dataSource=prometheus&amp;search=kubernetes](https://grafana.com/dashboards?dataSource=prometheus&search=kubernetes).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana项目维护了一系列仪表板，您可以搜索并直接使用，或者用作灵感来修改和创建自己的仪表板。您可以搜索已共享的仪表板，例如，将其限制为来自Prometheus并与Kubernetes相关的仪表板。您将看到各种各样的仪表板可供浏览，其中一些包括屏幕截图，网址为[https://grafana.com/dashboards?dataSource=prometheus&amp;search=kubernetes](https://grafana.com/dashboards?dataSource=prometheus&search=kubernetes)。
- en: 'You can import these into your instance of Grafana using the dashboard number.
    For example, dashboards 1621 and 162 are common dashboards for monitoring the
    health of the overall cluster:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用仪表板编号将其导入到Grafana的实例中。例如，仪表板1621和162是用于监视整个集群健康状况的常见仪表板：
- en: '![](assets/ee7702a3-ee8c-4556-9a53-292187d189f7.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ee7702a3-ee8c-4556-9a53-292187d189f7.png)'
- en: The best value of these dashboards is to show you how to configure your own
    graphs and make your own dashboards. Within every dashboard, you may select the
    graphs and choose Edit to see the queries used and display choices made, and tweak
    them to your values. Every dashboard can also be shared back to the Grafana hosting
    site, or you can view the JSON that is the configuration and save it locally.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这些仪表板的最佳价值在于向您展示如何配置自己的图形并制作自己的仪表板。在每个仪表板中，您可以选择图形并选择编辑以查看使用的查询和显示选择，并根据您的值进行微调。每个仪表板也可以共享回Grafana托管站点，或者您可以查看配置的JSON并将其保存在本地。
- en: The work going on with the Prometheus operator is working towards making it
    easier to bring up Prometheus and Grafana together, pre-configured and running
    to monitor your cluster and the applications within your cluster. If you are interested
    in trying it out, see the project README that is hosted by CoreOS at [https://github.com/coreos/prometheus-operator/tree/master/helm](https://github.com/coreos/prometheus-operator/tree/master/helm),
    which can also be installed with Helm.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus运营商正在努力使启动Prometheus和Grafana变得更容易，预先配置并运行以监视您的集群和集群内的应用程序。如果您有兴趣尝试一下，请参阅CoreOS托管的项目README
    [https://github.com/coreos/prometheus-operator/tree/master/helm](https://github.com/coreos/prometheus-operator/tree/master/helm)，也可以使用Helm进行安装。
- en: Now that you have Grafana and Prometheus installed, you may use them to follow
    a similar process to determine the CPU and memory utilization of your own software,
    while running load tests. One of the benefits of running Prometheus locally is
    the ability it provides to collect metrics about your application.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经安装了Grafana和Prometheus，您可以使用它们来遵循类似的过程，以确定您自己软件的CPU和内存利用率，同时运行负载测试。在本地运行Prometheus的一个好处是它提供了收集有关您的应用程序的指标的能力。
- en: Using Prometheus to view application metrics
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Prometheus查看应用程序指标
- en: While you could add a job within Prometheus to include the configuration to
    scrape Prometheus metrics from a specific endpoint, the installation we did earlier
    includes a configuration that will update what it is looking at dynamically, based
    on annotations on pods. One of the benefits of Prometheus is that it has support
    for automatically detecting changes in your cluster, based on annotations, and
    it can look up the endpoints for the pods that back a service.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以在Prometheus中添加作业以包括从特定端点抓取Prometheus指标的配置，但我们之前进行的安装包括一个将根据Pod上的注释动态更新其查看内容的配置。
    Prometheus的一个好处是它支持根据注释自动检测集群中的更改，并且可以查找支持服务的Pod的端点。
- en: Since we deployed Prometheus using Helm, you can find the relevant configuration
    embedded within the `values.yaml` file. Look for the Prometheus job `kubernetes-service-endpoints`,
    and you will find both the configuration and some documentation of how it can
    be used. If you don't have the files locally, you can view this configuration
    at [https://github.com/kubernetes/charts/blob/master/stable/prometheus/values.yaml#L747-L776.](https://github.com/kubernetes/charts/blob/master/stable/prometheus/values.yaml#L747-L776)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用Helm部署了Prometheus，您可以在`values.yaml`文件中找到相关的配置。查找Prometheus作业`kubernetes-service-endpoints`，您将找到配置和一些关于如何使用它的文档。如果您没有本地文件，可以在[https://github.com/kubernetes/charts/blob/master/stable/prometheus/values.yaml#L747-L776](https://github.com/kubernetes/charts/blob/master/stable/prometheus/values.yaml#L747-L776)上查看此配置。
- en: This configuration looks for services within the cluster that have an annotation
     `prometheus.io/scrape` . If this is set to `true`, then Prometheus will automatically
    attempt to add that endpoint to the list of targets it is watching. By default,
    it will attempt to access the metrics at the URI `/metrics` and use the same port
    as the service. You can use additional annotations to change those defaults, for
    example, `prometheus.io/path = "/alternatemetrics"` will attempt to read the metrics
    from the path `/alternatemetrics`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置查找集群中具有注释`prometheus.io/scrape`的服务。如果设置为`true`，那么Prometheus将自动尝试将该端点添加到其正在监视的目标列表中。默认情况下，它将尝试访问URI`/metrics`上的指标，并使用与服务相同的端口。您可以使用其他注释来更改这些默认值，例如，`prometheus.io/path
    = "/alternatemetrics"`将尝试从路径`/alternatemetrics`读取指标。
- en: By using the service as the means of organizing metric gathering, we have a
    mechanism that will automatically scale with the number of pods. Whereas in other
    environments you might have to reconfigure the monitoring every time you add or
    remove instances, Prometheus and Kubernetes working together seamlessly capture
    this data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用服务作为组织指标收集的手段，我们有一个机制，它将根据Pod的数量自动扩展。而在其他环境中，您可能需要每次添加或删除实例时重新配置监控，Prometheus和Kubernetes无缝协作捕获这些数据。
- en: This capability allows us to easily expose custom metrics from our application
    and have them picked up by Prometheus. There are several ways this can be used,
    but the most obvious is simply getting better visibility on how your application
    is doing. With Prometheus collecting the metrics and Grafana installed as a dashboard
    tool, you could also use the combination to create your own application dashboards.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力使我们能够轻松地从我们的应用程序中公开自定义指标，并让Prometheus捕获这些指标。这可以有几种用途，但最明显的是更好地了解应用程序的运行情况。有了Prometheus收集指标和Grafana作为仪表板工具，您还可以使用这种组合来创建自己的应用程序仪表板。
- en: The Prometheus project supports client libraries in a wide variety of languages
    that make it easier to collect and expose metrics in its format. We will use some
    of these libraries to show you how to instrument our Python and Node.js examples.
    Before you dive in to directly using these libraries yourself, it is very worthwhile
    reading the documentation that the Prometheus project provides on how to write
    metric exporters, and their expected conventions for metric names. You can find
    this documentation at the project website: [https://prometheus.io/docs/instrumenting/writing_exporters/](https://prometheus.io/docs/instrumenting/writing_exporters/).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus项目支持多种语言的客户端库，使其更容易收集和公开指标。我们将使用其中一些库来向您展示如何为我们的Python和Node.js示例进行仪器化。在直接使用这些库之前，非常值得阅读Prometheus项目提供的有关如何编写指标导出器以及其对指标名称的预期约定的文档。您可以在项目网站找到这些文档：[https://prometheus.io/docs/instrumenting/writing_exporters/](https://prometheus.io/docs/instrumenting/writing_exporters/)。
- en: Flask metrics with Prometheus
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Prometheus的Flask指标
- en: 'You can find the library to expose metrics from Python at [https://github.com/prometheus/client_python](https://github.com/prometheus/client_python) and
    can install it using `pip` with this command:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/prometheus/client_python](https://github.com/prometheus/client_python)找到从Python公开指标的库，并可以使用以下命令使用`pip`进行安装：
- en: '[PRE37]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Depending on your setup, you may need to use `**sudo pip install prometheus_client**` to
    install the client with `pip`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的设置，您可能需要使用`**sudo pip install prometheus_client**`使用`pip`安装客户端。
- en: 'For our `flask` example, you can download the full sample code from [https://github.com/kubernetes-for-developers/kfd-flask](https://github.com/kubernetes-for-developers/kfd-flask)
    from branch 0.5.0\. The commands to get this updated sample are:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的`flask`示例，您可以从[https://github.com/kubernetes-for-developers/kfd-flask](https://github.com/kubernetes-for-developers/kfd-flask)的0.5.0分支下载完整的示例代码。获取此更新示例的命令如下：
- en: '[PRE38]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If you look within `exampleapp.py`, you can see the code where we use two metrics,
    a histogram and a counter, and use the flask framework to add in callbacks at
    the beginning of a request, and the end of a request, and capture that time difference:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看`exampleapp.py`，您可以看到我们使用两个指标的代码，即直方图和计数器，并使用flask框架在请求开始和请求结束时添加回调，并捕获该时间差：
- en: '[PRE39]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The library also includes a helper application to make it very easy to generate
    the metrics to be scraped by Prometheus:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 该库还包括一个辅助应用程序，使得生成Prometheus要抓取的指标非常容易：
- en: '[PRE40]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This code has been made into a container image, `quay.io/kubernetes-for-developers/flask:0.5.0`.
    With those additions in place, we only need to add the annotation to `flask-service`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码已制作成容器映像`quay.io/kubernetes-for-developers/flask:0.5.0`。有了这些添加，我们只需要将注释添加到`flask-service`：
- en: '[PRE41]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Once deployed with `kubectl apply -f deploy/` from the example's directory,
    the service will be backed by a single pod, and Prometheus will begin to pick
    it up as a target. If you use the `kubectl proxy` command, you can see the specific
    metric response that this generates. In our case, the pods name is `flask-6596b895b-nqqqz`,
    so the metrics can be easily queried at `http://localhost:8001/api/v1/proxy/namespaces/default/pods/flask-6596b895b-nqqqz/metrics`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 从示例目录中使用`kubectl apply -f deploy/`部署后，该服务将由单个pod支持，并且Prometheus将开始将其作为目标。如果您使用`kubectl
    proxy`命令，您可以查看此生成的特定指标响应。在我们的情况下，pod的名称是`flask-6596b895b-nqqqz`，因此可以轻松查询指标`http://localhost:8001/api/v1/proxy/namespaces/default/pods/flask-6596b895b-nqqqz/metrics`。
- en: 'A sample of those metrics is as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标的示例如下：
- en: '[PRE42]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: You can see the metrics named `flask_request_latency_seconds` and `flask_request_count`
    in this sample, and you can query for those same metrics in the Prometheus browser
    interface.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此示例中看到名为`flask_request_latency_seconds`和`flask_request_count`的指标，并且您可以在Prometheus浏览器界面中查询相同的指标。
- en: Node.js metrics with Prometheus
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Prometheus的Node.js指标
- en: 'JavaScript has similar client libraries to Python. In fact, it is even easier
    to instrument Node.js express applications with the use of `express-prom-bundle`,
    which in turn uses `prom-client`. You can install this library for your use with
    this command:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript具有与Python类似的客户端库。实际上，使用`express-prom-bundle`来为Node.js express应用程序提供仪表板甚至更容易，该库反过来使用`prom-client`。您可以使用以下命令安装此库以供您使用：
- en: '[PRE43]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You can then use it in your code. The following will set up a middleware for
    express:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以在您的代码中使用它。以下内容将为express设置一个中间件：
- en: '[PRE44]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'And then, you simply include the middleware, as you''re setting up this application:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您只需包含中间件，就像您正在设置此应用程序一样：
- en: '[PRE45]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The example code at [https://github.com/kubernetes-for-developers/kfd-nodejs](https://github.com/kubernetes-for-developers/kfd-nodejs)
    has these updates, and you can check out this code from branch 0.5.0 with the
    command:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kubernetes-for-developers/kfd-nodejs](https://github.com/kubernetes-for-developers/kfd-nodejs)上的示例代码已经更新，您可以使用以下命令从0.5.0分支检查此代码：'
- en: '[PRE46]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Like the Python code, the Node.js example includes updating the service with
    the annotation `prometheus.io/scrape: "true"`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '与Python代码一样，Node.js示例包括使用注释`prometheus.io/scrape: "true"`更新服务：'
- en: '[PRE47]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Service signals in Prometheus
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Prometheus中的服务信号
- en: You can tell the health and status of your service with three key metrics. It
    has become relatively common for service dashboards to instrument and build on
    these metrics as a baseline for understanding how your service is running.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过三个关键指标来了解您服务的健康和状态。服务仪表板通常会基于这些指标进行仪表化和构建，作为了解您的服务运行情况的基线，这已经相对普遍。
- en: 'These key metrics for a web-based service are:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络服务的关键指标是：
- en: Error rate
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误率
- en: Response time
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应时间
- en: Throughput
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吞吐量
- en: Error rate can be gathered by using the labels within the `http_request_duration_seconds_count`
    metric, which is included from `express-prom-bundle`. The query we can use in
    Prometheus. We can match on the format of the response code and count the increase
    in the number of 500 responses versus all responses.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 错误率可以通过使用`http_request_duration_seconds_count`指标中的标签来收集，该指标包含在`express-prom-bundle`中。我们可以在Prometheus中使用的查询。我们可以匹配响应代码的格式，并计算500个响应与所有响应的增加数量。
- en: 'The Prometheus query could be:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus查询可以是：
- en: '[PRE48]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: With little load on our own example services and probably no errors, this query
    isn't likely to return any data points, but you can use it as an example to explore
    building your own error response queries.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们自己的示例服务上几乎没有负载，可能没有错误，这个查询不太可能返回任何数据点，但你可以用它作为一个示例来探索构建你自己的错误响应查询。
- en: 'Response time is complex to measure and understand, especially with busy services.
    The reason that we typically include a histogram metric for the time it takes
    to process the request is to be able to view the distribution of those requests
    over time. Using a histogram, we can aggregate the requests across a window, and
    then look at the rate of those requests. In our preceding Python example, the
    `flask_request_latency_seconds` is a histogram, and each request gets label with
    where it was in the histogram bucket, the HTTP method used, and the URI endpoint.
    We can aggregate the rates of those requests using those labels, and look at the
    median, 95^(th), and 99^(th) percentile with the following Prometheus queries:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 响应时间很难测量和理解，特别是对于繁忙的服务。我们通常包括一个用于处理请求所需时间的直方图指标的原因是为了能够查看这些请求随时间的分布。使用直方图，我们可以在一个窗口内聚合请求，然后查看这些请求的速率。在我们之前的Python示例中，`flask_request_latency_seconds`是一个直方图，每个请求都带有它在直方图桶中的位置标签，使用的HTTP方法和URI端点。我们可以使用这些标签聚合这些请求的速率，并使用以下Prometheus查询查看中位数、95^(th)和99^(th)百分位数：
- en: 'Median:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '中位数:'
- en: '[PRE49]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '95^(th) percentile:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 95^(th)百分位数：
- en: '[PRE50]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '99^(th) percentile:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 99^(th)百分位数：
- en: '[PRE51]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Throughput is about measuring the total number of requests over a given timeframe,
    and can be derived directly from `flask_request_latency_seconds_count` and viewed
    against endpoint and method:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐量是关于在给定时间范围内测量请求总数的，可以直接从`flask_request_latency_seconds_count`中派生，并针对端点和方法进行查看：
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced Prometheus and showed how to install it, used
    it to capture metrics from your Kubernetes cluster, and showed how to install
    and use Grafana to provide dashboards, using the metrics captured and temporarily
    stored in Prometheus. We then looked at how you can expose custom metrics from
    your own code and leverage Prometheus to capture them, along with a few examples
    of metrics you might be interested in tracking, such as error rate, response time,
    and throughput.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了Prometheus，并展示了如何安装它，使用它从您的Kubernetes集群中捕获指标，并展示了如何安装和使用Grafana来提供仪表板，使用在Prometheus中临时存储的指标。然后，我们看了一下如何从您自己的代码中暴露自定义指标，并利用Prometheus来捕获它们，以及一些您可能有兴趣跟踪的指标的示例，如错误率、响应时间和吞吐量。
- en: In the next chapter, we continue to look into observability for our applications
    with tools to help us capture logs and traces.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续使用工具来帮助我们捕获日志和跟踪，以便观察我们的应用程序。
