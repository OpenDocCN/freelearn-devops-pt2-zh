- en: Monitoring, Logging, and Troubleshooting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控、日志记录和故障排除
- en: In [Chapter 2](4a72bd61-c0d6-430b-b146-15a1ca3391da.xhtml), *Creating Kubernetes
    Clusters*, you learned how to create Kubernetes clusters in different environments,
    experimented with different tools, and created a couple of clusters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](4a72bd61-c0d6-430b-b146-15a1ca3391da.xhtml)中，*创建Kubernetes集群*，您学习了如何在不同环境中创建Kubernetes集群，尝试了不同的工具，并创建了一些集群。
- en: Creating a Kubernetes cluster is just the beginning of the story. Once the cluster
    is up and running, you need to make sure that it is operational, all the necessary
    components are in place and properly configured, and enough resources are deployed
    to satisfy the requirements. Responding to failures, debugging, and troubleshooting
    is a major part of managing any complicated system, and Kubernetes is no exception.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Kubernetes集群只是故事的开始。一旦集群运行起来，您需要确保它是可操作的，所有必要的组件都齐全并正确配置，并且部署了足够的资源来满足要求。响应故障、调试和故障排除是管理任何复杂系统的重要部分，Kubernetes也不例外。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Monitoring with Heapster
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Heapster进行监控
- en: Performance analytics with Kubernetes dashboard
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes仪表板进行性能分析
- en: Central logging
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中央日志记录
- en: Detecting problems at the node level
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在节点级别检测问题
- en: Troubleshooting scenarios
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障排除场景
- en: Using Prometheus
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Prometheus
- en: At the end of this chapter, you will have a solid understanding of the various
    options available to monitor Kubernetes clusters, how to access logs, and how
    to analyze them. You will be able to look at a healthy Kubernetes cluster and
    verify that everything is OK. You will also be able to look at an unhealthy Kubernetes
    cluster and methodically diagnose it, locate the problems, and address them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将对监视Kubernetes集群的各种选项有扎实的了解，知道如何访问日志以及如何分析它们。您将能够查看健康的Kubernetes集群并验证一切正常。您还将能够查看不健康的Kubernetes集群，并系统地诊断它，定位问题并解决它们。
- en: Monitoring Kubernetes with Heapster
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Heapster监控Kubernetes
- en: 'Heapster is a Kubernetes project that provides a robust monitoring solution
    for Kubernetes clusters. It runs as a pod (of course), so it can be managed by
    Kubernetes itself. Heapster supports Kubernetes and CoreOS clusters. It has a
    very modular and flexible design. Heapster collects both operational metrics and
    events from every node in the cluster, stores them in a persistent backend (with
    a well-defined schema), and allows visualization and programmatic access. Heapster
    can be configured to use different backends (or sinks, in Heapster''s parlance)
    and their corresponding visualization frontends. The most common combination is
    InfluxDB as the backend and Grafana as the frontend. The Google Cloud Platform
    integrates Heapster with the Google monitoring service. There are many other less
    common backends, as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Heapster是一个为Kubernetes集群提供强大监控解决方案的Kubernetes项目。它作为一个pod（当然）运行，因此可以由Kubernetes本身管理。Heapster支持Kubernetes和CoreOS集群。它具有非常模块化和灵活的设计。Heapster从集群中的每个节点收集操作指标和事件，将它们存储在持久后端（具有明确定义的模式）中，并允许可视化和编程访问。Heapster可以配置为使用不同的后端（或在Heapster术语中称为sinks）及其相应的可视化前端。最常见的组合是InfluxDB作为后端，Grafana作为前端。谷歌云平台将Heapster与谷歌监控服务集成。还有许多其他不太常见的后端，如下所示：
- en: Log
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志
- en: Google Cloud monitoring
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌云监控
- en: Google Cloud logging
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌云日志
- en: Hawkular-Metrics (metrics only)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hawkular-Metrics（仅指标）
- en: OpenTSDB
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenTSDB
- en: Monasca (metrics only)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monasca（仅指标）
- en: Kafka (metrics only)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka（仅指标）
- en: Riemann (metrics only)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riemann（仅指标）
- en: Elasticsearch
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: 'You can use multiple backends by specifying sinks on the command line:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在命令行上指定sinks来使用多个后端：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: cAdvisor
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cAdvisor
- en: cAdvisor is part of the kubelet, which runs on every node. It collects information
    about the CPU/cores' usage, memory, network, and filesystems of each container.
    It provides a basic UI on port `4194`, but, most importantly for Heapster, it
    provides all this information through the Kubelet. Heapster records the information
    collected by cAdvisor on each node and stores it in its backend for analysis and
    visualization.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: cAdvisor是kubelet的一部分，它在每个节点上运行。它收集有关每个容器的CPU/核心使用情况、内存、网络和文件系统的信息。它在端口`4194`上提供基本UI，但是对于Heapster来说，最重要的是它通过Kubelet提供了所有这些信息。Heapster记录了由cAdvisor在每个节点上收集的信息，并将其存储在其后端以进行分析和可视化。
- en: The cAdvisor UI is useful if you want to quickly verify that a particular node
    is set up correctly, for example, while creating a new cluster when Heapster is
    not hooked up yet.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想快速验证特定节点是否设置正确，例如，在Heapster尚未连接时创建新集群，那么cAdvisor UI非常有用。
- en: 'Here is what it looks like:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的样子：
- en: '![](Images/994b310a-76bb-4d78-baf2-e16507c117e8.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/994b310a-76bb-4d78-baf2-e16507c117e8.png)'
- en: Installing Heapster
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Heapster
- en: 'Heapster components may or may not be installed in your Kubernetes cluster.
    If Heapster is not installed, you can install it with a few simple commands. First,
    let''s clone the Heapster repo:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Heapster组件可能已安装或尚未安装在您的Kubernetes集群中。如果Heapster尚未安装，您可以使用几个简单的命令进行安装。首先，让我们克隆Heapster存储库：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In earlier versions of Kubernetes, Heapster exposed the services as `NodePort`
    by default. Now, they are exposed by default as `ClusterIP`, which means that
    they are available only inside the cluster. To make them available locally, I
    added type: `NodePort` to the spec of each service in `deploy/kube-config/influxdb`.
    For example, for `deploy/kube-config/influxdb/influxdb.yaml`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '在早期版本的Kubernetes中，Heapster默认将服务公开为`NodePort`。现在，默认情况下，它们被公开为`ClusterIP`，这意味着它们仅在集群内可用。为了使它们在本地可用，我在`deploy/kube-config/influxdb`中的每个服务的规范中添加了type:
    `NodePort`。例如，对于`deploy/kube-config/influxdb/influxdb.yaml`：'
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'I made a similar change to `deploy/kube-config/influxdb/grafana.yaml`, which
    has `+ type: NodePort` this line commented out, so I just uncommented it. Now,
    we can actually install InfluxDB and Grafana:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '我对`deploy/kube-config/influxdb/grafana.yaml`进行了类似的更改，其中`+ type: NodePort`这一行被注释掉了，所以我只是取消了注释。现在，我们实际上可以安装InfluxDB和Grafana：'
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should see the following output:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: InfluxDB backend
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: InfluxDB后端
- en: InfluxDB is a modern and robust distributed time-series database. It is very
    well-suited and used broadly for centralized metrics and logging. It is also the
    preferred Heapster backend (outside the Google Cloud Platform). The only thing
    is InfluxDB clustering; high availability is part of enterprise offering.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: InfluxDB是一个现代而强大的分布式时间序列数据库。它非常适合用于集中式指标和日志记录，并被广泛使用。它也是首选的Heapster后端（在谷歌云平台之外）。唯一的问题是InfluxDB集群；高可用性是企业提供的一部分。
- en: The storage schema
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储模式
- en: 'The InfluxDB storage schema defines the information that Heapster stores in
    InfluxDB, and it is available for querying and graphing later. The metrics are
    divided into multiple categories, named measurements. You can treat and query
    each metric separately, or you can query a whole category as one measurement and
    receive the individual metrics as fields. The naming convention is `<category>/<metrics
    name>` (except for uptime, which has a single metric). If you have an SQL background,
    you can think of measurements as tables. Each metric is stored per container.
    Each metric is labeled with the following information:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: InfluxDB存储模式定义了Heapster在InfluxDB中存储的信息，并且可以在以后进行查询和绘图。指标分为多个类别，称为测量。您可以单独处理和查询每个指标，或者您可以将整个类别作为一个测量进行查询，并将单独的指标作为字段接收。命名约定是`<category>/<metrics
    name>`（除了正常运行时间，它只有一个指标）。如果您具有SQL背景，可以将测量视为表。每个指标都存储在每个容器中。每个指标都带有以下信息标签：
- en: '`pod_id`: A unique ID of a pod'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pod_id`: 一个pod的唯一ID'
- en: '`pod_name`: A user-provided name of a pod'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pod_name`: pod的用户提供的名称'
- en: '`pod_namespace`: The namespace of a pod'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pod_namespace`: pod的命名空间'
- en: '`container_base_image`: A base image for the container'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`container_base_image`: 容器的基础镜像'
- en: '`container_name`: A user-provided name of the container or full `cgroup` name
    for system containers'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`container_name`: 容器的用户提供的名称或系统容器的完整`cgroup`名称'
- en: '`host_id`: A cloud-provider-specified or user-specified identifier of a node'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host_id`: 云服务提供商指定或用户指定的节点标识符'
- en: '`hostname`: The hostname where the container ran'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hostname`: 容器运行的主机名'
- en: '`labels`: The comma-separated list of user-provided labels; format is `key:value`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`: 用户提供的标签的逗号分隔列表；格式为`key:value`'
- en: '`namespace_id`: The UID of the namespace of a pod'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`namespace_id`: pod命名空间的UID'
- en: '`resource_id`: A unique identifier used to differentiate multiple metrics of
    the same type, for example, FS partitions under filesystem/usage'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resource_id`: 用于区分同一类型多个指标的唯一标识符，例如，文件系统/使用下的FS分区'
- en: Here are all the metrics grouped by category, as you can see, it is quite extensive.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是按类别分组的所有指标，可以看到，它非常广泛。
- en: CPU
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU
- en: 'The CPU metrics are:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: CPU指标包括：
- en: '`cpu/limit`: CPU hard limit in millicores'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpu/limit`: 毫核的CPU硬限制'
- en: '`cpu/node_capacity`: CPU capacity of a node'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpu/node_capacity`: 节点的CPU容量'
- en: '`cpu/node_allocatable`: CPU allocatable of a node'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpu/node_allocatable`: 节点的可分配CPU'
- en: '`cpu/node_reservation`: Share of CPU that is reserved on the node allocatable'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpu/node_reservation`: 节点可分配的CPU保留份额'
- en: '`cpu/node_utilization`: CPU utilization as a share of node allocatable'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpu/node_utilization`: CPU利用率占节点可分配资源的份额'
- en: '`cpu/request`: CPU request (the guaranteed amount of resources) in millicores'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpu/request`: CPU请求（资源的保证数量）（毫核）'
- en: '`cpu/usage`: Cumulative CPU usage on all cores'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpu/usage`: 所有核心的累积CPU使用率'
- en: '`cpu/usage_rate`: CPU usage on all cores in millicores'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cpu/usage_rate`: 所有核心的CPU使用率（毫核）'
- en: Filesystem
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件系统
- en: 'The Filesystem metrics are:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统指标包括：
- en: '`filesystem/usage`: The total number of bytes consumed on a filesystem'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filesystem/usage`: 文件系统上消耗的总字节数'
- en: '`filesystem/limit`: The total size of the filesystem in bytes'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filesystem/limit`: 文件系统的总大小（字节）'
- en: '`filesystem/available`: The number of available bytes remaining in the filesystem'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filesystem/available`: 文件系统中剩余的可用字节数'
- en: Memory
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存
- en: 'The memory metrics are:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 内存指标包括：
- en: '`memory/limit`: Memory hard limit in bytes'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/limit`: 内存的硬限制（字节）'
- en: '`memory/major_page_faults`: The number of major page faults'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/major_page_faults`: 主要页面错误的数量'
- en: '`memory/major_page_faults_rate`: The number of major page faults per second'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/major_page_faults_rate`: 每秒的主要页面错误数'
- en: '`memory/node_capacity`: Memory capacity of a node'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/node_capacity`: 节点的内存容量'
- en: '`memory/node_allocatable`: Memory allocatable of a node'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/node_allocatable`: 节点的可分配内存'
- en: '`memory/node_reservation`: Share of memory that is reserved on the node allocatable'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/node_reservation`: 节点可分配内存上保留的份额'
- en: '`memory/node_utilization`: Memory utilization as a share of memory allocatable'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/node_utilization`: 内存利用率占内存可分配资源的份额'
- en: '`memory/page_faults`: The number of page faults'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/page_faults`: 页面错误的数量'
- en: '`memory/page_faults_rate`: The number of page faults per second'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/page_faults_rate`: 每秒的页面错误数'
- en: '`memory/request`: Memory request (the guaranteed amount of resources) in bytes'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/request`: 内存请求（资源的保证数量）（字节）'
- en: '`memory/usage`: Total memory usage'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/usage`: 总内存使用量'
- en: '`memory/working_set`: Total working set usage; working set is the memory being
    used and is not easily dropped by the kernel'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory/working_set`: 总工作集使用量；工作集是内存的使用部分，不容易被内核释放'
- en: Network
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络
- en: 'The network metrics are:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 网络指标包括：
- en: '`network/rx`: Cumulative number of bytes received over the network'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network/rx`: 累积接收的网络字节数'
- en: '`network/rx_errors`: Cumulative number of errors while receiving over'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network/rx_errors`: 接收时的累积错误数'
- en: the network
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 网络
- en: '`network/rx_errors_rate`: The number of errors per second while receiving over
    the network'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network/rx_errors_rate`：在网络接收过程中每秒发生的错误次数'
- en: '`network/rx_rate`: The number of bytes received over the network per second'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network/rx_rate`：每秒通过网络接收的字节数'
- en: '`network/tx`: Cumulative number of bytes sent over the network'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network/tx`：通过网络发送的累积字节数'
- en: '`network/tx_errors`: Cumulative number of errors while sending over the network'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network/tx_errors`：在网络发送过程中的累积错误次数'
- en: '`network/tx_errors_rate`: The number of errors while sending over the network'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network/tx_errors_rate`：在网络发送过程中发生的错误次数'
- en: '`network/tx_rate`: The number of bytes sent over the network per second'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`network/tx_rate`：每秒通过网络发送的字节数'
- en: Uptime
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正常运行时间
- en: Uptime is the number of milliseconds since the container was started.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正常运行时间是容器启动以来的毫秒数。
- en: 'You can work with InfluxDB directly if you''re familiar with it. You can either
    connect to it using its own API or use its web interface. Type the following command
    to find its port and endpoint:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉InfluxDB，可以直接使用它。您可以使用其自己的API连接到它，也可以使用其Web界面。键入以下命令以查找其端口和端点：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, you can browse the InfluxDB web interface using the HTTP port. You''ll
    need to configure it to point to the API port. The `Username` and `Password` are
    `root` and `root` by default:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用HTTP端口浏览InfluxDB Web界面。您需要将其配置为指向API端口。默认情况下，`用户名`和`密码`为`root`和`root`：
- en: '![](Images/9ae3efb3-3192-44a7-be2b-3b9174c1d30c.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/9ae3efb3-3192-44a7-be2b-3b9174c1d30c.png)'
- en: Once you're set up, you can select what database to use (see the top-right corner).
    The Kubernetes database is named `k8s`. You can now query the metrics using the
    InfluxDB query language.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成后，您可以选择要使用的数据库（请参阅右上角）。Kubernetes数据库的名称为`k8s`。现在，您可以使用InfluxDB查询语言查询指标。
- en: Grafana visualization
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Grafana可视化
- en: 'Grafana runs in its own container and serves a sophisticated dashboard that
    works well with InfluxDB as a data source. To locate the port, type the following
    command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana在其自己的容器中运行，并提供一个与InfluxDB作为数据源配合良好的复杂仪表板。要找到端口，请键入以下命令：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, you can access the Grafana web interface on that port. The first thing
    you need to do is set up the data source to point to the InfluxDB backend:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以在该端口上访问Grafana Web界面。您需要做的第一件事是设置数据源指向InfluxDB后端：
- en: '![](Images/79a74148-198b-4a7a-ac70-86144ee6ca9f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/79a74148-198b-4a7a-ac70-86144ee6ca9f.png)'
- en: Make sure to test the connection and then go explore the various options in
    the dashboards. There are several default dashboards, but you should be able to
    customize them to your preferences. Grafana is designed to let you adapt it to
    your needs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 确保测试连接，然后去探索仪表板中的各种选项。有几个默认的仪表板，但您应该能够根据自己的喜好进行自定义。Grafana旨在让您根据自己的需求进行调整。
- en: Discovery and load balancing
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现和负载平衡
- en: 'The discovery and load balancing category is often where you start from. Services
    are the public interface to your Kubernetes cluster. Serious problems will affect
    your services, which will affect your users:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 发现和负载平衡类别通常是您开始的地方。服务是您的Kubernetes集群的公共接口。严重的问题将影响您的服务，从而影响您的用户：
- en: '![](Images/ac735902-2574-44a9-9489-ae1b94edd493.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ac735902-2574-44a9-9489-ae1b94edd493.png)'
- en: When you drill down by clicking on a service, you get some information about
    the service (most important is the label selector) and a pods view.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当您通过单击服务进行深入了解时，您将获得有关服务的一些信息（最重要的是标签选择器）和一个Pods视图。
- en: Performance analysis with the dashboard
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用仪表板进行性能分析
- en: 'My favorite tool by far, when I just want to know what''s going on in the cluster,
    is the Kubernetes dashboard. There are a couple of reasons for this, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我最喜欢的工具，当我只想知道集群中发生了什么时，就是Kubernetes仪表板。以下是几个原因：
- en: It is built-in (always in sync and tested with Kubernetes)
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是内置的（始终与Kubernetes同步和测试）
- en: It's fast
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它很快
- en: It provides an intuitive drill-down interface, from the cluster level all the
    way down to individual container
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一个直观的深入界面，从集群级别一直到单个容器
- en: It doesn't require any customization or configuration
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不需要任何定制或配置
- en: Although Heapster, InfluxDB, and Grafana are better for customized and heavy-duty
    views and queries, the Kubernetes dashboard's predefined views can probably answer
    all your questions 80-90% of the time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Heapster、InfluxDB和Grafana更适合定制和重型视图和查询，但Kubernetes仪表板的预定义视图可能能够在80-90%的时间内回答所有你的问题。
- en: You can also deploy applications and create any Kubernetes resource using the
    dashboard by uploading the proper YAML or JSON file, but I will not cover this
    because it is an anti-pattern for manageable infrastructure. It may be useful
    when playing around with a test cluster, but for actually modifying the state
    of the cluster, I prefer the command line. Your mileage may vary.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过上传适当的YAML或JSON文件，使用仪表板部署应用程序并创建任何Kubernetes资源，但我不会涉及这个，因为这对于可管理的基础设施来说是一种反模式。在玩测试集群时可能有用，但对于实际修改集群状态，我更喜欢使用命令行。您的情况可能有所不同。
- en: 'Let''s find the port first:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先找到端口：
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Top-level view
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顶层视图
- en: The dashboard is organized with a hierarchical view on the left (it can be hidden
    by clicking the hamburger menu) and dynamic, context-based content on the right.
    You can drill down into the hierarchical view to get deeper into the information
    that's relevant.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 仪表板以左侧的分层视图组织（可以通过单击汉堡菜单隐藏），右侧是动态的、基于上下文的内容。您可以深入分层视图，以深入了解相关信息。
- en: 'There are several top-level categories:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个顶层类别：
- en: Cluster
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群
- en: Overview
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述
- en: Workloads
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作负载
- en: Discovery and load balancing
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现和负载平衡
- en: Config and storage
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置和存储
- en: You can also filter everything by a particular namespace or choose all namespaces.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过特定命名空间过滤所有内容或选择所有命名空间。
- en: Cluster
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群
- en: 'The Cluster view has five sections: Namespaces, Nodes, PersistentVolumes, Roles,
    and Storage Classes. It is mostly about observing the physical resources of the
    cluster:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 集群视图有五个部分：命名空间、节点、持久卷、角色和存储类。它主要是观察集群的物理资源：
- en: '![](Images/3dbe8557-cce3-4e5b-9922-e7ed73042915.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/3dbe8557-cce3-4e5b-9922-e7ed73042915.png)'
- en: 'You get, in a glance, a lot of information: CPU and memory usage of all the
    nodes, what namespaces are available, their Status, and Age. For each node, you
    can see its Age, Labels, and if it''s ready or not. If there were persistent volumes
    and roles, you would see them as well, then the storage classes (just host path
    in this case).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一眼就可以获得大量信息：所有节点的CPU和内存使用情况，可用的命名空间，它们的状态和年龄。对于每个节点，您可以看到它的年龄、标签，以及它是否准备就绪。如果有持久卷和角色，您也会看到它们，然后是存储类（在这种情况下只是主机路径）。
- en: 'If we drill down the nodes and click on the minikube node itself, we get a
    detailed screen of information about that node and the allocated resources in
    a nice pie chart. This is critical for dealing with performance issues. If a node
    doesn''t have enough resources, then it might not be able to satisfy the needs
    of its pods:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们深入节点并点击minikube节点本身，我们会得到有关该节点和分配资源的详细信息，以一个漂亮的饼图显示。这对处理性能问题至关重要。如果一个节点没有足够的资源，那么它可能无法满足其pod的需求：
- en: '![](Images/8143ad1c-4616-4b6c-b176-b8f28598c4cd.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/8143ad1c-4616-4b6c-b176-b8f28598c4cd.png)'
- en: 'If you scroll down, you''ll see even more interesting information. The Conditions
    pane is where it''s at. You get a great, concise view of memory and disk pressure
    at the individual node level:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您向下滚动，您会看到更多有趣的信息。条件窗格是最重要的地方。您可以清晰、简洁地查看每个节点的内存和磁盘压力：
- en: '![](Images/4b7585a5-ab73-4762-9170-f6e87759ad63.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/4b7585a5-ab73-4762-9170-f6e87759ad63.png)'
- en: There are also Pods and Events panes. We'll talk about pods in the next section.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 还有Pods和Events窗格。我们将在下一节讨论pod。
- en: Workloads
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作负载
- en: 'The Workloads category is the main one. It organizes many types of Kubernetes
    resources, such as CronJobs, Daemon Sets, Deployments, Jobs, Pods, Replica Sets,
    Replication Controllers, and Stateful Sets. You can drill down along any of these
    dimensions. Here is the top-level Workloads view for the default namespace that
    currently has only the echo service deployed. You can see the Deployments, Replica
    Sets, and Pods:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载类别是主要类别。它组织了许多类型的Kubernetes资源，如CronJobs、Daemon Sets、Deployments、Jobs、Pods、Replica
    Sets、Replication Controllers和Stateful Sets。您可以沿着任何这些维度进行深入。这是默认命名空间的顶级工作负载视图，目前只部署了echo服务。您可以看到部署、副本集和pod：
- en: '![](Images/0a80dca8-6665-4f68-ad1f-2e01735b4c88.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/0a80dca8-6665-4f68-ad1f-2e01735b4c88.png)'
- en: 'Let''s switch to all namespaces and dive into the Pods subcategory. This is
    a very useful view. In each row, you can tell if the pod is running or not, how
    many times it restarted, its IP, and the CPU and memory usage histories are even
    embedded as nice little graphs:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们切换到所有命名空间并深入研究Pods子类别。这是一个非常有用的视图。在每一行中，您可以看出pod是否正在运行，它重新启动了多少次，它的IP，甚至嵌入了CPU和内存使用历史记录作为漂亮的小图形：
- en: '![](Images/da3368c6-80dc-4056-8fc0-3b186dc40164.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/da3368c6-80dc-4056-8fc0-3b186dc40164.png)'
- en: 'You can also view the Logs for any pod by clicking the text symbol (second
    from the right). Let''s check the Logs of the InfluxDB pod. It looks like everything
    is in order and Heapster is successfully writing to it:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以通过点击文本符号（从右边数第二个）查看任何pod的日志。让我们检查InfluxDB pod的日志。看起来一切都井井有条，Heapster成功地向其写入：
- en: '![](Images/0358b581-6b6a-46f9-aa3e-a7995142fc90.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/0358b581-6b6a-46f9-aa3e-a7995142fc90.png)'
- en: 'There is one more level of detail that we haven''t explored yet. We can go
    down to the container level. Let''s click on the kubedns pod. We get the following
    screen, which shows the individual containers and their `run` command; we can
    also view their logs:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个我们尚未探讨的更详细的层次。我们可以进入容器级别。让我们点击kubedns pod。我们得到以下屏幕，显示了各个容器及其`run`命令；我们还可以查看它们的日志：
- en: '![](Images/eb484adb-241d-444b-bcf7-ac80ffb637cd.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/eb484adb-241d-444b-bcf7-ac80ffb637cd.png)'
- en: Adding central logging
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加中央日志记录
- en: Central logging or cluster-level logging is a fundamental requirement for any
    cluster with more than a couple of nodes, pods, or containers. First, it is impractical
    to view the logs of each pod or container independently. You can't get a global
    picture of the system, and there will be just too many messages to sift through.
    You need a solution that aggregates the log messages and lets you slice and dice
    them easily. The second reason is that containers are ephemeral. Problematic pods
    will often just die, and their replication controller or replica set will just
    start a new instance, losing all the important log info. By logging to a central
    logging service, you preserve this critical troubleshooting information.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 中央日志记录或集群级日志记录是任何具有多个节点、pod或容器的集群的基本要求。首先，单独查看每个pod或容器的日志是不切实际的。您无法获得系统的全局图片，而且将有太多的消息需要筛选。您需要一个聚合日志消息并让您轻松地切片和切块的解决方案。第二个原因是容器是短暂的。有问题的pod通常会死掉，它们的复制控制器或副本集将启动一个新实例，丢失所有重要的日志信息。通过记录到中央日志记录服务，您可以保留这些关键的故障排除信息。
- en: Planning central logging
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划中央日志记录
- en: Conceptually, central logging is very simple. On each node, you run a dedicated
    agent that intercepts all log messages from all the pods and containers on the
    node, and sends them, along with enough metadata, to a central repository where
    they are stored safely.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上，中央日志非常简单。在每个节点上，您运行一个专用代理，拦截节点上所有pod和容器的所有日志消息，并将它们连同足够的元数据发送到一个中央存储库，其中它们被安全地存储。
- en: As usual, if you run on the Google platform, then GKE's got you covered, and
    there is a Google central-logging service integrated nicely. For other platforms,
    a popular solution is fluentd, Elasticsearch, and Kibana. There is an official
    add-on to set up the proper services for each component. The `fluentd-elasticsearch`
    add-on is at [http://bit.ly/2f6MF5b](http://bit.ly/2f6MF5b).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，如果您在谷歌平台上运行，那么GKE会为您提供支持，并且有一个谷歌集中日志服务集成得很好。对于其他平台，一个流行的解决方案是fluentd、Elasticsearch和Kibana。有一个官方的附加组件来为每个组件设置适当的服务。`fluentd-elasticsearch`附加组件位于[http://bit.ly/2f6MF5b](http://bit.ly/2f6MF5b)。
- en: It is installed as a set of services for Elasticsearch and Kibana, and the fluentd
    agent is installed on each node.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 它被安装为Elasticsearch和Kibana的一组服务，并且在每个节点上安装了fluentd代理。
- en: Fluentd
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fluentd
- en: 'Fluentd is a unified logging layer that sits between arbitrary data sources
    and arbitrary data sinks and makes sure that log messages can stream from A to
    B. Kubernetes comes with an add-on that has a Docker image that deploys the fluentd
    agent, which knows how to read various logs that are relevant to Kubernetes, such
    as `Docker` logs, `etcd` logs, and `Kube` logs. It also adds labels to each log
    message to make it easy for users to filter later by label. Here is a snippet
    from the `fluentd-es-configmap.yaml` file:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd是一个统一的日志记录层，位于任意数据源和任意数据接收器之间，并确保日志消息可以从A流向B。Kubernetes带有一个附加组件，其中有一个部署fluentd代理的Docker镜像，它知道如何读取与Kubernetes相关的各种日志，如`Docker`日志、`etcd`日志和`Kube`日志。它还为每条日志消息添加标签，以便用户以后可以轻松地按标签进行过滤。这是`fluentd-es-configmap.yaml`文件的一部分：
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Elasticsearch
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: 'Elasticsearch is a great document store and full-text search engine. It is
    a favorite in the enterprise because it is very fast, reliable, and scalable.
    It is used in the Kubernetes central logging add-on as a Docker image, and it
    is deployed as a service. Note that a fully-fledged production cluster of Elasticsearch
    (which will be deployed on a Kubernetes cluster) requires its own master, client,
    and data nodes. For large-scale and highly-available Kubernetes clusters, the
    central logging itself will be clustered. Elasticsearch can use self-discovery.
    Here is an enterprise grade solution: [https://github.com/pires/kubernetes-elasticsearch-cluster](https://github.com/pires/kubernetes-elasticsearch-cluster).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch是一个很棒的文档存储和全文搜索引擎。它在企业中很受欢迎，因为它非常快速、可靠和可扩展。它作为一个Docker镜像在Kubernetes中央日志附加组件中使用，并且部署为一个服务。请注意，一个完全成熟的Elasticsearch生产集群（将部署在Kubernetes集群上）需要自己的主节点、客户端节点和数据节点。对于大规模和高可用的Kubernetes集群，中央日志本身将被集群化。Elasticsearch可以使用自我发现。这是一个企业级的解决方案：[https://github.com/pires/kubernetes-elasticsearch-cluster](https://github.com/pires/kubernetes-elasticsearch-cluster)。
- en: Kibana
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kibana
- en: Kibana is Elasticsearch's partner in crime. It is used to visualize and interact
    with the data stored and indexed by Elasticsearch. It is also installed as a service
    by the add-on. Here is the Kibana Dockerfile template ([http://bit.ly/2lwmtpc](http://bit.ly/2lwmtpc)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana是Elasticsearch的搭档。它用于可视化和与Elasticsearch存储和索引的数据进行交互。它也作为一个服务被附加组件安装。这是Kibana的Dockerfile模板([http://bit.ly/2lwmtpc](http://bit.ly/2lwmtpc))。
- en: Detecting node problems
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测节点问题
- en: 'In Kubernetes'' conceptual model, the unit of work is the pod. However, pods
    are scheduled on nodes. When it comes to monitoring and reliability, the nodes
    are what require the most attention, because Kubernetes itself (the scheduler
    and replication controllers) takes care of the pods. Nodes can suffer from a variety
    of problems that Kubernetes is unaware of. As a result, it will keep scheduling
    pods to the bad nodes and the pods might fail to function properly. Here are some
    of the problems that nodes may suffer while still appearing functional:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes的概念模型中，工作单位是pod。但是，pod被调度到节点上。在监控和可靠性方面，节点是最需要关注的，因为Kubernetes本身（调度器和复制控制器）负责pod。节点可能遭受各种问题，而Kubernetes并不知晓。因此，它将继续将pod调度到有问题的节点上，而pod可能无法正常运行。以下是节点可能遭受的一些问题，尽管看起来是正常的：
- en: Bad CPU
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU问题
- en: Bad memory
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存问题
- en: Bad disk
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘问题
- en: Kernel deadlock
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核死锁
- en: Corrupt filesystem
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损坏的文件系统
- en: Problems with the Docker daemon
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker守护进程问题
- en: The kubelet and cAdvisor don't detect these issues, another solution is needed.
    Enter the node problem detector.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: kubelet和cAdvisor无法检测到这些问题，需要另一个解决方案。进入节点问题检测器。
- en: Node problem detector
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点问题检测器
- en: The node problem detector is a pod that runs on every node. It needs to solve
    a difficult problem. It needs to detect various problems across different environments,
    different hardware, and different OSes. It needs to be reliable enough not to
    be affected itself (otherwise, it can't report the problem), and it needs to have
    relatively-low overhead to avoid spamming the master. In addition, it needs to
    run on every node. Kubernetes recently received a new capability named DaemonSet
    that addresses that last concern.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 节点问题检测器是在每个节点上运行的一个pod。它需要解决一个困难的问题。它需要检测不同环境、不同硬件和不同操作系统上的各种问题。它需要足够可靠，不受影响（否则，它无法报告问题），并且需要具有相对较低的开销，以避免向主节点发送大量信息。此外，它需要在每个节点上运行。Kubernetes最近收到了一个名为DaemonSet的新功能，解决了最后一个问题。
- en: The source code is at [https://github.com/kubernetes/node-problem-detector](https://github.com/kubernetes/node-problem-detector).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码位于[https://github.com/kubernetes/node-problem-detector](https://github.com/kubernetes/node-problem-detector)。
- en: DaemonSet
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 守护进程集
- en: DaemonSet is a pod for every node. Once you define DaemonSet, every node that's
    added to the cluster automatically gets a pod. If that pod dies, Kubernetes will
    start another instance of that pod on that node. Think about it as a fancy replication
    controller with 1:1 node-pod affinity. Node problem detector is defined as a DaemonSet,
    which is a perfect match for its requirements. It is possible to use affinity,
    anti-affinity, and taints to have more fine-grained control over DaemonSet scheduling.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet是每个节点的一个pod。一旦定义了DaemonSet，集群中添加的每个节点都会自动获得一个pod。如果该pod死掉，Kubernetes将在该节点上启动该pod的另一个实例。可以将其视为带有1:1节点-
    pod亲和性的复制控制器。节点问题检测器被定义为一个DaemonSet，这与其要求完全匹配。可以使用亲和性、反亲和性和污点来更精细地控制DaemonSet的调度。
- en: Problem daemons
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题守护进程
- en: The problem with node problem detector (pun intended) is that there are too
    many problems which it needs to handle. Trying to cram all of them into a single
    codebase can lead to a complex, bloated, and never-stabilizing codebase. The design
    of the node problem detector calls for separation of the core functionality of
    reporting node problems to the master from the specific problem detection. The
    reporting API is based on generic conditions and events. The problem detection
    should be done by separate problem daemons (each in its own container). This way,
    it is possible to add and evolve new problem detectors without impacting the core
    node problem detector. In addition, the control plane may have a remedy controller
    that can resolve some node problems automatically, therefore implementing self-healing.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 节点问题检测器的问题（双关语）在于它需要处理太多问题。试图将所有这些问题都塞进一个代码库中会导致一个复杂、臃肿且永远不稳定的代码库。节点问题检测器的设计要求将报告节点问题的核心功能与特定问题检测分离开来。报告API基于通用条件和事件。问题检测应该由单独的问题守护程序（每个都在自己的容器中）来完成。这样，就可以添加和演进新的问题检测器，而不会影响核心节点问题检测器。此外，控制平面可能会有一个补救控制器，可以自动解决一些节点问题，从而实现自愈。
- en: At this stage (Kubernetes 1.10), problem daemons are baked into the node problem
    detector binary, and they execute as Goroutines, so you don't get the benefits
    of the loosely-coupled design just yet.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段（Kubernetes 1.10），问题守护程序已经嵌入到节点问题检测器二进制文件中，并且它们作为Goroutines执行，因此您还没有获得松耦合设计的好处。
- en: In this section, we covered the important topic of node problems, which can
    get in the way of successful scheduling of workloads, and how the node problem
    detector can help. In the next section, we'll talk about various failure scenarios
    and how to troubleshoot them using Heapster, central logging, the Kubernetes dashboard,
    and node problem detector.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们涵盖了节点问题的重要主题，这可能会妨碍工作负载的成功调度，以及节点问题检测器如何帮助解决这些问题。在下一节中，我们将讨论各种故障场景以及如何使用Heapster、中央日志、Kubernetes仪表板和节点问题检测器进行故障排除。
- en: Troubleshooting scenarios
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除场景
- en: There are so many things that can go wrong in a large Kubernetes cluster, and
    they will, this is expected. You can employ best practices and minimize some of
    them (mostly human errors) using stricter processes. However, some issues such
    as hardware failures and networking issues can't be totally avoided. Even human
    errors should not always be minimized if it means slower development time. In
    this section, we'll discuss various categories of failures, how to detect them,
    how to evaluate their impact, and consider the proper response.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个大型的Kubernetes集群中，有很多事情可能会出错，而且它们确实会出错，这是可以预料的。您可以采用最佳实践并最小化其中一些问题（主要是人为错误），通过严格的流程来减少一些问题。然而，一些问题，比如硬件故障和网络问题是无法完全避免的。即使是人为错误，如果这意味着开发时间变慢，也不应该总是被最小化。在这一部分，我们将讨论各种故障类别，如何检测它们，如何评估它们的影响，并考虑适当的应对措施。
- en: Designing robust systems
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计健壮的系统
- en: When you want to design a robust system, you first need to understand the possible
    failure modes, the risk/probability of each failure, and the impact/cost of each
    failure. Then, you can consider various prevention and mitigation measures, loss-cutting
    strategies, incident-management strategies, and recovery procedures. Finally,
    you can come up with a plan that matches risks to mitigation profiles, including
    cost. A comprehensive design is important and needs to be updated as the system
    evolves. The higher the stakes, the more thorough your plan should be. This process
    has to be tailored for each organization. A corner of error recovery and robustness
    is detecting failures and being able to troubleshoot. The following subsections
    describe common failure categories, how to detect them, and where to collect additional
    information.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想设计一个强大的系统时，首先需要了解可能的故障模式，每种故障的风险/概率以及每种故障的影响/成本。然后，您可以考虑各种预防和缓解措施、损失削减策略、事件管理策略和恢复程序。最后，您可以制定一个与风险相匹配的缓解方案，包括成本。全面的设计很重要，并且需要随着系统的发展而进行更新。赌注越高，您的计划就应该越彻底。这个过程必须为每个组织量身定制。错误恢复和健壮性的一个角落是检测故障并能够进行故障排除。以下小节描述了常见的故障类别，如何检测它们以及在哪里收集额外信息。
- en: Hardware failure
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件故障
- en: 'Hardware failures in Kubernetes can be divided into two groups:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的硬件故障可以分为两组：
- en: The node is unresponsive
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点无响应
- en: The node is responsive
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点有响应
- en: When the node is not responsive, it can be difficult sometimes to determine
    if it's a networking issue, a configuration issue, or actual hardware failure.
    You obviously can't use any information like logs or run diagnostics on the node
    itself. What can you do? First, consider if the node was ever responsive. If it's
    a node that was just added to the cluster, it is more likely a configuration issue.
    If it's a node that was part of the cluster then you can look at historical data
    from the node on Heapster or central logging and see if you detect any errors
    in the logs or degradation in performance that may indicate failing hardware.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点无响应时，有时很难确定是网络问题、配置问题还是实际的硬件故障。显然，您无法使用节点本身的日志或运行诊断。你能做什么？首先，考虑节点是否曾经有响应。如果这是一个刚刚添加到集群中的节点，更有可能是配置问题。如果这是集群中的一个节点，那么您可以查看来自Heapster或中央日志的节点的历史数据，并查看日志中是否有任何错误或性能下降的迹象，这可能表明硬件故障。
- en: When the node is responsive, it may still suffer from the failure of redundant
    hardware, such as non-OS disk or some cores. You can detect the hardware failure
    if the node problem detector is running on the node and raises some event or node
    condition to the attention of master. Alternatively, you may note that pods keep
    getting restarted or jobs take longer to complete. All of these may be signs of
    hardware failure. Another strong hint for hardware failure is if the problems
    are isolated to a single node and standard maintenance operations such as reboot
    don't alleviate the symptoms.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点有响应时，它可能仍然遭受冗余硬件的故障，例如非操作系统磁盘或一些核心。如果节点问题检测器在节点上运行并引起一些事件或节点条件引起主节点的注意，您可以检测硬件故障。或者，您可能会注意到Pod不断重新启动或作业完成时间较长。所有这些都可能是硬件故障的迹象。另一个硬件故障的强烈暗示是，如果问题局限在单个节点上，并且标准维护操作（如重新启动）不能缓解症状。
- en: If your cluster is deployed in the cloud, replacing a node which you suspect
    as having hardware problems is trivial. It is simple to just manually provision
    a new VM and remove the bad VM. In some cases, you may want to employ a more automated
    process and employ a remedy controller, as suggested by the node problem detector
    design. Your remedy controller will listen to problems (or missing health checks)
    and can automatically replace bad nodes. This approach can work even for private
    hosting or bare metal if you keep a pool of extra nodes ready to kick in. Large-scale
    clusters can function just fine, even with reduced capacity most of the time.
    Either you can tolerate slightly reduced capacity when a small number of nodes
    are down, or you can over-provision a little bit. This way, you have some headway
    when a node goes down.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的集群部署在云中，替换一个您怀疑存在硬件问题的节点是微不足道的。只需手动提供一个新的VM并删除坏的VM即可。在某些情况下，您可能希望采用更自动化的流程并使用一个补救控制器，正如节点问题检测器设计所建议的那样。您的补救控制器将监听问题（或缺少的健康检查），并可以自动替换坏的节点。即使在私有托管或裸金属中，这种方法也可以运行，只要您保留一些额外的节点准备投入使用。大规模集群即使在大部分时间内容量减少也可以正常运行。您可以容忍少量节点宕机时的轻微容量减少，或者您可以略微过度配置。这样，当一个节点宕机时，您就有了一些余地。
- en: Quotas, shares, and limits
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配额、份额和限制
- en: 'Kubernetes is a multitenant system. It is designed to use resources efficiently,
    but it schedules pods and allocates resources based on a system of checks and
    balances between available quotas and limits per namespace, and requests for guaranteed
    resources from pods and containers. We will dive into the details later in the
    book. Here, we''ll just consider what can go wrong and how to detect it. There
    are several bad outcomes you can run into:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个多租户系统。它旨在高效利用资源，但是它根据命名空间中可用配额和限制以及pod和容器对保证资源的请求之间的一套检查和平衡系统来调度pod并分配资源。我们将在本书的后面深入讨论细节。在这里，我们只考虑可能出现的问题以及如何检测它。您可能会遇到几种不良结果：
- en: '**Insufficient resources**: If a pod requires a certain amount of CPU or memory,
    and there is no node with available capacity, then the pod can''t be scheduled.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源不足：如果一个pod需要一定数量的CPU或内存，而没有可用容量的节点，那么该pod就无法被调度。
- en: '**Under-utilization**: A pod may declare that it requires a certain amount
    of CPU or memory, and Kubernetes will oblige, but then the pod may only use a
    small percentage of its requested resources. This is just wasteful.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源利用不足：一个pod可能声明需要一定数量的CPU或内存，Kubernetes会满足，但是pod可能只使用其请求资源的一小部分。这只是浪费。
- en: '**Mismatched node configuration**: A pod that requires a lot of CPU but very
    little memory may be scheduled to a high-memory node and use all its CPU resources,
    thereby hogging the node, so no other pod can be scheduled but the unused memory
    is wasted.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点配置不匹配：一个需要大量CPU但很少内存的pod可能被调度到一个高内存的节点上，并使用所有的CPU资源，从而占用了节点，因此无法调度其他pod，但未使用的内存却被浪费了。
- en: 'Checking out the dashboard is a great way to look for suspects visually. Nodes
    and pods that are either over-subscribed or under-utilized are candidates for
    quota and resource request mismatches:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 查看仪表板是一种通过视觉寻找可疑情况的好方法。过度订阅或资源利用不足的节点和pod都是配额和资源请求不匹配的候选者。
- en: '![](Images/37fed9ea-0d5c-4666-a98f-cc4e23b9195d.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/37fed9ea-0d5c-4666-a98f-cc4e23b9195d.png)'
- en: Once you detect a candidate, you can dive into using the `describe` command
    at the node or pod level. In a large-scale cluster, you should have automated
    checks that compare the utilization against capacity planning. This is important
    because most large systems have some level of fluctuation and a uniform load is
    not expected. Make sure that you understand the demands on your system and that
    your cluster's capacity is within the normal range or can adjust elastically,
    as needed.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您检测到一个候选项，您可以深入使用`describe`命令来查看节点或pod级别。在大规模集群中，您应该有自动化检查，以比较利用率与容量规划。这很重要，因为大多数大型系统都有一定程度的波动，而不会期望均匀的负载。确保您了解系统的需求，并且您的集群容量在正常范围内或可以根据需要弹性调整。
- en: Bad configuration
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误的配置
- en: Bad configuration is an umbrella term. Your Kubernetes cluster state is configuration;
    your containers' command-line arguments are configuration; all the environment
    variables used by Kubernetes, your application services, and any third-party services
    are configuration; and all the configuration files are configuration. In some
    data-driven systems, configuration is stored in various data stores. Configuration
    issues are very common because, usually, there aren't any established good practices
    to test them. They often have various fallbacks (for example, search path for
    configuration files) and defaults, and the production-environment configuration
    is different to the development or staging environment.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 错误的配置是一个总称。您的Kubernetes集群状态是配置；您的容器的命令行参数是配置；Kubernetes、您的应用服务和任何第三方服务使用的所有环境变量都是配置；所有配置文件都是配置。在一些数据驱动的系统中，配置存储在各种数据存储中。配置问题非常常见，因为通常没有建立良好的实践来测试它们。它们通常具有各种回退（例如，配置文件的搜索路径）和默认值，并且生产环境配置与开发或暂存环境不同。
- en: 'At the Kubernetes cluster level, there are many possible configuration problems,
    as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes集群级别，可能存在许多可能的配置问题，如下所示：
- en: Incorrect labeling of nodes, pods, or containers
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点、pod或容器的标签不正确
- en: Scheduling pods without a replication controller
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在没有复制控制器的情况下调度pod
- en: Incorrect specification of ports for services
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务端口的规范不正确
- en: Incorrect ConfigMap
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不正确的ConfigMap
- en: Most of these problems can be addressed by having a proper automated deployment
    process, but you must have a deep understanding of your cluster architecture and
    how Kubernetes resources fit together.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些问题可以通过拥有适当的自动化部署流程来解决，但您必须深入了解您的集群架构以及Kubernetes资源如何配合。
- en: Configuration problems typically occur after you change something. It is critical,
    after each deployment or manual change to the cluster, to verify its state.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 配置问题通常发生在您更改某些内容之后。在每次部署或手动更改集群后，验证其状态至关重要。
- en: Heapster and the dashboard are great options here. I suggest starting from the
    services and verifying that they are available, responsive, and functional. Then,
    you can dive deeper and verify that the system also operates within the expected
    performance parameters.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Heapster和仪表板在这里是很好的选择。我建议从服务开始，并验证它们是否可用、响应和功能正常。然后，您可以深入了解系统是否也在预期的性能参数范围内运行。
- en: The logs also provide helpful hints and can pinpoint specific configuration
    options.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 日志还提供了有用的提示，并可以确定特定的配置选项。
- en: Cost versus performance
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本与性能
- en: Large clusters are not cheap. This is especially true if you run in the cloud.
    A major part of operating massive-scale systems is keeping track of the expense.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 大型集群并不便宜。特别是在云中运行时。操作大规模系统的一个重要部分是跟踪开支。
- en: Managing cost on the cloud
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云上管理成本
- en: One of the greatest benefits of the cloud is that it can satisfy elastic demand
    that caters for systems that expand and contract automatically by allocating and
    deallocating resources as needed. Kubernetes fits this model very well and can
    be extended to provision more nodes as necessary. The risk here is that, if not
    constrained properly, a denial-of-service attack (malicious, accidental, or self-inflicted)
    can lead to arbitrary provisioning of expensive resources. This needs to be monitored
    carefully, so it can be caught early on. Quotas on namespaces can avoid it, but
    you still need to be able to dive in and pinpoint the core issue. The root cause
    can be external (a botnet attack), misconfiguration, an internal test gone awry,
    or a bug in the code that detects or allocate resources.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 云的最大好处之一是它可以满足弹性需求，满足系统根据需要自动扩展和收缩，通过根据需要分配和释放资源。Kubernetes非常适合这种模型，并且可以扩展以根据需要提供更多节点。风险在于，如果不适当地限制，拒绝服务攻击（恶意的、意外的或自我造成的）可能导致昂贵资源的任意分配。这需要仔细监控，以便及早发现。命名空间的配额可以避免这种情况，但您仍然需要能够深入了解并准确定位核心问题。根本原因可能是外部的（僵尸网络攻击），配置错误，内部测试出错，或者是检测或分配资源的代码中的错误。
- en: Managing cost on bare metal
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在裸金属上管理成本
- en: On bare metal, you typically don't have to worry about runaway allocation, but
    you can easily run into a wall if you need extra capacity and can't provision
    more resources fast enough. Capacity planning and monitoring your system's performance
    to detect the need early are primary concerns for OPS. Heapster can show historical
    trends and help identify both peak times and overall growth in demand.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在裸金属上，您通常不必担心资源分配失控，但是如果您需要额外的容量并且无法快速提供更多资源，您很容易遇到瓶颈。容量规划和监控系统性能以及及早检测需求是OPS的主要关注点。Heapster可以显示历史趋势，并帮助识别高峰时段和总体需求增长。
- en: Managing cost on hybrid clusters
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理混合集群的成本
- en: Hybrid clusters run on both bare metal and the cloud (and possibly on private
    hosting services too). The considerations are similar, but you may need to aggregate
    your analysis. We will discuss hybrid clusters in more detail later.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 混合集群在裸金属和云上运行（可能还在私人托管服务上）。考虑因素是相似的，但您可能需要汇总您的分析。我们将在稍后更详细地讨论混合集群。
- en: Using Prometheus
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Prometheus
- en: Heapster and the default monitoring and logging that come in the box with Kubernetes
    are a great starting point. However, the Kubernetes community is bursting with
    innovation and several alternative solutions are available. One of the most popular
    solutions is Prometheus. In this section, we will explore the new world of operators,
    the Prometheus Operator, how to install it, and how to use it to monitor your
    cluster.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Heapster和Kubernetes默认的监控和日志记录是一个很好的起点。然而，Kubernetes社区充满了创新，有几种替代方案可供选择。其中最受欢迎的解决方案之一是Prometheus。在本节中，我们将探索运营商的新世界，Prometheus运营商，如何安装它以及如何使用它来监视您的集群。
- en: What are operators?
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是运营商？
- en: Operators are a new class of software that encapsulates the operational knowledge
    needed to develop, manage, and maintain applications on top of Kubernetes. The
    term was introduced by CoreOS in late 2016\. An operator is an application-specific
    controller that extends the Kubernetes API to create, configure, and manage instances
    of complex stateful applications on behalf of a Kubernetes user. It builds upon
    the basic Kubernetes resource and controller concepts, but includes domain or
    application-specific knowledge to automate common tasks.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 运营商是一种新型软件，它封装了在Kubernetes之上开发、管理和维护应用程序所需的操作知识。这个术语是由CoreOS在2016年底引入的。运营商是一个特定于应用程序的控制器，它扩展了Kubernetes
    API，以代表Kubernetes用户创建、配置和管理复杂有状态应用程序的实例。它建立在基本的Kubernetes资源和控制器概念之上，但包括领域或应用程序特定的知识，以自动化常见任务。
- en: The Prometheus Operator
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Prometheus Operator
- en: Prometheus ([https://prometheus.io](https://prometheus.io)) is an open source
    systems monitoring and alerting toolkit for monitoring applications in clusters.
    It was inspired by Google's Borgmon and designed for the Kubernetes model of assigning
    and scheduling units of work. It joined CNCF in 2016, and it has been adopted
    widely across the industry. The primary differences between InfluxDB and Prometheus
    is that Prometheus uses a pull model where anyone can hit the /metrics endpoint,
    and its query language is very expressive, but simpler than the SQL-like query
    language of InfluxDB.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus ([https://prometheus.io](https://prometheus.io))是一个用于监控集群中应用程序的开源系统监控和警报工具包。它受Google的Borgmon启发，并设计用于Kubernetes模型的工作单元分配和调度。它于2016年加入CNCF，并在整个行业广泛采用。InfluxDB和Prometheus之间的主要区别在于，Prometheus使用拉模型，任何人都可以访问/metrics端点，其查询语言非常表达性强，但比InfluxDB的类似SQL的查询语言更简单。
- en: Kubernetes has built-in features to support Prometheus metrics, and Prometheus
    awareness of Kuberneres keeps improving. The Prometheus Operator packages all
    that monitoring goodness into an easy to install and use bundle.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes具有内置功能来支持Prometheus指标，而Prometheus对Kuberneres的认识不断改进。Prometheus Operator将所有这些监控功能打包成一个易于安装和使用的捆绑包。
- en: Installing Prometheus with kube-prometheus
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用kube-prometheus安装Prometheus
- en: 'The easiest way to install Prometheus is using kube-prometheus. It uses the
    Prometheus Operator as well as Grafana for dashboarding and `AlertManager` for
    managing alerts. To get started, clone the repo and run the `deploy` script:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Prometheus的最简单方法是使用kube-prometheus。它使用Prometheus Operator以及Grafana进行仪表板和`AlertManager`的管理。要开始，请克隆存储库并运行`deploy`脚本：
- en: '[PRE9]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The script creates a monitoring namespace and lots of Kubernetes entities and
    supporting components:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本创建一个监控命名空间和大量的Kubernetes实体和支持组件。
- en: The Prometheus Operator itself
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prometheus Operator本身
- en: The Prometheus node_exporter
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prometheus node_exporter
- en: kube-state metrics
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kube-state metrics
- en: A Prometheus configuration covering monitoring of all Kubernetes core components
    and exporters
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖监控所有Kubernetes核心组件和导出器的Prometheus配置
- en: A default set of alerting rules on the cluster components' health
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群组件健康的默认一组警报规则
- en: A Grafana instance serving dashboards on cluster metrics
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为集群指标提供仪表板的Grafana实例
- en: A three node highly available Alertmanager cluster
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个由三个节点组成的高可用性Alertmanager集群
- en: 'Let''s verify that everything is in order:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证一切是否正常：
- en: '[PRE10]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that `alertmanager-main-2` is pending. I suspect that this is due to Minikube
    running on two cores. It is not causing any problem in practice in my setup.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`alertmanager-main-2`处于挂起状态。我怀疑这是由于Minikube在两个核心上运行。在我的设置中，这实际上并没有造成任何问题。
- en: Monitoring your cluster with Prometheus
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Prometheus监控您的集群
- en: 'Once the Prometheus Operator is up and running along with Grafana and the Alertmanager,
    you can access their UIs and interact with the different components:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Prometheus Operator与Grafana和Alertmanager一起运行，您就可以访问它们的UI并与不同的组件进行交互：
- en: Prometheus UI on node port `30900`
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点端口`30900`上的Prometheus UI
- en: Alertmanager UI on node port `30903`
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点端口`30903`上的Alertmanager UI
- en: Grafana on node port `30902`
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点端口`30902`上的Grafana
- en: 'Prometheus supports a dizzying array of metrics to choose from. Here is a screenshot
    that shows the duration of HTTP requests in microseconds broken down by container:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus支持选择的指标种类繁多。以下是一个屏幕截图，显示了按容器分解的微秒级HTTP请求持续时间：
- en: '![](Images/e55c149a-84ac-4c8c-91a8-1addb887cc63.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/e55c149a-84ac-4c8c-91a8-1addb887cc63.png)'
- en: 'To limit the view to only the 0.99 quantile for the `prometheus-k8s` service,
    use the following query:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要将视图限制为`prometheus-k8s`服务的仅0.99分位数，请使用以下查询：
- en: '![](Images/348f4501-a178-41e3-818c-d69d69a48a33.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/348f4501-a178-41e3-818c-d69d69a48a33.png)'
- en: '[PRE11]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](Images/a11d115c-7f30-4aea-871e-8c616f00f73d.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/a11d115c-7f30-4aea-871e-8c616f00f73d.png)'
- en: The Alertmanager is another important part of the Prometheus monitoring story.
    Here is a screenshot of the web UI that lets you define and configure alerts based
    on arbitrary metrics.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Alertmanager是Prometheus监控故事的另一个重要部分。这是一个Web UI的截图，让您可以根据任意指标定义和配置警报。
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at monitoring, logging, and troubleshooting. This
    is a crucial aspect of operating any system and, in particular, a platform such
    as Kubernetes with so many moving pieces. My greatest worry whenever I'm responsible
    for something is that something will go wrong and I will have no systematic way
    to figure out what's wrong and how to fix it. Kubernetes has ample tools and facilities
    built in, such as Heapster, logging, DaemonSets, and node problem detector. You
    can also deploy any kind of monitoring solution you prefer.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了监控、日志记录和故障排除。这是操作任何系统的关键方面，特别是像Kubernetes这样有许多移动部件的平台。每当我负责某件事情时，我最担心的是出现问题，而我没有系统化的方法来找出问题所在以及如何解决它。Kubernetes内置了丰富的工具和设施，如Heapster、日志记录、DaemonSets和节点问题检测器。您还可以部署任何您喜欢的监控解决方案。
- en: In [Chapter 4](0b446f8f-3748-4bb4-8406-78f2af468e14.xhtml), *High Availability
    and Reliability*, we will look at highly available and scalable Kubernetes clusters.
    This is arguably the most important use case for Kubernetes, where it shines compared
    with other orchestration solutions.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](0b446f8f-3748-4bb4-8406-78f2af468e14.xhtml)中，*高可用性和可靠性*，我们将看到高可用和可扩展的Kubernetes集群。这可以说是Kubernetes最重要的用例，它在与其他编排解决方案相比的时候表现出色。
