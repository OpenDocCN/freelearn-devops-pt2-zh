- en: 15\. Monitoring and Autoscaling in Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15. Kubernetes中的监控和自动扩展
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter will introduce you to how Kubernetes enables you to monitor your
    cluster and workloads, and then use the data collected to automatically drive
    certain decisions. You will learn about the Kubernetes Metric Server, which aggregates
    all cluster runtime information, allowing you to use this information to drive
    application runtime scaling decisions. We will walk you through setting up monitoring
    using the Kubernetes Metrics server and Prometheus and then use Grafana to visualize
    those metrics. By the end of this chapter, you will also have learned how to automatically
    scale up your application to completely utilize the resources on the provisioned
    infrastructure, as well as automatically scale your cluster infrastructure as needed.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍Kubernetes如何使您能够监视集群和工作负载，然后使用收集的数据自动驱动某些决策。您将了解Kubernetes Metric Server，它汇总了所有集群运行时信息，使您能够使用这些信息来驱动应用程序运行时的扩展决策。我们将指导您如何使用Kubernetes
    Metrics服务器和Prometheus设置监控，然后使用Grafana来可视化这些指标。到本章结束时，您还将学会如何自动扩展您的应用程序以充分利用所提供的基础设施的资源，以及根据需要自动扩展您的集群基础设施。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Let's take a moment to reflect on our progress through this series of chapters
    beginning from *Chapter 11*, *Build Your Own HA Cluster*. We started by setting
    up a Kubernetes cluster using kops to configure AWS infrastructure in a highly
    available manner. Then, we used Terraform and some scripting to improve the stability
    of our cluster and deploy our simple counter app. After this, we began hardening
    the security and increasing the availability of our app using Kubernetes/cloud-native
    principles. Finally, we learned how to run a stateful database responsible for
    using transactions to ensure that we always get a series of increasing numbers
    from our application.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间回顾一下我们在这一系列章节中的进展，从第11章“构建您自己的HA集群”开始。我们首先使用kops设置了一个Kubernetes集群，以高可用的方式配置AWS基础设施。然后，我们使用Terraform和一些脚本来提高集群的稳定性，并部署我们的简单计数器应用程序。之后，我们开始加固安全性，并使用Kubernetes/云原生原则增加我们应用程序的可用性。最后，我们学会了运行一个负责使用事务来确保我们始终从我们的应用程序中获得一系列递增数字的有状态数据库。
- en: In this chapter, we are going to explore how to leverage the data that already
    exists in Kubernetes about our applications to drive and automate decision-making
    processes around scaling them so that they are always the right size for our load.
    Because it takes time to observe application metrics, schedule and start containers,
    and bootstrap nodes from scratch, this scaling is not instantaneous but will eventually
    (usually within minutes) balance the number of pods and nodes needed to perform
    the work of the load on the cluster. To achieve this, we need a way of getting
    this data, understanding/interpreting this data, and feeding back instructions
    to Kubernetes with this data. Luckily, there are already tools in Kubernetes that
    will help us do this. These are the **Kubernetes Metric Server**, **HorizontalPodAutoscalers**
    (**HPAs**), and the **ClusterAutoscaler**.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何利用Kubernetes已有的关于我们应用程序的数据，驱动和自动化关于调整其规模的决策过程，以便始终使其适合我们的负载。由于观察应用程序指标、调度和启动容器以及从头开始引导节点需要时间，因此这种扩展并非瞬间发生，但最终（通常在几分钟内）会平衡集群上执行负载工作所需的Pod和节点数量。为了实现这一点，我们需要一种获取这些数据、理解/解释这些数据并利用这些数据向Kubernetes反馈指令的方法。幸运的是，Kubernetes中已经有一些工具可以帮助我们做到这一点。这些工具包括Kubernetes
    Metric Server、HorizontalPodAutoscalers（HPAs）和ClusterAutoscaler。
- en: Kubernetes Monitoring
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes监控
- en: 'Kubernetes has built-in support for providing useful monitoring information
    about infrastructure components as well as various Kubernetes objects. The Kubernetes
    Metrics server is a component (which does not come built-in) that gathers and
    exposes the metrics data at an API endpoint on the API server. Kubernetes uses
    this data to manage the scaling of Pods, but this data can also be scraped by
    a third-party tool such as Prometheus for use by cluster operators. Prometheus
    has a few very basic data visualization functions and primarily serves as a metric-gathering
    and storage tool, so you can use a more powerful and useful data visualization
    tool such as Grafana. Grafana allows cluster admins to create useful dashboards
    to monitor their clusters. You can learn more about how monitoring in Kubernetes
    is architected at this link: [https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes内置支持提供有关基础设施组件以及各种Kubernetes对象的有用监控信息。 Kubernetes Metrics服务器是一个组件（不是内置的），它会在API服务器的API端点上收集和公开指标数据。
    Kubernetes使用这些数据来管理Pod的扩展，但这些数据也可以被第三方工具（如Prometheus）抓取，供集群操作员使用。 Prometheus具有一些非常基本的数据可视化功能，并且主要用作度量收集和存储工具，因此您可以使用更强大和有用的数据可视化工具，如Grafana。
    Grafana允许集群管理员创建有用的仪表板来监视其集群。 您可以在此链接了解有关Kubernetes监控架构的更多信息：[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md)。
- en: 'Here''s how this will look for us in a diagram:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在图表中的展示：
- en: '![Figure 15.1: An overview of the monitoring pipeline that'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.1：监控管道概述'
- en: we will implement in this chapter
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章实施的
- en: '](image/B14870_15_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_01.jpg)'
- en: 'Figure 15.1: An overview of the monitoring pipeline that we will implement
    in this chapter'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：我们将在本章实施的监控管道概述
- en: 'This diagram represents how the monitoring pipeline is going to be implemented
    through various Kubernetes objects. In summary, the monitoring pipeline will work
    as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表代表了通过各种Kubernetes对象实施监控管道的方式。 总之，监控管道将按以下方式工作：
- en: The various components of Kubernetes are already instrumented to provide various
    metrics. The Kubernetes Metrics server will fetch these metrics from the components.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes的各个组件已经被调整，以提供各种指标。 Kubernetes Metrics服务器将从这些组件中获取这些指标。
- en: The Kubernetes Metrics server will then expose these metrics on an API endpoint.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes Metrics服务器将在API端点上公开这些指标。
- en: Prometheus will access this API endpoint, scrape these metrics, and add it to
    its special database.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prometheus将访问此API端点，抓取这些指标，并将其添加到其特殊数据库中。
- en: Grafana will query the Prometheus database to gather these metrics and present
    it in a neat dashboard with graphs and other visual representations.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Grafana将查询Prometheus数据库，收集这些指标，并在整洁的仪表板上以图表和其他可视化形式呈现。
- en: Now, let's look at each of the previously mentioned components to understand
    them better.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐个查看之前提到的每个组件，以更好地理解它们。
- en: Kubernetes Metrics API/Metrics Server
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes Metrics API/Metrics服务器
- en: The Kubernetes Metrics server (formerly known as Heapster) gathers and exposes
    metric data on the running state of all Kubernetes components and objects in Kubernetes.
    Nodes, control plane components, running pods, and really any Kubernetes objects
    are all observable via the Metrics server. Some examples of the metrics that it
    collects are the number of pods that are desired in a Deployment/ReplicaSet, the
    number of pods posting a `Ready` status in that Deployment, and the CPU and memory
    utilization of each container.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Metrics server（以前称为Heapster）收集并公开所有Kubernetes组件和对象的运行状态的度量数据。节点、控制平面组件、运行中的pod以及任何Kubernetes对象都可以通过Metrics
    server进行观察。它收集的一些度量的例子包括Deployment/ReplicaSet中所需pod的数量，该部署中处于“Ready”状态的pod的数量，以及每个容器的CPU和内存利用率。
- en: We will mostly be using the default exposed metrics while gathering the information
    relevant to the Kubernetes objects that we are orchestrating our application.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集与我们编排应用程序相关的信息时，我们将主要使用默认公开的度量。
- en: Prometheus
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Prometheus
- en: Prometheus is a metric collector, a time-series database, and an alert manager
    for just about anything. It makes use of a scraping function to pull metrics from
    running processes that expose those metrics in Prometheus format at a defined
    interval. Those metrics are then stored in their own time-series database and
    you can run queries on this data to get a snapshot of the state of your running
    applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus是一个度量收集器、时间序列数据库和警报管理器，几乎可以用于任何事情。它利用抓取功能从运行中的进程中提取度量，这些度量以Prometheus格式在定义的间隔内暴露。然后这些度量将存储在它们自己的时间序列数据库中，您可以对这些数据运行查询，以获取运行应用程序状态的快照。
- en: It also comes with an alert manager function, which allows you to set up triggers
    to alert your on-call admins. As an example, you can configure the alert manager
    to automatically trigger an alert if the CPU utilization on one of your nodes
    is above 90% for 15 minutes. The alert manager can interface with several third-party
    services to send the alert via various means, such as email, chat messages, or
    SMS phone alerts.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 它还带有警报管理器功能，允许您设置触发器以警报您的值班管理员。例如，您可以配置警报管理器，如果您的一个节点的CPU利用率在15分钟内超过90%，则自动触发警报。警报管理器可以与多个第三方服务进行接口，通过各种方式发送警报，如电子邮件、聊天消息或短信电话警报。
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: 'If you want to learn more about Prometheus, you can refer to this book: [https://www.packtpub.com/virtualization-and-cloud/hands-infrastructure-monitoring-prometheus](https://www.packtpub.com/virtualization-and-cloud/hands-infrastructure-monitoring-prometheus).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于Prometheus的信息，可以参考这本书：[https://www.packtpub.com/virtualization-and-cloud/hands-infrastructure-monitoring-prometheus](https://www.packtpub.com/virtualization-and-cloud/hands-infrastructure-monitoring-prometheus)。
- en: Grafana
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Grafana
- en: Grafana is an open-source tool that can be used to visualize data and create
    useful dashboards. Grafana will query the Prometheus database for metrics and
    graph them on dashboard charts that are easier for humans to understand and spot
    trends or discrepancies. These tools are indispensable when running a production
    cluster as they help us spot issues in the infrastructure quickly and resolve
    issues.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana是一个开源工具，可用于可视化数据并创建有用的仪表板。Grafana将查询Prometheus数据库以获取度量，并在仪表板图表上绘制它们，这样人类更容易理解并发现趋势或差异。在运行生产集群时，这些工具是必不可少的，因为它们帮助我们快速发现基础设施中的问题并解决问题。
- en: Monitoring Your Applications
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控您的应用程序
- en: While application monitoring is beyond the scope of this book, we will provide
    some rough guidelines so that you can explore more on this topic. We would recommend
    that you expose your application's metrics in Prometheus format and use Prometheus
    to scrape them; there are many libraries for most languages that can help with
    this.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然应用程序监控超出了本书的范围，但我们将提供一些粗略的指南，以便您可以在这个主题上进行更多的探索。我们建议您以Prometheus格式公开应用程序的指标，并使用Prometheus对其进行抓取；大多数语言都有许多库可以帮助实现这一点。
- en: 'Another way is to use Prometheus exporters that are available for various applications.
    Exporters gather the metrics from an application and expose them to an API endpoint
    so that Prometheus can scrape it. You can find several open-source exporters for
    common applications at this link: [https://prometheus.io/docs/instrumenting/exporters/](https://prometheus.io/docs/instrumenting/exporters/).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用适用于各种应用程序的Prometheus导出器。导出器从应用程序中收集指标并将它们暴露给API端点，以便Prometheus可以对其进行抓取。您可以在此链接找到一些常见应用程序的开源导出器：[https://prometheus.io/docs/instrumenting/exporters/](https://prometheus.io/docs/instrumenting/exporters/)。
- en: 'For your custom applications and frameworks, you can create your own exporters
    using the libraries provided by Prometheus. You can find the relevant guidelines
    at this link: [https://prometheus.io/docs/instrumenting/writing_exporters/](https://prometheus.io/docs/instrumenting/writing_exporters/).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您的自定义应用程序和框架，您可以使用Prometheus提供的库创建自己的导出器。您可以在此链接找到相关的指南：[https://prometheus.io/docs/instrumenting/writing_exporters/](https://prometheus.io/docs/instrumenting/writing_exporters/)。
- en: Once you have exposed and scraped the metrics from your applications, you can
    present them in a Grafana dashboard, similar to the one we will create for monitoring
    Kubernetes components.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您从应用程序中公开并抓取了指标，您可以在Grafana仪表板中呈现它们，类似于我们将为监控Kubernetes组件创建的仪表板。
- en: 'Exercise 15.01: Setting up the Metrics Server and Observing Kubernetes Objects'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习15.01：设置Metrics Server并观察Kubernetes对象
- en: 'In this exercise, we are going to be setting up monitoring for Kubernetes objects
    in our cluster and running a few queries and creating visualizations to see what''s
    going on. We''re going to be installing Prometheus, Grafana, and the Kubernetes
    Metrics server:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将为我们的集群设置Kubernetes对象的监控，并运行一些查询和创建可视化来查看发生了什么。我们将安装Prometheus、Grafana和Kubernetes
    Metrics server：
- en: 'To begin with, we will recreate your EKS cluster from the Terraform file in
    *Exercise 12.02*, *Creating a Cluster with EKS Using Terraform*. If you already
    have the `main.tf` file, you can work with it. Otherwise, you can run the following
    command to get it:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将从*练习12.02*中的Terraform文件重新创建您的EKS集群，*使用Terraform创建EKS集群*。如果您已经有了`main.tf`文件，您可以使用它。否则，您可以运行以下命令来获取它：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, use the following two commands one after the other to get your cluster
    resources up and running:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，依次使用以下两个命令来启动和运行您的集群资源：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You will need `jq` for the following command. `jq` is a simple tool to manipulate
    JSON data. If you don''t already have it installed, you can do so by using this
    command: `sudo apt install jq`.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要`jq`来运行以下命令。`jq`是一个简单的操作JSON数据的工具。如果您还没有安装它，您可以使用以下命令来安装：`sudo apt install
    jq`。
- en: 'To set up the Kubernetes Metrics server in our cluster, we need to run the
    following in sequence:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在我们的集群中设置Kubernetes Metrics server，我们需要按顺序运行以下命令：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see a response similar to the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到类似以下的响应：
- en: '![Figure 15.2: Deploying all the objects required for the Metrics server'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.2：部署Metrics server所需的所有对象'
- en: '](image/B14870_15_02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_02.jpg)'
- en: 'Figure 15.2: Deploying all the objects required for the Metrics server'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：部署Metrics server所需的所有对象
- en: 'To test this, let''s run the following command:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试这一点，让我们运行以下命令：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you are getting a `ServiceUnavailable` error, please check whether your firewall
    rules are allowing the API server to communicate with the node running the Metrics
    server.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您收到“ServiceUnavailable”错误，请检查防火墙规则是否允许API服务器与运行Metrics服务器的节点进行通信。
- en: 'We have been frequently using the `kubectl get` commands by naming the object.
    We have seen in *Chapter 4*, *How to Communicate with Kubernetes (API Server)*,
    that Kubectl interprets the request, points the request to the appropriate endpoint,
    and formats the results in a readable format. But here, since we have created
    a custom endpoint at our API server, we have to point toward it using the `--raw`
    flag. You should see a response similar to the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常使用`kubectl get`命令来命名对象。我们在*第4章*中看到，Kubectl解释请求，将请求指向适当的端点，并以可读格式格式化结果。但是在这里，由于我们在API服务器上创建了自定义端点，我们必须使用`--raw`标志指向它。您应该看到类似以下的响应：
- en: '![Figure 15.3: Response from the Kubernetes Metrics server'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.3：来自Kubernetes Metrics服务器的响应'
- en: '](image/B14870_15_03.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_03.jpg)'
- en: 'Figure 15.3: Response from the Kubernetes Metrics server'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：来自Kubernetes Metrics服务器的响应
- en: As we can see here, the response contains JSON blobs that define a metric namespace,
    metric values, and metric metadata, such as a node name and availability zones.
    However, these metrics are not very readable. We will make use of Prometheus to
    aggregate them and then use Grafana to present the aggregated metrics in a concise
    dashboard.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里看到的，响应包含定义度量命名空间、度量值和度量元数据的JSON块，例如节点名称和可用区。但是，这些指标并不是非常可读的。我们将利用Prometheus对它们进行聚合，然后使用Grafana将聚合指标呈现在简洁的仪表板中。
- en: 'Now, we have metric data being aggregated. Let''s start scraping and visualizing
    with Prometheus and Grafana. For this, we will install Prometheus and Grafana
    using Helm. Run the following command:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们正在聚合度量数据。让我们开始使用Prometheus和Grafana进行抓取和可视化。为此，我们将使用Helm安装Prometheus和Grafana。运行以下命令：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you are installing and running helm for the first time, you will need to
    run the following command to get stable repos:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是第一次安装和运行helm，您需要运行以下命令来获取稳定的存储库：
- en: '`help repo add stable https://kubernetes-charts.storage.googleapis.com/`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`help repo add stable https://kubernetes-charts.storage.googleapis.com/`'
- en: 'You should see an output similar to the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似以下的输出：
- en: '![Figure 15.4: Installing the Helm chart for Prometheus'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.4：安装Prometheus的Helm图表'
- en: '](image/B14870_15_04.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_04.jpg)'
- en: 'Figure 15.4: Installing the Helm chart for Prometheus'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4：安装Prometheus的Helm图表
- en: 'Now, let''s install Grafana in a similar fashion:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们以类似的方式安装Grafana：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should see the following response:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 15.5: Installing the Helm chart for Grafana'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.5：安装Grafana的Helm图表'
- en: '](image/B14870_15_05.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_05.jpg)'
- en: 'Figure 15.5: Installing the Helm chart for Grafana'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5：安装Grafana的Helm图表
- en: In this screenshot, notice the `NOTES:` section, which lists two steps. Follow
    these steps to get your Grafana admin password and your endpoint to access Grafana.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在此截图中，请注意“NOTES:”部分，其中列出了两个步骤。按照这些步骤获取Grafana管理员密码和访问Grafana的端点。
- en: 'Here, we are running the first command that Grafana showed in the output of
    the previous step:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们正在运行Grafana在上一步输出中显示的第一个命令：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Please use the version of the commands that you got; the command will be customized
    for your instance. This command gets your password, which is stored in a Secret,
    decodes it, and echoes it in your terminal output so that you can copy it for
    use in further steps. You should see a response similar to the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请使用您获得的命令版本；命令将被定制为您的实例。此命令获取您的密码，该密码存储在一个秘密中，解码它，并在您的终端输出中回显它，以便您可以将其复制以供后续步骤使用。您应该看到类似以下的响应：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s run the next two commands that Grafana asked us to run, as seen
    in *Figure 15.5*:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们运行Grafana要求我们运行的下两个命令，如*图15.5*中所示：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Again, use the command that you obtain for your instance as this will be customized.
    These commands find the Pod that Grafana is running on and then map a port from
    our local machine to it so that we can easily access it. You should see the following
    response:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用您的实例获取的命令，因为这将是定制的。这些命令会找到Grafana正在运行的Pod，然后将本地机器的端口映射到它，以便我们可以轻松访问它。您应该会看到以下响应：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'At this step, if you are facing any issues with getting the proper Pod name,
    you can simply run `kubectl get pods` to find the name of the Pod running Grafana
    and use that name instead of the shell (`$POD_NAME`) variable. So, your command
    will look similar to this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，如果您在获取正确的Pod名称时遇到任何问题，您可以简单地运行`kubectl get pods`来找到运行Grafana的Pod的名称，并使用该名称代替shell（`$POD_NAME`）变量。因此，您的命令将类似于这样：
- en: '`kubectl --namespace default port-forward grafana-1591658222-7cd4d8b7df-b2hlm
    3000`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl --namespace default port-forward grafana-1591658222-7cd4d8b7df-b2hlm
    3000`。'
- en: 'Now, open your browser and visit `http://localhost:3000` to access Grafana.
    You should see the following landing page:![Figure 15.6: The log-in page for the
    Grafana dashboard'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，打开浏览器并访问`http://localhost:3000`以访问Grafana。您应该会看到以下着陆页面：![图15.6：Grafana仪表板的登录页面
- en: '](image/B14870_15_06.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_06.jpg)'
- en: 'Figure 15.6: The log-in page for the Grafana dashboard'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6：Grafana仪表板的登录页面
- en: The default username is `admin` and the password is the value echoed in the
    output of *step 6*. Use that to log in.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 默认用户名是`admin`，密码是*步骤6*输出中的值。使用该值登录。
- en: 'After a successful login, you should see this page:![Figure 15.7: The Grafana
    Home dashboard'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成功登录后，您应该会看到此页面：![图15.7：Grafana主页仪表板
- en: '](image/B14870_15_07.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_07.jpg)'
- en: 'Figure 15.7: The Grafana Home dashboard'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7：Grafana主页仪表板
- en: 'Now, let''s create a dashboard for Kubernetes metrics. To do so, we need to
    set up Prometheus as a data source for Grafana. On the left sidebar, click on
    `Configuration` and then on `Data Sources`:![Figure 15.8: Selecting Data Sources
    from the Configuration menu'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们为Kubernetes指标创建一个仪表板。为此，我们需要将Prometheus设置为Grafana的数据源。在左侧边栏中，点击`配置`，然后点击`数据源`：![图15.8：从配置菜单中选择数据源
- en: '](image/B14870_15_08.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_08.jpg)'
- en: 'Figure 15.8: Selecting Data Sources from the Configuration menu'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8：从配置菜单中选择数据源
- en: 'You will see this page:![Figure 15.9: The Add data source option'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将看到此页面：![图15.9：添加数据源选项
- en: '](image/B14870_15_09.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_09.jpg)'
- en: 'Figure 15.9: The Add data source option'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9：添加数据源选项
- en: Now, click on the `Add data source` button.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，点击`添加数据源`按钮。
- en: 'You should see this page with several database options. Prometheus should be
    on top. Click on that:![Figure 15.10: Choosing Prometheus as our data source for
    the Grafana dashboard'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该会看到带有几个数据库选项的页面。Prometheus应该在顶部。点击它：![图15.10：选择Prometheus作为Grafana仪表板的数据源
- en: '](image/B14870_15_10.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_10.jpg)'
- en: 'Figure 15.10: Choosing Prometheus as our data source for the Grafana dashboard'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10：选择Prometheus作为Grafana仪表板的数据源
- en: Now, before we move on to the next screen, here, we need to get the URL that
    Grafana will use to access the Prometheus database from inside the cluster. We
    will do that in the next step.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们继续到下一个屏幕之前，在这里，我们需要获取Grafana将用于从集群内部访问Prometheus数据库的URL。我们将在下一步中执行此操作。
- en: 'Open a new terminal window and run the following command:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的终端窗口并运行以下命令：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You should see a response similar to the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到类似以下的响应：
- en: '![Figure 15.11: Getting the list of all services'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.11：获取所有服务的列表'
- en: '](image/B14870_15_11.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_11.jpg)'
- en: 'Figure 15.11: Getting the list of all services'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.11：获取所有服务的列表
- en: Copy the name of the service that starts with `prometheus` and ends in `server`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 复制以`prometheus`开头并以`server`结尾的服务的名称。
- en: 'After *step 12*, you will have arrived at the screen shown in the following screenshot:![Figure
    15.12: Entering the address of our Prometheus service in Grafana'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*步骤12*之后，您将看到以下屏幕截图所示的屏幕：![图15.12：在Grafana中输入我们Prometheus服务的地址
- en: '](image/B14870_15_12.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_12.jpg)'
- en: 'Figure 15.12: Entering the address of our Prometheus service in Grafana'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.12：在Grafana中输入我们Prometheus服务的地址
- en: 'In the `URL` field of the `HTTP` section, enter the following value:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在`HTTP`部分的`URL`字段中，输入以下值：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that you should see `Data source is working`, as shown in the preceding
    screenshot. Then, click on the `Save and Test` button at the bottom. The reason
    we have added `.default` to our URL is that we deployed this Helm chart to the
    `default` Kubernetes namespace. If you deployed it to another namespace, you should
    replace `default` with the name of your namespace.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您应该看到`数据源正在工作`，如前面的屏幕截图所示。然后，点击底部的`保存并测试`按钮。我们在URL中添加`.default`的原因是我们将此Helm图表部署到了`default`
    Kubernetes命名空间。如果您将其部署到另一个命名空间，您应该用您的命名空间的名称替换`default`。
- en: 'Now, let''s set up the dashboard. Back on the Grafana home page (`http://localhost:3000`),
    click on the `+` symbol on the left sidebar, and then click on `Import`, as shown
    here:![Figure 15.13: Navigating to import Dashboard option'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们设置仪表板。回到Grafana主页（`http://localhost:3000`），点击左侧边栏上的`+`符号，然后点击`Import`，如下所示：![图15.13：导航到导入仪表板选项
- en: '](image/B14870_15_13.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_13.jpg)'
- en: 'Figure 15.13: Navigating to import Dashboard option'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.13：导航到导入仪表板选项
- en: 'On the next page, you should see the `Grafana.com Dashboard` field, as shown
    here:![Figure 15.14: Entering the source to import the dashboard from'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一页，您应该看到`Grafana.com仪表板`字段，如下所示：![图15.14：输入要从中导入仪表板的源
- en: '](image/B14870_15_14.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_14.jpg)'
- en: 'Figure 15.14: Entering the source to import the dashboard from'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.14：输入要从中导入仪表板的源
- en: 'Paste the following link into the `Grafana.com Dashboard` field:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下链接粘贴到“Grafana.com仪表板”字段中：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is an officially supported Kubernetes dashboard. Once you click anywhere
    outside the file, you should automatically advance to the next screen.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个官方支持的Kubernetes仪表板。一旦你在文件外的任何地方点击，你应该自动进入下一个屏幕。
- en: 'The previous step should lead you to this screen:![Figure 15.15: Setting Prometheus
    as the data source for the imported dashboard'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一步应该引导您到这个屏幕：![图15.15：将Prometheus设置为导入仪表板的数据源
- en: '](image/B14870_15_15.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_15.jpg)'
- en: 'Figure 15.15: Setting Prometheus as the data source for the imported dashboard'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.15：将Prometheus设置为导入仪表板的数据源
- en: Where you see the `prometheus`, click on the drop-down list next to it, select
    `Prometheus`, and hit `Import`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在你看到`prometheus`的地方，点击旁边的下拉列表，选择`Prometheus`，然后点击`Import`。
- en: 'The result should look like this:![Figure 15.16: The Grafana dashboard to monitor
    our cluster'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果应该如下所示：![图15.16：用于监视我们集群的Grafana仪表板
- en: '](image/B14870_15_16.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_16.jpg)'
- en: 'Figure 15.16: The Grafana dashboard to monitor our cluster'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.16：用于监视我们集群的Grafana仪表板
- en: As you can see, we have a concise dashboard for monitoring workloads in Kubernetes.
    In this exercise, we deployed our Metric Server to collect and expose Kubernetes
    object metrics, then we deployed Prometheus to store those metrics and Grafana
    to help us visualize the collected metrics in Prometheus, which will inform us
    as to what's going on in our cluster at any point in time. Now, it's time to use
    that information to scale things.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们在Kubernetes中有一个简洁的仪表板来监视工作负载。在这个练习中，我们部署了我们的Metric Server来收集和公开Kubernetes对象指标，然后我们部署了Prometheus来存储这些指标，并使用Grafana来帮助我们可视化Prometheus中收集的指标，这将告诉我们在任何时候我们集群中发生了什么。现在，是时候利用这些信息来扩展事物了。
- en: Autoscaling in Kubernetes
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的自动扩展
- en: 'Kubernetes allows you to automatically scale your workloads to adapt to changing
    demands on your applications. The information gathered from the Kubernetes Metrics
    server is the data that is used for driving the scaling decisions. In this book,
    we will be covering two types of scaling action—one that impacts the number of
    running pods in a Deployment and another that impacts the number of running nodes
    in a cluster. Both are examples of horizontal scaling. Let''s briefly gain an
    intuition for what both the horizontal scaling of pods and the horizontal scaling
    of nodes would entail:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes允许您自动扩展工作负载以适应应用程序的不断变化的需求。从Kubernetes度量服务器收集的信息是用于驱动扩展决策的数据。在本书中，我们将涵盖两种类型的扩展操作——一种影响部署中运行的pod数量，另一种影响集群中运行的节点数量。这两种都是水平扩展的例子。让我们简要地直观了解一下pod的水平扩展和节点的水平扩展将涉及什么：
- en: '**Pods**: Assuming that you filled out the `resources:` section of `podTemplate`
    when creating a Deployment in Kubernetes, each container within that pod will
    have the `requests` and `limits` fields, as designated by the corresponding `cpu`
    and `memory` fields. When the resources needed to process a workload exceed that
    which you have allocated, then by adding additional replicas of a pod to the Deployment,
    you are horizontally scaling to add capacity to your Deployment. By letting a
    software process decide the number of replicas of a Pod in a Deployment for you
    based on load, you are *autoscaling* your deployment to keep the number of replicas
    consistent with the metric you have defined to express your application''s load.
    One such metric for application load could be the percentage of the allocated
    CPU that is currently being consumed.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods：假设您在创建Kubernetes中的部署时填写了`podTemplate`的`resources:`部分，那么该pod中的每个容器都将具有由相应的`cpu`和`memory`字段指定的`requests`和`limits`字段。当处理工作负载所需的资源超出您分配的资源时，通过向部署添加pod的额外副本，您可以水平扩展以增加部署的容量。通过让软件进程根据负载为您决定部署中Pod的副本数量，您正在*自动扩展*部署，以使副本数量与您定义的用于表示应用程序负载的指标保持一致。应用程序负载的一个指标可能是当前正在使用的分配CPU的百分比。
- en: '**Nodes**: Every node has a certain amount of CPU (typically expressed by the
    number of cores) and memory (typically expressed in gigabytes) that it has available
    for consumption by Pods. When the total capacity of all worker nodes is exhausted
    by all running pods (meaning that the CPU and memory requests/limits for all the
    Pods are equal to or greater than that of the whole cluster), then we have saturated
    the resources of our cluster. In order to allow more Pods to be run on the cluster,
    or to allow more autoscaling to take place in the cluster, we need to add capacity
    in the form of additional worker nodes. When we allow a software process to make
    this decision for us, we are considered to be *autoscaling* the total capacity
    of our cluster. In Kubernetes, this is handled by the ClusterAutoscaler.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点：每个节点都有一定数量的CPU（通常以核心数表示）和内存（通常以升表示），可供Pod消耗。当所有工作节点的总容量被所有运行的pod耗尽时（这意味着所有Pod的CPU和内存请求/限制都等于或大于整个集群的请求/限制），那么我们已经饱和了集群的资源。为了允许在集群上运行更多的Pod，或者允许集群中发生更多的自动扩展，我们需要以额外的工作节点的形式增加容量。当我们允许软件进程为我们做出这个决定时，我们被认为是*自动扩展*我们集群的总容量。在Kubernetes中，这由ClusterAutoscaler处理。
- en: Note
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When you increase the number of pod replicas of an application, it is known
    as horizontal scaling and is handled by the **HorizontalPodAutoscaler**. If, instead,
    you were to increase the resource limits for your replicas, that would be called
    vertical scaling. Kubernetes also offers **VerticalPodAutoscaler**, but we are
    leaving it out for brevity, and due to the fact that it is not yet generally available
    and safe for use in production.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当你增加应用程序的Pod副本数量时，这被称为水平扩展，由**HorizontalPodAutoscaler**处理。相反，如果你增加副本的资源限制，那就被称为垂直扩展。Kubernetes还提供**VerticalPodAutoscaler**，但由于尚未普遍可用且在生产中使用时不安全，我们在此略过。
- en: Using both HPAs and ClusterAutoscalers in conjunction with each other can be
    an effective way for companies to ensure that they always have the right amount
    of application resources deployed for their load and that they aren't paying too
    much for it at the same time. Let's examine both of them in the following subsections.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 同时使用HPA和ClusterAutoscalers可以是公司确保始终有正确数量的应用程序资源部署以满足其负载，并且同时不会支付过多费用的有效方式。让我们在以下小节中分别研究它们。
- en: HorizontalPodAutoscaler
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HorizontalPodAutoscaler
- en: HPAs are responsible for making sure that the number of replicas of your application
    in a Deployment match whatever the current demand as measured by a metric. This
    is useful because we can use real-time metric data, which is already gathered
    by Kubernetes, to always ensure that our application is meeting the demands we
    have set forth in our thresholds. This may be a new concept to some application
    owners who are not used to running applications using data, but once you begin
    to leverage tools that can right-size your deployments, you will never want to
    go back.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: HPA负责确保部署中应用程序的副本数量与度量标准测得的当前需求相匹配。这很有用，因为我们可以使用实时度量数据，这些数据已经被Kubernetes收集，以始终确保我们的应用程序满足我们在阈值中设定的需求。这对一些不习惯使用数据运行应用程序的应用程序所有者可能是一个新概念，但一旦开始利用可以调整部署大小的工具，你就永远不想回头了。
- en: Kubernetes has an API resource in the `autoscaling/v1` and `autoscaling/v2beta2`
    groups to provide a definition of autoscaling triggers that can run against another
    Kubernetes resource, which is most often a Kubernetes Deployment object. In the
    case of `autoscaling/v1`, the only supported metric is the current CPU consumption,
    and in the case of `autoscaling/v2beta2`, there is support for any custom metrics.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在`autoscaling/v1`和`autoscaling/v2beta2`组中有一个API资源，用于提供可以针对另一个Kubernetes资源运行的自动缩放触发器的定义，这往往是一个Kubernetes部署对象。在`autoscaling/v1`的情况下，唯一支持的度量标准是当前的CPU消耗，在`autoscaling/v2beta2`的情况下，支持任何自定义度量标准。
- en: HPA queries the Kubernetes Metric Server to look at the metrics for the particular
    deployment. Then, the autoscaling resource will determine whether or not the currently
    observed metric is beyond the threshold for a scaling target. If it is, then it
    will change the number of Pods desired by the deployment to be higher or lower
    depending on the load.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: HPA查询Kubernetes Metric Server来查看特定部署的度量标准。然后，自动缩放资源将确定当前观察到的度量标准是否超出了缩放目标的阈值。如果是，它将根据负载将部署所需的Pod数量增加或减少。
- en: As an example, consider a shopping cart microservice hosted by an e-commerce
    company. The shopping cart service experiences a heavy load during the coupon
    code-entry process because it must traverse all items in the cart and search for
    active coupons on them before validating a coupon code. On a random Tuesday morning,
    there are many shoppers online using the service and they all want to use coupons.
    Normally, the service would become overwhelmed and requests would start to fail.
    However, if you were able to use an HPA, Kubernetes would use the spare computing
    power of your cluster to ensure that there are enough Pods of this shopping cart
    service to be able to handle the load.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，考虑一个由电子商务公司托管的购物车微服务。在输入优惠券代码的过程中，购物车服务经历了重负载，因为它必须遍历购物车中的所有商品，并在验证优惠券代码之前搜索其中的活动优惠券。在一个随机的周二早晨，有许多在线购物者使用该服务，并且他们都想使用优惠券。通常情况下，服务会不堪重负，请求会开始失败。然而，如果您能够使用HPA，Kubernetes将利用集群的空闲计算能力，以确保有足够的该购物车服务的Pod来处理负载。
- en: Note that simply autoscaling a Deployment is not a "one-size-fits-all" solution
    to performance problems in an application. There are many places in modern applications
    where slowdowns can occur, so careful consideration should be made about your
    application architecture to see where you can identify other bottlenecks not solved
    by simple autoscaling. One such example would be slow query performance on a database.
    However, for this chapter, we will be focusing on application problems that can
    be solved by autoscaling in Kubernetes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，简单地对部署进行自动缩放并不是解决应用程序性能问题的“一刀切”解决方案。现代应用程序中有许多可能出现减速的地方，因此应该仔细考虑您的应用程序架构，以查看您可以识别其他瓶颈的地方，而这些瓶颈并不是简单的自动缩放可以解决的。一个这样的例子是数据库上的慢查询性能。然而，在本章中，我们将专注于可以通过Kubernetes中的自动缩放来解决的应用程序问题。
- en: 'Let''s look at the structure of an HPA to understand a bit better:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下HPA的结构，以便更好地理解一些内容：
- en: with_autoscaler.yaml
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: with_autoscaler.yaml
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can find the full code at this link: [https://packt.live/3bE9v28](https://packt.live/3bE9v28).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此链接找到完整的代码：[https://packt.live/3bE9v28](https://packt.live/3bE9v28)。
- en: 'In this spec, observe the following fields:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个规范中，观察以下字段：
- en: '`scaleTargetRef`: This is the reference to the object that is being scaled.
    In this case, it is a pointer to the Deployment of a shopping-cart microservice.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaleTargetRef`：这是被缩放的对象的引用。在这种情况下，它是指向购物车微服务的部署的指针。'
- en: '`minReplicas`: The minimum replicas in the Deployment, regardless of scaling triggers.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minReplicas`：部署中的最小副本数，不考虑缩放触发器。'
- en: '`maxReplicas`: The maximum number of replicas in the Deployment, regardless
    of scaling triggers.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxReplicas`：部署中的最大副本数，不考虑缩放触发器。'
- en: '`targetCPUUtilizationPercentage`: The goal percentage of average CPU utilization
    across all Pods in this deployment. Kubernetes will re-evaluate this metric constantly
    and increase and decrease the number of pods so that the actual average CPU utilization
    matches this target.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`targetCPUUtilizationPercentage`：部署中所有Pod的平均CPU利用率的目标百分比。Kubernetes将不断重新评估此指标，并增加和减少Pod的数量，以使实际的平均CPU利用率与此目标相匹配。'
- en: To simulate stress on our application, we will use **wrk**, because it is simple
    to configure and has a Docker container already made for us. wrk is an HTTP load-testing
    tool. It is simple to use and only has a few options; however, it will be able
    to generate large amounts of load by making requests over and over using multiple
    simultaneous HTTP connections against a specified endpoint.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟对我们的应用程序的压力，我们将使用**wrk**，因为它很容易配置，并且已经为我们制作了一个Docker容器。wrk是一个HTTP负载测试工具。它使用简单，并且只有少量选项；但是，它将能够通过使用多个同时的HTTP连接反复发出请求来生成大量负载，针对指定的端点。
- en: Note
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find out more about wrk at this link: [https://github.com/wg/wrk](https://github.com/wg/wrk).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此链接找到有关wrk的更多信息：[https://github.com/wg/wrk](https://github.com/wg/wrk)。
- en: 'For the following exercise, we will use a modified version of the application
    we''ve been running to help drive scaling behavior. In this revision of our application,
    we have modified it such that the application will perform a Fibonacci sequence
    calculation in a naïve way out to the 10,000,000th entry so that it will be slightly
    more computationally expensive and exceed our CPU autoscaling trigger. If you
    examine the source code, you can see that we have added this function:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将使用我们一直在运行的应用程序的修改版本来帮助驱动扩展行为。在我们的应用程序的这个修订版中，我们已经修改了它，使得应用程序以天真的方式执行斐波那契数列计算，直到第10,000,000个条目，以便它会稍微更加计算密集，并超过我们的CPU自动缩放触发器。如果您检查源代码，您会发现我们已经添加了这个函数：
- en: main.go
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: main.go
- en: '[PRE14]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can find the full code at this link: [https://packt.live/3h5wCEd](https://packt.live/3h5wCEd).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此链接找到完整的代码：[https://packt.live/3h5wCEd](https://packt.live/3h5wCEd)。
- en: Other than this, we will be using an Ingress, which we learned about in *Chapter
    12*, *Your Application and HA*, and the same SQL database that we built in the
    previous chapter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们将使用Ingress，我们在*第12章*中学到的，并且在上一章中构建的相同的SQL数据库。
- en: Now, with all of that said, let's dig into the implementation of these autoscalers
    in the following exercise.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，说了这么多，让我们深入研究以下练习中这些自动缩放器的实现。
- en: 'Exercise 15.02: Scaling Workloads in Kubernetes'
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习15.02：在Kubernetes中扩展工作负载
- en: 'In this exercise, we''re going to be putting together a few different pieces
    from before. Since our application has several moving parts at this point, we
    need to lay out some steps that we''re going to take so that you understand where
    we''re headed:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将整合之前的一些不同部分。由于我们的应用程序目前有几个移动部分，我们需要列出一些步骤，以便您了解我们的方向：
- en: We need to have our EKS cluster set up as we have in *Exercise 12.02*, *Creating
    a Cluster with EKS Using Terraform*.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要像*练习12.02*中一样设置我们的EKS集群，使用Terraform创建一个集群。
- en: We need to have the required components for the Kubernetes Metrics server set up.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要为Kubernetes Metrics服务器设置所需的组件。
- en: Note
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Considering these two points, you need to complete the previous exercise successfully
    to be able to perform this exercise.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这两点，您需要成功完成上一个练习才能执行这个练习。
- en: We need to install our counter application using a modification so that it will
    be a computationally intensive exercise to get the next number in a sequence.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要使用修改后的计数器应用程序进行安装，以便它成为一个计算密集型的练习，以获取序列中的下一个数字。
- en: We need to install the HPA and set a metric target for the CPU percentage.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要安装HPA并设置CPU百分比的度量目标。
- en: We need to install the ClusterAutoscaler and give it the permissions to change
    the **Autoscaling Group** (**ASG**) size in AWS.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要安装ClusterAutoscaler并赋予它在AWS中更改**Autoscaling Group**（**ASG**）大小的权限。
- en: We need to stress test our application by generating enough load to be able
    to scale the application out and cause the HPA to trigger a cluster-scaling action.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要通过生成足够的负载来对我们的应用进行压力测试，以便能够扩展应用并导致HPA触发集群扩展操作。
- en: We will use a Kubernetes Ingress resource to load test using traffic external
    to our cluster so that we can create an even more realistic simulation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Kubernetes Ingress资源来使用集群外部的流量进行负载测试，以便我们可以创建一个更真实的模拟。
- en: 'After doing this, you''ll be a Kubernetes captain, so let''s dive in:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，您将成为Kubernetes的船长，所以让我们开始吧：
- en: 'Now, let''s deploy the `ingress-nginx` setup by running the following commands
    one after the other:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过依次运行以下命令来部署`ingress-nginx`设置：
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should see the following responses:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 15.17: Deploying the nginx Ingress controller'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.17：部署nginx Ingress控制器'
- en: '](image/B14870_15_17.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_17.jpg)'
- en: 'Figure 15.17: Deploying the nginx Ingress controller'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.17：部署nginx Ingress控制器
- en: 'Now, let''s fetch the manifest for our application with HA MySQL, Ingress,
    and an HPA:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们获取具有HA MySQL、Ingress和HPA的应用程序清单：
- en: '[PRE16]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Before we apply it, let''s look at our autoscaling trigger:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用之前，让我们看一下我们的自动缩放触发器：
- en: with_autoscaler.yaml
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: with_autoscaler.yaml
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The full code can be found at this link: [https://packt.live/3bE9v28](https://packt.live/3bE9v28).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在此链接找到：[https://packt.live/3bE9v28](https://packt.live/3bE9v28)。
- en: Here, we are starting with two replicas of this deployment and allowing ourselves
    to grow up to 1000 replicas while trying to keep the CPU at a constant 10% utilization.
    Recall from our Terraform template that we are using m4.large EC2 instances to
    run these Pods.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从此部署的两个副本开始，并允许自己增长到1000个副本，同时尝试保持CPU利用率恒定在10％。回想一下，根据我们的Terraform模板，我们使用m4.large
    EC2实例来运行这些Pod。
- en: 'Let''s deploy this application by running the following command:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令来部署此应用程序：
- en: '[PRE18]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You should see the following response:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 15.18: Deploying our application'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.18：部署我们的应用程序'
- en: '](image/B14870_15_18.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_18.jpg)'
- en: 'Figure 15.18: Deploying our application'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.18：部署我们的应用程序
- en: 'With that, we are ready to load test. Before we begin, let''s check on the
    number of Pods in our deployment:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这个，我们准备进行负载测试。在开始之前，让我们检查一下部署中的Pod数量：
- en: '[PRE19]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This may take up to 5 minutes to show a percentage, after which you should
    see something like this:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要长达5分钟才能显示百分比，之后您应该看到类似于这样的东西：
- en: '![Figure 15.19: Getting details about our HPA'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.19：获取有关我们的HPA的详细信息'
- en: '](image/B14870_15_19.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_19.jpg)'
- en: 'Figure 15.19: Getting details about our HPA'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.19：获取有关我们的HPA的详细信息
- en: The `Deployment pods:` field shows `2 current / 2 desired`, meaning our HPA
    has changed the desired replica count from 3 to 2 because we have a CPU utilization
    of 0%, which is below the target of 10%.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: “Deployment pods:”字段显示“2 current / 2 desired”，这意味着我们的HPA已将期望的副本计数从3更改为2，因为我们的CPU利用率为0％，低于10％的目标。
- en: Now, we need to get some load going. We're going to run a load test from our
    computer to the cluster using wrk as a Docker container. But first, we need to
    get the Ingress endpoint to access our cluster.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要进行一些负载。我们将从我们的计算机到集群运行一个负载测试，使用wrk作为Docker容器。但首先，我们需要获取Ingress端点以访问我们的集群。
- en: 'Run the following command to first get your Ingress endpoint:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先运行以下命令以获取您的Ingress端点：
- en: '[PRE20]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should see the following response:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 15.20: Checking our Ingress endpoint'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.20：检查我们的Ingress端点'
- en: '](image/B14870_15_20.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_20.jpg)'
- en: 'Figure 15.20: Checking our Ingress endpoint'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.20：检查我们的Ingress端点
- en: 'In another terminal session, run a `wrk` load test using the following command:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个终端会话中，使用以下命令运行`wrk`负载测试：
- en: '[PRE21]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s quickly understand these parameters:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解这些参数：
- en: '`-t10`: The number of threads to use for this test, which is 10 in this case.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`-t10`：此测试要使用的线程数，在本例中为10。'
- en: '`-c1000`: The total number of connections to hold open. In this case, each
    thread is handling 1,000 connections each.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`-c1000`：要保持打开的连接总数。在本例中，每个线程处理1000个连接。'
- en: '`-d600`: The number of seconds to run this test (which in this case is 600
    seconds or 10 minutes).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`-d600`：运行此测试的秒数（在本例中为600秒或10分钟）。'
- en: 'You should get output like the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该获得以下输出：
- en: '![Figure 15.21: Running a load test to our Ingress endpoint'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.21：对我们的Ingress端点运行负载测试'
- en: '](image/B14870_15_21.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_21.jpg)'
- en: 'Figure 15.21: Running a load test to our Ingress endpoint'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.21：对我们的Ingress端点运行负载测试
- en: 'In another session, let''s keep an eye on the pods for our application:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个会话中，让我们留意我们应用程序的Pod：
- en: '[PRE22]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should be able to see a response similar to this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该能够看到类似于这样的响应：
- en: '![Figure 15.22: Watching pods backing our application'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.22：观察支持我们应用程序的Pod'
- en: '](image/B14870_15_22.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_22.jpg)'
- en: 'Figure 15.22: Watching pods backing our application'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.22：观察支持我们应用程序的Pod
- en: In this terminal window, you should see the number of Pods increasing. Note
    that we can also check the same in our Grafana dashboard.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个终端窗口中，您应该看到Pod数量的增加。请注意，我们也可以在Grafana仪表板中检查相同的情况。
- en: Here, it is increased by 1; but soon, these pods will exceed all the available
    space.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，它增加了1；但很快，这些Pod将超出所有可用空间。
- en: 'In yet another terminal session, you can again set up port forwarding to Grafana
    to observe the dashboard:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个终端会话中，您可以再次设置端口转发到Grafana以观察仪表板：
- en: '[PRE23]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You should see the following response:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE24]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, access the dashboard on your browser at `localhost:3000`:![Figure 15.23:
    Observing our cluster in the Grafana dashboard'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在浏览器上访问`localhost:3000`上的仪表板：![图15.23：在Grafana仪表板中观察我们的集群
- en: '](image/B14870_15_23.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_23.jpg)'
- en: 'Figure 15.23: Observing our cluster in the Grafana dashboard'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.23：在Grafana仪表板中观察我们的集群
- en: You should be able to see the number of Pods increasing here as well. Thus,
    we have successfully deployed an HPA that is automatically scaling up the number
    of Pods as the load on our application increases.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应该能够在这里看到Pod数量的增加。因此，我们已成功部署了一个自动扩展Pod数量的HPA，随着应用程序负载的增加而增加Pod数量。
- en: ClusterAutoscaler
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClusterAutoscaler
- en: If the HPA ensures that there are always the right number of Pods running in
    a Deployment, then what happens when we run out of capacity on the cluster for
    all of those Pods? We need more of them, but we also don't want to be paying for
    that additional cluster capacity when we don't need it. This is where the ClusterAutoscaler
    comes in.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果HPA确保部署中始终有正确数量的Pod在运行，那么当我们的集群对所有这些Pod的容量用完时会发生什么？我们需要更多的Pod，但我们也不希望在不需要时为这些额外的集群容量付费。这就是ClusterAutoscaler的作用。
- en: 'The ClusterAutoscaler will work inside your cluster to ensure that the number
    of nodes running in the ASG (in the case of AWS) always has enough capacity to
    run the currently deployed application components of your cluster. So, if 10 pods
    in a Deployment can fit on 2 nodes, then when you need an 11th Pod, the ClusterAutoscaler
    will ask AWS to add a 3rd node to your Kubernetes cluster to get that Pod scheduled.
    When that Pod is no longer needed, that Node goes away, too. Let''s look at a
    brief architecture diagram to understand how the ClusterAutoscaler works:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterAutoscaler将在您的集群内工作，以确保在ASG（在AWS的情况下）中运行的节点数量始终具有足够的容量来运行集群当前部署的应用程序组件。因此，如果部署中的10个Pod可以放在2个节点上，那么当您需要第11个Pod时，ClusterAutoscaler将要求AWS向您的Kubernetes集群添加第3个节点以安排该Pod。当不再需要该Pod时，该节点也会消失。让我们看一个简短的架构图，以了解ClusterAutoscaler的工作原理：
- en: '![Figure 15.24: Cluster with nodes at full capacity'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.24：节点满负荷的集群'
- en: '](image/B14870_15_24.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_24.jpg)'
- en: 'Figure 15.24: Cluster with nodes at full capacity'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.24：节点满负荷的集群
- en: Note that in this example, we have an EKS cluster running two worker nodes and
    all available cluster resources are taken up. So, here's what the ClusterAutoscaler
    does.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在此示例中，我们运行了两个工作节点的EKS集群，并且所有可用的集群资源都已被占用。因此，ClusterAutoscaler的作用如下。
- en: When a request for a Pod that won't fit arrives at the control plane, it remains
    in a `Pending` state. When the ClusterAutoscaler observes this, it will communicate
    with the AWS EC2 API and request for the ASG, which has our worker nodes deployed
    in them, to scale up by another node. This requires the ClusterAutoscaler to be
    able to communicate with the API for the cloud provider it is running in in order
    to change worker node count. In the case of AWS, this also means that we will
    either have to generate IAM credentials for the ClusterAutoscaler or allow it
    to use the IAM role of the machine to access the AWS APIs.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当控制平面收到一个无法容纳的Pod的请求时，它将保持在`Pending`状态。当ClusterAutoscaler观察到这一点时，它将与AWS EC2
    API通信，并请求ASG进行扩展，其中我们的工作节点部署在其中。这需要ClusterAutoscaler能够与云提供商的API通信，以便更改工作节点计数。在AWS的情况下，这还意味着我们要么必须为ClusterAutoscaler生成IAM凭据，要么允许它使用机器的IAM角色来访问AWS
    API。
- en: 'A successful scaling action should look like the following:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的扩展操作应该如下所示：
- en: '![Figure 15.25: Additional node provisioned to run the additional pods'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.25：额外的节点用于运行额外的Pod'
- en: '](image/B14870_15_25.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_25.jpg)'
- en: 'Figure 15.25: Additional node provisioned to run the additional pods'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.25：额外的节点用于运行额外的Pod
- en: We will implement the ClusterAutoscaler in the following exercise, and then
    load test it in the activity after that.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一个练习中实现ClusterAutoscaler，然后在随后的活动中进行负载测试。
- en: 'Exercise 15.03: Configuring the ClusterAutoscaler'
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习15.03：配置ClusterAutoscaler
- en: 'So, now that we''ve seen our Kubernetes Deployment scale, it''s time to see
    it scale to the point where it needs to add more node capacity to the cluster.
    We will be continuing where the last lesson left off and run the exact same application
    and load test but let it run a little longer:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们已经看到我们的Kubernetes部署扩展，现在是时候看它扩展到需要向集群添加更多节点容量的地步了。我们将继续上一课的内容，并运行完全相同的应用程序和负载测试，但让它运行更长一点：
- en: 'To create a ClusterAutoscaler, first, we need to create an AWS IAM account
    and give it the permissions to manage our ASGs. Create a file called `permissions.json`
    with the following contents:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建ClusterAutoscaler，首先，我们需要创建一个AWS IAM账户，并赋予它管理我们ASG的权限。创建一个名为`permissions.json`的文件，内容如下：
- en: '[PRE25]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, let''s run the following command to create an AWS IAM policy:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们运行以下命令来创建一个AWS IAM策略：
- en: '[PRE26]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You should see the following response:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 15.26: Creating an AWS IAM policy'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.26：创建一个AWS IAM策略'
- en: '](image/B14870_15_26.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_26.jpg)'
- en: 'Figure 15.26: Creating an AWS IAM policy'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.26：创建一个AWS IAM策略
- en: Note down the value of the `Arn:` field from the output that you get.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 记下您获得的输出中`Arn:`字段的值。
- en: 'Now, we need to create an IAM user and then attach a policy to it. First, let''s
    create the user:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个IAM用户，然后将策略附加到它。首先，让我们创建用户：
- en: '[PRE27]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should see this response:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 15.27: Creating an IAM user to use our policy'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.27：创建一个IAM用户来使用我们的策略'
- en: '](image/B14870_15_27.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_27.jpg)'
- en: 'Figure 15.27: Creating an IAM user to use our policy'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.27：创建一个IAM用户来使用我们的策略
- en: 'Now, let''s attach the IAM policy to the user:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将IAM策略附加到用户：
- en: '[PRE28]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Use the ARN value that you obtained in *step 2*.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您在*步骤2*中获得的ARN值。
- en: 'Now, we need the secret access key for this IAM user. Run the following command:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要这个IAM用户的秘密访问密钥。运行以下命令：
- en: '[PRE29]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You should get this response:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到以下响应：
- en: '![Figure 15.28: Fetching the secret access key for the created IAM user'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.28：获取创建的IAM用户的秘密访问密钥'
- en: '](image/B14870_15_28.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_28.jpg)'
- en: 'Figure 15.28: Fetching the secret access key for the created IAM user'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.28：获取创建的IAM用户的秘密访问密钥
- en: In the output of this command, note `AccessKeyId` and `SecretAccessKey`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在此命令的输出中，请注意`AccessKeyId`和`SecretAccessKey`。
- en: 'Now, get the manifest file for ClusterAutoscaler that we have provided:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，获取我们提供的ClusterAutoscaler的清单文件：
- en: '[PRE30]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We need to create a Kubernetes Secret to expose these credentials to the ClusterAutoscaler.
    Open the `cluster_autoscaler.yaml` file. In the first entry, you should see the
    following:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要创建一个Kubernetes Secret来将这些凭据暴露给ClusterAutoscaler。打开`cluster_autoscaler.yaml`文件。在第一个条目中，您应该看到以下内容：
- en: cluster_autoscaler.yaml
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: cluster_autoscaler.yaml
- en: '[PRE31]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You can find the full code at this link: [https://packt.live/2DCDfzZ](https://packt.live/2DCDfzZ).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此链接找到完整的代码：[https://packt.live/2DCDfzZ](https://packt.live/2DCDfzZ)。
- en: You need to replace `YOUR_AWS_ACCESS_KEY_ID` and `YOUR_AWS_SECRET_ACCESS_KEY`
    with the Base64-encoded versions of the values returned by AWS in *step 5*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要使用AWS在*步骤5*中返回的值的Base64编码版本替换`YOUR_AWS_ACCESS_KEY_ID`和`YOUR_AWS_SECRET_ACCESS_KEY`。
- en: 'To encode in Base64 format, run the following command:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要以Base64格式编码，运行以下命令：
- en: '[PRE32]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Run this twice, using `AccessKeyID` and `SecretAccessKey` in place of `<YOUR_VALUE>`
    to get the corresponding Base64-encoded version that you need to enter into the
    secret fields. Here''s what it should look like when complete:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 运行两次，使用`AccessKeyID`和`SecretAccessKey`替换`<YOUR_VALUE>`，以获取相应的Base64编码版本，然后将其输入到secret字段中。完成后应如下所示：
- en: '[PRE33]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, in the same `cluster_autoscaler.yaml` file, go to line 188\. You will
    need to replace the value of `YOUR_AWS_REGION` with the value of the region you
    deployed your EKS cluster into, such as `us-east-1`:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在相同的`cluster_autoscaler.yaml`文件中，转到第188行。您需要将`YOUR_AWS_REGION`的值替换为您部署EKS集群的区域的值，例如`us-east-1`：
- en: cluster_autoscaler.yaml
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放器.yaml
- en: '[PRE34]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can find the entire code at this link: [https://packt.live/2F8erkb](https://packt.live/2F8erkb).'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此链接找到整个代码：[https://packt.live/2F8erkb](https://packt.live/2F8erkb)。
- en: 'Now, apply this file by running the following:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过运行以下命令应用此文件：
- en: '[PRE35]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You should see a response similar to the following:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似以下的响应：
- en: '![Figure 15.29: Deploying our ClusterAutoscaler'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.29：部署我们的ClusterAutoscaler'
- en: '](image/B14870_15_29.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_29.jpg)'
- en: 'Figure 15.29: Deploying our ClusterAutoscaler'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.29：部署我们的ClusterAutoscaler
- en: 'Note that we need to now modify our ASG in AWS to allow for a scale-up; otherwise,
    the ClusterAutoscaler will not attempt to add any nodes. To do this, we have provided
    a modified `main.tf` file that has only one line changed: `max_size = 5` (*line
    299*). This will allow the cluster to add up a maximum of five EC2 nodes to itself.'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，我们现在需要修改AWS中的ASG以允许扩展；否则，ClusterAutoscaler将不会尝试添加任何节点。为此，我们提供了一个修改过的`main.tf`文件，只更改了一行：`max_size
    = 5`（*第299行*）。这将允许集群最多添加五个EC2节点到自身。
- en: 'Navigate to the same location where you downloaded the previous Terraform file,
    and then run the following command:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到您下载了先前Terraform文件的相同位置，然后运行以下命令：
- en: '[PRE36]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You should see this response:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 15.30: Downloading the modified Terraform file'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.30：下载修改后的Terraform文件'
- en: '](image/B14870_15_30.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_30.jpg)'
- en: 'Figure 15.30: Downloading the modified Terraform file'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.30：下载修改后的Terraform文件
- en: 'Now, apply the modifications to the Terraform file:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将修改应用到Terraform文件：
- en: '[PRE37]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Verify that the changes are only applied to the ASG max capacity, and then
    type `yes` when prompted:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 验证更改仅应用于ASG的最大容量，然后在提示时输入`yes`：
- en: '![Figure 15.31: Applying our Terraform modifications'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.31：应用我们的Terraform修改'
- en: '](image/B14870_15_31.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_31.jpg)'
- en: 'Figure 15.31: Applying our Terraform modifications'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.31：应用我们的Terraform修改
- en: Note
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will test this ClusterAutoscaler in the following activity. Hence, do not
    delete your cluster and API resources for now.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下活动中测试此ClusterAutoscaler。因此，现在不要删除您的集群和API资源。
- en: At this point, we have deployed our ClusterAutoscaler and configured it to access
    the AWS API. Thus, we should be able to scale the number of nodes as required.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经部署了我们的ClusterAutoscaler，并配置它以访问AWS API。因此，我们应该能够根据需要扩展节点的数量。
- en: Let's proceed to the following activity, where we will load test our cluster.
    You should plan to do this activity as soon as possible in order to keep costs
    down.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行下一个活动，在那里我们将对我们的集群进行负载测试。您应该尽快进行此活动，以便降低成本。
- en: 'Activity 15.01: Autoscaling Our Cluster Using ClusterAutoscaler'
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动15.01：使用ClusterAutoscaler对我们的集群进行自动扩展
- en: In this activity, we are going to run another load test and this time, we are
    going to run it for longer and observe the changes to the infrastructure as the
    cluster expands to meet demands. This activity should repeat the previous steps
    (as shown in *Exercise 15.02, Scaling Workloads in Kubernetes*) to run the load
    test but this time, it should be done with the ClusterAutoscaler installed so
    that when your cluster runs out of capacity for the Pods, it will scale the number
    of nodes to fit the new Pods. The goal of this is to see a load test increase
    the node count.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将运行另一个负载测试，这一次，我们将运行更长时间，并观察基础架构的变化，因为集群扩展以满足需求。这个活动应该重复之前的步骤（如*练习15.02，在Kubernetes中扩展工作负载*中所示）来运行负载测试，但这一次，应该安装ClusterAutoscaler，这样当您的集群对Pod的容量不足时，它将扩展节点的数量以适应新的Pod。这样做的目的是看到负载测试增加节点数量。
- en: 'Follow these guidelines to complete your activity:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下指南完成您的活动：
- en: We will use the Grafana dashboard to observe the cluster metrics, paying close
    attention to the number of running Pods and the number of nodes.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用Grafana仪表板来观察集群指标，特别关注运行中的Pod数量和节点数量。
- en: Our HPA should be set up so that when our application receives more load, we
    can scale the number of Pods to meet the demand.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的HPA应该设置好，这样当我们的应用程序接收到更多负载时，我们可以扩展Pod的数量以满足需求。
- en: Ensure that your ClusterAutoscaler has been successfully set up.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您的ClusterAutoscaler已成功设置。
- en: Note
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To fulfill the three aforementioned requirements, you will need to have successfully
    completed all the exercises in this chapter. We will be using the resources created
    in those exercises.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足前面提到的三个要求，您需要成功完成本章中的所有练习。我们将使用在这些练习中创建的资源。
- en: Run a load test, as shown in *step 2* of *Exercise 15.02*. You may choose a
    longer or more intense test if you wish.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行负载测试，如*练习15.02*的*步骤2*所示。如果愿意，您可以选择更长或更强烈的测试。
- en: 'By the end of this activity, you should be able to observe an increase in the
    number of nodes by describing the AWS ASG like so:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成此活动时，您应该能够通过描述AWS ASG来观察节点数量的增加，如下所示：
- en: '![Figure 15.32: Increase in the number of nodes observed'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.32：观察到节点数量的增加'
- en: by describing the AWS scaling group
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 通过描述AWS扩展组
- en: '](image/B14870_15_32.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_32.jpg)'
- en: 'Figure 15.32: Increase in the number of nodes observed by describing the AWS
    scaling group'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.32：通过描述AWS扩展组观察节点数量的增加
- en: 'You should also be able to observe the same in your Grafana dashboard:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应该能够在Grafana仪表板中观察到相同的情况：
- en: '![Figure 15.33: Increase in the number of nodes observed in the Grafana dashboard'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '![图15.33：在Grafana仪表板中观察到节点数量的增加'
- en: '](image/B14870_15_33.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_15_33.jpg)'
- en: 'Figure 15.33: Increase in the number of nodes observed in the Grafana dashboard'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.33：在Grafana仪表板中观察到节点数量的增加
- en: Note
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The solution to this activity can be found at the following address: [https://packt.live/304PEoD](https://packt.live/304PEoD).
    Make sure you delete the EKS cluster by running the command `terraform destroy`.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可在以下地址找到：[https://packt.live/304PEoD](https://packt.live/304PEoD)。确保通过运行命令`terraform
    destroy`删除EKS集群。
- en: Deleting Your Cluster Resources
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除您的集群资源
- en: 'This is the last chapter where we will use our EKS cluster. Hence, we recommend
    that you delete your cluster resources using the following command:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将使用EKS集群的最后一章。因此，我们建议您使用以下命令删除您的集群资源：
- en: '[PRE38]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This should stop the billing for the EKS cluster that we created using Terraform.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该停止使用Terraform创建的EKS集群的计费。
- en: Summary
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Let's reflect a bit on how far we've come from *Chapter 11*, *Build Your Own
    HA Cluster*, when we started to talk about running Kubernetes in a highly available
    manner. We covered how to set up a production cluster that was secure in the cloud
    and created using infrastructure as code tools such as Terraform, as well as secured
    the workloads that it runs. We also looked at necessary modifications to our applications
    in order to scale them well—both for the stateful and stateless versions of the application.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微回顾一下我们从*第11章*《构建自己的HA集群》开始讨论如何以高可用的方式运行Kubernetes所走过的路程。我们讨论了如何设置一个安全的生产集群，并使用Terraform等基础设施即代码工具创建，以及保护其运行的工作负载。我们还研究了必要的修改，以便良好地扩展我们的应用程序——无论是有状态的还是无状态的版本。
- en: Then, in this chapter, we looked at how we can extend the management of our
    application runtimes using data specifically when introducing Prometheus, Grafana,
    and the Kubernetes Metrics server. We then used that information to leverage the
    HPA and the ClusterAutoscaler so that we can rest assured that our cluster is
    always appropriately sized and ready to respond to spikes in demand automatically
    without having to pay for hardware that is overprovisioned.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在本章中，我们看了如何使用数据来扩展我们的应用程序运行时，特别是在引入Prometheus、Grafana和Kubernetes Metrics服务器时。然后，我们利用这些信息来利用HPA和ClusterAutoscaler，以便我们可以放心地确保我们的集群始终具有适当的大小，并且可以自动响应需求的激增，而无需支付过度配置的硬件。
- en: In the following series of chapters, we will explore some advanced concepts
    in Kubernetes, starting with admission controllers in the next chapter.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的一系列章节中，我们将探索Kubernetes中的一些高级概念，从下一章开始介绍准入控制器。
