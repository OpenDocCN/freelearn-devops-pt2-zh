- en: 11\. Build Your Own HA Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11. 构建您自己的HA集群
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will learn how Kubernetes enables us to deploy infrastructure
    with remarkable resilience and how to set up a high-availability Kubernetes cluster
    in the AWS cloud. This chapter will help you understand what enables Kubernetes
    to be used for highly available deployments and, in turn, enable you to make the
    right choices while architecting a production environment for your use case. By
    the end of the chapter, you will be able to set up a suitable cluster infrastructure
    on AWS to support your **highly available** (**HA**) Kubernetes cluster. You will
    also be able to deploy an application in a production environment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习Kubernetes如何使我们能够部署具有显著弹性的基础设施，以及如何在AWS云中设置一个高可用性的Kubernetes集群。本章将帮助您了解是什么使Kubernetes能够用于高可用性部署，并帮助您在为您的用例设计生产环境时做出正确的选择。在本章结束时，您将能够在AWS上设置一个适当的集群基础设施，以支持您的高可用性（HA）Kubernetes集群。您还将能够在生产环境中部署应用程序。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapters, you learned about application containerization, how
    Kubernetes works, and some of the "proper nouns" or "objects" in Kubernetes that
    allow you to create a declarative-style application architecture that Kubernetes
    will execute on your behalf.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，您了解了应用程序容器化、Kubernetes的工作原理，以及Kubernetes中的一些“专有名词”或“对象”，这些对象允许您创建一种声明式的应用程序架构，Kubernetes将代表您执行。
- en: Software and hardware instability are a reality in all environments. As applications
    need higher and higher availability, shortcomings in the infrastructure become
    more obvious. Kubernetes was purpose-built to help solve this challenge for containerized
    applications. But what about Kubernetes itself? As cluster operators, do we shift
    from watching our individual servers like hawks to watching our single Kubernetes
    control infrastructure?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 软件和硬件的不稳定在所有环境中都是现实。随着应用程序对更高可用性的需求越来越高，基础设施的缺陷变得更加明显。Kubernetes是专门为帮助解决容器化应用程序的这一挑战而构建的。但是Kubernetes本身呢？作为集群操作员，我们是不是要从像鹰一样监视我们的单个服务器，转而监视我们的单个Kubernetes控制基础设施呢？
- en: 'As it turns out, this aspect was one of the design considerations for Kubernetes.
    One of the design goals of Kubernetes is to be able to withstand instability in
    its own infrastructure. This means that when set up properly, the Kubernetes control
    plane could withstand quite a few disasters, including:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这一方面是Kubernetes设计考虑的一个方面。Kubernetes的设计目标之一是能够经受住其自身基础设施的不稳定性。这意味着当正确设置时，Kubernetes控制平面可以经受相当多的灾难，包括：
- en: Network splits/partitions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络分裂/分区
- en: Control plane (master) server failure
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制平面（主节点）服务器故障
- en: Data corruption in etcd
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd中的数据损坏
- en: Many other less severe events that impact availability events
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多其他影响可用性的不太严重的事件
- en: Not only can Kubernetes help your application tolerate failure, but you can
    rest easy at night knowing that Kubernetes can also tolerate failures in its own
    control infrastructure. In this chapter, we are going to build a cluster of our
    very own and make sure that it is highly available. High availability implies
    that the system is very reliable and almost always available. This does not mean
    that everything in it always works perfectly; it just means that whenever the
    user or client wants something, the architecture stipulates that the API server
    should be **available** to do the job. This means that we have to design a system
    for our applications to automatically respond to and take corrective measures
    in response to any faults.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅可以 Kubernetes 帮助您的应用程序容忍故障，而且您可以放心，因为 Kubernetes 也可以容忍其自身控制基础设施的故障。在本章中，我们将建立一个属于我们自己的集群，并确保它具有高可用性。高可用性意味着系统非常可靠，几乎总是可用的。这并不意味着其中的一切总是完美运行；它只意味着每当用户或客户端需要某些东西时，架构规定
    API 服务器应该“可用”来完成工作。这意味着我们必须为我们的应用程序设计一个系统，以自动响应并对任何故障采取纠正措施。
- en: In this chapter, we will look at how Kubernetes integrates such measures to
    tolerate faults in its own control architecture. Then, you will have the chance
    to extend this concept a bit further by designing your application to take advantage
    of this horizontally scalable, fault-tolerant architecture. But first, let's look
    at how the different cogs in the machine turn together to enable it to be highly
    available.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看看 Kubernetes 如何整合这些措施来容忍其自身控制架构中的故障。然后，您将有机会进一步扩展这个概念，通过设计您的应用程序来利用这种横向可扩展、容错的架构。但首先，让我们看看机器中不同齿轮如何一起转动，使其具有高可用性。
- en: How the Components of Kubernetes Work Together to Achieve High Availability
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 组件如何一起实现高可用性
- en: You have learned in *Chapter 2*, *An Overview of Kubernetes*, how the pieces
    of Kubernetes work together to provide a runtime for your application containers.
    But we need to investigate deeper how these components work together to achieve
    high availability. To do that, we'll start with the memory bank of Kubernetes,
    otherwise known as etcd.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在《第2章》《Kubernetes 概述》中学到了 Kubernetes 的各个部分是如何一起工作，为您的应用程序容器提供运行时的。但我们需要更深入地研究这些组件如何一起实现高可用性。为了做到这一点，我们将从
    Kubernetes 的内存库，也就是 etcd 开始。
- en: etcd
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: etcd
- en: As you have learned in earlier chapters, etcd is the place where all Kubernetes
    configuration is stored. This makes it arguably the single most important component
    of the cluster since changes in etcd affect the state of everything. More specifically,
    any change to a key-value pair in etcd will cause the other components of Kubernetes
    to react to this change, which could mean disruptions to your application. In
    order to achieve high availability for Kubernetes, it is wise to have more than
    one etcd node.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在之前的章节中学到的，etcd 是存储所有 Kubernetes 配置的地方。这使得它可以说是集群中最重要的组件，因为 etcd 中的更改会影响一切的状态。更具体地说，对
    etcd 中的键值对的任何更改都会导致 Kubernetes 的其他组件对此更改做出反应，这可能会导致对您的应用程序的中断。为了实现 Kubernetes
    的高可用性，最好有多个 etcd 节点。
- en: But many more challenges arise when you add multiple nodes to an eventually
    consistent datastore like etcd. Do you have to write to every node to persist
    a change of state? How does replication work? Do we read from just one node or
    as many as are available? How does it handle networking failures and partitions?
    Who is the master of the cluster and how does leader election work? The short
    answer is that, by design, etcd makes these challenges either non-existent or
    easy to deal with. etcd uses a consensus algorithm called **Raft** to achieve
    replication and fault tolerance in relation to many of the aforementioned issues.
    Thus, if we're building a Kubernetes HA cluster, we need to make sure that we
    set up multiple nodes (preferably an odd number to make leader election tie-breaking
    easier) of an etcd cluster properly, and we can rely on that from there.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当您将多个节点添加到像etcd这样的最终一致性数据存储中时，会出现更多的挑战。您是否必须向每个节点写入以保持状态的更改？复制是如何工作的？我们是从一个节点读取还是尽可能多地读取？它如何处理网络故障和分区？谁是集群的主节点，领导者选举是如何工作的？简短的答案是，通过设计，etcd使这些挑战要么不存在，要么易于处理。etcd使用一种称为**Raft**的共识算法来实现复制和容错，以解决上述许多问题。因此，如果我们正在构建一个Kubernetes高可用性集群，我们需要确保正确设置多个节点（最好是奇数，以便更容易进行领导者选举）的etcd集群，并且我们可以依靠它。
- en: Note
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Leader election in etcd is a process where multiple instances of the database
    software collectively vote on which host will be an authority for dealing with
    any issues that arise in achieving database consensus. For more details, refer
    to this link: [https://raft.github.io/](https://raft.github.io/)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: etcd中的领导者选举是一个过程，数据库软件的多个实例共同投票，决定哪个主机将成为处理实现数据库一致性所需的任何问题的权威。有关更多详细信息，请参阅此链接：[https://raft.github.io/](https://raft.github.io/)
- en: Networking and DNS
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络和DNS
- en: Many of the applications that run on Kubernetes require some form of network
    to be useful. Therefore, networking is an important consideration when designing
    a topology for your clusters. For example, your network should be able to support
    all of the protocols that your application uses, including the ones for Kubernetes.
    Kubernetes itself uses TCP for all of its communication between masters, nodes,
    and etcd, and it uses UDP for internal domain name resolution, which is otherwise
    known as service discovery. Your network should also be provisioned to have at
    least as many IP addresses as the number of nodes that you plan to have in the
    cluster. For example, if you planned to have more than 256 machines (nodes) in
    your cluster, you probably shouldn't use an IP CIDR address space of /24 or higher
    since that only has 255 or fewer available IP addresses.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 许多在Kubernetes上运行的应用程序都需要某种形式的网络才能发挥作用。因此，在为您的集群设计拓扑时，网络是一个重要考虑因素。例如，您的网络应该能够支持应用程序使用的所有协议，包括Kubernetes使用的协议。Kubernetes本身在主节点、节点和etcd之间的所有通信都使用TCP，它还使用UDP进行内部域名解析，也就是服务发现。您的网络还应该配置为至少具有与您计划在集群中拥有的节点数量一样多的IP地址。例如，如果您计划在集群中拥有超过256台机器（节点），那么您可能不应该使用/24或更高的IP
    CIDR地址空间，因为这样只有255个或更少的可用IP地址。
- en: Later in this workshop, we will talk about the security decisions you will need
    to make as a cluster operator. However, in this section, we will not discuss them
    because they do not directly relate to Kubernetes' ability to achieve high availability.
    We will deal with the security of Kubernetes in *Chapter 13*, *Runtime and Network
    Security in Kubernetes*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次研讨会的后续部分，我们将讨论作为集群操作员需要做出的安全决策。然而，在本节中，我们不会讨论这些问题，因为它们与Kubernetes实现高可用性的能力没有直接关系。我们将在
    *第13章* *Kubernetes中的运行时和网络安全* 中处理Kubernetes的安全性。
- en: One final thing to take into consideration about the network where your master
    and worker nodes will run is that every master node should be able to communicate
    with every worker node. The reason this is important is that each master node
    communicates with the Kubelet process running on the worker node in order to determine
    the state of the full cluster.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要考虑的一件事是你的主节点和工作节点所在的网络，即每个主节点都应该能够与每个工作节点通信。这一点很重要，因为每个主节点都要与工作节点上运行的Kubelet进程通信，以确定整个集群的状态。
- en: Nodes' and Master Servers' Locations and Resources
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点和主服务器的位置和资源
- en: Because of the design of etcd's Raft algorithm, which allows distributed consensus
    to happen in the key-value store of Kubernetes, we are able to run multiple master
    nodes, each of which is capable of controlling the entire cluster without the
    fear of them behaving independently from each other (in other words, going rogue).
    As a reminder of why master nodes being out of sync is a problem in Kubernetes,
    consider that the runtime of your application is being controlled by commands
    that Kubernetes issues on your behalf. If those commands conflict with each other
    because of state sync problems between master nodes, then your application runtime
    will suffer as a result. By introducing multiple master nodes, we again provide
    resistance to faults and network partitions that could potentially sacrifice the
    availability of the cluster.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于etcd的Raft算法的设计，它允许Kubernetes的键值存储中发生分布式一致性，我们能够运行多个主节点，每个主节点都能够控制整个集群，而不必担心它们会独立行动（换句话说，变得不受控制）。提醒一下，主节点不同步在Kubernetes中是一个问题，考虑到你的应用程序的运行时是由Kubernetes代表你发出的命令来控制的。如果由于主节点之间的状态同步问题而导致这些命令发生冲突，那么你的应用程序运行时将受到影响。通过引入多个主节点，我们再次提供了对可能危及集群可用性的故障和网络分区的抵抗力。
- en: Kubernetes is actually able to run in a "headless" mode. This means whatever
    instructions the Kubelets (worker nodes) have last received from the master nodes
    will continue to be carried out until communication with the master nodes can
    be re-established. In theory, this means an application that was deployed on Kubernetes
    could run indefinitely, even if the entire control plane (all master nodes) went
    down and nothing else changed on the worker nodes where the Pods running the application
    were scheduled. Obviously, this is a worst-case scenario for the availability
    of a cluster, but it is reassuring to know that, even in the worst case, applications
    don't necessarily have to suffer downtime.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes实际上能够以“无头”模式运行。这意味着Kubelets（工作节点）最后从主节点接收的任何指令都将继续执行，直到可以重新与主节点通信。理论上，这意味着部署在Kubernetes上的应用程序可以无限期地运行，即使整个控制平面（所有主节点）崩溃，应用程序所在的工作节点上的Pods没有发生任何变化。显然，这是集群可用性的最坏情况，但令人放心的是，即使在最坏的情况下，应用程序不一定会遭受停机时间。
- en: 'When you are planning the design and capacity for a high-availability deployment
    of Kubernetes, it is important to know a few things about the design of your network,
    which we discussed previously. For example, if you are running a cluster in a
    popular cloud provider, they likely have a concept of "availability zones". A
    similar concept for data center environments would be physically isolated data
    centers. If possible, there should be at least one master node and multiple worker
    nodes per availability zone. This is important because, in the event of an availability
    zone (data center) outage, your cluster is still able to operate within the remaining
    availability zones. This is illustrated in the following diagrams:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当您计划设计和容量高可用性部署Kubernetes时，重要的是要了解一些关于您的网络设计的事情，我们之前讨论过。例如，如果您在流行的云提供商中运行集群，它们可能有“可用区”的概念。数据中心环境的类似概念可能是物理隔离的数据中心。如果可能的话，每个可用区应至少有一个主节点和多个工作节点。这很重要，因为在可用区（数据中心）停机的情况下，您的集群仍然能够在剩余的可用区内运行。这在以下图表中有所说明：
- en: '![Figure 11.1: The cluster before the outage of an availability zone'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.1：可用区停机前的集群'
- en: '](image/B14870_11_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_01.jpg)'
- en: 'Figure 11.1: The cluster before the outage of an availability zone'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：可用区停机前的集群
- en: 'Let''s assume that there is a total outage of Availability Zone – C, or at
    least we are no longer able to communicate with any servers that are running inside
    it. Here is how the cluster now behaves:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设可用区C完全停机，或者至少我们不再能够与其中运行的任何服务器进行通信。现在集群的行为如下：
- en: '![Figure 11.2: The cluster following the outage of an availability zone'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.2：可用区停机后的集群'
- en: '](image/B14870_11_02.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_02.jpg)'
- en: 'Figure 11.2: The cluster following the outage of an availability zone'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：可用区停机后的集群
- en: As you can see in the diagram, Kubernetes can still execute. Additionally, if
    the loss of the nodes running in Availability Zone - C causes an application to
    no longer be in its desired state, which is dictated by the application's Kubernetes
    manifest, the remaining master nodes will work to schedule the interrupted workload
    on the remaining worker nodes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在图表中所看到的，Kubernetes仍然可以执行。此外，如果在可用区C中运行的节点的丢失导致应用程序不再处于其期望的状态，这是由应用程序的Kubernetes清单所决定的，剩余的主节点将工作以在剩余的工作节点上安排中断的工作负载。
- en: Note
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Depending on the number of worker nodes in your Kubernetes cluster, you may
    have to plan for additional resource constraints because of the amount of CPU
    power needed to run a master connected to several worker nodes. You can use the
    chart at this link to determine the resource requirements of the master nodes
    you should deploy for controlling your cluster: [https://kubernetes.io/docs/setup/best-practices/cluster-large/](https://kubernetes.io/docs/setup/best-practices/cluster-large/)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的Kubernetes集群中工作节点的数量，您可能需要计划额外的资源约束，因为运行连接到多个工作节点的主节点所需的CPU功率。您可以使用此链接中的图表来确定应该部署用于控制您的集群的主节点的资源要求：[https://kubernetes.io/docs/setup/best-practices/cluster-large/](https://kubernetes.io/docs/setup/best-practices/cluster-large/)
- en: Container Network Interface and Cluster DNS
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器网络接口和集群DNS
- en: The next decision you need to make with respect to your cluster is how the containers
    themselves communicate across each of the nodes. Kubernetes itself has a container
    network interface called **kubenet**, which is what we will use in this chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 关于您的集群，您需要做出的下一个决定是容器本身如何在每个节点之间进行通信。Kubernetes本身有一个容器网络接口称为**kubenet**，这是我们在本章中将使用的。
- en: For smaller deployments and simple operations, kubenet more than exceeds the
    needs of those clusters from a **Container Network Interface** (**CNI**) perspective.
    However, it does not work for every workload and network topology. So, Kubernetes
    provides support for several different CNIs. When considering container network
    interfaces from a high-availability perspective, you will want the most performant
    and stable option possible. It is beyond the scope of this introduction to Kubernetes
    to discuss each of the CNI offerings at length.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的部署和简单的操作，从容器网络接口（CNI）的角度来看，kubenet已经超出了这些集群的需求。然而，它并不适用于每种工作负载和网络拓扑。因此，Kubernetes提供了对几种不同CNI的支持。在考虑容器网络接口的高可用性时，您会希望选择性能最佳且稳定的选项。本文介绍Kubernetes的范围超出了讨论每种CNI提供的内容。
- en: Note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you plan to use a managed Kubernetes service provider or plan to have a
    more complex network topology such as multiple subnets inside a single VPC, kubenet
    will not work for you. In this case, you will have to pick one of the more advanced
    options. More information on selecting the right CNI for your environment can
    be found here: [https://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/](https://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划使用托管的Kubernetes服务提供商或计划拥有更复杂的网络拓扑，比如单个VPC内的多个子网，kubenet将无法满足您的需求。在这种情况下，您将不得不选择更高级的选项。有关选择适合您环境的正确CNI的更多信息，请参阅此处：[https://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/](https://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/)
- en: Container Runtime Interfaces
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器运行时接口
- en: One of the final decisions you will have to make is how your containers will
    run on your worker nodes. The Kubernetes default for this is the Docker container
    runtime interface, and Kubernetes was initially built to work with Docker. Since
    then, however, open standards have been developed and other container runtime
    interfaces are now compatible with the Kubernetes API. Generally, cluster operators
    tend to stick with Docker because it is extremely well established. Even if you
    want to explore alternatives, keep in mind when designing a topology capable of
    maintaining high availability for your workloads and Kubernetes that you'll probably
    want to go with more established and stable options like Docker.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您将不得不做出的最终决定之一是您的容器将如何在工作节点上运行。Kubernetes的默认选择是Docker容器运行时接口，最初Kubernetes是为了与Docker配合而构建的。然而，自那时以来，已经开发了开放标准，其他容器运行时接口现在与Kubernetes
    API兼容。一般来说，集群操作员倾向于坚持使用Docker，因为它非常成熟。即使您想探索其他选择，也请记住，在设计能够维持工作负载和Kubernetes高可用性的拓扑时，您可能会选择更成熟和稳定的选项，比如Docker。
- en: Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find some of the other container runtime interfaces that are compatible
    with Kubernetes on this page: [https://kubernetes.io/docs/setup/production-environment/container-runtimes/](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此页面找到与Kubernetes兼容的其他一些容器运行时接口：[https://kubernetes.io/docs/setup/production-environment/container-runtimes/](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)
- en: Container Storage Interfaces
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器存储接口
- en: 'Recent versions of Kubernetes have introduced improved ways of interacting
    with the persistence tools that are available in data centers and cloud providers
    such as storage arrays and blob storage. The most important improvement has been
    the introduction and standardization of the container storage interface for managing
    `StorageClass`, `PersistentVolume`, and `PersistentVolumeClaim` in Kubernetes.
    The consideration for highly available clusters you will need to make with regard
    to storage is more specific per application. For example, if your application
    makes use of Amazon EBS volumes, which must reside within an availability zone,
    then you will have to ensure appropriate redundancy is available in your worker
    nodes so that the Pod that depends on that volume can be rescheduled in the event
    of an outage. More information on CSI drivers and implementations can be found
    here: [https://kubernetes-csi.github.io/docs/](https://kubernetes-csi.github.io/docs/)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的Kubernetes版本引入了与数据中心和云提供商中可用的持久性工具进行交互的改进方法，例如存储阵列和blob存储。最重要的改进是引入和标准化了用于管理Kubernetes中的`StorageClass`，`PersistentVolume`和`PersistentVolumeClaim`的容器存储接口。对于高可用集群的考虑，您需要针对每个应用程序做出更具体的存储决策。例如，如果您的应用程序使用亚马逊EBS卷，这些卷必须驻留在一个可用区内，那么您将需要确保工作节点具有适当的冗余，以便在发生故障时可以重新安排依赖于该卷的Pod。有关CSI驱动程序和实现的更多信息，请访问：[https://kubernetes-csi.github.io/docs/](https://kubernetes-csi.github.io/docs/)
- en: Building a High-Availability Focused Kubernetes Cluster
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个以高可用性为重点的Kubernetes集群
- en: Hopefully, by reading the previous section, you're starting to realize that
    Kubernetes is less magical than it may seem when you first approached the topic.
    It is an extremely powerful tool on its own, but Kubernetes really shines when
    we take full advantage of its capability of running in a highly available configuration.
    So now we're going to see how to implement it and actually build a cluster using
    a cluster life cycle management tool. But before we do that, we need to know the
    different ways that we can deploy and manage a Kubernetes cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 希望通过阅读前面的部分，您开始意识到当您首次接触这个主题时，Kubernetes并不像看起来那么神奇。它本身是一个非常强大的工具，但当我们充分利用其在高可用配置中运行的能力时，Kubernetes真正发挥作用。现在我们将看到如何实施它，并实际使用集群生命周期管理工具构建一个集群。但在我们这样做之前，我们需要了解我们可以部署和管理Kubernetes集群的不同方式。
- en: Self-Managed versus Vendor-Managed Kubernetes Solutions
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自管理与供应商管理的Kubernetes解决方案
- en: Amazon Web Services, Google Cloud Platform, Microsoft Azure, and practically
    every other major cloud services provider has a managed Kubernetes offering. So,
    when you are deciding how you are going to build and run your cluster, you should
    consider some of the different managed providers and their strategic offerings
    to see whether or not they align with your business needs and goals. For example,
    if you use Amazon Web Services, then Amazon EKS might be a viable solution for
    you.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊网络服务，谷歌云平台，微软Azure，以及几乎所有其他主要的云服务提供商都提供了托管的Kubernetes解决方案。因此，当您决定如何构建和运行您的集群时，您应该考虑一些不同的托管提供商及其战略性的提供，以确定它们是否符合您的业务需求和目标。例如，如果您使用亚马逊网络服务，那么Amazon
    EKS可能是一个可行的解决方案。
- en: There are trade-offs with choosing a managed service provider over an open-source
    and self-managed solution. For example, a lot of the hard work of cluster assembly
    is done for you, but you forfeit a great deal of control in the process. So, you
    need to decide how much value you place on being able to control the Kubernetes
    master plane and whether or not you would like to be able to pick your container
    networking interface or container runtime interface. For the purposes of this
    tutorial, we are going to use an open-source solution because it can be deployed
    anywhere, and it also helps us understand how Kubernetes works and how it is supposed
    to be configured.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 选择托管服务提供商而不是开源和自我管理的解决方案存在一些权衡。例如，很多集群组装的繁重工作都已经为您完成，但在这个过程中您放弃了很多控制权。因此，您需要决定您对能够控制Kubernetes主平面有多少价值，以及您是否希望能够选择您的容器网络接口或容器运行时接口。出于本教程的目的，我们将使用开源解决方案，因为它可以部署在任何地方，并且还可以帮助我们理解Kubernetes的工作原理以及应该如何配置。
- en: Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Please ensure that you have an AWS account and are able to access it using
    the AWS CLI: [https://aws.amazon.com/cli](https://aws.amazon.com/cli).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您拥有AWS账户并能够使用AWS CLI访问：[https://aws.amazon.com/cli](https://aws.amazon.com/cli)。
- en: If you are unable to access it, then please follow the instructions at the preceding
    link.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您无法访问它，请按照上面的链接中的说明操作。
- en: Assuming for now that we want more control over our cluster and are comfortable
    with managing it by ourselves, let's look at some open-source tools that can be
    used for setting up a cluster.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们现在想要对我们的集群有更多的控制，并且愿意自己管理它，让我们看一些可以用于设置集群的开源工具。
- en: kops
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: kops
- en: We will use one of the more popular open-source installation tools to do this
    called **kops**, which stands for **Kubernetes Operations**. It is a complete
    cluster life cycle management tool and has a very easy API to understand. As a
    part of the cluster creation/updating process, kops can generate Terraform configuration
    files so you can run the infrastructure upgrade process as part of your own pipeline.
    It also has good tooling to support the upgrade path between versions of Kubernetes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个更受欢迎的开源安装工具来完成这个过程，这个工具叫做**kops**，它代表**Kubernetes Operations**。它是一个完整的集群生命周期管理工具，并且具有非常易于理解的API。作为集群创建/更新过程的一部分，kops可以生成Terraform配置文件，因此您可以将基础设施升级过程作为自己流程的一部分运行。它还具有良好的工具支持Kubernetes版本之间的升级路径。
- en: Note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Terraform is an infrastructure life cycle management tool that we will briefly
    learn about in the next chapter.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Terraform是一个基础设施生命周期管理工具，我们将在下一章中简要了解。
- en: Some of the drawbacks of kops are that it tends to be about two versions of
    Kubernetes behind, it has not always been able to respond to vulnerability announcements
    as fast as other tools, and it is currently limited to creating clusters in AWS,
    GCP, and OpenStack.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: kops的一些缺点是它往往落后于Kubernetes的两个版本，它并不总是能够像其他工具那样快速响应漏洞公告，并且目前仅限于在AWS、GCP和OpenStack中创建集群。
- en: 'The reason we have decided to use kops for our cluster life cycle management
    in this chapter is four-fold:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定在本章中使用kops来管理我们的集群生命周期的原因有四个：
- en: We wanted to select a tool that would abstract away some of the more confusing
    bits of the Kubernetes setup as we ease you into cluster administration.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望选择一个工具，可以将一些更令人困惑的Kubernetes设置抽象化，以便让您更容易进行集群管理。
- en: It supports more cloud platforms than just AWS, so you don't have to be locked
    into Amazon if you choose not to be.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持的云平台不仅仅是AWS，因此如果您选择不使用亚马逊，您不必被锁定在亚马逊上。
- en: It supports a broad array of customizations to the Kubernetes infrastructure,
    such as choosing CNI providers, deciding on a VPC network topology, and node instance
    group customizations.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持对Kubernetes基础设施进行广泛的定制，例如选择CNI提供程序、决定VPC网络拓扑和节点实例组定制。
- en: It has first-class support for zero-downtime cluster version upgrades and handles
    the process automatically.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对零停机集群版本升级有一流的支持，并自动处理该过程。
- en: Other Commonly Used Tools
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他常用工具
- en: 'Besides kops, there are several other tools that can be used to set up a Kubernetes
    cluster. You can find the full list at this link: [https://kubernetes.io/docs/setup/#production-environment](https://kubernetes.io/docs/setup/#production-environment).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了kops之外，还有其他几种工具可以用来设置Kubernetes集群。您可以在此链接找到完整的列表：[https://kubernetes.io/docs/setup/#production-environment](https://kubernetes.io/docs/setup/#production-environment)。
- en: 'We will mention a couple of them here so you get an idea of what''s available:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提到其中一些，以便您了解有哪些可用的工具：
- en: '**kubeadm**: This is generated from the Kubernetes source code and is the tool
    that will allow the greatest level of control over each component of Kubernetes.
    It can be deployed in any environment.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kubeadm**：这是从Kubernetes源代码生成的工具，它将允许对Kubernetes的每个组件进行最大程度的控制。它可以部署在任何环境中。'
- en: Using kubeadm requires an expert level knowledge of Kubernetes to be useful.
    It gives cluster administrators little room for error, and it is complicated to
    upgrade a cluster using kubeadm.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用kubeadm需要对Kubernetes有专家级的了解才能发挥作用。它给集群管理员留下了很少的错误空间，并且使用kubeadm升级集群是复杂的。
- en: '**Kubespray**: This uses Ansible/Vagrant-style configuration management, which
    is familiar to many IT professionals. It is better for environments where the
    infrastructure is more static rather than dynamic (such as the cloud). Kubespray
    is very composable and configurable from a tooling perspective. It also allows
    the deployment of a cluster on bare-metal servers. The key to watch out for here
    is coordinating software upgrades of cluster components and hardware and operating
    systems. Since you are providing much of the functionality a cloud provider does,
    you have to make sure your upgrade processes won''t break the applications running
    on top of the cluster.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubespray**：这使用Ansible/Vagrant风格的配置管理，这对许多IT专业人士来说是熟悉的。它更适用于基础设施更为静态而非动态的环境（如云）。Kubespray非常可组合和可配置，从工具的角度来看。它还允许在裸机服务器上部署集群。关键是要注意协调集群组件和硬件和操作系统的软件升级。由于您提供了云提供商所做的许多功能，您必须确保您的升级过程不会破坏运行在集群之上的应用程序。'
- en: 'Because Kubespray uses Ansible for provisioning, you are restricted by the
    underlying limitations of Ansible for provisioning large clusters and keeping
    them in spec. Currently, Kubespray is limited to the following environments: AWS,
    GCP, Azure, OpenStack, vSphere, Packet, Oracle Cloud Infrastructure, or your own
    bare-metal installations.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因为Kubespray使用Ansible进行配置，您受到了用于配置大型集群并保持其规范性的Ansible底层限制的限制。目前，Kubespray仅限于以下环境：AWS、GCP、Azure、OpenStack、vSphere、Packet、Oracle
    Cloud Infrastructure或您自己的裸机安装。
- en: Authentication and Identity in Kubernetes
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes中的身份验证和身份
- en: 'Kubernetes uses two concepts for authentication: ServiceAccounts are meant
    to identify processes running inside Pods, and User Accounts are meant to identify
    human users. We will take a look at ServiceAccounts in a later topic in this chapter,
    but first, let''s understand User Accounts.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用两个概念进行身份验证：ServiceAccounts用于标识在Pods内运行的进程，而User Accounts用于标识人类用户。我们将在本章的后续主题中查看ServiceAccounts，但首先让我们了解User
    Accounts。
- en: From the very beginning, Kubernetes has tried to remain incredibly agnostic
    to any form of authentication and identity for user accounts, because most companies
    have a very specific way of authenticating users. Some use Microsoft Active Directory
    and Kerberos, some may use Unix passwords and UGW permission sets, and some may
    use a cloud provider or software as a service-based IAM solution. In addition,
    there are a number of different authentication strategies that may be used by
    an organization.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从一开始，Kubernetes一直试图对用户帐户的任何形式的身份验证和身份保持非常中立，因为大多数公司都有一种非常特定的用户身份验证方式。有些使用Microsoft
    Active Directory和Kerberos，有些可能使用Unix密码和UGW权限集，有些可能使用云提供商或基于软件的IAM解决方案。此外，组织可能使用多种不同的身份验证策略。
- en: Because of this, Kubernetes does not have built-in identity management or a
    required single way of authenticating those identities. Instead, it has a concept
    of authentication "strategies." A strategy is essentially a way for Kubernetes
    to delegate the verification of identity to another system or method.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Kubernetes没有内置的身份管理或必需的身份验证方式。相反，它有身份验证“策略”的概念。策略本质上是Kubernetes将身份验证的验证委托给另一个系统或方法的方式。
- en: In this chapter, we will be using x509 certificate-based authentication. X509
    certificate authentication essentially makes use of the Kubernetes Certificate
    Authority and common names/organization names. Since Kubernetes RBAC rules use
    `usernames` and `group names` to map authenticated identities to permission sets,
    x509 `common names` become the `usernames` of Kubernetes, and `organization names`
    become the `group names` in Kubernetes. kops automatically provisions x509-based
    authentication certificates for you so there is little to worry about; but when
    it comes to adding your own users, you will want to be aware of this.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用基于x509证书的身份验证。X509证书身份验证基本上利用了Kubernetes证书颁发机构和通用名称/组织名称。由于Kubernetes
    RBAC规则使用`用户名`和`组名`将经过身份验证的身份映射到权限集，x509`通用名称`成为Kubernetes的`用户名`，而`组织名称`成为Kubernetes中的`组名`。kops会自动为您提供基于x509的身份验证证书，因此几乎不用担心；但是当涉及添加自己的用户时，您需要注意这一点。
- en: Note
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Kubernetes RBAC stands for Role-Based Access Control, which allows us to allow
    or deny certain access to our users based on their roles. This will be covered
    in more depth in *Chapter 13*, *Runtime and Network Security in Kubernetes*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes RBAC代表基于角色的访问控制，它允许我们根据用户的角色允许或拒绝对某些访问的访问。这将在*第13章*《Kubernetes中的运行时和网络安全》中更深入地介绍。
- en: An interesting feature of kops is that you can use it in a similar way to manage
    cluster resources as you would use kubectl to manage cluster resources. kops handles
    a node similar to how Kubernetes would handle a Pod. Just as Kubernetes has a
    resource called "Deployment" to manage a bunch of Pods, kops has a resource called
    **InstanceGroup** (which can also be referred to by its short form, `ig`) to manage
    a bunch of nodes. In the case of AWS, a kops InstanceGroup effectively creates
    an AWS EC2 Autoscaling group.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: kops的一个有趣特性是，你可以像使用kubectl管理集群资源一样使用它来管理集群资源。kops处理节点的方式类似于Kubernetes处理Pod的方式。就像Kubernetes有一个名为“Deployment”的资源来管理一组Pods，kops有一个名为**InstanceGroup**的资源（也可以用它的简写形式`ig`）来管理一组节点。在AWS的情况下，kops
    InstanceGroup实际上创建了一个AWS EC2自动扩展组。
- en: Extending this comparison, `kops get instancegroups` or `kops get ig` is analogous
    to `kubectl get deployments`, and `kops edit` works similarly to `kubectl edit`.
    We will make use of this feature in the activity later in the chapter, but first,
    let's get our basic HA cluster infrastructure up and running in the following exercise.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展这个比较，`kops get instancegroups`或`kops get ig`类似于`kubectl get deployments`，`kops
    edit`的工作方式类似于`kubectl edit`。我们将在本章后面的活动中使用这个功能，但首先，让我们在下面的练习中启动和运行我们的基本HA集群基础设施。
- en: Note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this chapter, the commands have been run using the Zsh shell. However, they
    are completely compatible with Bash.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，命令是使用Zsh shell运行的。但是，它们与Bash完全兼容。
- en: 'Exercise 11.01: Setting up Our Kubernetes Cluster'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习11.01：设置我们的Kubernetes集群
- en: Note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This exercise will exceed the free tier of AWS that is normally given to new
    account holders for the first 12 months. Pricing information on EC2 can be found
    here: [https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习将超出AWS免费套餐的范围，该套餐通常赠送给新账户持有者的前12个月。EC2的定价信息可以在这里找到：[https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/)
- en: Also, you should remember to delete your instances at the end of the chapter
    to stop being billed for your consumed AWS resources.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您应该记得在本章结束时删除您的实例，以停止对您消耗的AWS资源进行计费。
- en: In this exercise, we will prepare our infrastructure for running a Kubernetes
    cluster on AWS. There's nothing particularly special about the choice of AWS;
    Kubernetes is platform-agnostic, though it already has code that allows it to
    integrate with native AWS services (EBS, EC2, and IAM) on behalf of cluster operators.
    This is also true for Azure, GCP, IBM Cloud, and many other cloud platforms.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将准备在AWS上运行Kubernetes集群的基础设施。选择AWS并没有什么特别之处；Kubernetes是平台无关的，尽管它已经有了允许它与本地AWS服务（EBS、EC2和IAM）集成的代码，代表集群运营商。这对于Azure、GCP、IBM
    Cloud和许多其他云平台也是如此。
- en: 'We will set up a cluster with the following specifications:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将建立一个具有以下规格的集群：
- en: Three master nodes
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个主节点
- en: Three etcd nodes (to keep things simple, we will run these on the master nodes)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个etcd节点（为了简单起见，我们将在主节点上运行这些节点）
- en: Two worker nodes
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个工作节点
- en: At least two availability zones
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少两个可用区
- en: 'Once we have our cluster set up, we will deploy an application on it in the
    next exercise. Now follow these steps to complete this exercise:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们设置好了我们的集群，我们将在下一个练习中在其上部署一个应用程序。现在按照以下步骤完成这个练习：
- en: 'Ensure that you have installed kops as per the instructions in the *Preface*.
    Verify that kops is properly installed and configured using the following command:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您已按*前言*中的说明安装了kops。使用以下命令验证kops是否已正确安装和配置：
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should see the following response:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now before we move on to the following steps, we need to do some setup in AWS.
    Most of the following settings are configurable, but we will be making a few decisions
    for you for the sake of convenience.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在我们继续以下步骤之前，我们需要在AWS中进行一些设置。以下大部分设置都是可配置的，但为了方便起见，我们将为您做出一些决定。
- en: 'First, we will set up an AWS IAM user that kops will use to provision your
    infrastructure. Run the following commands one after the other in your terminal:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将设置一个AWS IAM用户，kops将用它来提供您的基础设施。在您的终端中依次运行以下命令：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see output similar to this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似于这样的输出：
- en: '![Figure 11.3: Setting up an IAM user for kops'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.3：为kops设置IAM用户'
- en: '](image/B14870_11_03.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_03.jpg)'
- en: 'Figure 11.3: Setting up an IAM user for kops'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：为kops设置IAM用户
- en: Note the highlighted `AccessKeyID` and `SecretAccessKey` fields you will receive
    for your output. This is sensitive information, and the keys in the preceding
    screenshot will, of course, be invalidated by the author. We will need the highlighted
    information for our next step.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意突出显示的`AccessKeyID`和`SecretAccessKey`字段，这是您将收到的输出。这是敏感信息，前面截图中的密钥当然将被作者作废。我们将需要突出显示的信息进行下一步操作。
- en: 'Next, we need to export the created credentials for kops as environment variables
    for our terminal session. Use the highlighted information from the screenshot
    in the previous step:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将为kops创建的凭据导出为环境变量，用于我们的终端会话。使用前一步截图中的突出信息：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we need to create an S3 bucket for kops to store its state. To create
    a random bucket name, run the following command:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要为kops创建一个S3存储桶来存储其状态。要创建一个随机的存储桶名称，请运行以下命令：
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The second command outputs the name of the S3 bucket created, and you should
    see a response similar to the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个命令输出创建的S3存储桶的名称，您应该看到类似以下的响应：
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Run the following command to create the required bucket using the AWS CLI:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令，使用AWS CLI创建所需的存储桶：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we are using the `us-west-2` region. You can use a region closer to you
    if you want. You should see the following response for a successful bucket creation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`us-west-2`地区。如果您愿意，您可以使用离您更近的地区。对于成功创建存储桶，您应该看到以下响应：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have our S3 bucket, we can begin to set our cluster up. There are
    numerous options we can choose, but right now we're going to work with the defaults.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了S3存储桶，我们可以开始设置我们的集群。我们可以选择许多选项，但现在我们将使用默认设置。
- en: 'Export the name of your cluster and the S3 bucket that kops will use to store
    its state:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导出您的集群名称和kops将用于存储其状态的S3存储桶的名称：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Generate all the config and store it in the S3 bucket from earlier to create
    a Kubernetes cluster using the following command:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成所有的配置并将其存储在之前的S3存储桶中，使用以下命令创建一个Kubernetes集群：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By passing the `--zones` argument, we are specifying the availability zones
    we want our cluster to span, and by specifying the `master-count=3` parameter,
    we are effectively saying we want to use a highly available Kubernetes cluster.
    By default, kops will create two worker nodes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递`--zones`参数，我们正在指定我们希望集群跨越的可用区域，并通过指定`master-count=3`参数，我们有效地表示我们要使用一个高可用的Kubernetes集群。默认情况下，kops将创建两个工作节点。
- en: Note that this did not actually create the cluster, but it created a pre-flight
    set of checks so we can create a cluster in just a moment. It is informing us
    that in order to access our AWS instances, we need to provide a public key – the
    default search location is `~/.ssh/id_rsa.pub`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这实际上并没有创建集群，而是创建了一系列的预检查，以便我们可以在短时间内创建一个集群。它通知我们，为了访问AWS实例，我们需要提供一个公钥 -
    默认搜索位置是`~/.ssh/id_rsa.pub`。
- en: 'Now, we need to create an SSH key to be added to all of the master and worker
    nodes so we can log in to them with SSH. Use the following command:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个SSH密钥，以添加到所有的主节点和工作节点，这样我们就可以用SSH登录到它们。使用以下命令：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The type of secret (`sshpublickey`) is a special keyword reserved to kops for
    this operation. More information can be found at this link: [https://github.com/kubernetes/kops/blob/master/docs/cli/kops_create_secret_sshpublickey.md](https://github.com/kubernetes/kops/blob/master/docs/cli/kops_create_secret_sshpublickey.md).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 秘钥类型（`sshpublickey`）是kops为此操作保留的特殊关键字。更多信息可以在此链接找到：[https://github.com/kubernetes/kops/blob/master/docs/cli/kops_create_secret_sshpublickey.md](https://github.com/kubernetes/kops/blob/master/docs/cli/kops_create_secret_sshpublickey.md)。
- en: Note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The key being specified here at `~/.ssh/id_rsa.pub` will be the key that kops
    is going to distribute to all master and worker nodes and can be used for SSH
    from your local computer to the running server for diagnostic or maintenance purposes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里指定的密钥`~/.ssh/id_rsa.pub`将是kops要分发到所有主节点和工作节点并可用于从本地计算机到运行服务器进行诊断或维护目的的密钥。
- en: 'You can use the following command to use the key to log in with an admin account:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令使用密钥以管理员帐户登录：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: While this is not required for this exercise, you will find this useful for
    a later chapter.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这对于这个练习并不是必需的，但你会发现这对以后的章节很有用。
- en: 'To view our configuration, let''s run the following command:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看我们的配置，请运行以下命令：
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will open your text editor with the definition of our cluster, as shown
    here:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开您的文本编辑器，并显示我们集群的定义，如下所示：
- en: '![Figure 11.4: Examining the definition of our cluster'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.4：检查我们集群的定义'
- en: '](image/B14870_11_04.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_04.jpg)'
- en: 'Figure 11.4: Examining the definition of our cluster'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：检查我们集群的定义
- en: 'We have truncated this screenshot for brevity. At this point, you can make
    any edits, though, for this exercise, we will proceed without making any changes.
    We will keep the description of this spec out of the scope of this workshop for
    brevity. If you want more details about the various elements in the `clusterSpec`
    of kops, you can find more details here: [https://github.com/kubernetes/kops/blob/master/docs/cluster_spec.md](https://github.com/kubernetes/kops/blob/master/docs/cluster_spec.md).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们已经截取了这个屏幕截图。在这一点上，你可以进行任何编辑，但是对于这个练习，我们将继续进行而不进行任何更改。为了简洁起见，我们将不在本研讨会的范围内保留此规范的描述。如果您想了解kops的`clusterSpec`中各种元素的更多细节，可以在这里找到更多详细信息：[https://github.com/kubernetes/kops/blob/master/docs/cluster_spec.md](https://github.com/kubernetes/kops/blob/master/docs/cluster_spec.md)。
- en: 'Now, take the configuration we generated and stored in S3 and actually run
    commands to reconcile the AWS infrastructure with what we said we wanted it to
    be in our config files:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，拿出我们在S3中生成并存储的配置，并实际运行命令，以使AWS基础设施与我们在配置文件中所说的想要的状态相一致：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: All commands in kops are dry-run (nothing will actually happen except some validation
    steps) by default unless you specify the `--yes` flag. This is a protectionary
    measure, so you don't accidentally do something harmful to your cluster in production.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，kops中的所有命令都是dry-run（除了一些验证步骤外，实际上什么都不会发生），除非您指定`--yes`标志。这是一种保护措施，以防止您在生产环境中意外地对集群造成危害。
- en: 'This will take a long time, but after it''s done, we''ll have a working Kubernetes
    HA cluster. You should see the following response:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要很长时间，但完成后，我们将拥有一个可工作的Kubernetes HA集群。您应该看到以下响应：
- en: '![Figure 11.5: Updating the cluster to match the generated definition'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.5：更新集群以匹配生成的定义'
- en: '](image/B14870_11_05.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_05.jpg)'
- en: 'Figure 11.5: Updating the cluster to match the generated definition'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：更新集群以匹配生成的定义
- en: 'To validate that our cluster is running, let''s run the following command.
    This may take up to 5-10 minutes to fully work:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证我们的集群是否正在运行，让我们运行以下命令。这可能需要5-10分钟才能完全运行：
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You should see the following response:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 11.6: Validating our cluster'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.6：验证我们的集群'
- en: '](image/B14870_11_06.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_06.jpg)'
- en: 'Figure 11.6: Validating our cluster'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：验证我们的集群
- en: From this screenshot, we can see we have three Kubernetes master nodes running
    in separate availability zones, and two worker nodes spread across two of the
    three availability zones (making this a highly available cluster). Also, all of
    the nodes as well as the cluster appear to be healthy.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个屏幕截图中，我们可以看到我们有三个Kubernetes主节点分布在不同的可用区，并且两个工作节点分布在三个可用区中的两个（使这个集群具有高可用性）。此外，所有节点以及集群似乎都是健康的。
- en: Note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Remember your cluster resources are still running. If you plan to proceed to
    the next exercise after a significant amount of time, you may want to delete this
    cluster to stop the billing for the AWS resources. To delete this cluster, you
    can use the following command:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您的集群资源仍在运行。如果您计划在一段时间后继续进行下一个练习，您可能希望删除此集群以停止对AWS资源的计费。要删除此集群，您可以使用以下命令：
- en: '`kops delete cluster --name ${NAME} --yes`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`kops delete cluster --name ${NAME} --yes`'
- en: Kubernetes Service Accounts
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes Service Accounts
- en: As we learned earlier, a Kubernetes ServiceAccount object serves as an identification
    marker for a process inside a Pod. While Kubernetes does not manage and authenticate
    the identity of human users, it does manage and authenticate ServiceAccount objects.
    And then, similar to users, you can allow role-based access to Kubernetes resources
    for ServiceAccount.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前学到的，Kubernetes ServiceAccount对象用作Pod内部进程的标识标记。虽然Kubernetes不管理和验证人类用户的身份，但它管理和验证ServiceAccount对象。然后，类似于用户，您可以允许ServiceAccount对Kubernetes资源进行基于角色的访问。
- en: ServiceAccount acts as a way of authenticating to the cluster using **JSON Web
    Token** (**JWT**) style, header-based authentication. Every ServiceAccount is
    paired with a token stored in a secret that is created by the Kubernetes API and
    then mounted into the Pod associated with that ServiceAccount. Whenever any process
    in the Pod needs to make an API request, it passes the token along with it to
    the API server, and Kubernetes maps that request to the ServiceAccount. Based
    on that identity, Kubernetes can then determine the level of access to the resources/objects
    (authorization) that a process should be granted. Typically, service accounts
    are given to Pods inside the cluster as they are intended only to be used internally.
    A ServiceAccount is a Kubernetes namespace-scoped object.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ServiceAccount充当使用**JSON Web Token**（**JWT**）样式、基于标头的身份验证方式对集群进行身份验证的一种方式。每个ServiceAccount都与一个令牌配对，该令牌存储在由Kubernetes
    API创建的秘密中，然后挂载到与该ServiceAccount关联的Pod中。每当Pod中的任何进程需要发出API请求时，它会将令牌与请求一起传递给API服务器，Kubernetes会将该请求映射到ServiceAccount。基于该身份，Kubernetes可以确定应该授予该进程对资源/对象（授权）的访问级别。通常，ServiceAccount只分配给集群内部的Pod使用，因为它们只用于内部使用。ServiceAccount是一个Kubernetes命名空间范围的对象。
- en: 'An example spec for a ServiceAccount would look as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ServiceAccount的示例规范如下：
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will use this example in the next exercise. You would attach this ServiceAccount
    to an object by including this field in the definition of an object such as a
    Kubernetes deployment:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一个练习中使用这个示例。您可以通过在对象的定义中包含这个字段来将这个ServiceAccount附加到一个对象，比如一个Kubernetes部署：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you create a Kubernetes object without specifying a service account, it will
    be created with the `default` service account. A `default` service account is
    created by Kubernetes for each namespace.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您创建一个Kubernetes对象而没有指定服务账户，它将会被创建为`default`服务账户。`default`服务账户是Kubernetes为每个命名空间创建的。
- en: In the following exercise, we will deploy the Kubernetes Dashboard on our cluster.
    Kubernetes Dashboard is arguably one of the most helpful tools to have running
    in any Kubernetes cluster. It is useful for debugging issues with configuring
    workloads in Kubernetes.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将在我们的集群上部署Kubernetes仪表板。Kubernetes仪表板可以说是任何Kubernetes集群中运行的最有用的工具之一。它对于调试Kubernetes中的工作负载配置问题非常有用。
- en: Note
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find more information about it here: [https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到更多信息：[https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/)。
- en: 'Exercise 11.02: Deploying an Application on Our HA Cluster'
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习11.02：在我们的HA集群上部署应用程序
- en: In this exercise, we will use the same cluster that we deployed in the previous
    exercise and deploy Kubernetes Dashboard. If you have deleted your cluster resources,
    then please rerun the previous exercise. kops will automatically add the required
    information to connect to the cluster in your local Kube config file (found at
    `~/.kube/config`) and set that cluster as the default context.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用在上一个练习中部署的相同集群，并部署Kubernetes仪表板。如果您已经删除了集群资源，请重新运行上一个练习。kops将自动将所需的信息添加到本地Kube配置文件中以连接到集群，并将该集群设置为默认上下文。
- en: 'Since the Kubernetes Dashboard is an application that helps us in administration
    tasks, the `default` ServiceAccount does not have sufficient privileges. We will
    be creating a new ServiceAccount with generous privileges in this exercise:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Kubernetes仪表板是一个帮助我们进行管理任务的应用程序，`default` ServiceAccount没有足够的权限。在这个练习中，我们将创建一个具有广泛权限的新ServiceAccount：
- en: 'To begin with, we will apply the Kubernetes Dashboard manifest sourced directly
    from the official Kubernetes repository. This manifest defines all the objects
    that we will need for our application. Run the following command:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将应用直接从官方Kubernetes存储库获取的Kubernetes仪表板清单。这个清单定义了我们应用程序所需的所有对象。运行以下命令：
- en: '[PRE17]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You should see the following response:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 11.7: Applying the manifest for Kubernetes Dashboard'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.7：应用Kubernetes仪表板的清单'
- en: '](image/B14870_11_07.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_07.jpg)'
- en: 'Figure 11.7: Applying the manifest for Kubernetes Dashboard'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：应用Kubernetes仪表板的清单
- en: 'Next, we need to configure a ServiceAccount to access the dashboard. To do
    this, create a file called `sa.yaml` with the following content:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要配置一个ServiceAccount来访问仪表板。为此，请创建一个名为`sa.yaml`的文件，并包含以下内容：
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We are giving this user very liberal permissions, so please treat the access
    token with care. ClusterRole and ClusterRoleBinding objects are a part of RBAC
    policies, which are covered in *Chapter 13*, *Runtime and Network Security in
    Kubernetes*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给这个用户非常宽松的权限，所以请小心处理访问令牌。ClusterRole和ClusterRoleBinding对象是RBAC策略的一部分，这在《第13章》《Kubernetes中的运行时和网络安全》中有所涵盖。
- en: 'Next, run the following command:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，运行以下命令：
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You should see this response:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到这个响应：
- en: '[PRE20]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, let''s confirm the ServiceAccount details by running the following command:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过运行以下命令来确认ServiceAccount的详细信息：
- en: '[PRE21]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You should see the following response:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 11.8: Examining our ServiceAccount'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.8：检查我们的ServiceAccount'
- en: '](image/B14870_11_08.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_08.jpg)'
- en: 'Figure 11.8: Examining our ServiceAccount'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8：检查我们的ServiceAccount
- en: When you create a ServiceAccount in Kubernetes, it will also create a Secret
    in the same namespace with the contents of the JWT needed to make API calls against
    the API server. As we can see from the previous screenshot, the Secret in this
    case is named `admin-user-token-vx84g`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在Kubernetes中创建一个ServiceAccount时，它还会在相同的命名空间中创建一个包含用于对API服务器进行API调用所需的JWT内容的Secret。正如我们从前面的截图中所看到的，这种情况下的Secret的名称是`admin-user-token-vx84g`。
- en: 'Let''s examine the `secret` object:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查`secret`对象：
- en: '[PRE22]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should see the following output:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '![Figure 11.9: Examining the token in our ServiceAccount'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.9：检查我们的ServiceAccount中的令牌'
- en: '](image/B14870_11_09.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_09.jpg)'
- en: 'Figure 11.9: Examining the token in our ServiceAccount'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：检查我们的ServiceAccount中的令牌
- en: This is a truncated screenshot of the output. As we can see, we have a token
    here in this secret. Note that this is Base64 encoded, which we will decode in
    the next step.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出的一个截断截图。正如我们所看到的，我们在这个秘密中有一个令牌。请注意，这是Base64编码的，我们将在下一步中解码。
- en: 'Now we need the content of the token for the account Kubernetes just created
    for us, so let''s use this command:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要账户Kubernetes为我们创建的令牌的内容，所以让我们使用这个命令：
- en: '[PRE23]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s break this command down. The command gets the secret called `admin-user`
    because we created a ServiceAccount with that name. When a ServiceAccount is created
    in Kubernetes, it places a secret named the same with the token we use to authenticate
    to the cluster. The rest of the command is syntactic sugar to decode the result
    in a useful form for copying and pasting into the dashboard. You should get an
    output as shown in the following screenshot:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个命令。该命令获取名为`admin-user`的密钥，因为我们创建了一个具有该名称的ServiceAccount。当在Kubernetes中创建ServiceAccount时，它会放置一个与我们用于对集群进行身份验证的令牌同名的密钥。命令的其余部分是用于将结果解码为有用的形式以便复制和粘贴到仪表板中的语法糖。您应该得到如下截图所示的输出：
- en: '![Figure 11.10: Getting the content of the token associated'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.10：获取与令牌相关的内容'
- en: with the admin-user ServiceAccount
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 与admin-user ServiceAccount
- en: '](image/B14870_11_10.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_10.jpg)'
- en: 'Figure 11.10: Getting the content of the token associated with the admin-user
    ServiceAccount'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10：获取与admin-user ServiceAccount关联的令牌的内容
- en: Copy the output you receive, while being careful not to copy the `$` or `%`
    signs (seen in Bash or Zsh, respectively) seen at the very end of the output.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 复制您收到的输出，但要小心不要复制输出末尾看到的`$`或`%`符号（在Bash或Zsh中看到）。
- en: 'By default, Kubernetes Dashboard is not exposed to the public internet outside
    our cluster. So, in order to access it with our browser, we need a way to allow
    our browser to communicate with Pods inside the Kubernetes container network.
    One useful way is to use the proxy built into `kubectl`:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，Kubernetes仪表板不会暴露给集群外的公共互联网。因此，为了使用浏览器访问它，我们需要一种允许浏览器与Kubernetes容器网络内的Pod进行通信的方式。一个有用的方法是使用内置在`kubectl`中的代理：
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You should see this response:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到这个响应：
- en: '[PRE25]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Open your browser and navigate to the following URL:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开浏览器并导航到以下URL：
- en: '[PRE26]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You should see the following prompt:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下提示：
- en: '![Figure 11.11: Entering the token to sign in to Kubernetes Dashboard'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.11：输入令牌以登录Kubernetes仪表板'
- en: '](image/B14870_11_11.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_11.jpg)'
- en: 'Figure 11.11: Entering the token to sign in to Kubernetes Dashboard'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11：输入令牌以登录Kubernetes仪表板
- en: Paste your token copied from *step 4*, and then click on the `SIGN IN` button.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 粘贴从*步骤4*复制的令牌，然后单击`SIGN IN`按钮。
- en: 'After logging in successfully, you should see the dashboard as shown in the
    following screenshot:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 成功登录后，您应该看到仪表板如下截图所示：
- en: '![Figure 11.12: Kubernetes Dashboard landing page'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.12：Kubernetes仪表板登陆页面'
- en: '](image/B14870_11_12.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_12.jpg)'
- en: 'Figure 11.12: Kubernetes Dashboard landing page'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12：Kubernetes仪表板登陆页面
- en: In this exercise, we have deployed Kubernetes Dashboard to the cluster to allow
    you to administer your application from a convenient GUI. During the course of
    deploying this application, we have seen how we can create ServiceAccounts for
    our cluster.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们已经部署了Kubernetes仪表板到集群，以便您可以从方便的GUI管理您的应用程序。在部署此应用程序的过程中，我们已经看到了如何为我们的集群创建ServiceAccounts。
- en: Throughout this chapter, you've learned how to create the cloud infrastructure
    using kops to make a highly available Kubernetes cluster. Then, we deployed the
    Kubernetes Dashboard and learned about ServiceAccounts in the process. Now that
    you have seen the steps required to make a cluster and get an application running
    on it, we will make another cluster and see its resilience in action in the following
    activity.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经学会了如何使用kops创建云基础架构，以创建一个高可用的Kubernetes集群。然后，我们部署了Kubernetes仪表板，并在此过程中了解了ServiceAccounts。现在您已经看到了创建集群并在其上运行应用程序所需的步骤，我们将创建另一个集群，并在接下来的活动中看到其弹性。
- en: 'Activity 11.01: Testing the Resilience of a Highly Available Cluster'
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动11.01：测试高可用集群的弹性
- en: 'In this activity, we will test out the resiliency of a Kubernetes cluster we
    create ourselves. Here are some guidelines for proceeding with this activity:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将测试我们自己创建的Kubernetes集群的弹性。以下是进行此活动的一些指南：
- en: Deploy Kubernetes Dashboard. But this time, set the replica count of the deployment
    running the application to something higher than `1`.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署Kubernetes仪表板。但是这次，将运行应用程序的部署的副本计数设置为高于`1`的值。
- en: The Kubernetes Dashboard application is run on Pods managed by a deployment
    named `kubernetes-dashboard`, which runs in a namespace called `kubernetes-dashboard`.
    This is the deployment that you need to manipulate.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Dashboard应用程序在由名为`kubernetes-dashboard`的部署管理的Pod上运行，该部署在名为`kubernetes-dashboard`的命名空间中运行。这是您需要操作的部署。
- en: Now, start shutting down various nodes from the AWS console to remove nodes,
    delete Pods, and do what you can to make the underlying system unstable.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，开始从AWS控制台关闭各种节点，以删除节点，删除Pod，并尽力使底层系统不稳定。
- en: 'After each attempt you make to take down the cluster, refresh the Kubernetes
    console if the console is still accessible. So long as you get any response from
    the application, this means that the cluster and our application (in this case,
    Kubernetes Dashboard) is still online. As long as the application is online, you
    should be able to access the Kubernetes Dashboard as shown in the following screenshot:![Figure
    11.13: Kubernetes Dashboard prompt for entering a token'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您尝试关闭集群的每次尝试后，如果控制台仍然可访问，请刷新Kubernetes控制台。只要从应用程序获得任何响应，这意味着集群和我们的应用程序（在本例中为Kubernetes仪表板）仍然在线。只要应用程序在线，您应该能够访问Kubernetes仪表板，如下截图所示：![图11.13：Kubernetes仪表板提示输入令牌
- en: '](image/B14870_11_13.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_13.jpg)'
- en: 'Figure 11.13: Kubernetes Dashboard prompt for entering a token'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13：Kubernetes仪表板提示输入令牌
- en: This screenshot shows just the prompt where you need to enter your token, but
    it is a good enough indicator that our application is online. If your request
    times out, this means that our cluster is no longer functional.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 此截图仅显示您需要输入令牌的提示，但足以表明我们的应用程序在线。如果您的请求超时，这意味着我们的集群不再可用。
- en: Join another node to this cluster.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加入另一个节点到这个集群。
- en: 'To achieve this, you need to find and edit the InstanceGroup resource that
    is managing the nodes. The spec contains `maxSize` and `minSize` fields, which
    you can manipulate to control the number of nodes. When you update your cluster
    to match the modified specification, you should be able to see three nodes, as
    shown in the following screenshot:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，您需要找到并编辑管理节点的InstanceGroup资源。规范包含`maxSize`和`minSize`字段，您可以操纵这些字段来控制节点的数量。当您更新您的集群以匹配修改后的规范时，您应该能够看到三个节点，如下截图所示：
- en: '![Figure 11.14: Number of master and worker nodes in the cluster'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.14：集群中主节点和工作节点的数量'
- en: '](image/B14870_11_14.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_14.jpg)'
- en: 'Figure 11.14: Number of master and worker nodes in the cluster'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14：集群中主节点和工作节点的数量
- en: Note
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The solution to this activity can be found at the following address: [https://packt.live/304PEoD](https://packt.live/304PEoD).
    Make sure you have deleted your clusters once you have completed the activity.
    More details on how to delete your clusters are presented in the following section
    (*Deleting Our Cluster*).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可在以下地址找到：[https://packt.live/304PEoD](https://packt.live/304PEoD)。确保在完成活动后删除您的集群。有关如何删除集群的更多详细信息，请参见以下部分（*删除我们的集群*）。
- en: Deleting Our Cluster
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除我们的集群
- en: 'Once we''re done with all the exercises and activities in this chapter, you
    should delete the cluster by running the following command:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了本章中的所有练习和活动，您应该通过运行以下命令来删除集群：
- en: '[PRE27]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should see this response:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到这个响应：
- en: '![Figure 11.15: Deleting our cluster'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.15：删除我们的集群'
- en: '](image/B14870_11_15.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_11_15.jpg)'
- en: 'Figure 11.15: Deleting our cluster'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15：删除我们的集群
- en: At this point, you should no longer be receiving charges from AWS for the Kubernetes
    infrastructure you have spun up in this chapter.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您不应该再从AWS那里收到本章中您所创建的Kubernetes基础架构的费用。
- en: Summary
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Highly available infrastructure is one of the key components to achieving high
    availability for applications. Kubernetes is an extremely well-designed tool and
    has many built-in resiliency features that make it able to withstand major networking
    and compute events. It works to keep those events from impacting your application.
    During our exploration of high-availability systems, we investigated some components
    of Kubernetes and how they work together to achieve high availability. Then, we
    constructed a cluster of our own on AWS that was designed to be highly available
    using the kops cluster life cycle management tool.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用基础架构是实现应用程序高可用性的关键组成部分之一。Kubernetes是一个设计非常精良的工具，具有许多内置的弹性特性，使其能够经受住重大的网络和计算事件。它致力于防止这些事件影响您的应用程序。在我们探索高可用性系统时，我们调查了Kubernetes的一些组件以及它们如何共同实现高可用性。然后，我们使用kops集群生命周期管理工具在AWS上构建了一个旨在实现高可用性的集群。
- en: In the next chapter, we're going to take a look at how we make our applications
    more resilient by leveraging Kubernetes primitives to ensure high availability.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看看如何通过利用Kubernetes原语来确保高可用性，使我们的应用程序更具弹性。
