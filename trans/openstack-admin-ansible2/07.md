# 第七章。管理云上的容器

在本章中，我们将介绍当前讨论最多的技术方法之一，容器。容器的受欢迎程度在不断提高，这是理所当然的，谁不想要一种更简单的部署应用的方法和消耗计算资源的整合方法呢？我喜欢用的最好的类比，除了船上明显的集装箱类比，当谈论集装箱时，想象一下把你所有的代码放进一辆汽车或 SUV。然后一个搬运工出现在你家门口来取你的车。您的车辆的维护将是最少的，甚至没有，因为车辆承运人正在做所有的工作。你只需要担心确保车辆承运人正在工作。这是容器背后的原理，我们将深入容器概念，并了解如何利用 OpenStack 和/或 Ansible 来构建和部署它们。按照我们通常的方式，当我们浏览每一节时，我们将创建几个简单的例子，说明如何管理各种不同的容器格式。我们将在本章中讨论以下主题:

*   解释容器概念
*   构建和部署容器
    *   用可折叠集装箱建造集装箱
    *   在 OpenStack 上部署 Kubernetes
    *   用 Ansible 管理 CoreOS 和 Docker
    *   在开放堆栈上部署新 LXD
    *   审查行动手册和角色

# 容器概念解释

我不得不相信，大多数对技术感兴趣的人已经知道什么是集装箱化(又名集装箱)，但是在我的假设是错误的这种奇怪的可能性下，从解释它到底是什么开始感觉是个好主意。我将尽力不仅仅给你维基百科的定义，并试图围绕为什么容器模型是资源虚拟化的一个非常有用的补充给出一些坚实的意义。

随着传统虚拟化的开始，人们意识到我可以将我的服务器分割成可消费的块。不再需要将整个服务器专用于 web 或应用服务器。随着许多人开始意识到他们没有正确使用这些虚拟化资源，很快就采用了云。虚拟机闲置在那里，没有任何用处，或者有太多不需要的资源。云的主要卖点之一是这样一个事实，即您只能消费您需要的东西，并且这些资源是可处置的，也就是说使用它，然后再处置它。一直以来，这些技术使得消耗计算资源变得更加容易，但它们都没有真正帮助改进您部署应用的方式。

请记住，您为什么需要这些虚拟机和实例，是为了运行应用。如果仍然需要几天时间来部署新的应用，那么更快地获得资源有什么意义？在我看来，这就是为什么要精心设计集装箱化方法的基础。开发人员和系统管理员(主要是系统管理员)想要一种更有效的方法来部署应用。我个人还记得部署一个新应用或应用编程接口的极其痛苦的过程。它包括尝试浏览由开发人员编写的部署文档，开发人员很可能以前从未登录过服务器或管理过 web/应用服务器软件。我们只能说它充满了遗漏的步骤、错误的命令，并且永远无法考虑任何可能需要的环境修改(例如相关的软件版本)。

快进到现在，你现在有更多的选择。现在有相当多不同的容器技术允许开发人员将一个应用打包到一个容器中，并将其运送到您选择的容器平台。不再有部署文档，不再有 2 AM 部署方，最重要的是不再有部署错误。因为容器由应用的完整运行时环境组成，所以您需要管理的只是容器技术本身及其运行的操作系统。容器也很容易在环境之间或跨系统移动，因为唯一的依赖是运行相同容器技术的服务器。

现在您已经掌握了一些关于容器的事实，您必须选择最适合您需求的平台。一些最受欢迎的容器技术是 Docker([https://www.docker.com](https://www.docker.com))、Kubernetes([http://Kubernetes . io](http://kubernetes.io))、CoreOS([https://coreos.com](https://coreos.com))和 LXC/LXD([https://linuxcontainers.org](https://linuxcontainers.org))。

所以在你问之前，你可能会想，既然集装箱相对较新，它能被信任吗，集装箱化的概念已经被证明有效了吗？答案是肯定的，因为容器并不是一个新概念。集装箱，或集装箱化的概念，已经存在了 10 年。第一个容器技术是 LXC，多年来它一直是 Linux 内核的一部分。话虽如此，但我可以肯定地说，它已经过真正的测试，肯定是一项可以添加到您组织的产品组合中的技术。

我们现在可以开始进一步探索容器的旅程，并就如何在您的 OpenStack 云上自动构建和部署容器制定策略。我们在旅途中需要走的第一条路是建造我们的第一个集装箱。

# 构建和部署容器

在本节中，我们将学习如何设计、构建和部署容器到各种容器技术中。我们将在此讨论的主题细分如下:

*   用可转移集装箱建造集装箱
*   在 OpenStack 上部署 Kubernetes
*   用 Ansible 管理 CoreOS 和 Docker
*   在开放堆栈上部署新 LXD

如前所述，我们将首先学习如何使用我认为最简单的容器工具 Ansible Container 构建我们的第一个容器。希望你是兴奋的，因为我肯定是，我们走吧！

## 用可运输集装箱建造集装箱

什么是Ansible容器？

> *Ansible 将 Ansible Container 描述为“容器开发、测试和部署的最终工作流程”——([https://docs.ansible.com/ansible-container](https://docs.ansible.com/ansible-container))*

将其视为一种工作流工具，使您不仅可以构建 Docker 映像，还可以使用 Ansible 行动手册协调映像和应用的部署。我给你一秒钟让你振作起来。是的，我们在 Ansible 的朋友又做了一次，并交付了另一个奇妙的工具放入我们的工具箱。不再需要完全依赖 Dockerfile。Ansible 为桌面带来的所有功能现在都可以直接与构建、运行、部署甚至将容器映像推送到所选的注册表相结合。您想了解的关于 Ansible Container 的所有信息都可以在这里找到:[http://docs.ansible.com/ansible-container](http://docs.ansible.com/ansible-container)。

就像其他工具一样，Ansible 使焦点集中在 Ansible 容器上，简单易用。就我个人而言，我能够在几个小时内安装并部署我的第一个容器。Ansible Container 的一个关键特性是能够利用 ansi ble Galaxy([https://galaxy.ansible.com/intro](https://galaxy.ansible.com/intro))的共享容器构建，在设计您的容器图像上获得一个跳跃的开始。请记住，开源就是与社区共享。

### 自动化考虑因素

第一步是安装它，因为 Ansible 文档与众不同，所以我没有必要重新发明轮子。安装选项和细节可在以下位置找到:[http://docs.ansible.com/ansible-container/installation.html](http://docs.ansible.com/ansible-container/installation.html)。在你运行它之后，我建议下一步是查看这里的入门指南:

我们现在将逐步介绍一个我为开始使用而创建的示例 Ansible 容器项目。对我来说，这是学习新技术的最好方法。花点时间在上面，把你的手弄脏，然后变得更有知识。

#### 第一步

开始 Ansible 容器项目，从创建一个新目录这样简单的事情开始。创建新目录后，您需要移动到该目录中，并执行 Ansible 容器初始化命令。这些命令的一个工作示例如下:

```
$ mkdir elk-containers
$ cd elk-containers
$ ansible-container init

```

该命令的输出如下所示:

![Step 1](graphics/image_07_001.jpg)

在所示的例子中，我们的项目将被命名为`elk-containers`，并将在一个名为相同的目录中初始化。现在您已经初始化了您的项目，您会发现 Ansible 容器文件是在名为`ansible`的目录中创建的。项目的目录结构如下所示:

```
ansible/ 
  container.yml 
  main.yml 
  meta.yml 
  requirements.txt 
  requirements.yml 
  ansible.cfg 

```

这里创建的文件是框架文件，为您提供了一个入门的外壳。如果检查两个最重要的文件`container.yml`和`main.yml`，它们看起来会是这样的:

**容器. yml**

```
version: "1" 
services: 
 # Add your containers here, specifying the base image you want to build from 
 # For example: 
 # 
 # web: 
 #    image: ubuntu:trusty 
   #  ports: 
   #     - "80:80" 
   #  command: ['/usr/bin/dumb-init', '/usr/sbin/apache2ctl', '-D', 'FOREGROUND'] 
 #    dev_overrides: 
 #   environment: 
 #    - "DEBUG=1" 
 # 
registries: {} 
 # Add optional registries used for deployment. For example: 
 # google: 
 #  url: https://gcr.io 
 #  namespace: my-cool-project-xxxxxx  

```

**main.yml**

```
# This should be your Ansible playbooks to provision your containers. 
# An inventory will be automatically created using the names of the services 
# from your container.yml file. 
# Add any roles or other modules you'll need to this directory too. 
# For many examples of roles, check out Ansible Galaxy: https://galaxy.ansible.com/ 
# 
--- 
- hosts: all 
 gather_facts: false 

```

#### 第二步

现在，我们可以手动配置我们的容器和/或利用 Ansible Galaxy 上托管的许多预打包 Ansible 容器配置中的任何一个。对于这里的例子，我们将从 Ansible Galaxy 下拉并使用三种不同的配置。我们的示例项目将部署三个容器，它们将共同运行 ELK 堆栈(Elasticsearch、Logstash 和 Kibana)。

### 注

在执行以下命令之前，请确保您已经安装了 Ansible Container 和所有必备软件。详见 Ansible Container 安装说明:[https://docs . ansi ble . com/ansi ble-Container/installation . html](https://docs.ansible.com/ansible-container/installation.html)。

这里提到了处理这个的命令；执行时，确保您在项目目录的`root`目录中:

```
$ cd elk-containers
$ ansible-container install chouseknecht.kibana-container
$ ansible-container install chouseknecht.elasticsearch-container
$ ansible-container install chouseknecht.logstash-container

```

该命令的输出如下所示:

![Step 2](graphics/image_07_002.jpg)

一旦下载了基础映像，Ansible Container 将把它加载到一个虚拟容器中，该容器包含所有可能的映像依赖项，以便为构建做好准备。

![Step 2](graphics/image_07_003.jpg)

#### 第三步

接下来我们将回顾之前的`ansible-container`安装命令对我们的项目做了什么。如果我们现在查看一下我们的`container.yml`和`main.yml`文件，我们会注意到我们需要将 ELK 堆栈部署到容器中的所有自动化代码现在都在那里。让我们看看这些文件的变化:

**容器. yml**

```
version: '1' 
services: 
 kibana: 
    image: centos:7 
    ports: 
    - 5601:5601 
    user: kibana 
    links: 
    - elasticsearch 
    working_dir: /opt/kibana/bin 
    command: [./kibana] 
 elasticsearch: 
    image: centos:7 
    ports: 
    - 9200:9200 
    expose: 
    - 9300 
    restart: always 
    user: elasticsearch 
    working_dir: /usr/share/elasticsearch/bin 
    command: [./elasticsearch] 
 logstash: 
 image: centos:7 
    ports: 
    - 5044:5044 
    links: 
    - elasticsearch 
    restart: always 
    working_dir: /opt/logstash/bin 
    command: [./logstash, agent, -f, /etc/logstash/conf.d] 
    environment: 
    - JAVACMD=/usr/bin/java 

 # volumes: 
   # - your_configuration_volume:/etc/logstash/conf.d 
 # Add your containers here, specifying the base image you want to build from 
 # For example: 
 # 
 # web: 
 #    image: ubuntu:trusty 
 #    ports: 
 #       - "80:80" 
   #  command: ['/usr/bin/dumb-init', '/usr/sbin/apache2ctl', '-D', 'FOREGROUND'] 
 #    dev_overrides: 
   #     environment: 
   #        - "DEBUG=1" 
 # 
registries: {} 
 # Add optional registries used for deployment. For example: 
   # google: 
 #      url: https://gcr.io 
   #    namespace: my-cool-project-xxxxxx 

```

**main.yml**

```
- hosts: all 
 gather_facts: false 
- hosts: kibana 
 roles: 
 - role: chouseknecht.kibana-container 
    kibana_host: 0.0.0.0 
    kibana_port: 5601 
    kibana_elasticsearch_url: http://elasticsearch:9200 
    kibana_index: .kibana 
    kibana_log_dest: stdout 
    kibana_logging_silent: false 
    kibana_logging_quiet: false 
    kibana_logging_verbose: true 
- hosts: elasticsearch 
 roles: 
 - role: chouseknecht.elasticsearch-container 
    elasticsearch_network_host: 0.0.0.0 
    elasticsearch_http_port: 9200 
    elasticsearch_script_inline: true 
    elasticsearch_script_indexed: true 
    elasticsearch_data: /usr/share/elasticsearch/data 
    elasticsearch_logs: /usr/share/elasticsearch/logs 
    elasticsearch_config: /usr/share/elasticsearch/config 
    java_home: '' 
- hosts: logstash 
 roles: 
 - role: chouseknecht.logstash-container 
    logstash_elasticsearch_hosts: 
    - http://elasticsearch:9200 

    logstash_listen_port_beats: 5044 

    logstash_local_syslog_path: /var/log/syslog 
    logstash_monitor_local_syslog: true 

    logstash_ssl_dir: /etc/pki/logstash 
  logstash_ssl_certificate_file: '' 
  logstash_ssl_key_file: '' 

  logstash_enabled_on_boot: yes 

  logstash_install_plugins: 
  - logstash-input-beats 

```

我们现在还需要检查的另一个文件是`requirements.yml`文件。由于我们使用的是预打包的配置，因此将在此文件中添加指向这些配置的链接:

**要求. yml**

```
- src: chouseknecht.kibana-container 
- src: chouseknecht.elasticsearch-container 
- src: geerlingguy.java 
- src: chouseknecht.logstash-container 

```

此时，如果需要调整变量、特定的应用更改或添加额外的编排步骤，您可以选择对文件进行更改。最棒的是，你也可以选择不做任何改变。您可以直接构建和运行这个容器项目。

#### 第四步

在我们的最后一步中，这里我们将采用我们已经设计好的内容，执行 Ansible 容器构建过程，最后在本地部署这些容器。同样，对于我们的示例，我们不需要对容器设计文件进行任何更改。

构建过程非常强大，因为为了创建容器映像，将实现所有的容器依赖关系和编排。当您希望部署容器时，将使用这些图像。以下是用于构建容器的命令:

```
$ ansible-container build

```

该命令的输出片段如下所示:

![Step 4](graphics/B06086_07_04_resized-769x1024.jpg)

最后，当然也是最重要的一点，我们准备测试我们全新的集装箱。就像他们在集装箱化世界里说的那样，*只要装运就行了！。*使用 Ansible Container 在本地部署容器映像以测试它们是另一个非常有意义的特性。您将使用`ansible-container run`命令将容器部署到本地配置的 **Docker 引擎**安装中:

```
$ ansible-container run -d

```

一旦运行，命令的输出将类似于这样，我们可以通过执行`docker ps command`来确认我们的容器部署:

![Step 4](graphics/image_07_005.jpg)

如您所见，我们做得很好，因为我们现在有三个容器在本地运行。*我们做到了*！我们的第一个容器已经完成了设计、配置、构建和部署(尽管如此，所有这些都不到一个小时)。在我们继续下一步之前，我们可能应该停下来或者移走我们的容器。请使用以下命令停止或移除您的容器:

```
$ docker stop <container ID>
$ docker rm <container ID>

```

## 在 OpenStack 上部署 Kubernetes

在撰写本文时，Kubernetes 已经成为容器编排的市场选择，是顶级 GitHub 项目之一，并成为管理容器的领先企业选择。Kubernetes 的一些高级功能能够执行滚动升级、零停机部署、管理大规模复杂工作负载，以及开箱即用的高可用性/容错能力。如果您希望在生产环境中管理容器集群，那么您应该尝试一下 Kubernetes。

有鉴于此，经常会出现这样的问题，为什么我要在 OpenStack 这样的平台上运行 Kubernetes？很多人经常忘记 OpenStack 是一个虚拟机管理程序管理器，而不是虚拟机管理程序本身。OpenStack 使操作员能够管理许多不同类型的虚拟机管理程序，可以说容器编排软件只是另一种虚拟机管理程序。在 OpenStack 中，您有几种方法可以选择管理和部署 Kubernetes 集群。它可以通过 Magnum 来完成，Magnum 是 OpenStack 中的容器管理项目。另一种方法是使用 Heat 模板将 Kubernetes 集群作为一个堆栈进行管理。最后，您可以使用名为 **kargo** 的 GitHub 项目，该项目允许您在许多不同的系统和云平台上使用 Ansible 部署 Kubernetes。

对于这里的例子，我们将介绍最后一个选项，并使用 kargo 在我们的 OpenStack 云上部署 Kubernetes。通过创建我们自己的 Ansible 行动手册/角色来部署 Kubernetes，尝试重新发明轮子，感觉并不是很好地利用时间。卡戈项目可以在:[https://github.com/kubernetes-incubator/kargo](https://github.com/kubernetes-incubator/kargo)找到。存储库中有一些说明，将指导您如何进行设置，以便运行设置行动手册。

### 注

请记住，kargo 是一个开源项目，就像所有其他开源项目一样，它会发生变化。更改可能包括重新组织存储库布局、更改部署说明，甚至折旧。在写这篇文章的时候，这个项目还在运行。

OpenStack 的具体说明可以在这里找到:[https://github . com/kubernetes-孵化器/kargo/blob/master/docs/OpenStack . MD](https://github.com/kubernetes-incubator/kargo/blob/master/docs/openstack.md)。首先，您可以将 kargo 存储库克隆到 OpenStack 云上的实用程序容器中:

```
$ git clone https://github.com/kubernetes-incubator/kargo.git
$ cd kargo

```

### 自动化考虑因素

大部分情况下，安装会顺利进行。我确实不得不调整两件小事来让剧本顺利完成。第一次调整是在我的 OpenRC 文件中。正如您将在说明中注意到的，第二步是在运行安装剧本之前获取 OpenRC 文件。我的文件缺少剧本检查的两个参数；这是`OS_TENANT_ID`和`OS_REGION_NAME`参数。我的 OpenRC 文件的一个工作示例如下:

```
# Ansible managed: /etc/ansible/roles/openstack_openrc/templates/openrc.j2 
export LC_ALL=C 

# COMMON CINDER ENVS 
export CINDER_ENDPOINT_TYPE=publicURL 

# COMMON NOVA ENVS 
export NOVA_ENDPOINT_TYPE=publicURL 

# COMMON OPENSTACK ENVS 
export OS_ENDPOINT_TYPE=publicURL 
export OS_USERNAME=admin 
export OS_PASSWORD=passwd 
export OS_PROJECT_NAME=admin 
export OS_TENANT_NAME=admin 
export OS_TENANT_ID=bcf04d870b4c469cb1728e71ef9a6422 
export OS_AUTH_URL=https://192.168.0.249:5000/v3 
export OS_NO_CACHE=1 
export OS_USER_DOMAIN_NAME=Default 
export OS_PROJECT_DOMAIN_NAME=Default 
export OS_INTERFACE=publicURL 
export OS_REGION_NAME=RegionOne 

# For openstackclient 
export OS_IDENTITY_API_VERSION=3 
export OS_AUTH_VERSION=3 

```

我不得不做的另一个调整是调整一个特定的依赖于 Kubernetes 的软件容器是如何被拉出的。容器存储库标签已更改，kargo 项目尚未对其进行更新。对项目内的`roles/download/defaults/main.yml`文件进行更新。原始文件的片段如下所示:

```
... 
exechealthz_version: 1.1 
exechealthz_image_repo: "gcr.io/google_containers/exechealthz-amd64" 
exechealthz_image_tag: "{{ exechealthz_version }}" 
hyperkube_image_repo: "quay.io/coreos/hyperkube" 
hyperkube_image_tag: "{{ kube_version }}_coreos.0"

```

该文件需要更改为如下所示:

```
... 
exechealthz_version: 1.1 
exechealthz_image_repo: "gcr.io/google_containers/exechealthz-amd64" 
exechealthz_image_tag: "{{ exechealthz_version }}" 
hyperkube_image_repo: "quay.io/coreos/hyperkube" 
hyperkube_image_tag: "v{{ kube_version }}_coreos.0"

```

有了这两个变化，您所需要做的就是加速实例，以充当 Kubernetes 主实例、etcd 实例和节点。实例可以是您希望的任何基于 Linux 的操作系统。您布局 Kubernetes 集群的方式因环境类型和最终的用例而异。一个稳定的 Kubernetes 集群的参考体系结构是将两个实例作为主实例，三个实例作为 etcds，并利用讽刺来部署至少三个裸机服务器作为节点。当然，出于测试目的，您可以将整个集群作为实例部署在您的 OpenStack 云上。

下一步是配置您的清单文件，以包含您作为 Kubernetes 集群启动的实例。我的库存文件被命名为`os-inventory`。清单文件的工作示例如下:

```
[kube-master] 
kubes-1 
kubes-2 
[etcd] 
kubes-3 
kubes-4 

[kube-node] 
kubes-5 
kubes-6 
kubes-7 

[k8s-cluster:children] 
kube-node 
kube-master 
etcd 

```

信不信由你，您现在已经准备好运行安装脚本来部署您的 Kubernetes 集群了。这样做的命令如下，请确保您在卡戈存储库的`root`目录内:

```
$ ansible-playbook -i inventory/os-inventory -b cluster.yml

```

安装将运行一段时间，但在安装结束时，您将拥有一个可以使用的 Kubernetes 集群。我们现在将过渡到另一种容器编排技术，并尝试如何使用 Ansible 来管理容器，同时也利用 OpenStack。

## 用 Ansible 管理 CoreOS 和 Docker

CoreOS 似乎是在 OpenStack 之上运行的另一个绝佳选择，因为它是:

> *为集群部署设计的轻量级 Linux 操作系统，为您最关键的应用提供自动化、安全性和可扩展性–([https://coreos.com/why/#cluster](https://coreos.com/why/#cluster))*

CoreOS 的重点是提供一个默认具有集群感知能力的操作系统，使其非常适合容器技术等平台。Docker 也是尝试容器的明显选择，因为正是它让容器再次流行起来。此外，Docker 还有各种各样的图像，可以按原样进行下拉和部署。对于我们的例子，这里我们将回顾一个非常简单的剧本，它将在 CoreOS 上的容器中部署 ELK 堆栈。

### 自动化考虑因素

这个过程的第一步是旋转至少三个实例，具有至少 2 GB 的内存和稳定的 CoreOS 映像。因为我喜欢使用热来做这样的事情，所以我使用热模板来加速我的实例。我为此创建的模板可以在这里找到:[https://github . com/wbentley 15/open stack-heat-templates/tree/master/coreos](https://github.com/wbentley15/openstack-heat-templates/tree/master/coreos)。然后用 Heat 部署堆栈的命令如下所示:

```
$ heat stack-create coreos --template-file=heat-coreos-prod.yaml -- 
  parameters="key-name=my-key;user-data=cloud-config-prod.yaml;
  network=24b9b982-b847-4d0e-9088-61acbf92a37f"

```

![Automation considerations](graphics/image_07_006.jpg)

### 编写行动手册和角色

一旦您的 CoreOS 堆栈上线，您就可以执行我们现在将创建的行动手册。对于本例，所有任务都将在名为`base.yml`的行动手册中，该手册位于`playbook`目录的`root`目录中。该文件的开始内容如下所示:

```
--- 
# This playbook deploys the ELK stack on CoreOS 

- name: Bootstrap CoreOS 
 hosts: coreos 
 gather_facts: False 
 roles: 
 - defunctzombie.coreos-bootstrap 

```

剧本中的第一个任务是针对目标 CoreOS 实例运行诸如 Ansible 之类的包的关键。由于 CoreOS 是一个最小的操作系统，它没有任何 Python 版本。正如我们所知，在主机上运行 Ansible 的主要先决条件之一是安装 Python。为了规避这个限制，我们将使用名为`defunctzombie.coreos-bootstrap`的角色，它将在我们的 CoreOS 实例上安装`pypy`。我们将在后面学习如何告诉 Ansible 在这些节点上哪里可以找到我们的 Python 解释器。

您可以通过执行以下命令从 Galaxy 中删除该角色:

```
$ ansible-galaxy install defunctzombie.coreos-bootstrap

```

接下来的两个任务将在 CoreOS 实例上设置环境，以容器的形式运行 Docker 映像。请注意，我们将把`docker-py`和`docker-compose`包固定到特定版本；这是由于`docker_image`和`docker_container`模块的一个已知错误。一旦 bug 得到解决，这种依赖性就可以消除，或者这些版本可能需要随着时间的推移进行调整:

```
- name: Deploy ELK Stack 
 hosts: coreos 
 remote_user: core 
 become: false 
 tasks: 
    - name: Start etcd 
       service: name=etcd.service state=started 
       become: true 

    - name: Install docker-py 
       shell: /home/core/bin/pip install docker-py==1.9.0 docker-compose==1.8.0 

```

最后剩下的任务将处理为 ELK 堆栈拉下 Docker 映像，然后在 CoreOS 集群上启动这些容器:

```
 - name: Pull Elasticsearch container 
       docker_image: name=elasticsearch 

    - name: Pull Kibana container 
       docker_image: name=kibana 

    - name: Pull Logstash container 
     docker_image: name=logstash 

  - name: Launch Elasticsearch container 
   docker_container: 
    name: elasticsearch-cont 
          image: elasticsearch 
          state: started 

    - name: Launch Kibana container 
       docker_container: 
          name: kibana-cont 
          image: kibana 
          state: started 

    - name: Launch Logstash container 
     docker_container: 
    name: logstash-cont 
    image: logstash 
    state: started 

```

Docker 映像从 https://hub.docker.com 的存储库中取出，然后部署在托管 Docker 的 CoreOS 实例上。

我们这个例子的`hosts`文件又有点独特了，因为我们必须为 CoreOS 安装定制的 Python 解释器。我们需要配置 Ansible 来使用那个替代的 Python 解释器。在下面的工作示例中，您会发现我们将 Ansible 配置为使用位于`/home/core/bin/python`的 Python 解释器和位于`/home/core/bin/pip`的 pip 包:

```
[coreos] 
162.209.96.54 

[coreos:vars] 
ansible_ssh_user=core 
ansible_python_interpreter=/home/core/bin/python 
ansible_pip_interpreter=/home/core/bin/pip 

```

在本章的后面，我们将通过再次回顾这些行动手册和角色来总结，然后也以一个测试来结束，以便看到最终的结果。

## 在开放堆栈上部署新星 LXD

最后，但肯定不是最不重要的，我们将以真正开始这一切的容器选项来结束这一章，LXC，或者更确切地说，它更新的大兄弟 LXD。LXD 被描述为:

> *一个容器“管理程序”和 LXC 的新用户体验–([https://www.ubuntu.com/cloud/lxd](https://www.ubuntu.com/cloud/lxd))*

LXD 的三个主要组件是其系统范围的守护进程( **lxd** )、命令行客户端( **lxc** )和 OpenStack Nova 插件。正是这个插件将使我们能够在 OpenStack 控制下运行 LXD 作为一个管理程序，并使用传统的 OpenStack 命令来旋转容器。有了这样的东西，您可以在同一控制平面下的不同计算节点上运行实例和容器。LXD 被进一步描述为设计安全、可扩展、直观、形象、基于以及能够执行实时迁移。

对我们来说幸运的是，真主听到并回应了我们的祈祷。在牛顿版本下的**开放堆栈Ansible项目** ( **OSA** )中，您现在可以部署 LXD 作为 KVM 的替代虚拟机管理程序。现在，在部署 OSA 云之前，只需编辑两个配置文件就可以了。我们将概述这些变化，并演示如何使用 OpenStack 加速您的第一个 LXD 容器。

在我们开始之前，您应该知道在 OSA 上启用 LXD 的详细说明可以在这里找到:

### 自动化考虑因素

OSA 部署围绕部署节点上`/etc/openstack_deploy`目录中的三个主要配置文件展开。您需要编辑`user_variables.yml`和`user_secrets.yml`文件。从`user_variables.yml`文件开始，您需要设置`nova_virt_type`变量来使用 LXD。一个工作示例如下:

```
# This defaults to KVM, if you are deploying on a host that is not KVM capable 
# change this to your hypervisor type: IE "qemu", "lxc". 
nova_virt_type: lxd

```

第二个需要编辑的文件是`user_secrets.yml`文件。您只需要为 LXD 信托提供一个密码。需要编辑的行的示例如下:

```
# LXD Options for nova compute 
lxd_trust_password: 

```

### 类型

如果您计划建立一个混合计算节点场，并且希望同时拥有 KVM 和 LXD 主机。您需要编辑`openstack_user_config.yml`文件并为每个主机设置`nova_virt_type`。在前面的文档链接中可以找到一个如何配置它的工作示例。

现在，您可以开始 OSA 安装，因为您知道您将能够加速 LXD 容器以及运行在 KVM 上的实例。安装完成后，您还有最后一步要完成。我们现在必须创建一个 LXD 兼容的图像，当你旋转你的容器时将使用它。LXD 要求使用原始图像，因此我们将拉低符合这些要求的图像。在 OSA 云的实用程序容器中，执行以下命令:

```
$ wget http://cloud-images.ubuntu.com/trusty/current/
  trusty-server-cloudimg-amd64-root.tar.gz
$ glance image-create --name=trusty-LXD --visibility=public --container-
  format=bare --disk-format=raw 
  --file=trusty-server-cloudimg-amd64-root.tar.gz

```

![Automation considerations](graphics/image_07_007.jpg)

有了你的新形象，你现在准备好旋转你的第一个 LXD 集装箱。LXD 容器的管理方式类似于在 KVM 上运行的实例。您可以通过地平线仪表板或通过开放堆栈客户端命令行界面创建一个容器。对于这个例子，我们将使用 OpenStack 客户端来创建容器。以下命令将创建您的容器:

```
$ nova boot --image=<image name> --flavor=<flavor> --nic net-id=<network ID> --security-group=<security group> --min-count <number of containers> <container name>
$ nova boot --image=trusty-LXD --flavor=m1.small --nic net-id=eb283939-2c65-4ecb-9d9f-cbbea9bf252c --security-group default --min-count 3 first-lxd-container

```

如果您的输出类似于以下内容，则可以将其视为成功:

![Automation considerations](graphics/image_07_008.jpg)

然后，您可以执行`openstack server list`命令来验证您的新容器是否正在运行。

![Automation considerations](graphics/image_07_009.jpg)

很酷，我的朋友，你又做得很棒了！我知道我们谈了很多，但是你现在已经是老职业选手了，所以不用担心。按照我们的传统，我们将在本章结束时快速回顾一下我们所学的内容以及下一章的内容。

# 回顾行动手册和角色

让我们直接开始检查我们之前创建的主剧本，在名为**的 CoreOS 上部署 Docker 容器。位于`ansible-coreos`目录根的名为`base.yml`的完整剧本和文件如下所示:**

```
--- 
# This playbook deploys the ELK stack on CoreOS 

- name: Bootstrap CoreOS 
 hosts: coreos 
 gather_facts: False 
 roles: 
    - defunctzombie.coreos-bootstrap 

- name: Deploy ELK Stack 
 hosts: coreos 
 remote_user: core 
 become: false 
 tasks: 
    - name: Start etcd 
       service: name=etcd.service state=started 
       become: true 

    - name: Install docker-py 
       shell: /home/core/bin/pip install docker-py==1.9.0 docker-compose==1.8.0 

    - name: Pull Elasticsearch container 
       docker_image: name=elasticsearch 

    - name: Pull Kibana container 
       docker_image: name=kibana 

    - name: Pull Logstash container 
       docker_image: name=logstash 

    - name: Launch Elasticsearch container 
       docker_container: 
          name: elasticsearch-cont 
          image: elasticsearch 
          state: started 

  - name: Launch Kibana container 
   docker_container: 
          name: kibana-cont 
          image: kibana 
          state: started 

    - name: Launch Logstash container 
       docker_container: 
          name: logstash-cont 
        image: logstash 
    state: started 

```

我们从 Galaxy 下拉的对应角色位于`ansible-coreos/roles/defunctzombie.coreos-bootstrap/tasks`目录，看起来是这样的:

```
- name: Check if bootstrap is needed 
 raw: stat $HOME/.bootstrapped 
 register: need_bootstrap 
 ignore_errors: True 

- name: Run bootstrap.sh 
 script: bootstrap.sh 
 when: need_bootstrap | failed 

- name: Check if we need to install pip 
 shell: "{{ansible_python_interpreter}} -m pip --version" 
 register: need_pip 
 ignore_errors: True 
 changed_when: false 
 when: need_bootstrap | failed 

- name: Copy get-pip.py 
 copy: src=get-pip.py dest=~/get-pip.py 
 when: need_pip | failed 

- name: Install pip 
 shell: "{{ansible_python_interpreter}} ~/get-pip.py" 
 when: need_pip | failed 

- name: Remove get-pip.py 
 file: path=~/get-pip.py state=absent 
 when: need_pip | failed 

- name: Install pip launcher 
 copy: src=runner dest=~/bin/pip mode=0755 
 when: need_pip | failed 

```

最后，我们创建了`hosts`文件，它也位于`playbook`目录的`root`目录中:

```
[coreos] 
162.209.96.54 

[coreos:vars] 
ansible_ssh_user=core 
ansible_python_interpreter=/home/core/bin/python 
ansible_pip_interpreter=/home/core/bin/pip 

```

### 注

完整的代码集也可以在下面的 GitHub 资源库中找到:[https://GitHub . com/OS-admin-with-ansi ble/OS-admin-with-ansi ble-v2](https://github.com/os-admin-with-ansible/os-admin-with-ansible-v2)。

我们终于准备好试一试这个剧本了。假设您已经克隆了以前的 GitHub 存储库，从部署节点测试剧本的命令如下:

```
$ cd os-admin-with-ansible-v2
$ cd ansible-coreos
$ ansible-playbook -i hosts base.yml

```

Assuming all goes well, the output should resemble the snippet in the following screenshot:

![Reviewing playbooks and roles](graphics/image_07_010.jpg)

通常，我还喜欢通过在 CoreOS 实例上执行`docker ps`命令来采取额外的步骤来验证容器是否正在运行。

![Reviewing playbooks and roles](graphics/image_07_011.jpg)

# 总结

越过终点线确实感觉不错。我希望容器能够为您提供的原始能力将激励您开始在您的 OpenStack 云上部署它们。拥有传统虚拟机和实例之外的选项总是让人感觉很好。

在结束本章之前，让我们花点时间回顾一下这一章。我们以探索集装箱化的概念和为什么它变得如此受欢迎开始这一章。您学习了如何使用 Ansible 容器来创建我们的第一个容器映像和构建。我们回顾了 kargo 项目，该项目使您能够在多个云平台(包括 OpenStack)上使用 Ansible 部署 Kubernetes。接下来，我们演示了如何使用 Ansible 来管理在 OpenStack 上运行 Docker 集群的 CoreOS。最后，我们回顾了使用 **openstack-ansible** 项目部署 LXD 所需的配置更改。

下一章也将是非常有趣的一章，因为作为云运营商，您最终将不得不考虑扩大/缩小您的云足迹。OpenStack 具有有用的内置特性，这使得缩放过程相当容易和简单。在下一章中，我们将介绍设置主动-主动云区域的概念，然后通过自动完成这项任务来提升这一概念，以减轻时间到来时的扩展压力。如果你已经准备好面对未知的水域，为[第 8 章](08.html "Chapter 8. Setting Up Active-Active Regions")、*设置活动-活动区域*设定一个路线！