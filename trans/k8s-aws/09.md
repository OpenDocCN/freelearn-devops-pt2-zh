# 存储状态

本章主要讲述如何利用 Kubernetes 与 AWS 原生存储解决方案**弹性块存储** ( **EBS** )的深度集成。亚马逊 EBS 提供网络连接存储即服务，是用于向 EC2 实例提供数据块存储的主要解决方案。

几乎每个启动的 EC2 实例都由一个 EBS 根卷(从 AMI 机器映像创建)支持。因为 EBS 存储是网络连接的，如果承载 EC2 实例的底层机器以某种方式出现故障，存储在卷上的数据是安全的，因为它会自动跨多个物理存储设备复制。

除了用于存储 EC2 实例的根文件系统之外，附加的 EBS 卷可以附加到 EC2 实例，并通过 AWS 应用编程接口按需装载。Kubernetes 与 AWS EBS 的集成利用了这一点，提供了可由您的吊舱使用的持久卷。如果一个 pod 被终止，并被另一个 EC2 实例上的 pod 替换，Kubernetes 将处理从旧的 EC2 实例中分离 EBS 卷并将其附加到新的实例，准备根据需要装入新的 pod。

在本章中，我们将从了解如何配置我们的豆荚来利用额外的卷开始。然后，我们将看看 Kubernetes 为处理提供持久性的存储(如 EBS)提供的抽象。我们将了解 Kubernetes 如何根据 pod 配置中要求的规格为我们自动调配 EBS 卷。

一旦您掌握了使用 Kubernetes 为您的 pods 配置持久存储，在本章的后半部分，我们将查看有状态集，Kubernetes 提供的运行一组 pods 的抽象，每个 pods 都可以有自己的附加存储和一个身份，即使重新安排到另一个节点，该身份仍然存在。如果您想在 Kubernetes 集群上运行复杂的有状态应用(如数据库)，这是最后一块难题。

在本章中，我们将涵盖以下主题:

*   卷
*   存储类
*   有状态集

# 卷

让我们先来看看如何将卷附加到我们的豆荚上。最简单的可用卷`emptyDir`只是一个临时目录，与 pod 的生命周期相关联。创建卷时，正如名称所示，它是空的，并保留在节点上，直到 pod 从节点中删除。存储在卷内的数据确实会在同一节点上 pod 重启之间保持不变，因此对于需要在文件系统上缓存昂贵计算的进程或检查其进度的进程非常有用。在[第 1 章](01.html)、*谷歌的“为我们其余人服务的基础设施”*中，我们讨论了`emptyDir`卷在一个容器内的不同容器之间共享文件的一些其他可能的用途。

在本例中，我们将使用`emptyDir`卷来部署一个应用，该应用期望写入容器中的`/data`目录，其中根文件系统已被设为只读。

该应用旨在说明 Kubernetes 中卷的一些属性。当它启动时，它会写入`/data`目录中的一个随机文件名。然后，它启动一个显示该目录内容的 web 服务器:

```
apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: randserver 
spec: 
  selector: 
    matchLabels: 
      app: randserver 
  template: 
    metadata: 
      labels: 
        app: example 
    spec: 
      containers: 
      - image: errm/randserver 
        name: randserver 
        volumeMounts: 
        - mountPath: /data 
          name: data 
        securityContext: 
          readOnlyRootFilesystem: true 
      volumes: 
      - name: data 
        emptyDir: {} 
```

看一下这种配置，关于我们如何在 pod 中使用卷，您应该注意一些事情。这些规则不仅适用于`emptyDir`卷，也适用于您可能遇到的所有其他类型的卷:

*   每个卷都在 pod 规范的顶层定义。即使一个容器中有多个容器使用一个卷，我们也只需要定义一次。
*   当您想要从容器中访问一个卷时，您必须指定一个卷挂载，在特定的点将该卷挂载到容器的文件系统中。当我们挂载一个卷时，我们用我们在`volumes`部分定义它时使用的名称来引用它。

一旦您部署了这个示例清单，您应该能够使用`kubectl port-forward`命令来访问在 pod 内部运行的 web 服务器:

```
$ kubectl port-forward deployment/randserver 3000:3000   
Forwarding from 127.0.0.1:3000 -> 3000
Forwarding from [::1]:3000 -> 3000
```

现在，您应该能够在浏览器中访问`http://localhost:3000`来查看容器启动时创建的随机文件:

![](assets/2ec3e254-b8ec-41e1-a6c3-42b261d94d2f.png)

如果删除此容器，则部署将重新创建一个新的容器。因为每当 pod 被破坏时`emptyDir`卷的内容都会丢失，所以在第一个 pod 启动时创建的文件将会丢失，并且会创建一个具有不同名称的新文件:

```
$ kubectl delete pod -l app=randserver
pod "randserver-79559c5fb6-htnxm" deleted  
```

您需要重新运行`kubectl port-forward`来选择新的 pod:

```
$ kubectl port-forward deployment/randserver 3000:3000  
```

![](assets/cda09f41-f300-412e-a001-43f1baa830a9.png)

A newly created file being served

# EBS 卷

让 Kubernetes 附加一个 EBS 卷，然后将其安装到我们吊舱中的容器中，这几乎和使用`emptyDir`卷一样简单。安装 EBS 卷的最低级别和最简单的方法是使用`awsElasticBlockStore`卷类型。这种卷类型处理将 EBS 卷附加到我们的 pod 将运行的节点，然后将卷装入我们的容器中的路径。

当使用这种卷类型时，Kubernetes 并不负责为我们实际创建卷，因此我们需要手动创建。我们可以使用 AWS 命令行界面来实现这一点:

```
$ aws ec2 create-volume --availability-zone=us-east-1a --size=5 --volume-type=gp2
{
 "AvailabilityZone": "us-east-1a",
 "CreateTime": "2018-11-17T15:17:54.000Z",
 "Encrypted": false,
 "Size": 5,
 "SnapshotId": "",
 "State": "creating",
 "VolumeId": "vol-04e744aad50d4911",
 "Iops": 100,
 "Tags": [],
 "VolumeType": "gp2"
} 
```

请记住，EBS 卷绑定到特定的可用性区域(就像`ec2`实例一样)，并且只能连接到同一可用性区域中的实例，因此您需要在与集群中的实例相同的区域中创建卷。

在这里，我们已经更新了我们在上一个示例中创建的部署，以使用`awsElasticBlockStore`卷类型并将我们刚刚创建的卷附加到我们的 pod。EBS 卷的标识作为参数传递给卷配置:

```
apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: randserver 
spec: 
  selector: 
    matchLabels: 
      app: randserver 
  template: 
    metadata: 
      labels: 
        app: randserver 
    spec: 
      containers: 
      - image: errm/randserver 
        name: randserver 
        volumeMounts: 
        - mountPath: /data 
          name: data 
        securityContext: 
          readOnlyRootFilesystem: true 
      volumes: 
      - name: data 
        awsElasticBlockStore: 
          volumeID: vol-04e744aad50d4911 
          fsType: ext4 
      nodeSelector: 
        "failure-domain.beta.kubernetes.io/zone": us-east-1a 
```

您将会看到，以这种方式手动附加 EBS 卷与使用更简单的`emptyDir`卷非常相似。

The special `failure-domain.beta.kubernetes.io/zone` label is added to each node automatically by the AWS cloud provider. Here, we are using it in `nodeSelector` of our pod definition to schedule the pod to a node in the same availability zone as we created the volume in. There are several other labels that Kubernetes will automatically add to the nodes in your cluster. You can read about them in the Kubernetes documentation at [https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/).

当您第一次提交此部署时，它的行为将与以前的版本完全相同。但是，当我们删除 pod 并替换它时，您会注意到，在此容器的先前运行中创建的文件将保留，并且每次启动时都会有一个新文件添加到列表中:

![](assets/dee678a3-6597-4cd5-ac44-c64a26143d86.png)

When our application is backed by an EBS volume, files survive pod rescheduling

# 持久卷

虽然我们当然可以用这种方式手动创建 EBS 卷，并在我们的清单中使用它们的标识，但这种方法存在一些问题。

对于希望在集群上运行其应用的用户来说，在修改清单以引用硬编码的标识之前，首先考虑配置应用所需的 EBS 卷是很难的，也很耗时。这意味着 pod 清单需要包含一个特定于在 AWS 上运行所讨论的应用的配置。理想情况下，我们希望尽可能多的配置可以在我们可能部署它的不同环境之间重用，以避免因不得不修改配置而引入错误的风险。

Kubernetes 提供了两个抽象概念来帮助我们管理 EBS 卷:`PersistentVolume`和`PersistentVolumeClaim`。

`PersistentVolume`对象代表集群中的一个物理存储块；在 AWS 上，这是一个 EBS 卷，就像`Node`对象代表集群中的 EC2 实例一样。该对象捕获存储如何实现的详细信息，因此对于 EBS 卷，它会记录其 ID，以便 Kubernetes 可以在计划使用该卷的 pod 时将其附加到正确的节点。

`PersistentVolumeClaim`是 Kubernetes 对象，它允许我们表达在 pod 中使用`PersistentVolume`的请求。当我们请求持久卷时，我们只需要请求我们需要的存储量和可选的存储类(参见下一节)。`PersistentVolumeClaim`通常嵌入在 pod 规范中。当一个吊舱被调度时，它的`PersistentVolumeClaim`与一个特定的`PersistentVolume`相匹配，这个特定的【】足够大以满足所请求的存储量。`PersistentVolume`绑定到其请求的`PersistentVolumeClaim`上，这样即使重新调度了一个 pod，相同的底层卷也将被附加到该 pod 上。

与手动配置 EBS 卷和在我们的配置中包含卷 ID 相比，这是一个很大的改进，因为我们不需要每次将 pod 部署到新环境时都修改清单。

如果您手动操作 Kubernetes(例如，在裸机部署中)，集群管理员可能会预配置一个`PersistentVolume`池，然后在创建每个`PersistentVolumeClaim`时将其匹配并绑定到它们。当使用 AWS 时，不需要预先配置存储，因为 Kubernetes 根据需要使用 AWS API 动态创建`PersistentVolume`。

# 持久卷示例

让我们看看如何使用持久卷来简化示例应用的部署。

To avoid additional charges on your AWS account, you might want to delete the EBS volume you created manually in the previous example.

First, delete the deployment that we created so Kubernetes can detach the volume:

`**$ kubectl delete deployment/randserver**`Then, you can use the AWS CLI to delete the EBS volume:

`**$ aws ec2 delete-volume --volume-id vol-04e744aad50d4911**`

在开始之前，请确保您已经向集群中至少添加了通用存储类。

使用 Kubernetes 动态卷供应创建 EBS 卷与使用`kubectl`创建任何其他资源一样简单:

```
apiVersion: v1 
kind: PersistentVolumeClaim 
metadata: 
  name: randserver-data 
spec: 
  accessModes: 
    - ReadWriteOnce 
  storageClassName: general-purpose 
  resources: 
    requests: 
      storage: 1Gi 
```

如果您将`storageclass.kubernetes.io/is-default-class`注释添加到集群中的存储类，则可以省略`storageClassName`字段。

使用`kubernetes.io/aws-ebs`置备程序为存储类创建`PersistantVolumeClaim`后，Kubernetes 将置备与您指定的大小和存储类参数匹配的 EBS 卷。完成后，您可以使用`kubectl describe`查看索赔；您可以看到状态已经更新为`Bound`，并且`Volume`字段显示了索赔绑定到的基础`PersistentVolume`:

```
$ kubectl describe pvc/randserver-data
Name:          randserver-data
Namespace:     default
StorageClass:  general-purpose
Status:        Bound
Volume:        pvc-5c2dab0d-f017-11e8-92ac-0a56f9f52542
Capacity:      1Gi
Access Modes:  RWO

```

如果我们使用`kubectl describe`来检查这个`PersistentVolume`，我们可以看到自动配置的底层 EBS 卷的详细信息:

```
$ kubectl describe pv/pvc-5c2dab0d-f017-11e8-92ac-0a56f9f52542
Name: pvc-5c2dab0d-f017-11e8-92ac-0a56f9f52542
StorageClass: general-purpose
Status: Bound
Claim: default/randserver-data
Reclaim Policy: Delete
Access Modes: RWO
Capacity: 1Gi
Source:
 Type: AWSElasticBlockStore (a Persistent Disk resource in AWS)
 VolumeID: aws://us-east-1a/vol-04ad625aa4d5da62b
 FSType: ext4
 Partition: 0
 ReadOnly: false
```

在我们的部署中，我们可以更新 pod 规范的`volumes`部分，通过名称引用`PersistentVolumeClaim`:

```
apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: randserver 
spec: 
  selector: 
    matchLabels: 
      app: randserver 
  template: 
    metadata: 
      labels: 
        app: randserver 
    spec: 
      containers: 
      - image: errm/randserver 
        name: randserver 
        volumeMounts: 
        - mountPath: /data 
          name: data 
        securityContext: 
          readOnlyRootFilesystem: true 
      volumes: 
      - name: data 
        persistentVolumeClaim: 
          claimName: randserver-data 
```

# 存储类

在 AWS 上，有几种不同类型的卷提供不同的价格和性能特征。

为了在调配卷时提供一种选择卷类型(和一些其他设置)的简单方法，我们创建了一个`StorageClass`对象，然后在创建`PersistentVolumeClaim`时可以通过名称引用该对象。

通过使用`kubectl`向 API 提交清单，存储类的创建方式与任何其他 Kubernetes 对象相同:

```
kind: StorageClass 
apiVersion: storage.k8s.io/v1 
metadata: 
  name: general-purpose 
  annotations: 
    "storageclass.kubernetes.io/is-default-class": "true" 
provisioner: kubernetes.io/aws-ebs 
parameters: 
  type: gp2 
```

该清单创建了一个名为`general-purpose`的存储类，该存储类使用`gp2`卷类型创建卷。如果您还记得我们在[第 6 章](06.html)、*生产规划*中关于 EBS 卷类型的讨论，这种固态硬盘支持的卷类型适合大多数通用应用，提供了性能和价格的良好平衡。

您还会注意到`storageclass.kubernetes.io/is-default-class`注释，该注释使`StorageClass`成为任何未指定存储类的`PersistentVolumeClaim`使用的默认注释。您应该仅将此注释应用于单个`StorageClass`。

`parameter`字段接受几个不同的选项。

最重要的参数字段是`type`，允许我们选择`gp2`(默认)、`io1`(预配 IOPS)、`sc1`(冷存储)或`st1`(吞吐量优化)之一。

如果您选择使用`io1`类型，您还应该使用`iopsPerGB`参数来指定将为请求的每 GB 磁盘存储调配的 IOPS 数。`io1` EBS 卷支持的最大 IOPS/GB 比率为 50:1。

Bear in mind that the cost of provisioned IOPS makes the cost of `io1` volumes much higher than the equivalent general-purpose volumes. An `io1` volume with IOPS provisioned to provide similar throughput to a `gp2` volume of the same size can be three times more expensive. So, you should only use `io1` volumes where you require performance in excess of that provided by `gp2` volumes. One trick that can optimize costs is to use `gp2` volumes larger than your application requires to provide extra IO credits.

例如，您可以使用`io1`类型创建几个不同的类，供具有不同性能要求的应用使用:

```
kind: StorageClass 
apiVersion: storage.k8s.io/v1 
metadata: 
  name: high-iops-ssd 
provisioner: kubernetes.io/aws-ebs 
parameters: 
  type: io1 
  iopsPerGB: "50" 
---
 kind: StorageClass 
apiVersion: storage.k8s.io/v1 
metadata: 
  name: medium-iops-ssd 
provisioner: kubernetes.io/aws-ebs 
parameters: 
  type: io1 
  iopsPerGB: "25" 
```

请注意，Kubernetes 需要为`iopsPerGb`字段输入一个字符串值，因此您需要引用该值。

如果您正在使用一个经过优化的应用对文件系统进行顺序读写，那么您可能会受益于使用`st1`卷类型，它使用优化的磁存储来提供高吞吐量的读写。不建议将此存储用于一般用途，因为进行随机存取读取或写入时性能会很差:

```
kind: StorageClass 
apiVersion: storage.k8s.io/v1 
metadata: 
  name: throughput 
provisioner: kubernetes.io/aws-ebs 
parameters: 
  type: st1 
```

`sc1`卷类型提供了作为 EBS 卷的最低成本存储，适用于不常访问的数据。与`st1`卷一样，`sc1`针对顺序读写进行了优化，因此在具有随机读写的工作负载上表现不佳:

```
kind: StorageClass 
apiVersion: storage.k8s.io/v1 
metadata: 
  name: cold-storage 
provisioner: kubernetes.io/aws-ebs 
parameters: 
  type: sc1 
```

It is a good idea to decide up front the different storage classes you want to make available in your cluster, and then provide documentation about when each class should be used to users of your cluster.

You should think about submitting a list of storage classes to your cluster as part of your provisioning process, as there are no storage classes created by default when you provision an EKS cluster.

# StatefulSet

到目前为止，我们已经看到了如何使用 Kubernetes 为`PersistentVolumeClaim`自动调配 EBS 卷。这对于许多应用非常有用，在这些应用中，我们需要单个卷来为单个 pod 提供持久性。

但是，一旦我们尝试扩大部署规模，就会遇到问题。运行在同一节点上的 Pods 最终可能会共享该卷。但是由于 EBS 卷在任何时候只能连接到单个实例，任何计划到另一个节点的 pods 都将停留在`ContainerCreating`状态，无休止地等待 EBS 卷被连接。

如果您正在运行一个应用，希望每个副本都有自己唯一的卷，我们可以使用有状态集。当我们想要部署每个副本都需要有自己的持久存储的应用时，有状态集比部署有两个主要优势。

首先，我们可以提供一个模板来为每个 pod 创建一个新的持久卷，而不是通过名称来引用单个持久卷。这允许我们为每个 pod 副本提供一个独立的 EBS 卷，只需通过扩展状态集。如果我们想通过部署来实现这一点，我们需要为每个复制副本创建一个单独的部署，每个复制副本通过名称引用不同的持久卷。

其次，当 pod 由`StatefulSet`调度时，每个副本都有一个一致且持久的主机名，即使 pod 被重新调度到另一个节点，该主机名也保持不变。这在运行软件时非常有用，在软件中，每个副本都希望能够连接到特定地址的对等方。在有状态集被添加到 Kubernetes 之前，将这样的软件部署到 Kubernetes 通常依赖于特殊的插件来使用 Kubernetes API 执行服务发现。

为了说明有状态集是如何工作的，我们将重写示例应用部署清单以使用`StatefulSet`。因为`StatefulSet`中的每个副本 pod 都有一个可预测的主机名，所以我们首先需要创建一个服务来允许到这些主机名的流量被路由到底层 pod:

```
apiVersion: v1 
kind: Service 
metadata: 
  name: randserver 
  labels: 
    app: randserver 
spec: 
  ports: 
  - port: 80 
    name: web 
    targetPort: 3000 
  clusterIP: None 
  selector: 
    app: randserver 
```

每个 pod 都将获得一个主机名，该主机名由有状态集的名称和该集中的 pod 号构成。主机名的域是服务的名称。

因此，当我们用三个副本创建一个名为`randserver`的有状态集合时。该组中的豆荚将被命名为`randserver-0`、`randserver-1`和`randserver-2`。集群内运行的其他服务将能够使用名称`randserver-0.randserver`、`randserver-1.randserver`和`randserver-2.randserver`连接到这些吊舱。

`StatefulSet`的配置与部署的配置非常相似。应该注意的主要区别是:

*   `serviceName`字段，我们需要在其中引用用于提供对 pods 的网络访问的服务。
*   在`volumeClaimTemplates`字段中，我们包含了为`StatefulSet`中的每个吊舱副本创建的`PersistentVolumeClaim`模板。您可以将其视为模板字段的模拟，为创建的每个 pod 提供模板:

```
apiVersion: apps/v1 
kind: StatefulSet 
metadata: 
  name: randserver 
spec: 
  selector: 
    matchLabels: 
      app: randserver 
  serviceName: randserver 
  replicas: 3 
  template: 
    metadata: 
      labels: 
        app: randserver 
    spec: 
      containers: 
      - image: errm/randserver 
        name: randserver 
        volumeMounts: 
        - mountPath: /data 
          name: data 
        securityContext: 
          readOnlyRootFilesystem: true 
  volumeClaimTemplates: 
    - metadata: 
        name: data 
      spec: 
        accessModes: 
          - ReadWriteOnce 
        storageClassName: general-purpose 
        resources: 
          requests: 
            storage: 1Gi 
```

一旦您将`StatefulSet`提交给 Kubernetes，您应该能够看到已经成功调度到集群的吊舱:

```
$ kubectl get pods
NAME           READY     STATUS    RESTARTS   AGE
randserver-0   1/1       Running   0          39s
randserver-1   1/1       Running   0          21s
randserver-2   1/1       Running   0          10s  
```

请注意，每个 pod 的名称都遵循一个可预测的模式，不像使用部署或副本集创建的 pod，每个 pod 都有一个随机的名称。

尝试删除有状态集中的一个 pod，注意它被一个与被删除的 pod 同名的 pod 替换:

```
$ kubectl delete pod/randserver-1
$ kubectl get pods
NAME           READY     STATUS    RESTARTS   AGE
randserver-0   1/1       Running   0          17m
randserver-1   1/1       Running   0          18s
randserver-2   1/1       Running   0          17m  
```

如果您查看持久卷声明，您会发现它们的名称也遵循一种可预测的模式，其中声明的名称由卷声明模板元数据中给出的名称、有状态集的名称和 pod 编号组成:

```
kubectl get pvc
NAME                STATUS    VOLUME
data-randserver-0   Bound     pvc-803210cf-f027-11e8-b16d
data-randserver-1   Bound     pvc-99192c41-f027-11e8-b16d
data-randserver-2   Bound     pvc-ab2b25b1-f027-11e8-b16d  
```

如果删除(或缩减)有状态集，则相关联的持久卷声明仍然存在。这是非常有利的，因为它使丢失应用创建的有价值的数据变得更加困难。如果您稍后重新创建(或扩展)有状态集，那么由于使用了可预测的名称，相同的卷将被重用。

如果您确实打算从集群中完全删除有状态集，您可能还需要另外删除相应的持久卷声明:

```
$ kubectl delete statefulset randserver
statefulset.apps "randserver" deleted
$ kubectl delete pvc -l app=randserver
persistentvolumeclaim "data-randserver-0" deleted
persistentvolumeclaim "data-randserver-1" deleted
persistentvolumeclaim "data-randserver-2" deleted  
```

# 摘要

在本章中，我们了解了 Kubernetes 为您的应用提供的丰富的存储资源调配工具。

您应该已经了解了以下内容:

*   如何为豆荚配置卷
*   如何将卷装入容器
*   如何使用持久卷声明自动调配 EBS 卷
*   如何通过配置存储类来调配不同的 EBS 卷类型
*   如何为有状态集中的每个 pod 动态调配卷

现在，您应该有足够的知识将许多类型的应用部署到 Kubernetes 集群中。

# 进一步阅读

如果您想了解更多关于如何在 Kubernetes 中利用存储的信息，这里有一些您可能会觉得有用的资源:

*   **Kubernetes Helm 图表包括大量使用持久卷**:[https://github.com/helm/charts](https://github.com/helm/charts)的知名数据存储区的许多配置示例
*   **Kubernetes 文档中有关于在 Kubernetes** 中使用存储的详细而广泛的信息:[https://kubernetes.io/docs/concepts/storage/](https://kubernetes.io/docs/concepts/storage/)
*   **Kubernetes EFS 置备程序提供了一个附加置备程序，可以部署该程序来置备由 AWS 弹性文件系统(EFS)支持的卷。如果你想让多个豆荚能够从同一个卷** :
    [中读写，这可能是一个有用的工具](https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs)