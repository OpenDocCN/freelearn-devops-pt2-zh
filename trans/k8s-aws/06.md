# 生产计划

Kubernetes 为开发人员快速构建高度灵活的分布式应用程序提供了一个出色的平台。通过在 Kubernetes 上运行我们的应用程序，我们可以使用许多工具来简化它们的操作，并使它们更加可靠、对错误有弹性，并最终实现高可用性。

为了让我们依赖于我们的应用程序可以从 Kubernetes 继承的一些保证和行为，我们必须了解 Kubernetes 的行为，以及对生产系统有影响的一些因素。

作为集群管理员，了解正在运行的应用程序的需求以及这些应用程序的用户非常重要。

了解 Kubernetes 在生产中的行为方式至关重要，因此在开始为任务关键型流量提供服务之前，获得一些在 Kubernetes 上运行应用程序的实际经验是非常宝贵的。例如，当 GitHub 将他们的主要应用程序迁移到 Kubernetes 时，他们首先将内部用户的流量转移到他们新的基于 Kubernetes 的基础架构，然后再切换他们的主要生产流量。

"The load from internal users helped us find problems, fix bugs, and start getting comfortable with Kubernetes in production. During this period, we worked to increase our confidence by simulating procedures we anticipated performing in the future, writing runbooks, and performing failure tests." —Jesse Newland ([https://githubengineering.com/kubernetes-at-github/](https://githubengineering.com/kubernetes-at-github/))

虽然我可以介绍在生产中的 AWS 上使用 Kubernetes 时您可能会遇到的一些事情，但重要的是要理解每个应用程序和组织都有令人惊讶的独特之处。您应该将 Kubernetes 视为一个工具包，它将使您能够为您的组织构建一个强大而灵活的环境。Kubernetes 不是一个消除对运营专业知识需求的灵丹妙药；它是一个帮助您管理应用程序的工具。

# 设计过程

设计过程如下所示:

![](assets/f50ea2e6-83e1-4fe8-8dbb-d53c03a585c6.png)

当您考虑准备使用 Kubernetes 来管理您的生产基础架构时，您不应该将 Kubernetes 视为您的最终目标。它是构建运行系统的平台的基础。

当您考虑构建一个平台来满足组织中不同人员的需求时，定义您将对 Kubernetes 提出的要求会变得简单得多。当试图规划生产环境时，您需要了解您的组织所具有的需求。显然，您想要管理的软件的技术要求很重要。但这也是理解您的组织需要支持的运营流程的关键。

采用 Kubernetes 为那些对他们运行的软件有复杂需求的组织提供了很多好处。不幸的是，这种复杂性也可能导致以成功的方式安全采用 Kubernetes 的挑战。

# 原规划

你应该考虑在你的初始推广中，你会把精力集中在哪里。您应该寻找一个既能快速交付有价值的结果，又能降低风险的应用程序。如果我们考虑一下 GitHub 的例子，他们最初专注于为内部用户构建一个基础设施，以快速测试他们软件的变化。通过专注于审查或准备基础设施，他们发现了一个适用于 Kubernetes 的应用程序，该应用程序既能为他们组织中的开发人员快速提供价值，又能为他们的业务提供一个低风险领域，因为只有内部用户才能访问该领域。

像这样兼具即时有用性和较低停机影响的应用程序非常有用。它们允许您的组织使用 Kubernetes 获得宝贵的运营经验，并在您尝试处理生产工作负载之前很好地排除 bug 和其他问题。

开始使用 Kubernetes 时，很容易选择您的组织运行的最简单的应用程序，并开始围绕它构建流程和工具。但是，这可能是一个错误，因为它可能会导致您对应用程序应该如何运行做出假设，这可能会使以后将相同的过程和配置应用于更复杂的应用程序变得更加困难。

如果您选择开始构建您的平台来支持一个不需要任何后端服务的简单应用程序，例如数据库，那么您可能会错过一些需要在部署过程中考虑的事情。例如，当部署新版本的应用程序时，由数据库支持的应用程序通常需要运行迁移脚本来更新模式。如果您从设计一个部署过程来满足一个非常简单的应用程序的需求开始，那么您可能要等到以后才能提出这些需求。请记住，部署一个只需要您的平台提供的功能子集的简单应用程序总是比部署一个需要您在设计时没有考虑到的设施的更复杂的应用程序要简单得多。

如果您选择将精力集中在最初采用 Kubernetes 的单个应用程序上，请确保您选择的应用程序能够代表您组织的需求。开始在绿地项目中使用 Kubernetes 可能很有诱惑力，因为您可以在考虑平台的情况下做出应用程序开发决策。但是请记住，一个新的应用程序可能比一个已经使用了很长时间的应用程序要简单得多。在 GitHub 的例子中，他们选择首先部署的应用程序是他们组织运营的最大的应用程序，提供许多核心服务。

如果您的组织有一个每次部署都需要大量操作时间和精力的应用程序，那么这可能是最初采用 Kubernetes 的好选择。您的开发和运营团队将非常了解像这样的应用程序的需求，他们将能够立即开始利用 Kubernetes 来解决以前需要花费时间和精力的问题。

# 为成功做计划

为了在采用 Kubernetes 的项目中成功交付，您应该尽量避免一些事情。

一个很容易陷入的陷阱是变化太多太快。如果您决定采用容器化和 Kubernetes，那么在此基础上采用许多新的流程和工具是非常诱人的。这可能会大大减缓您的进度，因为最初在容器中运行应用程序的项目可能会迅速发展到包含您的组织希望采用的许多其他工具和流程。

您应该致力于避免范围蔓延，并尽可能少地进行更改，以便尽快交付您对 Kubernetes 的初始采用。重要的是不要试图一次性实现太多集装箱化的承诺，因为它们会阻碍您的采用，并可能导致您整个项目的失败。

试着考虑一下您当前正在部署应用程序的环境，并打算首先复制它的设施，然后添加额外的功能。我们在本书其余部分讨论的许多工具和过程对于您的 Kubernetes 集群来说可能确实是可选的，您可以在以后添加这些项目来提供额外的有价值的服务，但不要将其视为采用的阻碍因素。

如果您有机会在额外部署时缩小 Kubernetes 部署提供的基础架构的范围，您应该考虑这样做。它减少了您的组织需要了解的新工具和流程的范围。这将使您有机会在以后更详细地关注这个主题，并参考您在 Kubernetes 上运行应用程序所获得的操作经验。

以日志管理为例，如果您当前的过程是使用 SSH 和尾日志文件登录服务器，您可以使用`kubectl logs`命令向 Kubernetes 集群的操作员提供相同的功能。实现一个解决方案来聚合和搜索集群生成的日志可能是可取的，但不一定是使用 Kubernetes 的障碍。

如果您当前将您的应用程序部署到运行 Linux 发行版的服务器上，该发行版可以作为容器映像随时获得，那么您应该坚持使用该发行版，而不是在这个阶段寻找替代版本，因为您的开发人员和操作人员已经了解它的工作原理，并且您不必投入时间来修复不兼容性。学习在 Kubernetes 上操作应用程序应该是你的重点，而不是学习如何配置新的操作系统发行版。

# 计划成功推出

动摇组织中的流程和职责是很有诱惑力的。但是作为采用像 Kubernetes 这样的新工具的一部分，尝试这样做是有风险的。例如，如果在您的组织中有一个运营团队负责部署和监控您的应用程序，那么您采用 Kubernetes 的时候并不是将这一职责交给其他人(例如您的开发团队)或者尝试自动化手动过程的正确时机。

这可能会令人沮丧，因为通常情况下，采用 Kubernetes 是更广泛的计划的一部分，以改进您的组织使用的流程和工具。您应该先等待成功建立 Kubernetes 的使用和操作。一旦你有了一个稳定的基础，这将使你更好地引入新的工具和过程。您应该将 Kubernetes 的采用视为构建了一个基础，该基础将足够灵活，以实现您将来想要对工具和流程进行的任何更改。

您会发现，一旦您的应用程序基础架构在 Kubernetes 上运行，实现新的工具、服务和流程就会变得简单得多。一旦你有了一个 Kubernetes 集群，你会发现尝试一个新工具的障碍大大减少了。您无需花费大量时间进行规划和资源调配，只需向集群提交新配置，就可以快速评估和试用新工具。

# 发现需求

设计要求如下图所示:

![](assets/5a21d4ce-edfe-4d5e-9fa1-f901d1f353fc.png)

**可用性**、**容量**和**性能**是我们在准备生产时应该考虑的关键属性。当收集集群的功能需求时，它可以帮助分类哪些需求暗示了对这些属性的一些考虑。

重要的是要理解，如果不进行一些权衡，就不可能对所有三种属性进行优化。例如，对于依赖非常高的网络性能的应用程序，AWS 提供了一种称为集群放置组的工具。这确保了最佳的网络性能，方法是配置 EC2 虚拟机，使它们之间可以实现快速的网络互连(大概是通过将它们放在 AWS 数据中心的附近)。通过这种方式调配实例，可以在群集放置组内的计算机之间实现最高的网络吞吐量(超过 5 GB)和最低的延迟。对于一些需要这些性能水平的应用程序，这可能是一个值得的优化。

但是，由于群集放置组中的 EC2 实例不能跨越多个可用性区域，因此这种设置的可靠性可能较低，因为潜在的电源或连接问题可能会影响特定区域中的所有实例，尤其是在为了最大化互连速度而部署它们的情况下。如果您的应用程序对这种高性能网络没有要求，那么用可靠性换取更高的性能确实是不明智的。

支配这些属性是生产系统的一个非常重要的属性— **可观察性**。可观察性实际上描述了集群操作员理解您的应用程序正在发生什么的能力。如果不能理解应用程序的性能和行为是否正常，就无法评估、改进和改进系统的设计。在设计集群时，这是一个重要的反馈循环，允许您根据运营经验维护和改进集群。如果在规划集群时不考虑可观察性，那么调试集群本身和应用程序的问题会困难得多。

当我们在计划阶段讨论应用程序需求时，很难理解您的应用程序的需求是什么。对集群的性能、底层硬件和运行在其上的应用程序具有良好的可观察性，这使您能够做出务实的决策，并足够灵活地进行更改以支持您的应用程序，因为您会发现更多关于它们在生产工作负载下的行为以及它们的功能随着时间的推移而发展。

最后，也许最需要考虑的属性是安全性。将集群的安全性留到规划过程的最后是一个错误。请记住，虽然安全本身不会导致您的项目成功，但是无法保护集群可能会导致灾难性的后果。

最近的研究和披露表明，不安全的 Kubernetes 集群已经成为那些利用您的计算能力进行加密货币挖掘和其他邪恶目的的人的一个有吸引力的目标，更不用说访问您的集群被用来访问您组织内的敏感数据的可能性了。

应该在集群的整个生命周期中考虑和监控安全性；事实上，您应该尝试理解每一个其他需求的安全含义。您需要考虑您的组织成员需要如何与 Kubernetes 进行交互，以及有一个计划来确保您的集群的配置和软件以及您在其上运行的应用程序的安全。

在本章的以下部分中，我们将介绍一些想法，以帮助您理解可能需要考虑的这些属性。希望这一章能让您有足够的理解来发现您的需求，并开始为生产规划您的集群。对于实施计划所需的具体知识，继续阅读；这本书的后半部分几乎完全集中在实施计划所需的实用知识上。

# 有效性

下图显示了可用性:

![](assets/ace2923e-7d64-418e-90af-66ec4b2d60bd.png)

规划生产系统时，要考虑的最重要的事情之一是可用性。几乎总是这样，我们运行软件是为了给用户提供服务。如果，无论出于什么原因，我们的软件不能满足用户对它的要求，那么，我们经常不能满足他们的期望。根据您的组织提供的服务，不可用可能会导致您的用户不高兴、不方便，甚至遭受损失或伤害。为任何生产系统制定适当计划的一部分是了解停机或错误可能如何影响您的用户。

可用性的定义取决于集群运行的工作负载种类和业务需求。规划 Kubernetes 集群的一个关键部分是了解用户对您正在运行的服务的需求。

例如，考虑一个批处理作业，它每天向用户发送一次业务报告。只要您能确保它每天至少在大致正确的时间运行一次，您就可以认为它是 100%可用的，而您的用户可以在白天或晚上的任何时间访问的网络服务器需要随时可用并且没有错误。

当你的组织的首席执行官早上 9 点到达工作地点，收件箱里有一份准备阅读的报告时，他们会很高兴。他们不会在意任务在午夜未能运行，几分钟后重试成功。但是，如果托管他们用来阅读电子邮件的 web 邮件应用程序的应用程序服务器在一天中的某个时刻不可用，它们可能会被中断并带来不便:

![](assets/c9aafa0a-5cfd-48aa-9ec5-0f6d085f4e19.png)

A simple formula to calculate the availability of a service

一般来说，系统工程师认为给定服务的可用性是向服务发出的请求总数中成功请求的百分比。

我们可以考虑一个甚至失败几次的批处理作业。我们对系统的要求(报告每天通过电子邮件发送给正确的人一次)只要求作业至少成功完成一次。如果我们通过重试优雅地处理失败，对我们的用户没有影响。

当然，您应该为您的系统计划满足的确切数量在很大程度上是用户需求和组织优先级的问题。然而，值得记住的是，与停机时间可以接受的类似系统相比，为更高可用性设计的系统几乎总是更复杂，需要更多的资源。随着服务接近 100%可用性，实现额外可靠性的成本和复杂性呈指数级增长。

如果您还不了解它们，那么在您的组织内发起关于可用性需求的讨论是合理的。您应该这样做，以便设定目标并了解用 Kubernetes 运行软件的最佳方式。以下是一些你应该尝试回答的问题:

*   *你知道你的用户是如何访问你的服务的吗？*例如，如果您的用户正在使用移动设备，那么无论如何，互联网连接都可能更加不可靠，从而掩盖了服务的正常运行时间(或其他方面)。
*   如果您正在将您的服务迁移到 Kubernetes，*您知道它目前有多可靠吗？*
*   *你能给不可用的东西估价吗？*例如，电子商务或广告技术组织将知道在一段停机时间内损失的确切金额。
*   *您的用户准备接受什么级别的不可用性？* *你有竞争对手吗？*

您可能已经注意到，所有这些问题都与您的用户和组织有关；它们中的任何一个都没有可靠的技术答案，但是您需要能够回答它们，以了解您正在构建的系统的需求。

为了提供在网络(如 web 服务器)上访问的高可用性服务，我们需要确保该服务可以随时响应请求。由于我们无法确保运行我们服务的底层机器 100%可靠，因此我们需要运行我们软件的多个实例，并将流量仅路由到那些能够响应请求的实例。

这个批处理作业的语义暗示(在合理的范围内)我们不太关心作业执行所花费的时间，而 web 服务器响应所花费的时间是相当重要的。有许多研究表明，即使是添加到网页加载时间长度上的几秒钟的延迟也会对用户产生显著的、可测量的影响。因此，即使我们能够隐藏失败(例如，通过重试失败的请求)，我们的回旋余地也要小得多，事实上，如果高优先级请求花费的时间超过特定阈值，我们甚至可能认为它们已经失败。

您可能选择在 Kubernetes 上运行应用程序的一个原因是，您已经听说了它的自我修复特性。Kubernetes 将管理我们的应用程序，并在需要时采取行动，以确保我们的应用程序继续以我们要求 Kubernetes 的方式运行。这是 Kubernetes 对配置的声明性方法的有益效果。

使用 Kubernetes，我们会要求在集群上运行一定数量的服务副本。控制平面能够采取措施来确保这种情况继续为真，即使发生了影响正在运行的应用程序的事情，无论是由于节点故障还是由于内存泄漏导致应用程序定期被终止。

与此形成对比的是，强制部署过程依赖于操作员选择运行应用程序的特定底层机器(或一组机器)。如果一台机器出现故障，或者即使应用程序的一个实例出现问题，也需要人工干预。我们希望不间断地为用户提供他们需要的服务。

对于始终在线或对延迟敏感的应用程序，如网络服务器，Kubernetes 为我们提供了运行应用程序的多个副本和测试服务运行状况的机制，以便可以从服务中删除失败的实例，甚至重新启动。

对于批处理作业，Kubernetes 将重试失败的作业，并在底层节点失败时将其重新调度到其他节点。这些重启和重新调度失败应用程序的语义依赖于 Kubernetes 控制平面来运行。一旦 pod 在特定节点上运行，它将继续运行，直到发生以下情况:

*   它存在
*   它因为使用了太多的内存而被库布雷杀死
*   应用编程接口服务器请求杀死它(也许是为了重新平衡集群或者为具有更高优先级的 pod 让路)

这意味着控制平面本身可以暂时不可用，而不会影响集群上运行的应用程序。但是，在控制平面再次可用之前，不会重新计划任何失败的吊舱，或者在失败的节点上运行的吊舱。显然，您还需要应用编程接口服务器可用，以便与它进行交互，因此您的组织将新配置推送到集群的需求(例如，部署新版本的应用程序)也应该考虑在内。

我们将在[第 7 章](03.html)*生产就绪集群*中讨论一些策略和工具，您可以使用它们来提供高可用性的控制平面。

# 容量

容量如下图所示:

![](assets/c53ff269-a214-41be-9afa-eeec3728a8a6.png)

运行像 Kubernetes 这样的系统意味着您可以在应用程序启动所需的时间内响应对服务的额外需求。这个过程甚至可以通过工具实现自动化，例如**水平吊舱自动缩放器**(我们将在[第 8 章](08.html)、*抱歉我的应用程序吃掉了集群*)。

当我们将这种灵活性与我们随意启动新 EC2 实例的能力结合起来时，容量规划的参与度会比过去低得多。Kubernetes 和 AWS 允许我们构建只消耗他们在任何给定时间需要使用的资源量的应用程序。我们可以对应用程序的使用需求做出反应，而不是预测应用程序的需求并预先承诺使用资源。Kubernetes 终于让我们兑现了云计算的一个承诺:我们将只为我们使用的资源付费的承诺。

为了最有效地利用您在 AWS 上付费使用的资源，您应该考虑以下几点。

# EC2 实例类型

当准备启动一个 Kubernetes 集群时，您可能会被吸引去思考组成集群的实例的类型和大小。您选择的实例会对 Kubernetes 集群的利用率、性能和运营成本产生很大影响。

当 Kubernetes 将您的 pod 调度到集群中的工作节点时，它会考虑作为 pod 定义一部分的资源请求和限制。

通常，您的 pod 规范会要求若干个 CPU(或其一部分)和一定数量的内存。在 AWS 上，Kubernetes 使用 AWS 的 vCPU 作为度量单位。vCPU(在大多数实例类型上)是单个 CPU(超级)线程，而不是一个 CPU 内核。如果您请求少量的 CPU，那么 Kubernetes 会为您的 pod 分配一份 vCPU。请求的内存以字节为单位。

EC2 实例有几种不同的类型，它们提供不同的 CPU 与内存比率。

# EC2 实例类型

EC2 实例类型如下表所示:

| **类别** | **类型** | **CPU 与内存之比:vCPU:GiB** | **注释** |
| 可爆裂的 | T3 | 1 个 CPU : 2 个 | 提供 5-40%的 CPU 基线+可突发的额外使用。 |
| 中央处理器优化 | 溴化五烃季胺 | 1 个 CPU : 2 个 |  |
| 通用 | M5 | 1 个 CPU : 4 个 |  |
| 内存优化 | R5 | 1 个 CPU : 8 个 |  |
|  | X1 | 1 个 CPU:15 个 |  |
|  | X1e | 1 个 CPU:30 个 |  |
| 如果您需要以下实例类型提供的额外资源(图形处理器和/或本地存储)，您应该只考虑这些类型: |
| 国家政治保卫局。参见 OGPU | P3 | 1 个 CPU:7.6 个 | 1 个 GPU : 8 个 CPU (NVIDIA Tesla V100) |
|  | P2 | 1 个 CPU:4 个 | i. 1 GPU ： 4 CPU （英伟达 K80） |
| 储存；储备 | 氕 | 1 个 CPU:4 个 | 2TB 硬盘:8 个中央处理器 |
|  | D2 | 1 个 CPU:7.6 个 | 3TB 硬盘:2 个中央处理器 |
|  | I3 | 1 个 CPU:7.6 个 | 475 提供 SSD : 2 个 CPU |

在准备集群时，我们应该考虑组成集群的实例类型和实例大小。

当 Kubernetes 将我们的豆荚安排到集群中的节点时，它的目标当然是将尽可能多的容器打包到集群中。然而，如果我们大多数豆荚中的 CPU 与内存请求的比率与底层节点明显不同，这可能会受到阻碍。

例如，考虑一个场景，其中我们部署了请求 1 个 CPU 和 2 个 GiB 内存的 pods 到我们的集群。如果我们的集群由`m5.xlarge`个实例组成(4 个 vCPU 和 16 个 GiB 内存)，我们的每个节点将能够运行 4 个 pod。一旦这四个单元在这个节点上运行，就没有更多的单元能够被调度到这个节点，但是有一半的内存没有被使用，实际上被搁浅了。

如果您的工作负载相当均匀，当然很容易计算出哪种实例类型将为您的应用程序提供最佳的 CPU 与内存比率。然而，大多数集群运行大量的应用程序，每个应用程序需要不同数量的内存和 CPU(甚至可能还需要其他资源)。

在[第 8 章](08.html)、*对不起我的应用程序吃了集群*中，我们讨论了使用集群自动缩放器来自动添加和删除 AWS 自动缩放组中的实例，以便调整集群的大小，以匹配任何给定时间的集群需求。我们还讨论了如何使用集群自动缩放器来缩放具有多种不同实例类型的集群，以解决集群中 CPU 与内存的比例匹配问题，在集群中，运行的工作负载的大小和形状是动态的，并且会不时发生变化。

# 宽度与深度

亚马逊为每个家庭提供了许多实例大小；例如，m5 和 c5 系列有六种不同的可用实例大小，并且每升级一级都会提供两倍的资源。因此，最大的实例拥有的资源是最小实例的 48 倍。*我们应该如何选择构建集群的实例大小？*

*   实例的大小限制了您可以在集群上运行的最大 pod。该实例需要比您最大的 pod 大 10-20%，以考虑系统服务的开销，例如日志记录或监控工具、Docker 和 Kubernetes 本身。
*   较小的实例将允许您以较小的增量扩展集群，从而提高利用率。
*   更少(更大)的实例可能更容易管理。
*   较大的实例可能会将较低比例的资源用于集群级任务，如日志传送和指标。
*   如果您想使用监视或日志记录工具，如 Datadog、Sysdig、NewRelic 等，其中定价是基于每个实例的模型，那么较少的较大实例可能更具成本效益。
*   更大的实例可以提供更多的磁盘和网络带宽，但是如果每个实例运行更多的进程，这可能不会带来任何好处。
*   较大的实例大小不太可能在虚拟机管理程序级别受到噪音邻居问题的影响。
*   更大的实例通常意味着你的吊舱有更多的位置。当目标是提高利用率时，这通常是有利的，但有时会导致意外的资源限制模式。

# 表演

下图显示了影响性能的集群关键组件:

![](assets/7d5d9899-71d1-4f75-b644-c548bd23330a.png)

# 磁盘性能

如果您的某些应用程序依赖于磁盘性能，了解连接到实例的 EBS 卷的性能特征会非常有用。

当前一代的 EC2 实例都依赖于 EBS 存储。EBS 存储实际上是一种共享的网络连接存储，因此性能会受到许多因素的影响。

如果您的集群运行在最新一代的 EC2 实例上，您将使用 EBS 优化。这意味着专用带宽可用于您的 EBS 卷上的 I/O 操作，有效地消除了 EBS 和其他网络活动之间的争用。

EBS 卷的总最大可用带宽由 EC2 实例的大小决定。在运行多个容器的系统中，每个容器可能连接有一个或多个 EBS 卷，您应该知道该上限适用于实例上使用的所有卷的聚合。

If you are planning to run workloads that expect to do large amounts of disk I/O, you may need to consider the total I/O available to the instance.

EBS 基于两种基本技术提供四种卷类型。`gp2`和`io2`卷基于固态硬盘技术，而 st1 和 sc1 卷基于硬盘技术。

这种类型的磁盘对我们很有用，因为大体上，我们可以将您的应用程序可能交付的工作负载分为两组。首先，需要对文件系统进行快速随机读取和/或写入的那些。属于这一类别的工作负载包括数据库、网络服务器和引导卷。对于这些工作负载，性能的限制因素通常是**每秒的 I/O 操作** ( **IOPS** )。其次，有些工作负载需要尽可能快地从磁盘进行顺序读取。这包括地图缩减、日志管理和数据存储等应用程序，如 Kafka 或 Casandra，它们经过专门优化，尽可能实现顺序读写。

There are hard upper limits at the instance level to the maximum performance you can achieve with EBS volumes. The maximum IOPS available to all EBS volumes attached to a single instance is 64,000 on the largest instance size available on c5 and m5 instances. The smallest c5 and m5 instances only provide 1,600 IOPS. It is worth bearing these limits in mind, either if you want to run workloads requiring the higher levels of disk performance on smaller EC2 instance types or are using multiple EBS volumes on the larger instance types.

# gp2

`gp2` EBS 卷应该是大多数通用应用程序的第一个调用端口。它们以适中的价格提供**固态硬盘** ( **固态硬盘**)性能。`gp2`卷的性能基于信用体系。这些卷提供了基准性能，并且还随着时间的推移累积信用，当需要时允许性能崩溃高达 3，000 IOPS，直到累积信用耗尽。

当一个`gp2`卷被创建时，它会自动收到一个信用余额，允许它在 30 分钟内爆发到 3000 IOPS。当卷用作引导卷时，这非常有用，或者当数据需要作为引导过程的一部分快速复制到卷时，这可以提供更好的性能。

突发信用累积的速率和`gp2`卷的基线性能与卷大小成正比。小于 33 千兆字节的卷的最低基准性能始终为 100 IOPS。大于 1 TB 的卷的基准性能大于 3，000 IOPS，因此您不需要考虑突发信用。对于 3.3 TB(或更大)的卷，单个`gp2`卷的最大可用性能为 10，000 IOPS。

如果您的工作负载需要从`gp2`卷获得更高的性能，快速解决方法可以是使用更大的卷(即使您的应用程序不需要它提供的存储)。

您可以通过将 IOPS 乘以数据块大小(256 KiB)来计算最大吞吐量。但是，`gp2`卷将总吞吐量限制在 160 MiB/s，因此大于 214 GiB 的卷只能提供 160 MiB/s。

拥有监控与磁盘使用相关的指标的工具，对于了解磁盘性能如何影响您的应用程序，以及确定您是否达到性能极限以及达到性能极限的位置，都是非常宝贵的。

# io2

对于可靠性能至关重要且`gp2`卷无法提供足够的 IOPS 的应用程序，可以使用`io2`卷(也称为调配的 IOPS 卷)。如果它们所连接的实例能够支持它们，则可以调配`io2`卷，以提供最多 32，000 个 IOPS 卷。创建`io2`实例时，会提前指定所需的 IOPS(我们将在[第 9 章](09.html)、*存储状态*中讨论如何使用 Kubernetes)。可以为单个卷调配的最大 IOPS 取决于卷的大小，IOPS 和 GiB 之间的存储比例为`50:1`。因此，为了提供最大 IOPS，您需要请求至少 640 千兆字节的卷。

对于需要的 IOPS 数量少于`gp2`卷支持的数量(10，000)以及需要的吞吐量少于 160 MiB/s 的情况，支持类似性能特征的`gp2`卷通常不到`io2`卷价格的一半。除非您知道您需要`io2`卷的增强性能特性，否则坚持使用`gp2`卷以用于大多数通用用途是有意义的。

# st1

对于已经针对顺序读取进行了优化的应用程序，其中主要关注的性能指标是吞吐量，考虑到固态硬盘目前的主导地位，注意到最佳性能仍然由旋转磁盘提供可能会令人惊讶。

**st1** (和 **sc1** )卷是 AWS 上可用的最新类型的 EBS 卷。它们旨在为工作负载提供高吞吐量，例如地图缩减、日志处理、数据仓库和流工作负载，例如卡夫卡。st1 卷提供高达 500 MiB/s 的吞吐量，成本不到 gp2 实例的一半。缺点是它们支持低得多的 IOPS，因此对于随机或小的写操作，性能会差得多。您可能对固态硬盘进行的 IOPS 计算略有不同，因为数据块大小要大得多(1 MB 对 256 KB)。因此，进行一次小的写操作所需的时间与写入一个完整的 1 MB 数据块(如果按顺序写入)的时间一样长。

如果您的工作负载被正确优化以利用 st1 卷的性能特征，则非常值得考虑使用它们，因为成本大约是 gp2 卷的一半。

就像 gp2 卷一样，st1 使用半身桶模型来表现。但是，累积的信用允许吞吐量突破基线性能。基准性能和积分累积速率与卷大小成正比。对于大于 2 TiB 的卷，最大突发性能为 500 MiB/s，对于大于 12.5 TiB 的卷，最大基线性能为 500 MiB/s，对于这种大小(或更大)的卷，由于性能不变，因此无需考虑突发特性。

# sc1

`sc1`卷提供了 AWS 上可用的最低成本的数据块存储。它们提供了与`st1`卷相似的性能特征，但速度大约是后者的一半，成本也是后者的一半。您可能会考虑将它们用于需要从文件系统存储和检索数据的应用程序，但是访问更不频繁，或者性能对您来说并不那么重要。

`sc1`卷可被视为档案或 blob 存储系统的替代方案，如`s3`，因为成本大致相似，但优点是不需要特殊的库或工具来从中读取和写入数据，当然，在数据被读取和使用之前，延迟要低得多。

在类似 Kafka 或日志管理的用例中，您可能会考虑将`sc1`卷用于您仍然需要保存在在线存储中的旧数据，这样它就可以立即使用，但是访问频率较低，因此您希望优化存储成本。

# 建立工作关系网

运行分布式系统时，网络性能可能是影响应用程序整体可观察性能的关键因素。

鼓励构建不同组件之间主要通过网络进行通信的应用程序的架构模式(例如，SOA 和微服务)会导致集群内联网成为性能瓶颈的应用程序。群集数据存储还会对群集内联网提出很高的要求，尤其是在写操作期间以及在扩展或维护操作期间重新平衡群集时。

当然，当运行暴露于互联网或其他广域网的服务时，网络性能也是一个需要考虑的因素。

最新一代的 EC2 实例类型受益于网络接口，AWS 将其描述为增强的网络。为此，您需要运行一个相对较新的实例类型(M5、C5 或 R4)，并为亚马逊的弹性网络适配器安装一个特殊的网络驱动程序。幸运的是，如果您正在使用任何主要 Linux 发行版的官方 AMI，这应该已经为您完成了。

您可以使用`modinfo`命令检查是否安装了正确的驱动程序:

```
    $ modinfo ena
    filename:          /lib/modules/4.4.11- 
    23.53.amzn1.x86_64/kernel/drivers/amazon/net/ena/ena.ko
    version:            0.6.6
    license:            GPL
    description:      Elastic Network Adapter (ENA)
    author:             Amazon.com, Inc. or its affiliates
    ... 
```

如果没有安装**弹性网络接口**的驱动程序，您会看到如下内容:

```
    $ modinfo ena
    ERROR: modinfo: could not find module ena
```

增强网络带来的性能提升不需要额外花费任何成本，因此无论何时准备生产，您都应该检查配置是否正确。不支持增强网络的唯一常用实例是 t2 可突发实例类型。

EC2 实例的网络性能与实例大小成正比，每个实例类型中只有最大的实例大小能够满足 10 或 20 GBps 的主要网络吞吐量。即使使用最大的 EC2 实例大小，标题网络吞吐量也只能在与集群放置组中的其他实例通信时实现。

集群放置组可用于请求亚马逊在其数据中心的特定区域启动您需要的每个实例，以便提供最快的速度(和最低的延迟)。为了提高网络性能，我们可以调整两个变量:

*   **增加实例大小**:这使得实例可以更快地联网，并且还增加了配置，因此更有可能在服务之间进行本地主机网络调用。
*   **将您的实例添加到集群放置组**:这可以确保您的实例在物理上位于附近，从而提高网络性能。

在做出这样的决定之前，您需要知道网络实际上是您的性能瓶颈，因为所有这些选择都会使您的集群更容易受到 AWS 基础架构中潜在故障的威胁。因此，除非您已经知道您的特定应用程序将对集群网络提出特定要求，否则您不应该尝试优化以获得更高的性能。

# 安全

此图显示了影响安全性的一些关键领域:

![](assets/989337d6-4468-42f6-9a96-069308556c62.png)

保护构成集群基础设施的配置和软件至关重要，尤其是如果您计划向互联网公开您在其上运行的服务。

您应该考虑到，如果您向公共互联网公开具有众所周知的软件漏洞或配置错误的服务，那么您的服务被用于扫描易受攻击系统的自动化工具检测到可能只需要几个小时。

将集群的安全性视为移动目标非常重要。这意味着您或您使用的工具需要了解新的软件漏洞和配置漏洞。

Kubernetes 软件以及您的主机的底层操作系统软件的漏洞将由 Kubernetes 社区和您的操作系统供应商进行更新和修补，只需操作员制定一个程序来应用可用的更新。

更关键的是您环境的配置，因为验证其安全性和正确性的责任完全落在您的肩上。除了花时间验证和测试配置之外，您还应该将配置的安全性视为一个移动目标。您应该确保在更新时花时间查看 Kubernetes 更改日志中的更改和建议。

# 总是在更新

Kubernetes 的新小版本大约每三个月发布一次。该项目可以每周发布一次补丁级别的更新。补丁级别的更新通常包括对更多主要错误的修复，以及对安全问题的修复。Kubernetes 社区目前在任何时候都支持三个次要版本，随着每个新的次要版本的发布，最早支持版本的定期补丁级别更新也将结束。这意味着，当您计划和构建一个集群时，您需要计划对 Kubernetes 软件进行两种维护:

*   **补丁级更新**:一个月最多更新几次:
    *   这些应该保持非常紧密的兼容性，并且执行起来很简单。
    *   它们应该易于执行，停机时间很少(或没有)。
*   **小版本升级**:每 3 到 9 个月:
    *   在次要版本之间升级时，您可能需要对集群的配置进行微小的更改。
    *   Kubernetes 确实保持了良好的向后兼容性，并且有一个在删除或更改配置选项之前取消它们的策略。只需记住在更改日志和日志输出中记录反对警告。
    *   如果您正在使用依赖于 beta 或 alpha APIs 的第三方应用程序(或者已经编写了自己的工具)，您可能需要在升级集群之前更新这些工具。仅使用稳定 API 的工具应该在小版本更新之间继续工作。

*   您用来更新组成集群的计算机上的软件的过程实际上取决于您正在使用的工具。

您可以采取两种主要策略——就地升级和基于映像的不可变更新策略。

# 就地更新

有几种工具允许您升级集群节点上的底层操作系统。基于 Debian 的系统的`unattended-upgrades`或基于红帽的系统的`yum-cron`等工具允许您在没有任何操作员输入的情况下在节点上安装更新的软件包。

当然，在生产环境中，如果特定的更新导致系统失败，这可能会有一定的风险。

通常，如果您正在管理一个具有自动更新的系统，您将使用包管理器将基本组件(如 Kubernetes 和 **etcd** )固定到特定版本，然后以更可控的方式处理这些组件的升级，可能使用配置管理工具，如 Puppet、Chef 或 Ansible。

以自动方式升级此类软件包时，当某些组件更新时，需要重新启动系统。诸如**KUbernetes REboot Daemon**(**Kured**)、([https://github.com/weaveworks/kured](https://github.com/weaveworks/kured))之类的工具可以监视特定节点需要重新启动的信号，并在集群中协调重新启动节点，以保持集群上运行的服务的正常运行时间。这是通过首先向 Kubernetes Scheduler 发送信号，让它将工作负载重新调度到其他节点，然后触发重新启动来实现的。

还有一种新的操作系统，如 CoreOS 的容器 Linux 或谷歌的容器优化操作系统，它们采用了略有不同的更新方法。这些新的容器优化的 Linux 发行版根本不提供传统的包管理器，而是要求您将所有不在基本系统中的东西(如 Kubernetes)作为容器运行。

这些系统处理基本操作系统的更新，更像消费电子产品中的固件更新系统。这些操作系统中的基本根文件系统是只读的，并从两个特殊分区之一装载。这允许系统在后台将新的操作系统映像下载到未使用的分区。当系统准备升级时，它会重新启动，并且来自第二个分区的新映像作为根文件系统装载。

这样做的好处是，如果升级失败或导致系统变得不稳定，很容易回滚到上一个版本；事实上，这个过程甚至可以自动化。

如果您使用的是容器 Linux，您可以使用**容器 Linux 更新操作符**来协调由于操作系统更新而导致的重新启动([https://github.com/coreos/container-linux-update-operator](https://github.com/coreos/container-linux-update-operator))。使用此工具，您可以确保主机上的工作负载在重新启动前重新计划。

# 不变的图像

虽然有一些工具可以帮助管理升级您的主机，但是采用使用不可变映像的策略也有一些好处。

一旦您使用 Kubernetes 管理了在您的基础架构上运行的应用程序，那么需要安装在您的节点上的软件就会标准化。这意味着将主机的配置更新为不可变的映像变得更加简单。

这可能很有吸引力，因为它允许您以类似于使用 Docker 构建应用程序容器的方式来管理构建和部署节点软件。

通常，如果您采用这种方法，您将希望使用一种工具来简化 AMI 格式的构建映像，并使它们可用于其他工具来启动新的 EC2 实例，以替换用以前的映像启动的实例。封隔器就是这样一种工具。

# 网络安全性

在 AWS 上运行 Kubernetes 时，需要配置四个不同的层，以便正确保护集群上的流量。

# 节点下网络

为了让流量在 pods 和集群上运行的服务之间传递，您需要配置应用于节点的 AWS 组以允许此流量。如果您使用的是覆盖网络，这通常意味着允许特定端口上的流量，因为所有通信都被封装为通过单个端口传递(通常是 UDP 数据包)。例如，法兰绒覆盖网络通常被配置为通过端口`7890`上的 UPD 进行通信。

当使用本地 VPC 网络解决方案时，例如`amazon-vpc-cni-k8s`，通常需要允许所有流量在节点之间通过。`amazon-vpc-cni-k8s`插件将多个 pod IP 地址与单个弹性网络接口相关联，因此通常不可能使用安全组以更精细的方式管理节点下网络。

# 节点主网络

在正常操作中，运行在您的节点上的 Kubernetes 需要连接到 Kubernetes API，以发现它预期运行的 pods 的定义。

通常，这意味着允许在端口`443`上建立从您的工作节点到您的控制平面安全组的 TCP 连接。

控制平面连接到端口`10250`上暴露的应用编程接口上的 kubelet。这是`logs`和`exec`功能所需要的。

# 外部网络

正确理解允许来自集群外部的哪些流量访问您的节点是保持集群安全的关键部分。

最近，一些研究人员发现了大量安全的集群，这些集群允许任何在互联网上访问它们的人无限制地访问 Kubernetes 仪表板，从而访问集群本身。

通常，在这些情况下，群集管理员未能正确配置仪表板来验证用户身份。但是，如果他们仔细考虑暴露在更广泛的互联网上的服务，这些违规可能已经避免。只有将此类敏感服务暴露给特定的 IP 地址或通过 VPN 访问您的 VPC 的用户，才能提供额外的安全层。

当您确实想要向更广泛的互联网公开服务(或入口控制器)时，Kubernetes 负载平衡器服务类型将为您配置适当的安全组(以及提供**弹性负载平衡器** ( **ELB** ))。

# 库比特网络

开箱即用，Kubernetes 不提供任何工具来控制集群上运行的吊舱之间的网络访问。集群上运行的任何 pod 都可以连接到任何其他 pod 或服务。

对于完全受信任的应用程序的较小部署来说，这可能是合理的。如果您想提供策略来限制集群上运行的不同应用程序之间的连接，您需要部署一个网络插件来强制执行 Kubernetes 网络策略，例如 Calico、Romana 或 WeaveNet。

Whist there is a large choice of network plugins that can be used to support the enforcement of the Kubernetes Network policy, if you have chosen to make use of AWS-supported native VPC networking, it is recommended to use Calico, as this configuration is supported by AWS. AWS provide example configuration to deploy Calico alongside the `amazon-vpc-cni-k8s` plugin in their GitHub repository: [https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s).

库本内特斯应用编程接口提供`NetworkPolicy`资源，以提供策略来控制来自吊舱的流量的进出。每个`NetworkPolicy`都以标签选择器和名称空间影响的荚为目标。由于默认情况下 pods 没有网络隔离，如果您希望严格提供一个默认的`NetworkPolicy`来阻止尚未提供特定网络策略的 pods 的流量，这将非常有用。

Check the Kubernetes documentation for some examples of default network policies to allow and deny all traffic by default at [https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies).

# IAM 角色

Kubernetes 附带了一些与 AWS 的深度集成。这意味着 Kubernetes 可以执行一些任务，例如配置 EBS 卷并将它们连接到集群中的 EC2 实例、设置 ELB 以及代表您配置安全组。

为了让 Kubernetes 拥有执行这些操作所需的访问权限，您需要提供 IAM 凭据，以允许控制平面和节点获得所需的访问权限。

通常，最方便的方法是附加一个与相关 IAM 角色相关联的实例配置文件，以授予在该实例上运行的 Kubernetes 进程所需的权限。你在[第三章](03.html)、*到达云端*中看到了这样的例子，当时我们使用`kubeadm`启动了一个小集群。规划生产集群时，您还应该考虑以下几点:

*   *你在运行多个集群吗？* *需要集群资源隔离吗？*
*   *您的集群上运行的应用程序是否也需要访问 AWS 内需要身份验证的资源？*
*   *集群中的节点是否需要使用 AWS IAM Authenticator 使用 Kubernetes API 进行身份验证？*如果您正在使用亚马逊 EKS，这也将适用。

如果您在您的 AWS 帐户中运行多个集群(例如，对于生产和试运行或开发环境)，值得考虑如何定制您的 IAM 角色，以防止集群相互干扰资源。

理论上，一个集群不应该干扰另一个集群创建的资源，但是您可能会重视每个环境的独立 IAM 角色所能提供的额外安全性。不在生产和开发环境或登台环境之间共享 IAM 角色是一种好的做法，可以防止一个环境中的配置错误(甚至 Kubernetes 中的错误)对与另一个集群相关联的资源造成损害。Kubernetes 与之交互的大多数资源都用`kubernetes.io/cluster/<cluster name>`标记。通过这些资源中的一些，IAM 提供了将某些操作限制在与该标签匹配的资源上的能力。以这种方式限制删除操作是减少潜在危害的一种方法。

当集群上运行的应用程序需要访问 AWS 资源时，有多种方法可以向 AWS 客户端库提供凭据，以便正确进行身份验证。您可以向应用程序提供凭据，将机密作为配置文件或环境变量装载。但是提供 IAM 凭证最方便的方法之一是使用与实例配置文件相同的机制将 IAM 角色与您的 pods 相关联。

工具如`kube2iam`或`kiam`拦截 AWS 客户端库对元数据服务的调用，并根据 pod 上的注释集提供令牌。这允许将 IAM 角色作为正常部署过程的一部分进行分配。

**kiam** ([https://github.com/uswitch/kiam](https://github.com/uswitch/kiam)) and **kube2iam** ([https://github.com/jtblin/kube2iam](https://github.com/jtblin/kube2iam)) are two similar projects designed to provide IAM credentials to Kubernetes pods. Both projects run as an agent on each node, adding network routes to route traffic destined for the AWS metadata service. kiam additionally runs a central server component that is responsible for requesting tokens from the AWS API and maintains a cache of the credentials required for all running pods. This approach is noted to be more reliable in production clusters and reduces IAM the permissions required by the node agents.

使用这些工具之一的另一个优点是，它可以防止在群集上运行的应用程序使用分配给底层实例的权限，从而降低应用程序错误或恶意访问提供控制平面服务的资源的风险。

# 确认

设置集群时，您可以做出许多不同的选择来配置集群。重要的是，您要有某种方法来快速验证您的集群是否能正常运行。

这是 Kubernetes 社区为了证明不同的 Kubernetes 分布是*一致*而解决的问题。为了获得特定 Kubernetes 发行版符合性的认可，有必要对集群运行一组集成测试。这些测试对于提供预打包的 Kubernetes 安装的供应商来说非常有用，可以证明它们的分发功能是正确的。对于集群操作员来说，使用它来快速验证软件更新的配置更改使您的集群处于可操作状态也非常有用。

Kubernetes 一致性测试是基于 Kubernetes 代码库中的一些特别自动化的测试。这些测试是针对测试集群运行的，作为 Kubernetes 代码库的端到端验证的一部分，并且必须在代码库的每次更改被合并之前通过。

当然可以下载 Kubernetes 代码库(并建立一个 Golang 开发环境)并将其配置为直接运行一致性测试。然而，有一个叫做**声纳浮标**的工具可以为你自动化这个过程。

Sonobuoy makes it simple to run a set of Kubernetes conformance tests on your clusters in a simple and standardized manner. The simplest way to get started with Sonobuoy is to use the hosted browser-based service at [https://scanner.heptio.com/](https://scanner.heptio.com/). This service gives you a manifest to submit to your cluster and then displays the test results once the tests have finished running. If you want to run everything on your own cluster, you can install a command-line tool that will let you run tests and collect the results yourself by following the instructions at [https://github.com/heptio/sonobuoy](https://github.com/heptio/sonobuoy).

Kubernetes 一致性测试很重要，因为它使用了广泛的 Kubernetes 功能，在您将可能使用这些功能的应用程序部署到集群之前，就可以对任何错误配置发出早期警告。在对集群的配置进行更改时，如果您的更改可能会影响集群的功能，那么发出警告会非常有帮助。

虽然 Kubernetes 一致性测试侧重于测试集群的功能，但安全基准测试会根据已知的不安全配置设置检查集群的配置，确保集群的配置符合当前的安全最佳实践。

**互联网安全中心**发布了一步一步的清单，您可以手动遵循这些清单，根据安全最佳实践测试您的集群。

You can download a copy of these benchmarks for free at [https://www.cisecurity.org/benchmark/kubernetes/](https://www.cisecurity.org/benchmark/kubernetes/).

在构建集群时，阅读并遵循这些清单中的建议可能会很有用，因为这将帮助您了解特定配置值的原因。

设置好集群后，在更新和更改时自动验证配置会很有用，以避免配置意外偏离安全配置。

`kube-bench` is a tool that provides an automated way to run the CIS benchmarks against your cluster: [https://github.com/aquasecurity/kube-bench](https://github.com/aquasecurity/kube-bench).

您可能会发现编写自己的集成测试也很有用，它可以检查您是否能够成功地部署和操作一些自己的应用程序。当快速开发集群的配置时，这样的测试可以作为一个重要的健全性检查。

有许多工具可以用来执行这样的测试。我会推荐您的组织中的工程师已经熟悉的任何测试自动化工具。您可以使用一个专门为运行自动化测试而设计的工具，比如黄瓜，但是一个简单的 shell 脚本将一个应用程序部署到您的集群中，然后检查它是否可访问，这也是一个很好的开始。

# 可观察性

可观察性如下图所示:

![](assets/e1f09642-520c-4579-809e-3ac616965ea2.png)

在为生产设计集群时，能够监控和调试集群是需要牢记的最重要的一点。幸运的是，有许多管理日志和度量的解决方案对 Kubernetes 有很好的支持。

# 记录

每当您想知道您的应用程序在做什么时，大多数操作员首先想到的是查看应用程序生成的日志。

日志很容易理解，并且它们不需要任何特殊的工具来生成，因为您的应用程序可能已经支持某种日志记录了。

开箱即用，Kubernetes 允许您查看和跟踪应用程序为标准输出和标准错误编写的日志。如果您在自己的机器或服务器上使用过`docker logs`命令，您应该对使用`kubectl logs`命令很熟悉。

查看特定容器生成的日志比登录每个节点更方便。除了查看特定豆荚的日志，`kubectl logs`还可以显示匹配特定标签表达式的所有豆荚的日志。

如果您需要搜索应用程序为特定事件生成的日志，或者如果您需要查看过去特定时间生成的日志，那么您需要考虑部署一个解决方案来聚合和管理您的日志。

实现该功能最广泛使用的工具是 **Fluentd** 。Fluentd 是一个非常灵活的工具，可以用来从各种来源收集日志，然后将它们推送到一个或多个目的地。如果您的组织已经维护或使用第三方工具来聚合应用程序日志，您几乎肯定会找到一种方法来配置 Fluentd，以便将运行在 Kubernetes 上的应用程序的应用程序日志存储在您选择的工具中。Fluentd 团队的成员和更广泛的社区维护着 800 多个不同的插件，这些插件支持许多不同的输入、输出和过滤选项。

As Fluentd is built upon the Ruby programming language, its plugins are distributed using the Rubygems package system. By convention, all Fluentd plugins have a name beginning with **fluent-plugin**, and all currently available plugins are listed here: [https://www.fluentd.org/plugins/all](https://www.fluentd.org/plugins/all). Because some of these plugins are maintained by the wider community, it is worth making some initial tests of a plugin you plan to use. The quality of plugins can be variable, depending on the stage of development a particular plugin is in and how often it is maintained. You can install and manage Fluentd plugins using the `gem install` command or control the exact versions of Fluentd plugins using the **bundler** tool. You can read more about installing plugins in your Fluentd installation here: [https://docs.fluentd.org/v1.0/articles/plugin-management](https://docs.fluentd.org/v1.0/articles/plugin-management).

# 监视

如果您知道应用程序有问题，并且想要调试原因，那么查看应用程序的日志输出会很有用。但是如果你不知道你的系统中哪里出现了问题，或者如果你只是想评估一个系统的健康状况，那就难多了。

您的日志非常灵活，因为您的应用程序可以以非结构化的方式将任何信息写入日志记录端点。这在大型系统中可能会变得非常难以承受，过滤和分析这些输出所需的工作量可能会变得复杂。

监控或度量收集采用不同的方法。通过定义反映您的系统、Kubernetes 和您的基础设施的性能和操作的度量，您可以更快地回答关于您的系统的健康和性能的问题。

收集的指标也是自动化警报系统最有用的来源之一。他们可以就应用程序或基础架构中的异常行为向组织成员发出警告。

有许多商业和开源工具可用于收集指标和创建警报。你做出的决定很可能会受到你的组织和你的要求的影响。

正如我已经说过的，试图同时向您的组织引入太多新工具或流程可能会危及您的成功。在许多情况下，许多监控工具已经支持与 Kubernetes 的集成。如果是这种情况，考虑继续使用您的组织习惯的现有工具可能是明智的。

无论您选择哪种工具来记录来自您的应用程序、集群和底层基础架构的指标，您都应该仔细考虑如何让负责开发和部署应用程序的组织成员更容易地展示他们的指标。作为规划集群的一部分，尝试编写过程文档，以公开开发人员在集群中部署新应用程序时应遵循的指标。你应该尽可能保持这个过程的简单。如果您需要自动执行流程的步骤并提供默认的配置值，您应该这样做，以便简化流程。如果从应用程序中导出新指标的过程很复杂，或者需要大量手动步骤，那么组织的应用程序就不太可能公开它们。

如果这个过程简单且没有摩擦，那么默认情况下灌输一种监控文化就会变得简单得多。例如，如果您选择使用普罗米修斯，您可以这样记录这个过程:

*   `*`暴露端口`9102`上的端点`/metrics`
*   将注释`"prometheus.io/scrape": true`添加到您的 pod 中

在这个例子中，通过为 Prometheus 配置合理的默认值，从 pod 中公开度量对开发人员来说变得又快又简单。对于普罗米修斯抓取指标的方式，可以公开更复杂的配置，但是通过使用众所周知的默认值，它使设置过程更简单，并使在应用程序中包含标准的普罗米修斯库更简单。无论您选择使用什么系统来收集指标，尽可能遵循这些原则。

直接从应用程序单元和基础架构中收集指标，可以提供关于应用程序行为的深入而丰富的信息。当您需要了解有关应用程序的细节时，这些信息非常有用，对于预防问题非常有用。例如，有关磁盘使用情况的指标可用于提供警报，警告操作员如果不解决可能导致应用程序故障的状态。

# 黑盒监控

虽然特定于应用程序的指标提供了对根本原因分析和先发制人的警报有用的深刻见解，但黑盒监控采取了相反的方法。通过将应用程序视为一个密封的实体，并使用面向用户的端点，您可以暴露出性能不佳的应用程序的症状。黑盒监控可以通过使用普罗米修斯黑盒导出器等工具来实现。但是另一种常见的模式是使用商业服务。这样做的主要优势是，它们通常允许您从多个位置(可能是全球)探查应用程序，真正实现用户和应用程序之间的完整基础架构。

# 发信号

记录关于您在 Kubernetes 上运行的系统状态的度量是使您的系统易于观察的第一个阶段。一旦您收集了度量标准，就有几种方法可以让您收集的数据变得易于操作。

大多数指标收集工具都提供了一些方法来为对组织的不同成员很重要的指标构建图表和仪表板。例如，普罗米修斯的许多用户使用 Grafana 构建仪表板来公开重要指标。

虽然仪表板是了解特定系统或业务流程执行情况的好方法，但是您的系统的某些方面需要更主动的方法。

任何值得一提的度量收集系统都将提供一种向组织成员发出警报的方式。但是，当您收集指标以及用于向团队发送警报的系统时，您应该考虑几个原则:

*   **警报应该是可操作的**:当将指标从仪表板上的图表或仪表提升为警报时，确保您只针对需要立即人工干预的状态发送警报，而不仅仅是警告或信息。警告或信息警报属于您的仪表板，而不是您的寻呼机。
*   **提醒应该谨慎使用**:提醒会打断人们此刻正在做的任何事情:工作、休息，或者最糟糕的是睡觉。如果一个人收到太多的警报，它们可能是压力的原因，并很快变得不那么有效，因为警报疲劳开始，他们失去了吸引注意力的能力。在设计警报机制时，您应该准备好记录组织成员被您的警报打断的频率。

应该对警报进行指导—您应该考虑谁应该对特定的警报负责，并对其进行适当的指导。警报可以指向许多系统，例如错误跟踪器、电子邮件、聊天系统，甚至寻呼机应用程序。重要的是，从您的组织接收到最关键任务警报的人能够掌控并管理响应。不太重要的警报可能会在错误跟踪工具中分配给团队或小组。如果您的组织使用聊天系统，如 Slack、HipChat 或 IRC，您可能希望将特定应用程序的警报定向到开发或负责该应用程序操作的团队使用的频道或房间。只要记住确保音量保持在可接受的水平，否则你的警报很快就会被需要了解它们的人忽略。

# 描摹

追踪是可观测性家族中最年轻的成员，因此通常是组织选择实施的最后一个。跟踪系统的思想是测量单个请求通过应用程序所花费的时间。

对于一个单一的应用程序来说，这可能不会暴露出比配置良好的度量标准更有趣的信息。但是对于具有分布式或*微服务*架构的大规模系统来说，单个请求可以通过几十个甚至几百个独立的进程，跟踪可以帮助准确定位性能问题发生的时间和地点。

当实现从应用程序收集跟踪信息的系统时，您有许多选择。

AWS's built-in solution for tracing includes X-Ray ships with support for Java, Go Node.js, Python, Ruby, and .NET applications. For these technologies, adding distributed tracing to your applications is simply a question of adding a library to your applications and configuring it correctly. [https://aws.amazon.com/xray/](https://aws.amazon.com/xray/).

与 AWS 的解决方案竞争的是许多工具，这些工具被设计成在 OpenTracing 的旗帜下协同工作。

OpenTracing provides client libraries for nine languages that are compatible with nine different open source and commercial tools designed to collect trace data. Because of the open nature of OpenTracing, several application frameworks, and infrastructure components, are choosing to add support for its trace format. You can find out more about OpenTracing at [http://opentracing.io](http://opentracing.io).

# 摘要

希望这一章让您了解了在生产环境中运行 Kubernetes 时可以做出的无数不同选择和决定。不要让选项和选择的深度和广度影响你，因为 Kubernetes 非常容易上手，尤其是在 AWS 上。

在下一章中，我们将开始实际工作，建立集群并做好工作准备。我们将无法涵盖所有选项，更不用说 Kubernetes 周围的社区产生的所有附加组件和附加工具，但我们将提供一个稳定的起点，您可以从这里开始实施自己的计划。

希望这一章可以作为您和您的团队讨论和规划满足组织需求的集群的指南。然后，您可以开始实现在阅读本章时可能已经确定的特性和功能。

如果在启动自己的集群时有一件事需要记住:保持简单，傻瓜。Kubernetes 可以让你在需要的时候简单地添加新工具，所以不要过于复杂或者过于快速。从可能可行的最简单的设置开始，即使您认为以后需要增加复杂性；通常，你会发现简单的解决方案效果很好。

利用 Kubernetes 本身将允许您快速发展您的基础架构这一事实，从小处着手，并在您需要时(而不是之前)向您的系统添加功能和工具！