# 七、生产就绪型集群

在最后一章中，我们花了一些时间思考一个规划 Kubernetes 集群的框架。希望您清楚，在构建集群时，要根据您运行的系统的需求做出许多决定。

在这一章中，我们将采取更实际的方法来解决这个问题。我将从做出一些选择开始，而不是试图涵盖我们可用的无数选项，然后我们将构建一个功能齐全的集群，作为许多不同用例的基础配置。

在本章中，我们将涵盖以下主题:

*   将（行星）地球化（以适合人类居住）
*   准备节点映像和节点组
*   供应加载项

# 构建集群

本章包含的信息只是构建和管理集群的一种可能方式。当构建一个 Kubernetes 集群时，有很多选择，几乎可以选择的工具也一样多。为了本章的目的，我选择使用一些工具来简单说明构建集群的过程。如果您或您的团队喜欢使用不同的工具，那么本章中概述的概念和体系结构将很容易转移到其他工具。

在本章中，我们将以更适合生产工作负载的方式启动我们的集群。从[第 3 章](03.html)、*到达云端*开始，我们在这里所做的很多事情对您来说都是熟悉的，但是我们将在我们在那里概述的流程的基础上，以两种关键方式进行构建。首先，在构建您所依赖的基础架构时，能够以可重复的方式快速推出新的基础架构实例是非常重要的。我们希望能够做到这一点，因为这使得以无风险的方式测试我们希望对基础架构进行的更改变得简单。通过自动配置 Kubernetes 集群，我们实现了前面讨论过的不可变基础设施的模式。我们可以快速调配一个替代集群，然后在将工作负载转移到新集群之前进行测试，而不是冒着升级或更改生产基础架构的风险。

为了实现这一点，我们将使用 Terraform 基础设施供应工具与 AWS 进行交互。Terraform 允许我们使用某种编程语言将基础设施定义为代码。通过将我们的基础设施定义为代码，我们能够使用版本控制等工具，并遵循其他软件开发实践来管理我们基础设施的发展。

在本章中，我们将对运行在 AWS 上的 Kubernetes 集群应该是什么样子以及应该如何管理做出大量决定。为了本章和我们将要讨论的示例的目的，我心中有以下要求。

*   **说明性的**:我们将看到满足一般生产用例需求的 Kubernetes 集群可能是什么样子。这个集群反映了我个人在设计用于实际生产的 Kubernetes 集群时所做的决定。为了使这一章尽可能清晰易懂，我尽量保持集群及其配置简单。
*   **灵活**:我们会创造一些你可以当作模板的东西，添加或者修改来满足你的需求。
*   **可扩展**:每当你在设计一个 Kubernetes 集群(或者实际上任何基础设施)的时候，你应该考虑一下你现在做出的可能会阻止你以后扩展或扩展那个基础设施的决定。

显然，当您构建自己的集群时，您应该对您的需求有一个更具体的想法，以便您能够根据自己的需求定制集群。我们将在这里构建的集群将是任何生产就绪系统的一个很好的起点，您可以根据需要对其进行定制和添加。

Much of the configuration in this chapter has been shortened. You can check out the full configuration used in this chapter at [https://github.com/PacktPublishing/Kubernetes-on-AWS/tree/master/chapter07](https://github.com/PacktPublishing/Kubernetes-on-AWS/tree/master/chapter07).

# Terraform 入门

Terraform 是一个命令行工具，您可以在工作站上运行它来更改您的基础架构。Terraform 是一个单一的二进制文件，只需要安装到您的路径上。

You can download Terraform from [https://www.terraform.io/downloads.html](https://www.terraform.io/downloads.html) for six different operating systems, including macOS, Windows, and Linux. Download the ZIP file for your operating system, extract it, and then copy the Terraform binary to a location on your path.

Terraform 使用扩展名为`.tf`的文件来描述您的基础设施。因为 Terraform 支持在许多不同的云平台上管理资源，所以它可以包含相关提供者的概念，这些概念是根据需要加载的，以支持不同云提供者公开的不同 API。

首先，让我们配置 AWS Terraform 提供程序，以便为构建 Kubernetes 集群做好准备。创建一个新目录来保存 Kubernetes 集群的 Terraform 配置，然后创建一个文件，我们将在其中配置 AWS 提供程序，如以下代码所示:

```
aws.tf
provider "aws" { 
  version = "~> 1.0" 

  region = "us-west-2" 
} 
```

保存文件，然后运行以下命令:

```
terraform.init
Initializing provider plugins... 
- Checking for available provider plugins on https://releases.hashicorp.com... 
- Downloading plugin for provider "aws" (1.33.0)... 
Terraform has been successfully initialized! 
```

当您使用受支持的提供商时，Terraform 可以为您发现并下载所需的插件。请注意，我们已经为提供商配置了一个名为`us-west-2`的 AWS 区域，因为在本例中，我们将在该区域启动我们的集群。

为了让 Terraform 与 AWS 应用编程接口通信，您需要向 AWS 提供者提供一些凭据。我们在[第三章](03.html)*触云*中学习了如何获取凭证。如果您遵循第 3 章[中的建议](03.html)、*到达云*，并使用`aws configure`命令设置您的凭据，那么 Terraform 将从您的本地配置文件中读取您的默认凭据。

或者，Terraform 可以从`AWS_ACCESS_KEY_ID`和`AWS_SECRET_ACCESS_KEY`环境变量中读取 AWS 凭据，或者，如果您在 EC2 实例上运行 Terraform，它可以使用 EC2 实例角色提供的凭据。

也可以通过在 AWS 提供程序块中内联添加一个`access_key`和`secret_key`参数来静态配置凭证，但是我并不真正推荐这种做法，因为这使得将您的配置检查到版本控制系统中变得更加困难。

By default, Terraform uses a local file called `terraform.tfstate` to keep track of the state of your infrastructure. This is so that it can keep track of the changes you have made to the configuration since you last ran Terraform.
If you are going to be the only person managing your infrastructure, then this might be acceptable, but you would need to securely back up the state file. It should be considered sensitive, and Terraform won't function correctly if you lose it.
If you are using AWS, I would recommend using S3 as a backend. You can read about how to set this up in the Terraform documentation at [https://www.terraform.io/docs/backends/types/s3.html](https://www.terraform.io/docs/backends/types/s3.html). If configured correctly, S3 storage is highly secure, and if you are working on a team, then you can utilize a DynamoDB table as a lock to ensure that multiple instances of Terraform are not running at the same time. If you want to make use of this, set the configuration in the `backend.tf` file, otherwise delete that file.

# 变量

Terraform 允许我们定义变量，以便使我们的配置更加可重用。如果您以后想要将您的配置作为一个模块来定义多个集群，这将特别有用。我们不会在本章中讨论这个问题，但是我们可以遵循最佳实践并定义一些关键变量，让您能够简单地塑造集群来满足您的需求。

标准做法是创建一个`variables.tf`文件来包含项目中的所有变量。这很有帮助，因为它充当了如何控制配置的高级文档。

如您所见，在为我的变量选择描述性名称和添加可选的描述字段之间，整个文件是非常不言自明的。因为我已经为每个变量提供了默认值，所以我们可以在不传递这些变量的任何值的情况下运行 Terraform，如以下代码所示:

```
variables.tf
variable "cluster_name" { 
  default = "lovelace" 
} 

variable "vpc_cidr" { 
  default     = "10.1.0.0/16" 
  description = "The CIDR of the VPC created for this cluster" 
} 

variable "availability_zones" { 
  default     = ["us-west-2a","us-west-2b"] 
  description = "The availability zones to run the cluster in" 
} 

variable "k8s_version" { 
  default     = "1.10" 
  description = "The version of Kubernetes to use" 
} 
```

# 建立工作关系网

我们将首先创建一个配置文件来描述我们的 Kubernetes 集群的网络设置。您可能会认出这个网络的设计，因为它与我们在[第 3 章](03.html)、*中手动创建的网络非常相似，但增加了一些内容，使其更适合生产设置。*

Terraform configuration files can be documented with comments, and to better illustrate this configuration, I have provided some commentary in the form of comments. You will notice that they are surrounded by `/*` and `*/`.

为了支持高可用性，我们将为多个可用性区域创建子网，如以下代码所示。这里，我们使用了两个，但是如果您想要更大的弹性，您可以轻松地向`availability_zones`变量添加另一个可用性区域:

```
networking.tf
/*  Set up a VPC for our cluster. 
*/resource "aws_vpc" "k8s" { 
  cidr_block           = "${var.vpc_cidr}" 
  enable_dns_hostnames = true 

  tags = "${ 
    map( 
     "Name", "${var.cluster_name}", 
     "kubernetes.io/cluster/${var.cluster_name}", "shared", 
    ) 
  }" 
} 

/*  In order for our instances to connect to the internet 
  we provision an internet gateway.*/ 
resource "aws_internet_gateway" "gateway" { 
  vpc_id = "${aws_vpc.k8s.id}" 

  tags { 
    Name = "${var.cluster_name}" 
  } 
} 

/*  For instances without a Public IP address we will route traffic  
  through a NAT Gateway. Setup an Elastic IP and attach it. 

  We are only setting up a single NAT gateway, for simplicity. 
  If the availability is important you might add another in a  
  second availability zone. 
*/ 
resource "aws_eip" "nat" { 
  vpc        = true 
  depends_on = ["aws_internet_gateway.gateway"] 
} 

resource "aws_nat_gateway" "nat_gateway" { 
  allocation_id = "${aws_eip.nat.id}" 
  subnet_id     = "${aws_subnet.public.*.id[0]}" 
} 
```

我们将为集群中使用的每个可用性区域配置两个子网。一个公共子网，它有一条到互联网的直接路由，Kubernetes 将在其中提供可访问互联网的负载平衡器。以及一个专用子网，Kubernetes 将使用该子网为 pods 分配 IP 地址。

因为私有子网中可用的地址空间将是 Kubernetes 能够启动的 pod 数量的限制因素，所以我们提供了一个具有 16382 个可用 IP 地址的大地址范围。这应该会给我们的集群一些扩展空间。

如果您只打算运行互联网无法访问的内部服务，那么您可以跳过公共子网。你可以在本章的示例文件中找到完整的`networking.tf`文件。

# 计划和应用

Terraform 允许我们通过添加和更改定义基础设施的代码来逐步构建基础设施。然后，如果您愿意，当您完成本章时，您可以一点一点地构建您的配置，或者您可以使用 Terraform 一次性构建您的整个集群。

每当您使用 Terraform 对您的基础架构进行更改时，它首先会生成一个它将要进行的更改的计划，然后应用该计划。当修改生产基础架构时，这种两阶段操作非常理想，因为它让您有机会在进行更改之前查看实际将应用于您的集群的更改。

一旦您将网络配置保存到文件中，我们就可以遵循一些步骤来安全地调配我们的基础架构。

我们可以通过运行以下命令来检查配置中的语法错误:

```
terraform validate  
```

如果您的配置良好，则不会有输出，但是如果您的文件有语法错误，您应该会看到一条解释该问题的错误消息。例如，缺少右大括号可能会导致错误，如`Error parsing networking.tf: object expected closing RBRACE got: EOF`。

一旦您确保您的文件为 Terraform 正确格式化，您就可以使用以下命令为您的基础结构创建一个更改计划:

```
terraform plan -out k8s.plan  
```

如果运行此计划，此命令将输出对基础架构所做更改的摘要。`-out`标志是可选的，但这是一个好主意，因为它允许我们稍后准确地应用这些更改。如果您在运行 Terraform 计划时注意到了输出，那么您应该会看到如下消息:

```
To perform exactly these actions, run the following command to apply:
terraform apply "k8s.plan"
```

当您使用预计算计划运行`terraform apply`时，它将进行计划生成时概述的更改。您也可以在不预先生成计划的情况下运行`terraform plan`命令，但是在这种情况下，它仍然会计划更改，然后在应用它们之前提示您。

Terraform 计算基础设施中不同资源之间的依赖关系，例如，它确保在创建路由表和其他资源之前创建 VPC。有些资源可能需要几秒钟才能创建，但是 Terraform 会等到它们可用后再继续创建相关资源。

如果您想删除 Terraform 在您的 AWS 帐户中创建的资源，您可以从相关的`.tf`文件中删除该定义，然后计划并应用您的更改。当您测试 Terraform 配置时，删除由特定配置创建的所有资源，以便从头开始测试配置基础架构，这将非常有用。如果需要这样做，`terraform destroy`命令非常有用；它将从您的基础设施中删除您的地形文件中定义的所有资源。但是，请注意，这可能会导致基本资源被终止和删除，因此您不应该在运行的生产系统上使用此方法。在移除任何资源之前，Terraform 会列出它们，然后询问您是否要移除它们。

# 制导机

为了给我们的集群提供一个灵活可靠的 Kubernetes 控制平面，我们将从我们在[第 3 章](03.html)、*到达云端*中构建的简单集群进行第一次大的改变。

正如我们在[第 1 章](01.html)*中了解到的，谷歌为我们其他人提供的基础设施*，Kubernetes 控制平面的关键组件是支持 etcd 存储、API 服务器、调度器和控制器管理器。如果我们想要构建和管理一个弹性控制平面，我们需要跨多个实例管理这些组件的运行，最好是跨多个可用性区域。

因为 API 服务器是无状态的，并且调度器和控制器管理器具有内置的领导者选举工具，所以在 AWS 上运行多个实例相对简单，例如，通过使用自动缩放组。

运行生产级 etcd 要稍微复杂一些，因为在添加或删除节点时应该仔细管理 etcd，以避免数据丢失和停机。在 AWS 上成功运行 etcd 集群是一项相当困难的任务，需要手动操作或复杂的自动化。

对我们来说幸运的是，AWS 开发了一项服务，该服务几乎消除了提供库本内特控制平面时涉及的所有操作复杂性——亚马逊 EKS ，或者用全名来说，亚马逊库本内特弹性容器服务。

通过 EKS，AWS 将代表您在多个可用性区域管理和运行构成 Kubernetes 控制平面的组件，从而避免任何单点故障。有了 EKS，您不再需要担心执行或自动化运行稳定的 etcd 集群所需的操作任务。

我们应该记住，在 EKS，我们集群基础设施的一个关键部分现在由第三方管理。你应该对这样一个事实感到满意，即 AWS 可以比你自己的团队做得更好，提供一个有弹性的控制平面。这并不排除您将集群设计为对控制平面的故障有一定的抵抗力——例如，如果 kubelet 不能连接到控制平面，那么运行的容器将保持运行，直到控制平面再次可用。您应该确保添加到集群中的任何附加组件都能够以类似的方式应对临时停机。

EKS 减少了管理 Kubernetes(控制平面)最复杂部分所需的工作量，从而减少了设计和维护集群所需的时间(和金钱)。此外，即使对于中等规模的集群，EKS 服务的成本也明显低于在多个 EC2 实例上运行您自己的控制平面的同等成本。

为了让 Kubernetes 控制平面管理您的 AWS 帐户中的资源，您需要为 EKS 提供一个 IAM 角色，该角色将由 EKS 自己承担。

EKS 在您的 VPC 内创建网络接口，以允许库本内特控制平面与库贝莱通信，从而提供诸如**日志流**和 **exec** 等服务。为了控制这种交流，我们需要在 EKS 启动时提供一个安全小组。在本章的示例文件中，您可以在`control_plane.tf`中找到用于配置控制平面的完整地形配置。

我们可以使用 EKS 集群的 Terraform 资源来查询它，以便获取 Kubernetes API 的端点和用于访问它的证书颁发机构。

这些信息，结合 Terraform 的模板工具，允许我们生成一个`kubeconfig`文件，其中包含连接到 EKS 提供的 Kubernetes API 所需的信息。我们可以稍后使用它来配置附加组件。

如果需要，您也可以使用这个文件通过 kubectl 手动连接到集群，方法是将文件复制到`~/.kube/config`处的默认位置，或者通过使用`--kubeconfig`标志或`KUBECONFIG`环境变量将其位置传递给 kubectl，如以下代码所示:

The `KUBECONFIG` environment variable can be useful if you are managing multiple clusters, as you can easily load multiple configs by separating their paths; for example:
`export KUBECONFIG=$HOME/.kube/config:/path/to/other/conf`.

```
kubeconfig.tpl 
apiVersion: v1 
kind: Config 
clusters: 
- name: ${cluster_name} 
  cluster: 
    certificate-authority-data: ${ca_data} 
    server: ${endpoint} 
users: 
- name: ${cluster_name} 
  user: 
    exec: 
      apiVersion: client.authentication.k8s.io/v1alpha1 
      command: aws-iam-authenticator 
      args: 
      - "token" 
      - "-i" 
      - "${cluster_name}" 
contexts: 
- name: ${cluster_name} 
  context: 
    cluster: ${cluster_name} 
    user: ${cluster_name} 
current-context: ${cluster_name} 
```

# 准备节点映像

正如我们在 [第 3 章](03.html)*到达云*中所做的那样，我们现在将为集群中的工作节点准备一个 AMI。但是，我们将通过使用**包装机**将其自动化来改进该过程。Packer 是一种工具，它使在 AWS(和其他平台)上构建机器映像变得简单。

# 安装封隔器

就像 Terraform 一样，Packer 是作为单个二进制文件分发的，只需要复制到路径上的一个位置。您可以在[https://www.packer.io/intro/getting-started/install.html](https://www.packer.io/intro/getting-started/install.html)的 Packer 网站上找到详细的安装说明。

Once you have installed Packer, you can run `packer version` to check that you have correctly copied it into to your path.

# 封隔器配置

Packer 配置了一个 JSON 格式的配置文件，可以在`ami/node.json`看到。

这里的示例配置有三个部分。第一个是变量列表。这里，我们使用变量来存储将要安装在映像中的重要软件的版本号。当 Kubernetes 软件的更新版本在未来可用时，这将使构建和测试映像变得简单。

配置的第二部分配置构建器。Packer 允许我们选择与一个或多个支持为不同云提供商构建映像的构建器一起构建我们的映像。由于我们想要构建一个在 AWS 上使用的映像，我们使用的是`amazon-ebs`构建器，它通过启动一个临时 EC2 实例，然后从其根 EBS 卷的内容创建一个 AMI 来创建一个映像(就像我们在[第 3 章](03.html)、*到达云端*中遵循的手动过程一样)。该构建器配置允许我们选择我们的机器将基于的基础映像；在这里，我们使用的是一个官方的 Ubuntu 服务器映像，一个可信的来源。生成器配置中的`ami-name`字段定义了输出映像的名称。我们已经包含了所使用的 Kubernetes 软件版本和时间戳，以确保该映像名称是唯一的。拥有唯一的映像名称可以让我们在部署使用它的服务器时准确定义要使用哪个映像。

最后，我们配置一个置备程序来安装我们的映像所需的软件。Packer 支持许多可以安装软件的不同供应商，包括完整的配置管理系统，如 Chef 或 Ansible。为了使这个例子简单，我们将使用一个 shell 脚本自动安装我们需要的软件。Packer 会将配置好的脚本上传到构建器实例，然后通过 SSH 执行它。

我们只是在使用一个简单的 shell 脚本，但是如果您的组织已经使用了一个配置管理工具，那么您可能更喜欢使用它来安装您的映像所需的软件，尤其是因为它使包含您的组织的基本配置变得简单。

在此脚本中，我们正在安装工作节点加入 EKS 集群并正常运行所需的软件和配置，如下列表所示。在实际部署中，除了这些之外，您可能还希望添加其他工具和配置。

*   **Docker** : Docker 是目前测试最好、最常用的与 Kubernetes 一起使用的容器运行时
*   **kuble te**:kuble node agent
*   **ekstrap** :配置 kubelet 以连接到 EKS 集群端点
*   **aws-iam-authenticator** :允许节点使用其 iam 凭证向 EKS 集群进行身份验证

我们使用以下代码安装这些元素:

```
install.sh
          #!/bin/bash
          set -euxo pipefail  
...
          # Install aws-iam-authenticator
curl -Lo /usr/local/bin/heptio-authenticator-aws https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.3.0/heptio-authenticator-aws_0.3.0_linux_amd64
chmod +x /usr/local/bin/heptio-authenticator-aws

apt-get install -y \
  docker-ce=$DOCKER_VERSION* \
  kubelet=$K8S_VERSION* \
  ekstrap=$EKSTRAP_VERSION*
# Cleanup
apt-get clean
rm -rf /tmp/*
   # Cleanup
   apt-get clean
   rm -rf /tmp/*
```

为 Packer 准备好配置后，您可以使用`packer build`命令在您的 AWS 帐户中构建 AMI，如以下代码所示。这将启动一个临时 EC2 实例。将新的 AMI 保存到您的帐户中，并清理临时实例:

```
packer build node.json  
```

If your organization uses a continuous integration service, you might want to configure it to build your node image on a regular schedule in order to pick up security updates to the base operating system.

# 节点组

现在，我们已经为集群中的工作节点准备了一个映像，我们可以设置一个自动缩放组来管理将组成集群的 EC2 实例的启动。

EKS 没有将我们与以任何特定方式管理节点联系在一起，因此自动缩放组不是管理集群中节点的唯一选择，但是使用它们是管理集群中多个工作实例的最简单方式之一。

如果您希望在集群中使用多个实例类型，可以为您希望使用的每个实例类型重复启动配置和自动缩放组配置。在此配置中，我们将按需启动`c5.large`实例，但您应该参考[第 6 章](06.html)、*生产计划*，了解有关为集群选择合适实例大小的更多信息。

配置的第一部分设置了一个 IAM 角色供我们的实例使用。这很简单，因为 AWS 提供的托管策略具有 Kubernetes 所需的权限。`AmazonEKSWorkerNodePolicy`代码短语允许 kubelet 查询关于 EC2 实例、附加卷和网络设置的信息，以及查询关于 EKS 集群的信息。`AmazonEKS_CNI_Policy`提供`vpc-cni-k8s`网络插件所需的权限，以便将网络接口附加到实例，并为这些接口分配新的 IP 地址。`AmazonEC2ContainerRegistryReadOnly`策略允许实例从 AWS 弹性容器注册表中提取 Docker 映像(您可以在[第 10 章](10.html)、*管理容器映像*中了解更多使用信息)。我们还将手动指定一个策略，该策略将允许`kube2iam`工具承担角色，以便为运行在集群上的应用提供凭据，如以下代码所示:

```
nodes.tf 
/* 
  IAM policy for nodes 
*/ 
data "aws_iam_policy_document" "node" { 
  statement { 
    actions = ["sts:AssumeRole"] 

    principals { 
      type        = "Service" 
      identifiers = ["ec2.amazonaws.com"] 
    } 
  } 
} 
... 

resource "aws_iam_instance_profile" "node" { 
  name = "${aws_iam_role.node.name}" 
  role = "${aws_iam_role.node.name}" 
} 
```

在我们的工作节点可以向 Kubernetes API 服务器注册自己之前，它们需要有正确的权限才能这样做。在 EKS，IAM 角色和用户之间的映射是通过向集群提交配置映射来配置的。

You can read more about how to map IAM users and roles to Kubernetes permissions in the EKS documentation at [https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html).

Terraform 将使用我们在设置控制平面时生成的`kubeconfig`文件，以便使用`kubectl`通过 local-exec provisioner 将此配置提交给集群，如以下`nodes.tf`继续代码所示:

```
/* 
  This config map configures which IAM roles should be trusted by Kubernetes 
*/ 

resource "local_file" "aws_auth" { 
  content = <<YAML 
apiVersion: v1 
kind: ConfigMap 
metadata: 
  name: aws-auth 
  namespace: kube-system 
data: 
  mapRoles: | 
    - rolearn: ${aws_iam_role.node.arn} 
      username: system:node:{{EC2PrivateDNSName}} 
      groups: 
        - system:bootstrappers 
        - system:nodes 
YAML 
  filename = "${path.module}/aws-auth-cm.yaml" 
  depends_on = ["local_file.kubeconfig"] 

  provisioner "local-exec" { 
    command = "kubectl --kubeconfig=${local_file.kubeconfig.filename} apply -f ${path.module}/aws-auth-cm.yaml" 
  } 
} 
```

接下来，我们需要准备安全组来控制进出节点的网络流量。

我们将设置一些规则，以允许我们的集群正常运行所需的以下通信流:

*   节点需要相互通信以实现集群内 pod 和服务通信。
*   运行在节点上的 Kubernetes 需要连接到 Kubernetes API 服务器，以便读取和更新关于集群状态的信息。
*   控制平面需要连接到`10250`端口上的 Kubelet API 这用于`kubectl exec`和`kubectl logs.`等功能

*   为了使用应用编程接口的代理功能将流量代理到吊舱和服务，控制平面需要连接到集群中运行的吊舱。在本例中，我们打开了所有端口，但是，例如，如果您只打开了 pods 上的非特权端口，那么您只需要允许 1024 以上端口的流量。

我们使用以下代码设置这些规则。`nodes.tf`的代码继续:

```
 resource "aws_security_group" "nodes" { 
  name        = "${var.cluster_name}-nodes" 
  description = "Security group for all nodes in the cluster" 
  vpc_id      = "${aws_vpc.k8s.id}" 

  egress { 
    from_port   = 0 
    to_port     = 0 
    protocol    = "-1" 
    cidr_blocks = ["0.0.0.0/0"] 
  } 

... 
resource "aws_security_group_rule" "nodes-control_plane-proxy" { 
  description              = "API (proxy) communication to pods" 
  from_port                = 0 
  to_port                  = 65535 
  protocol                 = "tcp" 
  security_group_id        = "${aws_security_group.nodes.id}" 
  source_security_group_id = \ 
                          "${aws_security_group.control_plane.id}" 
  type                     = "ingress" 
} 
```

现在我们已经准备好了运行节点的基础设施，我们可以准备一个启动配置，并将其分配给一个自动伸缩组来实际启动我们的节点，如下面的代码所示。

显然，我在这里选择的实例类型和磁盘大小可能不适合您的集群，因此在为您的集群选择实例大小时，您需要参考第 6 章、*生产规划*中的信息。所需的磁盘大小在很大程度上取决于应用的平均映像大小。`nodes.tf`的代码继续:

```
data "aws_ami" "eks-worker" { 
  filter { 
    name   = "name" 
    values = ["eks-worker-${var.k8s_version}*"] 
  } 

  most_recent = true 
  owners      = ["self"] 
} 

...                                                                    
  resource "aws_autoscaling_group" "node" { 
  launch_configuration = "${aws_launch_configuration.node.id}" 
  max_size             = 2 
  min_size             = 10 
  name                 = "eks-node-${var.cluster_name}" 
  vpc_zone_identifier  = ["${aws_subnet.private.*.id}"] 

  tag { 
    key                 = "Name" 
    value               = "eks-node-${var.cluster_name}" 
    propagate_at_launch = true 
  } 

  tag { 
    key              = "kubernetes.io/cluster/${var.cluster_name}" 
    value               = "owned" 
    propagate_at_launch = true 
  } 
} 

```

`ekstrap`工具使用`kubernetes.io/cluster/<node name>`标签来发现 EKS 端点，以便向集群注册该节点，`kubelet`使用该标签来验证该节点是否已连接到正确的集群。

# 供应加载项

Kubernetes 的强大之处很大程度上来自于这样一个事实，即通过添加额外的服务来提供额外的功能，它很容易扩展。

我们将通过部署`kube2iam`来看一个这样的例子。这是一个运行在集群中每个节点上的守护进程，它拦截运行在我们的 pods 中的进程对 AWS 元数据服务的调用。

像这样提供服务的一个简单方法是使用 DaemonSet 在集群中的每个节点上运行一个 pod，如下面的代码所示。这种方法已经在我们的集群中用于将`aws-vpc-cni`网络插件部署到每个节点，并运行`kube-proxy`，即在每个节点上运行的 Kubernetes 组件，该组件负责将去往服务 IP 的流量路由到底层 pods:

```
kube2iam.yaml
--- 
apiVersion: v1 
kind: ServiceAccount 
metadata: 
  name: kube2iam 
  namespace: kube-system 
--- 
apiVersion: v1 
kind: List 
items:         
...                                                           kube2iam.tf 
resource "null_resource" "kube2iam" { 
  triggers = { 
    manifest_sha1 = "${sha1(file("${path.module}/kube2iam.yaml"))}" 
  } 

  provisioner "local-exec" { 
    command = " kubectl --kubeconfig=${local_file.kubeconfig.filename} apply -f 
${path.module}/kube2iam.yaml" 
  } 
} 
```

# 管理变革

与我们在[第 3 章](03.html)、*触及云*中探索的手动方法相比，使用 Terraform 这样的工具管理 Kubernetes 集群具有很多优势。当您想要测试对配置的更改时，甚至当您要升级您的集群正在运行的 Kubernetes 版本时，能够快速轻松地重复配置集群的过程非常有用。

将基础架构定义为代码的另一个关键优势是，您可以使用版本控制工具来跟踪您对基础架构所做的更改。这样做的一个主要优点是，每次您做出更改时，都可以留下提交消息。您现在做出的决定可能看起来显而易见，但是记录您为什么选择以某种方式做某事肯定会帮助您和将来必须使用您的配置的其他人，尤其是那些其他人可能没有您进行更改时的相同上下文。

许多软件工程师已经写了很多关于编写好的提交消息的文章。最好的建议是确保你包含了尽可能多的信息来解释为什么需要你的改变。如果几个月后你必须回到配置，你未来的自己会感谢你。

考虑以下提交消息:

```
Update K8s Node Security Groups 

Open port 80 on the Node Security Group  
```

还要考虑以下提交消息:

```
Allow deveopers to access the guestbook app 

The guestbook is served from port 80\. We are allowing the control plane access to this port on the Node security groups, so developers can test the application using kubectl proxy. 

Once the application is in production and we provision a LoadBalancer, we can remove these rules. 
```

第一条提交消息是不好的，因为它只是解释了你做了什么，只要看看配置是如何变化的，这应该是显而易见的。第二条信息提供了更多的信息。重要的是，第二条消息解释了为什么需要进行更改，并提供了一些对将来更改集群的人有用的信息。如果没有这个重要的上下文，你可能会想为什么端口`80`会被打开，并担心如果你改变这些信息会发生什么。

在生产环境中运行 Kubernetes 集群不仅仅是在第一天如何启动集群；这是为了确保您可以随着时间的推移更新和扩展集群，以继续满足组织的要求。

# 摘要

我们在本章中构建的集群仍然非常简单，并且真正反映了我们可以在以下章节中构建的一个起点。但是，它确实满足以下生产准备的基本要求:

*   **可靠性**:通过使用 EKS，我们提供了一个可靠的控制平面，我们可以依靠它来管理我们的集群。
*   **可伸缩性**:通过自动伸缩组操作我们的节点，我们可以在几秒钟内简单地为集群增加额外的容量。
*   **可维护性**:通过使用 Terraform 将我们的基础设施定义为代码，我们使得将来管理我们的集群变得简单。通过为我们的节点机器使用的 AMI 设置构建流程，我们能够快速重建映像，以引入安全更新和节点软件的更新版本。