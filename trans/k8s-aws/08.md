# 八、抱歉，我的应用吃掉了集群

使用 Kubernetes 运行我们的应用使我们能够在集群中的机器上实现更高的资源利用率。Kubernetes 调度程序非常有效地将不同的应用打包到集群中，从而最大限度地利用每台机器上的资源。您可以调度低优先级作业(如批处理作业)和高优先级作业(如 web 服务器或数据库)的组合，这些作业在需要时可以重新启动。Kubernetes 将帮助您利用网络服务器等待请求时出现的空闲 CPU 周期。

这是一个好消息，如果你想减少你支付给 AWS 的金额，让 EC2 实例运行你的应用。了解如何配置您的 pods 非常重要，这样 Kubernetes 就可以说明您的应用的资源使用情况。如果您没有正确配置您的 pods，那么您的应用的可靠性和性能可能会受到影响，因为 Kubernetes 可能需要将您的 pods 从一个节点中逐出，因为它正在耗尽资源。

在这一章中，您将从学习如何计算豆荚将使用的内存和中央处理器开始。我们将学习如何配置具有不同服务质量的 pods，从而保证重要工作负载获得所需的资源，但不太重要的工作负载可以在空闲资源可用时利用这些资源，而无需专用资源。您还将学习如何利用 Kubernetes 自动缩放工具，在应用负载增加时向其添加额外的 pods，并在资源不足时向集群添加额外的节点。

在本章中，您将学习如何执行以下操作:

*   配置容器资源请求和限制
*   为期望的**服务质量** ( **服务质量**)等级配置您的豆荚
*   为每个命名空间的资源使用设置配额
*   使用水平 pod 自动缩放器自动缩放您的应用，以满足对它们的需求
*   随着集群使用情况的变化，使用集群自动缩放器自动调配和终止 EC2 实例

# 资源请求和限制

Kubernetes 允许我们通过将多个不同的工作负载调度到单个机器池来实现集群的高利用率。每当我们要求 Kubernetes 安排一个 pod 时，它需要考虑将它放在哪个节点上。如果我们能给它一些关于吊舱所需资源的信息，调度程序就能更好地决定在哪里放置吊舱；然后，它可以计算每个节点上的当前工作负载，并选择适合我们 pod 的预期资源使用的节点。我们可以通过资源**请求**选择性地给 Kubernetes 这个信息。当 pod 被调度到一个节点时，请求被考虑。请求并没有对 pod 在特定节点上运行后可能消耗的资源量进行任何限制，它们只是对我们(集群操作员)在请求将特定 pod 调度到集群时所做的请求进行了说明。

为了防止豆荚使用过多的资源，我们可以设置资源**限制**。这些限制可以由容器运行时强制实施，以确保 pod 不会使用比所需更多的特定资源。

我们可以说容器的 CPU 使用是可压缩的，因为如果我们限制它，它可能会导致我们的进程运行得更慢，但通常不会导致任何其他不良影响，而容器的内存使用是不可压缩的，因为如果容器使用超过其内存限制，唯一可用的补救措施是杀死有问题的容器。

将资源限制和请求的配置添加到 pod 规范中非常简单。在我们的清单中，每个容器规范都可以有一个包含请求和限制的`resources`字段。在这个例子中，我们请求为一个 Nginx 网络服务器容器分配 250 MiB 的内存和四分之一的中央处理器内核。因为限制设置得比请求高，所以这允许 pod 最多使用半个 CPU 内核，并且只有当容器的内存使用超过 128 Mi 时才会被杀死:

```
apiVersion: v1 
kind: Pod 
metadata: 
  name: webserver 
spec: 
  containers: 
  - name: nginx 
    image: nginx 
    resources: 
      limits: 
        memory: 128Mi 
        cpu: 500m 
      requests:
        memory: 64Mi 
        cpu: 250m 
```

# 资源单位

每当我们指定 CPU 请求或限制时，我们都是根据 CPU 内核来指定的。因为我们经常希望请求或限制 pod 的使用到整个 CPU 核心的某个部分，所以我们可以将 CPU 的这个部分指定为十进制或毫核心值。例如，0.5 的值代表半个核心。也可以用毫核心值配置请求或限制。由于单个内核有 1000 个毫内核，我们可以将半个 CPU 指定为 500 m。可以指定的最小 CPU 数量是 1 m 或 0.001。

I find that it can be more readable to use the millicore units in your manifests. When using `kubectl` or the Kubernetes dashboard, you will also notice that CPU limits and requests are formatted as millicore values. But if you are creating manifests with an automated process, you might use the floating point version.

内存的限制和请求以字节为单位。但是在你的清单中以这种方式指定它们将是非常笨拙和难以阅读的。因此，Kubernetes 支持引用多个字节的标准前缀；您可以选择使用十进制乘数，如 M 或 G，或者使用二进制等价物之一，如 Mi 或 Gi，它们更常用，因为它们反映了物理 RAM 的实际大小。

The binary versions of these units are actually what most people really mean when they are talking about megabytes or gigabytes, even though more correctly they are talking about mebibytes and gibibytes!

In practice, you should just always remember to use the units with an **i** on the end, or you will end up with slightly less memory than you expected. This notation was introduced in the ISO/IEC 80000 standard in 1998, in order to avoid confusion between the decimal and binary units.

| **十进制** | **二进制** |
| **名称** | 字节 | **字尾** | **名称** | 字节 | **字尾** |
| 千字节 | One thousand | K | kibibyte | One thousand and twenty-four | 基里巴斯 |
| 兆字节 | 1000 <sup>2</sup> | M | 莫比斯 | 1024 <sup>2</sup> | 大音阶的第三音 |
| 十亿字节 | 1000 <sup>3</sup> | G | 就像交换 | 1024 <sup>3</sup> | 镀锌铁 |
| 兆兆位 | 1000 <sup>4</sup> | T | 你好吗 | 1024 <sup>4</sup> | 钛 |
| 千兆兆字节 | 1000 <sup>5</sup> | P | -什么 | 1024 <sup>5</sup> | 圆周率 |
| 艾字节 | 1000 <sup>6</sup> | E | 交换条件 | 1024 <sup>6</sup> | 不，不 |

The memory units supported by Kubernetes

# 如何管理有资源限制的吊舱

当 Kubelet 启动一个容器时，CPU 和内存限制被传递给容器运行时，它负责管理该容器的资源使用。

如果您使用的是 Docker，则 CPU 限制(以毫秒为单位)将乘以 100，得出每 100 毫秒允许容器使用的 CPU 时间量。如果 CPU 处于负载状态，一旦容器使用了其配额，它将必须等到下一个 100 毫秒期间才能继续使用 CPU。

用于在运行于 cgroups 中的不同进程之间共享 CPU 资源的方法称为**完全公平调度器**或**CFS**；这是通过在不同的 cgroups 之间划分 CPU 时间来实现的。这通常意味着为一个组分配一定数量的切片。如果一个组中的进程处于空闲状态，并且没有使用分配给它们的 CPU 时间，这些共享将可供其他组中的进程使用。

这意味着，即使将限制设置得太低，一个 pod 也可能表现良好，但随后可能会停滞不前，这时另一个 pod 开始获得其分配的 CPU 份额。您可能会发现，如果您开始在空集群上设置单元的 CPU 限制并添加额外的工作负载，单元的性能就会开始下降。

在本章的后面，我们将讨论一些基本工具，这些工具可以让我们了解每个 pod 使用了多少 CPU。

如果达到内存限制，容器运行时将终止容器(并且可能会重新启动)。如果一个容器使用的内存超过了请求的数量，那么当节点的内存开始不足时，它就会成为驱逐的候选对象。

# 服务质量

当 Kubernetes 创建一个 pod 时，它被分配三个服务质量类之一。这些类用于决定 Kubernetes 如何调度和从节点中驱逐 pods。一般来说，具有保证服务质量等级的单元受到驱逐的干扰最少，而具有最佳服务质量等级的单元最有可能受到干扰:

*   **保证**:这是针对高优先级工作负载，尽可能避免从节点中被逐出，并且在 CPU 资源的较低服务质量类别中优先于吊舱，容器运行时保证在需要时可以获得限制中指定的全部 CPU。
*   **可突发**:这是针对不太重要的工作负载，例如，后台作业可以在可用时利用更大的 CPU，但只能保证达到 CPU 请求中指定的级别。当节点资源不足时，特别是当它们使用的内存超过请求的数量时，可突发的 pod 比那些在保证服务质量类中的 pod 更有可能被逐出节点。
*   **BestEffort** :如果一个节点资源不足，这个类的 Pods 最有可能被逐出。这个 QoS 类中的 Pods 也只能使用当时节点上空闲的任何 CPU 和内存，因此如果节点上运行的其他 pods 大量使用 CPU，这些 pods 可能最终会完全缺乏资源。如果您在这个类中安排 Pods，那么您应该确保您的应用在资源匮乏和频繁重启的情况下能够按预期运行。

在实践中，最好总是避免使用带有 BestEffort 服务质量类的单元，因为当集群负载较重时，这些单元会出现非常不寻常的行为。

当我们为容器设置资源和请求限制时，这些值的组合决定了容器的服务质量等级。

要获得最佳工作的服务质量等级，容器中的任何容器都不应设置任何 CPU 或内存请求或限制:

```
apiVersion: v1 
kind: Pod 
metadata: 
  name: best-effort 
spec: 
  containers: 
  - name: nginx 
    image: nginx 
```

没有资源限制或请求的 pod 将被分配最佳服务质量等级。

要获得保证的服务质量等级，容器需要对容器中的每个容器设置 CPU 和内存请求和限制。限制和请求必须相互匹配。作为一种快捷方式，如果容器仅设置了限制，Kubernetes 会自动为资源请求分配相等的值:

```
apiVersion: v1 
kind: Pod 
metadata: 
  name: guaranteed 
spec: 
  containers: 
  - name: nginx 
    image: nginx 
    resources: 
      limits: 
        memory: 256Mi 
        cpu: 500m 
```

将被分配有保证服务质量等级的吊舱。

任何介于这两种情况之间的都将被赋予可突发的服务质量等级。这适用于在任何 pod 上设置了任何 CPU 或内存限制或请求的任何 pod。但是如果它们不符合保证类的标准，例如，不在每个容器上设置两个限制，或者请求和限制不匹配:

```
apiVersion: v1 
kind: Pod 
metadata: 
  name: burstable 
spec: 
  containers: 
  - name: nginx 
    image: nginx 
    resources: 
      limits: 
        memory: 256Mi 
        cpu: 500m 
      requests: 
        memory: 128Mi 
        cpu: 250m 
```

将被分配可突发服务质量等级的吊舱。

# 资源配额

资源配额允许您限制特定命名空间可以使用的资源数量。根据您在组织中选择使用名称空间的方式，它们可以为您提供一种强大的方式来限制特定团队、应用或应用组使用的资源，同时仍然让开发人员可以自由调整每个单独容器的资源限制。

当您想要控制不同团队或应用的资源成本，但仍想实现将多个工作负载调度到同一个集群的利用率优势时，资源配额是一个有用的工具。

在 Kubernetes 中，资源配额由准入控制器管理。该控制器跟踪资源的使用情况，如 pods 和服务，如果超过限制，它将阻止新资源的创建。

资源配额接纳控制器由在命名空间中创建的一个或多个`ResourceQuota`对象配置。这些对象通常由集群管理员创建，但是您可以将创建它们集成到组织中更广泛的资源分配过程中。

让我们看一个如何使用配额来限制集群中 CPU 资源使用的示例。由于配额将影响一个名称空间内的所有 pods，我们将首先使用`kubectl`创建一个新的名称空间:

```
$ kubectl create namespace quota-example
namespace/quota-example created  
```

我们将首先创建一个简单的示例，确保创建的每个新 pod 都设置了 CPU 限制，并且总限制不超过两个内核:

```
apiVersion: v1 
kind: ResourceQuota 
metadata: 
  name: resource-quota 
  namespace: quota-example 
spec: 
  hard: 
    limits.cpu: 2 
```

通过使用`kubectl`向集群提交清单来创建`ResourceQuota`。

Once a `ResourceQuota` specifying resource requests or limits has been created in a namespace, it becomes mandatory for all pods to specify a corresponding request or limit before they can be created.

为了看到这种行为的发生，让我们在我们的名称空间中创建一个示例部署:

```
apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: example 
  namespace: quota-example 
spec: 
  selector: 
    matchLabels: 
      app: example 
  template: 
    metadata: 
      labels: 
        app: example 
    spec: 
      containers: 
      - name: nginx 
        image: nginx 
        resources: 
          limits: 
            cpu: 500m 
```

使用`kubectl`向 Kubernetes 提交部署清单后，检查 pod 是否正在运行:

```
$ kubectl -n quota-example get pods
NAME                      READY     STATUS    RESTARTS   AGE
example-fb556779d-4bzgd   1/1       Running   0          1m  
```

现在，扩大部署规模，观察是否创建了额外的吊舱:

```
$ kubectl -n quota-example scale deployment/example --replicas=4$ kubectl -n quota-example get pods
NAME                      READY     STATUS    RESTARTS   AGE
example-fb556779d-4bzgd   1/1       Running   0          2m
example-fb556779d-bpxm8   1/1       Running   0          1m
example-fb556779d-gkbvc   1/1       Running   0          1m
example-fb556779d-lcrg9   1/1       Running   0          1m  
```

因为我们指定了`500m`的 CPU 限制，所以将我们的部署扩展到四个副本没有问题，这使用了我们在配额中指定的两个内核。

但是，如果您现在尝试扩展部署，使其使用比配额中指定的更多的资源，您会发现 Kubernetes 没有安排额外的 pods:

```
$ kubectl -n quota-example scale deployment/example --replicas=5  
```

运行`kubectl get events`将向您显示一条消息，其中调度程序未能创建满足副本数量所需的额外 pod:

```
$ kubectl -n quota-example get events
...
Error creating: pods "example-fb556779d-xmsgv" is forbidden: exceeded quota: resource-quota, requested: limits.cpu=500m, used: limits.cpu=2, limited: limits.cpu=2  
```

# 默认限制

当您在命名空间上使用配额时，一个要求是命名空间中的每个容器都必须定义资源限制和请求。有时，这种需求会导致复杂性，并使快速使用 Kubernetes 变得更加困难。正确指定资源限制虽然是为生产准备应用的一个重要部分，但是在使用 Kubernetes 作为开发或测试工作负载的平台时，确实会增加额外的开销。

Kubernetes 提供了在名称空间级别提供默认请求和限制的工具。您可以使用它为特定应用或团队使用的名称空间提供一些合理的默认值。

我们可以使用`LimitRange`对象为命名空间中的容器配置默认限制和请求。这个对象允许我们为中央处理器或内存或者两者提供默认值。如果名称空间中存在一个`LimitRange`对象，那么任何没有在`LimitRange`中配置资源请求或限制而创建的容器将从限制范围继承这些值。

有两种情况`LimitRange`会在创建 pod 时影响资源限制或请求:

*   没有资源限制或请求的容器将从`LimitRange`对象继承资源限制和请求
*   没有资源限制但指定了请求的容器将从`LimitRange`对象继承资源限制

如果一个容器已经定义了限制和请求，那么`LimitRange`将不起作用。因为仅指定的容器将默认请求字段限制为相同的值，所以它们不会从`LimitRange`继承请求值。让我们看一个快速的例子。我们从创建一个新的命名空间开始:

```
$ kubectl create namespace limit-example
namespace/limit-example created  
```

为限制范围对象创建清单，并通过`kubectl`提交给集群:

```
apiVersion: v1 
kind: LimitRange 
metadata: 
  name: example 
  namespace: limit-example 
spec: 
  limits: 
  - default: 
      memory: 512Mi 
      cpu: 1 
    defaultRequest: 
      memory: 256Mi 
      cpu: 500m 
    type: Container 
```

如果您在此命名空间中创建一个没有资源限制的 pod，它将在创建时继承限制范围对象:

```
$ kubectl -n limit-example run --image=nginx example  
```

`deployment.apps/`示例已创建。

您可以通过运行`kubectl describe`来检查限值:

```
$ kubectl -n limit-example describe pods 
... 
    Limits: 
      cpu:     1 
      memory:  512Mi 
    Requests: 
      cpu:        500m 
      memory:     256Mi 
... 
```

# 水平吊舱自动缩放

一些应用可以通过添加额外的副本来扩大规模以处理增加的负载。无状态 web 应用就是一个很好的例子，因为添加额外的副本可以提供额外的容量来处理应用增加的请求。其他一些应用也设计成以这样一种方式运行，即增加额外的吊舱可以处理增加的负载；许多围绕处理来自中央队列的消息而设计的系统也可以用这种方式处理增加的负载。

当我们使用 Kubernetes 部署来部署 pod 工作负载时，使用`kubectl scale`命令上下扩展应用使用的副本数量非常简单。然而，如果我们希望我们的应用能够自动响应其工作负载的变化并进行扩展以满足需求，那么 Kubernetes 为我们提供了水平 Pod 自动扩展。

水平 Pod 自动缩放允许我们定义规则，这些规则将根据 CPU 利用率和可选的其他自定义指标在我们的部署中向上或向下缩放副本数量。在我们能够在集群中使用水平 Pod 自动缩放之前，我们需要部署 Kubernetes 度量服务器；该服务器提供用于发现 CPU 利用率和我们的应用生成的其他指标的端点。

# 部署度量服务器

在我们使用水平吊舱自动缩放之前，我们需要将 Kubernetes 度量服务器部署到我们的集群中。这是因为水平吊舱自动缩放控制器利用了由度量服务器提供的`metrics.k8s.io`应用编程接口提供的度量。

虽然默认情况下，Kubernetes 的某些安装可能会安装该插件，但在我们的 EKS 集群中，我们需要自己部署它。

有多种方法可以将附加组件部署到集群中:

*   如果您已经遵循了 [第 7 章](07.html)*生产就绪集群*中的建议，并且正在为您的集群配置 Terraform，则可以像我们在[第 7 章](07.html)*生产就绪集群*中配置 kube2iam 一样，使用`kubectl`配置所需的清单。
*   如果您使用 helm 来管理集群上的应用，您可以使用稳定/度量服务器图表。

*   在本章中，为了简单起见，我们将使用`kubectl.`来部署度量服务器清单
*   我喜欢将部署附加组件(如 metrics server 和 kube2iam)与配置集群的过程相集成，因为我将它们视为集群基础架构的组成部分。但是，如果您打算使用像 helm 这样的工具来管理向集群部署应用，那么您可能更喜欢使用相同的工具来管理集群上运行的所有内容。您做出的决定实际上取决于您和您的团队为管理集群和在集群上运行的应用而采用的流程。
*   度量服务器是在位于[https://github.com/kubernetes-incubator/metrics-server](https://github.com/kubernetes-incubator/metrics-server)的 GitHub 存储库中开发的，您可以在该存储库的部署目录中找到部署它所需的清单。

从从 GitHub 克隆配置开始。度量服务器开始支持 EKS 在 0.0.3 版中提供的身份验证方法，因此请确保您拥有的清单至少使用该版本。

您将在`deploy/1.8+`目录中找到许多清单。`auth-reader.yaml`和`auth-delegator.yaml`文件配置了度量服务器与 Kubernetes 授权基础设施的集成。`resource-reader.yaml`文件配置一个角色，赋予度量服务器从应用编程接口服务器读取资源的权限，以便发现 pods 运行的节点。基本上，`metrics-server-deployment.yaml`和`metrics-server-service.yaml`定义了用于运行服务本身的部署和能够访问服务的服务。最后，`metrics-apiservice.yaml`文件定义了一个`APIService`资源，该资源向 Kubernetes API 服务器聚合层注册 metrics.k8s.io API 组；这意味着对 metrics.k8s.io 组的 API 服务器的请求将被代理到 metrics server 服务。

使用`kubectl`部署这些清单很简单，只需使用`kubectl apply`将所有清单提交给集群:

```
$ kubectl apply -f deploy/1.8+  
```

您应该会看到一条关于正在集群上创建的每个资源的消息。

If you are using a tool like Terraform to provision your cluster, you might use it to submit the manifests for the metrics server when you create your cluster.

# 验证指标服务器并排除故障

在继续之前，我们应该花点时间检查一下我们的集群和指标服务器是否正确配置为一起工作。

当度量服务器在您的集群上运行并有机会从集群中收集度量(给它一分钟左右)后，您应该能够使用`kubectl top`命令查看集群中 pod 和节点的资源使用情况。

从运行`kubectl top nodes`开始。如果您看到这样的输出，则表明度量服务器配置正确，并且正在从您的节点收集度量:

```
$ kubectl top nodes
NAME             CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
ip-10-3-29-209   20m          1%        717Mi           19%
ip-10-3-61-119   24m          1%        1011Mi          28%  
```

如果您看到错误消息，您可以遵循许多故障排除步骤。

您应该首先描述指标服务器部署，并检查是否有一个副本可用:

```
kubectl -n kube-system describe deployment metrics-server  
```

如果不是，您应该通过运行`kubectl -n kube-system describe pod`来调试创建的 pod。查看事件，了解服务器不可用的原因。请确保您至少运行的是 0.0.3 版本的度量服务器，因为以前的版本不支持使用 EKS API 服务器进行身份验证。

如果度量服务器运行正常，运行`kubectl top`时仍会出现错误，问题是向聚合层注册的 APIservice 配置不正确。查看运行`kubectl describe apiservice v1beta1.metrics.k8s.io`时返回信息底部的事件输出。

一个常见的问题是 EKS 控制平面无法连接到端口`443`上的度量服务器服务。如果您遵循了[第 7 章](07.html)、*生产就绪集群*中的说明，您应该已经有了一个安全组规则，允许从控制平面到工作节点的流量，但是一些其他文档可以建议更严格的规则，这可能不允许端口`443`上的流量。

# 基于中央处理器使用情况自动缩放吊舱

一旦度量服务器安装到我们的集群中，我们将能够使用度量应用编程接口来检索关于集群中单元和节点的 CPU 和内存使用情况的信息。使用`kubectl top`命令就是一个简单的例子。

水平吊舱自动缩放器也可以使用相同的度量应用编程接口来收集关于组成部署的吊舱的当前资源使用情况的信息。

我们来看一个这样的例子；我们将部署一个在负载下使用大量 CPU 的示例应用，然后配置一个 Horizontal Pod Autoscaler 来扩展这个 Pod 的额外副本，以便在 CPU 利用率超过目标水平时提供额外的容量。

我们将作为示例部署的应用是一个简单的 Ruby web 应用，它可以计算斐波那契序列中的第 n 个数字，这个应用使用简单的递归算法，效率不是很高(非常适合我们进行自动缩放实验)。这个应用的部署非常简单。为 CPU 设置资源限制非常重要，因为目标 CPU 利用率基于该限制的百分比:

```
deployment.yaml 
apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: fib 
  labels: 
    app: fib 
spec: 
  selector: 
    matchLabels: 
      app: fib 
  template: 
    metadata: 
      labels: 
        app: fib 
    spec: 
      containers: 
      - name: fib 
        image: errm/fib 
        ports: 
        - containerPort: 9292 
        resources: 
          limits: 
            cpu: 250m 
            memory: 32Mi 
```

我们没有在部署规范中指定副本的数量；当我们第一次向集群提交此部署时，副本的数量将默认为 1。在创建部署时，这是一个很好的做法，我们打算通过水平 Pod 自动缩放器来调整副本，因为这意味着如果我们稍后使用`kubectl apply`来更新部署，我们将不会覆盖水平 Pod 自动缩放器设置的副本值(无意中缩小或扩大了部署)。

让我们将这个应用部署到集群中:

```
kubectl apply -f deployment.yaml  
```

You could run `kubectl get pods -l app=fib` to check that the application started up correctly.

我们将创建一个服务，这样我们就能够访问部署中的 pods，请求将被代理到每个副本，从而分散负载:

```
service.yaml 
kind: Service 
apiVersion: v1 
metadata: 
  name: fib 
spec: 
  selector: 
    app: fib 
  ports: 
  - protocol: TCP 
    port: 80 
    targetPort: 9292 
```

使用`kubectl`向集群提交服务清单:

```
kubectl apply -f service.yaml  
```

我们将配置一个水平 Pod 自动缩放器来控制部署中的副本数量。`spec`定义了我们希望自动缩放器如何工作；我们在这里定义了，我们希望自动缩放器维护 1 到 10 个应用副本，并在这些副本中实现 60 的目标平均 CPU 利用率。

当 CPU 利用率降至 60%以下时，自动缩放器将下调目标部署的副本数量；当超过 60%时，将添加副本:

```
hpa.yaml 
kind: HorizontalPodAutoscaler 
apiVersion: autoscaling/v2beta1 
metadata: 
  name: fib 
spec: 
  maxReplicas: 10 
  minReplicas: 1 
  scaleTargetRef: 
    apiVersion: app/v1 
    kind: Deployment 
    name: fib 
  metrics: 
  - type: Resource 
    resource: 
      name: cpu 
      targetAverageUtilization: 60 
```

用`kubectl`创建自动标尺:

```
kubectl apply -f hpa.yaml  
```

The `kubectl autoscale` command is a shortcut to create a `HorizontalPodAutoscaler`. Running `kubectl autoscale deployment fib --min=1 --max=10 --cpu-percent=60` would create an equivalent autoscaler.

一旦你创建了水平吊舱自动缩放器，你可以通过`kubectl describe`看到很多关于它当前状态的有趣信息:

```
$ kubectl describe hpa fib    
Name:              fib
Namespace:         default
CreationTimestamp: Sat, 15 Sep 2018 14:32:46 +0100
Reference:         Deployment/fib
Metrics:           ( current / target )
  resource cpu:    0% (1m) / 60%
Min replicas:      1
Max replicas:      10
Deployment pods:   1 current / 1 desired  
```

现在我们已经设置了我们的水平吊舱自动缩放器，我们应该在部署中在吊舱上生成一些负载来说明它是如何工作的。在这种情况下，我们将使用`ab` (Apache benchmark)工具反复要求我们的应用计算第三十个斐波那契数:

```
load.yaml
apiVersion: batch/v1 
kind: Job 
metadata: 
  name: fib-load 
  labels: 
    app: fib 
    component: load 
spec: 
  template: 
    spec: 
      containers: 
      - name: fib-load 
        image: errm/ab 
        args: ["-n1000", "-c4", "fib/30"] 
      restartPolicy: OnFailure 
```

该作业使用`ab`向端点发出 1000 个请求(并发量为 4)。将作业提交给集群，然后观察水平吊舱自动缩放器的状态:

```
kubectl apply -f load.yaml    
watch kubectl describe hpa fib
```

一旦加载作业开始发出请求，自动缩放器将扩展部署以处理加载:

```
Name:                   fib
Namespace:              default
CreationTimestamp: Sat, 15 Sep 2018 14:32:46 +0100
Reference:         Deployment/fib
Metrics:           ( current / target )
  resource cpu:    100% (251m) / 60%
Min replicas:      1
Max replicas:      10
Deployment pods:   2 current / 2 desired  
```

# 基于其他指标自动缩放吊舱

度量服务器提供了一些应用编程接口，水平吊舱自动缩放器可以使用这些接口获取集群中吊舱的中央处理器和内存利用率的信息。

可以像针对 CPU 指标那样设定利用率百分比，或者像针对内存指标那样设定绝对值:

```
hpa.yaml 
kind: HorizontalPodAutoscaler 
apiVersion: autoscaling/v2beta1 
metadata: 
  name: fib 
spec: 
  maxReplicas: 10 
  minReplicas: 1 
  scaleTargetRef: 
    apiVersion: app/v1 
    kind: Deployment 
    name: fib 
  metrics: 
  - type: Resource 
    resource: 
      name: memory 
      targetAverageValue: 20M 
```

水平吊舱自动缩放器还允许我们扩展由更全面的度量系统提供的其他度量。Kubernetes 允许为自定义和外部指标聚合指标 API。

自定义指标是除了与 pod 相关联的 CPU 和内存之外的指标。例如，您可以使用一个适配器，该适配器允许您使用像普罗米修斯这样的系统从您的吊舱中收集的指标。

如果您有关于应用利用率的更详细的可用指标，例如，公开繁忙工作进程计数的分叉 web 服务器，或者公开当前排队项目数的队列处理应用，这将非常有益。

外部度量适配器提供与 Kubernetes 中的任何对象都不相关的资源信息，例如，如果您使用的是外部排队系统，如 AWS SQS 服务。

总的来说，如果您的应用可以公开关于它们所依赖的使用外部度量适配器的资源的度量，那就更简单了，因为很难限制对特定度量的访问，而自定义度量与特定的 Pod 相关联，因此 Kubernetes 可以将访问限制为只对那些需要使用它们的用户和进程。

# 自动缩放集群

Kubernetes 水平 pod 自动缩放器的功能允许我们在应用的资源使用随时间变化时，在应用中添加和删除 Pod 副本。但是，这对我们集群的容量没有影响。如果我们的吊舱自动缩放器正在添加吊舱来处理负载的增加，那么最终我们可能会用完集群中的空间，并且其他吊舱将无法被调度。如果我们的应用负载减少，并且 pod 自动缩放器删除了 pod，那么我们就要为空闲的 EC2 实例支付 AWS。

当我们在[第 7 章](07.html)*中创建我们的集群时，我们使用自动缩放组来部署集群节点，因此随着部署到集群的应用的需求随着时间的推移而变化，我们应该能够使用它来增长和收缩集群。*

自动扩展组内置了根据实例的平均 CPU 利用率来扩展集群大小的支持。然而，在处理 Kubernetes 集群时，这并不真正适合，因为在我们集群的每个节点上运行的工作负载可能会有很大的不同，因此平均 CPU 利用率并不能很好地代表集群的可用容量。

值得庆幸的是，为了有效地为节点调度 pod，Kubernetes 跟踪每个节点的容量和每个 pod 请求的资源。通过利用这些信息，我们可以自动扩展集群以匹配工作负载的大小。

Kubernetes 自动缩放器项目为包括 AWS 在内的一些主要云提供商提供了集群自动缩放器组件。集群自动缩放器可以非常简单地部署到我们的集群中。除了能够向我们的集群添加实例之外，集群自动缩放器还能够在集群的容量可以减少时，从其中排出 pods，然后终止实例。

# 部署集群自动缩放器

将集群自动缩放器部署到我们的集群非常简单，因为它只需要运行一个简单的 pod。我们需要的只是一个简单的 Kubernetes 部署，就像我们在前面几章中使用的那样。

为了让集群自动缩放器更新我们的自动缩放组的所需容量，我们需要通过 IAM 角色授予它权限。如果您正在使用 kube2iam，正如我们在[第 7 章](07.html)*生产就绪集群*中所讨论的，我们将能够通过适当的注释为集群自动缩放器窗格指定该角色:

```
cluster_autoscaler.tf
data "aws_iam_policy_document" "eks_node_assume_role_policy" { 
  statement { 
    actions = ["sts:AssumeRole"] 
    principals { 
      type = "AWS" 
      identifiers = ["${aws_iam_role.node.arn}"] 
    } 
  } 
} 

resource "aws_iam_role" "cluster-autoscaler" { 
  name = "EKSClusterAutoscaler" 
  assume_role_policy = "${data.aws_iam_policy_document.eks_node_assume_role_policy.json}" 
} 

data "aws_iam_policy_document" "autoscaler" { 
  statement { 
    actions = [ 
      "autoscaling:DescribeAutoScalingGroups", 
      "autoscaling:DescribeAutoScalingInstances", 
      "autoscaling:DescribeTags", 
      "autoscaling:SetDesiredCapacity", 
      "autoscaling:TerminateInstanceInAutoScalingGroup" 
    ] 
    resources = ["*"] 
  } 
} 

resource "aws_iam_role_policy" "cluster_autoscaler" { 
  name = "cluster-autoscaler" 
  role = "${aws_iam_role.cluster_autoscaler.id}" 
  policy = "${data.aws_iam_policy_document.autoscaler.json}" 
} 
```

为了将集群自动缩放器部署到我们的集群，我们将使用`kubectl`提交部署清单，类似于我们在[第 7 章](07.html)、*生产就绪集群*中部署 kube2iam 的方式。我们将使用 Terraform 的模板系统来生成清单。

我们创建一个服务帐户，由自动缩放器用来连接到 Kubernetes API:

```
cluster_autoscaler.tpl
--- 
apiVersion: v1 
kind: ServiceAccount 
metadata: 
  labels: 
    k8s-addon: cluster-autoscaler.addons.k8s.io 
    k8s-app: cluster-autoscaler 
  name: cluster-autoscaler 
  namespace: kube-system 
```

集群自动缩放器需要读取有关集群当前资源使用情况的信息，并且需要能够从需要从集群中移除并终止的节点中逐出 pods。基本上，`cluster-autoscalerClusterRole`提供了这些操作所需的权限。以下是`cluster_autoscaler.tpl`的代码延续:

```
--- 
apiVersion: rbac.authorization.k8s.io/v1beta1 
kind: ClusterRole 
metadata: 
  name: cluster-autoscaler 
  labels: 
    k8s-addon: cluster-autoscaler.addons.k8s.io 
    k8s-app: cluster-autoscaler 
rules: 
- apiGroups: [""] 
  resources: ["events","endpoints"] 
  verbs: ["create", "patch"] 
- apiGroups: [""] 
  resources: ["pods/eviction"] 
  verbs: ["create"] 
- apiGroups: [""] 
  resources: ["pods/status"] 
  verbs: ["update"] 
- apiGroups: [""] 
  resources: ["endpoints"] 
  resourceNames: ["cluster-autoscaler"] 
  verbs: ["get","update"] 
- apiGroups: [""] 
  resources: ["nodes"] 
  verbs: ["watch","list","get","update"] 
- apiGroups: [""] 
  resources: ["pods","services","replicationcontrollers","persistentvolumeclaims","persistentvolumes"] 
  verbs: ["watch","list","get"] 
- apiGroups: ["extensions"] 
  resources: ["replicasets","daemonsets"] 
  verbs: ["watch","list","get"] 
- apiGroups: ["policy"] 
  resources: ["poddisruptionbudgets"] 
  verbs: ["watch","list"] 
- apiGroups: ["apps"] 
  resources: ["statefulsets"] 
  verbs: ["watch","list","get"] 
- apiGroups: ["storage.k8s.io"] 
  resources: ["storageclasses"] 
  verbs: ["watch","list","get"] 
--- 
apiVersion: rbac.authorization.k8s.io/v1beta1 
kind: ClusterRoleBinding 
metadata: 
  name: cluster-autoscaler 
  labels: 
    k8s-addon: cluster-autoscaler.addons.k8s.io 
    k8s-app: cluster-autoscaler 
roleRef: 
  apiGroup: rbac.authorization.k8s.io 
  kind: ClusterRole 
  name: cluster-autoscaler 
subjects: 
  - kind: ServiceAccount 
    name: cluster-autoscaler 
    namespace: kube-system 
```

注意`cluster-autoscaler`将状态信息存储在配置映射中，因此需要权限才能读写。这个角色允许。以下是`cluster_autoscaler.tpl`的代码延续:

```
--- 
apiVersion: rbac.authorization.k8s.io/v1beta1 
kind: Role 
metadata: 
  name: cluster-autoscaler 
  namespace: kube-system 
  labels: 
    k8s-addon: cluster-autoscaler.addons.k8s.io 
    k8s-app: cluster-autoscaler 
rules: 
- apiGroups: [""] 
  resources: ["configmaps"] 
  verbs: ["create"] 
- apiGroups: [""] 
  resources: ["configmaps"] 
  resourceNames: ["cluster-autoscaler-status"] 
  verbs: ["delete","get","update"] 
--- 
apiVersion: rbac.authorization.k8s.io/v1beta1 
kind: RoleBinding 
metadata: 
  name: cluster-autoscaler 
  namespace: kube-system 
  labels: 
    k8s-addon: cluster-autoscaler.addons.k8s.io 
    k8s-app: cluster-autoscaler 
roleRef: 
  apiGroup: rbac.authorization.k8s.io 
  kind: Role 
  name: cluster-autoscaler 
subjects: 
  - kind: ServiceAccount 
    name: cluster-autoscaler 
    namespace: kube-system 
```

最后，让我们考虑集群自动缩放器部署本身的清单。集群自动缩放器窗格包含运行集群自动缩放器控制循环的单个容器。您会注意到，我们正在将一些配置作为命令行参数传递给集群自动缩放器。最重要的是，`--node-group-auto-discovery`标志允许自动缩放器使用我们在[第 7 章](07.html)*中创建集群时在自动缩放组上设置的`kubernetes.io/cluster/<cluster_name>`标签对自动缩放组进行操作。这很方便，因为我们不必用集群自动缩放组显式配置自动缩放器。*

If your Kubernetes cluster has nodes in more than one availability zone and you are running pods that rely on being scheduled to a particular zone (for example, pods that are making use of EBS volumes), it is recommended to create an autoscaling group for each availability zone that you plan to use. If you use one autoscaling group that spans several zones, then the cluster autoscaler will be unable to specify the availability zone of the instances that it launches.

以下是`cluster_autoscaler.tpl`的代码延续:

```
--- 
apiVersion: extensions/v1beta1 
kind: Deployment 
metadata: 
  name: cluster-autoscaler 
  namespace: kube-system 
  labels: 
    app: cluster-autoscaler 
spec: 
  replicas: 1 
  selector: 
    matchLabels: 
      app: cluster-autoscaler 
  template: 
    metadata: 
      annotations: 
        iam.amazonaws.com/role: ${iam_role} 
      labels: 
        app: cluster-autoscaler 
    spec: 
      serviceAccountName: cluster-autoscaler 
      containers: 
        - image: k8s.gcr.io/cluster-autoscaler:v1.3.3 
          name: cluster-autoscaler 
          resources: 
            limits: 
              cpu: 100m 
              memory: 300Mi 
            requests: 
              cpu: 100m 
              memory: 300Mi 
          command: 
            - ./cluster-autoscaler 
            - --v=4 
            - --stderrthreshold=info 
            - --cloud-provider=aws 
            - --skip-nodes-with-local-storage=false 
            - --expander=least-waste 
            - --node-group-auto-discovery=asg:tag=kubernetes.io/cluster/${cluster_name} 
          env: 
            - name: AWS_REGION 
              value: ${aws_region} 
          volumeMounts: 
            - name: ssl-certs 
              mountPath: /etc/ssl/certs/ca-certificates.crt 
              readOnly: true 
          imagePullPolicy: "Always" 
      volumes: 
        - name: ssl-certs 
          hostPath: 
            path: "/etc/ssl/certs/ca-certificates.crt" 
```

最后，我们通过传入 AWS 区域、集群名和 IAM 角色的变量来呈现模板化清单，并使用`kubectl`将文件提交给 Kubernetes:

以下是`cluster_autoscaler.tpl`的代码延续:

```
data "aws_region" "current" {} 

data "template_file" " cluster_autoscaler " { 
  template = "${file("${path.module}/cluster_autoscaler.tpl")}" 

  vars { 
    aws_region = "${data.aws_region.current.name}" 
    cluster_name = "${aws_eks_cluster.control_plane.name}" 
    iam_role = "${aws_iam_role.cluster_autoscaler.name}" 
  } 
} 

resource "null_resource" "cluster_autoscaler" { 
  trigers = { 
    manifest_sha1 = "${sha1("${data.template_file.cluster_autoscaler.rendered}")}" 
  } 

  provisioner "local-exec" { 
    command = "kubectl  
--kubeconfig=${local_file.kubeconfig.filename} apply -f -<<EOF\n${data.template_file.cluster_autoscaler.rendered}\nEOF" 
  } 
} 
```

# 摘要

Kubernetes 是一个强大的工具；它非常有效地实现了比手动将应用调度到机器更高的计算资源利用率。通过设置正确的资源限制和请求，了解如何将资源分配给您的 pods 非常重要；如果不这样做，您的应用可能会变得不可靠或资源匮乏。

通过了解 Kubernetes 如何根据您分配的资源请求和限制为您的单元分配服务质量类，您可以精确控制单元的管理方式。通过确保您的关键应用(如网络服务器和数据库)使用保证类运行，您可以确保当需要重新安排 pods 时，它们将一致地运行，并遭受最小的中断。您可以通过对优先级较低的吊舱设置限制来提高集群的效率，这将导致它们被调度为可突发服务质量类。可突发的吊舱在可用时可以使用额外的资源，但在负载增加时不需要向集群添加额外的容量。

在管理用于运行多个应用的大型集群时，资源配额可能是无价的，甚至由组织中的不同团队使用，尤其是当您试图控制非生产工作负载(如测试和试运行环境)的成本时。

AWS 称其机器具有弹性是有原因的:它们可以在几分钟内扩大或缩小规模，以满足您的应用需求。如果您在负载可变的集群上运行工作负载，那么您应该利用这些属性和 Kubernetes 提供的工具来扩展您的部署，以匹配您的应用正在接收的负载，并将您的集群与需要调度的 pod 的大小相匹配。