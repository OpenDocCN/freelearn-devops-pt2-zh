# 用 Arista 和 Ansible 构建数据中心网络

在本章中，我们将概述如何在典型的数据中心环境中，在叶脊柱架构中自动化 Arista 交换机。我们将探讨如何使用 Ansible 与 Arista 设备进行交互，以及如何使用各种 Ansible 模块在 Arista 交换机上的**边界网关协议/以太网虚拟专用网络** ( **BGP/EVPN)** 设置中部署**虚拟局域网** ( **虚拟局域网**)和**虚拟可扩展局域网** ( **VXLANs** )。我们将基于以下基本叶脊柱**数据中心网络** ( **DCN** )的示例网络图进行说明:

![](assets/c1fb1ade-4c7d-4961-ad9d-24c18b2d42d8.png)

下表概述了示例拓扑中的设备及其各自的管理**互联网协议** ( **IPs** ):

| **装置** | **角色** | **供应商** | **管理(MGMT)港** | **MGMT IP** |
| Spine01 | 脊柱开关 | 艺术家见 4.20 | 管理 1 | `172.20.1.35` |
| Spine02 | 脊柱开关 | 艺术家见 4.20 | 管理 1 | `172.20.1.36` |
| 活页 01 | 叶开关 | 艺术家见 4.20 | 管理 1 | `172.20.1.41` |
| 活页 02 | 叶开关 | 艺术家见 4.20 | 管理 1 | `172.20.1.42` |
| 活页 03 | 叶开关 | 艺术家见 4.20 | 管理 1 | `172.20.1.43` |
| 活页 04 | 叶开关 | 艺术家见 4.20 | 管理 1 | `172.20.1.44` |

本章涵盖的主要配方如下:

*   建立Ansible网络清单
*   从 Ansible 连接并验证 Arista 设备
*   在 Arista 设备上启用**可扩展操作系统**(**EOS**)**API**(**eAPI**)
*   在 Arista 设备上配置通用系统选项
*   在 Arista 设备上配置接口
*   在 Arista 设备上配置底层 BGP
*   在 Arista 设备上配置覆盖 BGP/EVPN
*   在 Arista 设备上部署配置
*   在 Arista 设备上配置虚拟局域网
*   在 Arista 设备上配置 VXLAN 隧道
*   收集 Arista 设备事实
*   从 Arista 设备检索操作数据

# 技术要求

本章所有食谱的代码可在以下 GitHub repo 上找到:
[https://GitHub . com/PacktPublishing/Network-Automation-Cookbook/tree/master/CH4 _ arista](https://github.com/PacktPublishing/Network-Automation-Cookbook/tree/master/ch4_arista)。

本章基于以下软件版本:

*   运行 CentOS 7 的可移植机器
*   Ansible 2.9
*   Arista **虚拟化 EOS** ( **vEOS** )运行 EOS 4.20.1F

查看以下视频，了解《行动守则》:
[https://bit.ly/3coydTp](https://bit.ly/3coydTp)

# 建立Ansible网络清单

在本食谱中，我们将概述如何构建和组织 Ansible 库存，以描述我们的样本叶脊柱**直流** ( **DC** )网络。Ansible 库存是 Ansible 的一个关键部分，因为它概述了应该由 Ansible 管理的设备并对其进行分组。

# 准备好

我们需要创建一个新的文件夹来存放我们将在本章中创建的所有文件。新文件夹应该命名为`ch4_arista`。

# 怎么做...

1.  在新文件夹(`ch4_arista`)中，我们创建一个`hosts`文件，内容如下:

```
$ cat hosts

[leaf]
 leaf01 ansible_host=172.20.1.41
 leaf02 ansible_host=172.20.1.42
 leaf03 ansible_host=172.20.1.43
 leaf04 ansible_host=172.20.1.44

[spine]
 spine01 ansible_host=172.20.1.35
 spine02 ansible_host=172.20.1.36

[arista:children]
 leaf
 spine
```

2.  创建一个`ansible.cfg`文件，如下代码块所示:

```
$ cat ansible.cfg

[defaults]
 inventory=hosts
 retry_files_enabled=False
 gathering=explicit
 host_key_checking=False
```

# 它是如何工作的...

为了描述和分类我们网络中应该由 Ansible 管理的设备，定义 Ansible 清单是强制性的。在 Ansible 清单中，我们还使用`ansible_host`参数指定了 Ansible 将通过其与这些被管理设备通信的 IP 地址。

我们使用`hosts`文件构建了 Ansible 清单，并定义了多个组，以便将拓扑中的不同设备分组。这些群体如下:

*   我们创建了`leaf`组，它引用了拓扑中的所有`leaf`交换机。
*   我们创建了`spine`组，它引用了拓扑中的所有`spine`交换机。
*   我们创建了`arista`组，它同时引用了`leaf`组和`spine`组。

最后，我们创建了`ansible.cfg`文件，并将其配置为指向我们的`hosts`文件，用作 Ansible 库存文件。此外，我们禁用了`setup`模块(通过将`gathering`设置为`explicit)`，这在针对网络节点运行 Ansible 时是不需要的)。

# 从 Ansible 连接并验证 Arista 设备

在本食谱中，我们将概述如何通过**安全外壳** ( **SSH** )从 Ansible 连接到 Arista 设备，以便从 Ansible 开始管理设备。我们将使用用户名和密码来验证拓扑中的 Arista 设备。

# 准备好

为了遵循该配方，应根据之前的配方构建一个可翻译的库存文件。还必须实现 Ansible 控制机器和网络中所有设备之间的 IP 可达性。

# 怎么做...

1.  在`ch4_arista`文件夹内，创建一个`group_vars`文件夹。
2.  在`group_vars`文件夹中，创建一个包含以下内容的`arista.yml`文件:

```
ansible_network_os: eos
ansible_connection: network_cli
ansible_user: ansible
ansible_ssh_pass: ansible123
```

3.  在 Arista 交换机上，我们配置用户名和密码并启用 SSH，如以下代码块所示:

```
!
username Ansible privilege 15 role network-admin secret sha512
$6$mfU4Ei0AORd6rage$5YObhOI1g0wNBK5onaKDpYJhLZ9138maJKgcOznzFdpM25Tf3rb0PWSojUSM
RQY0Y7.cexCFj5aFLY17tuNU1
!

 !
management ssh
 idle-timeout 300
 authentication mode password
 login timeout 300
!
```

4.  在 Arista 交换机上，使用正确的 IP 地址配置管理接口，并将其置于所需的管理**虚拟路由和转发** ( **VRF** )中，如下所示:

```
vrf definition MGMT
!
 ip routing vrf MGMT
 !
interface Management1
 vrf forwarding MGMT
 ip address *$Ansible_host$
*   no lldp transmit
 no lldp receive
!
```

# 它是如何工作的...

我们在`group_vars`目录下的`arista.yml`文件中指定了将在所有 Arista 交换机上配置的用户名和密码。这将把这些参数应用于我们库存中的所有 Arista 开关。在 Arista 交换机上，我们设置了用户名和密码，启用了 SSH，并在管理界面上设置了正确的 IP 地址(在我们的清单中的`ansible_host`参数中使用的地址)。我们配置了管理 VRF，并将管理界面与这个 VRF 相关联。

We are specifying the SSH password in plaintext in our Ansible variables. This is only for a lab setup; however, for production, we should use the Ansible vault to secure any sensitive information, as outlined in the previous chapters.

在这个阶段，我们使用`network_cli`连接方法，以便使用 SSH 连接到 Arista 交换机。我们可以使用以下命令来验证 Ansible 控制器是否能够到达并正确登录设备:

```
$ ansible arista -m ping

 leaf03 | SUCCESS => {
 "changed": false,
 "ping": "pong"
}
leaf04 | SUCCESS => {
 "changed": false,
 "ping": "pong"
}
 <-- Output Omitted for brevity -->
```

# 在 Arista 设备上启用 eAPI

在本食谱中，我们将概述如何在 Arista 设备上启用 eAPI。eAPI 是 Arista 设备上的一个**表示状态转移** ( **REST** ful) API，它简化了此类设备的管理，并提供了一个一致且健壮的 API 来管理它们。这项任务至关重要，因为我们将在未来的所有食谱中使用 eAPI 来管理 Arista 设备。

# 准备好

作为该配方的先决条件，必须有一个可翻译的库存文件。SSH 身份验证也应该按照前面的方法进行部署和工作。

# 怎么做...

1.  在`group_vars`文件夹中创建一个`all.yml`文件，包含以下管理 VRF 数据:

```
$ cat  group_vars/all.yml

global:
 mgmt_vrf: MGMT
```

2.  创建一个名为`pb_eos_enable_eapi.yml`的新剧本，如以下代码块所示:

```
 $ cat pb_eos_eanble_eapi.yml
 ---
- name: "Enable eAPI on Arista Switches"
 hosts: arista
 vars:
 ansible_connection: network_cli
 tasks:
 - name: "Enable eAPI"
 eos_eapi:
 https_port: 443
 https: yes
 state: started
```

3.  使用以下任务更新`pb_eos_enable_eapi.yml`行动手册，以便在 VRF 的管理下启用 eAPI:

```
 - name: "Enable eAPI under VRF"
 eos_eapi:
 state: started
 vrf: "{{global.mgmt_vrf}}"
```

4.  用连接设置更新`group_vars`文件夹中的`arista.yml`文件，使用 eAPI 作为连接插件:

```
$ cat group_vars/arista.yml

ansible_network_os: eos
ansible_connection: httpapi
ansible_httpapi_use_ssl: yes
ansible_httpapi_validate_certs: no
```

# 它是如何工作的...

为了开始通过 eAPI 与 Arista 设备交互，我们需要首先启用它；因此，我们需要在最初将 SSH 引入设备，并启用 eAPI。这就是为什么，在这个食谱中，我们使用`network_cli` Ansible 连接，以便通过传统的 SSH 与 Arista 设备连接。由于我们将在未来所有食谱中与 Arista 设备的所有交互中使用 eAPI，因此我们仅在剧本级别的`vars`参数下启用`network_cli`，以便覆盖`ansible_connection`设置的任何组或主机级别设置。

我们创建了一个名为`pb_eos_enable_eapi.yml`的新剧本，在第一个任务中，我们使用`eos_eapi`模块在远程 Arista 设备上启用 eAPI 协议。我们指定将使用**超文本传输协议安全** ( **HTTPS** )和标准 HTTPS 端口，即`443`。在第二个任务中，我们使用了`eos_eapi`模块，以便仅在特定的 VRF 下启用 eAPI，这是我们用来管理设备的管理 VRF。

最后，为了开始使用 eAPI 管理 Arista 设备，我们修改了我们在`group_vars/arista.yml`文件中定义的 Ansible 连接设置，我们包括以下设置:

*   `ansible_connection`设置为`httpapi`。
*   `ansible_httpapi_use_ssl`设置为`yes`是为了强制使用 HTTPS 而不是 HTTP。
*   `ansible_httpapi_validate_certs`被设置为`no`以禁用证书验证(因为我们在 Arista 设备上使用默认证书，该证书不是由可信的**证书颁发机构** ( **CA** )签署的)。

运行剧本后，我们将看到所有 Arista 设备都配置了 eAPI，如以下代码块所示:

```
!
management api http-commands
 no shutdown
 !
 vrf MGMT
 no shutdown
!
```

我们可以使用以下命令验证我们使用的是正确的连接设置，并且 Ansible 能够使用 eAPI 与 Arista 设备通信:

```
$ ansible all -m ping -l leaf01 -vvvv 

<172.20.1.41> attempting to start connection
<172.20.1.41> using connection plugin httpapi
<172.20.1.41> loaded API plugin for network_os eos
**<172.20.1.41> ESTABLISH HTTP(S) CONNECTFOR USER: ansible TO** https://172.20.1.41:443
```

# 请参见...

欲了解更多关于`eos_eapi`模块和该模块支持的不同参数的信息，请访问以下网址:[https://docs . ansi ble . com/ansi ble/latest/modules/EOS _ eapi _ module . html](https://docs.ansible.com/ansible/latest/modules/eos_eapi_module.html)。

# 在 Arista 设备上配置通用系统选项

在本食谱中，我们将概述如何配置一些基本的系统选项，如主机名和**域名系统** ( **域名系统**)服务器，并在 Arista 设备上调配用户。我们将了解如何使用各种 Ansible 模块设置所有这些系统级参数，并将概述管理这些参数的不同方法。

# 准备好

为了遵循这个配方，假设已经建立了一个 Ansible 清单，并且按照前面的配方，在所有 Arista 设备上启用了 eAPI。

# 怎么做...

1.  用通用系统参数更新`group_vars/all.yml`文件，如下代码块所示:

```
$ cat  group_vars/all.yml

 <-- Output Omitted for brevity -->

 global:
 dns:
 - 172.20.1.1
 - 172.20.1.15
 site: DC1
 users:
 -   password: ansible123
 privilege: 15
 role: network-admin
 username: ansible
```

2.  创建新的行动手册`pb_arista_basic_config.yml`，并添加以下任务来设置域名系统和主机名:

```
$ cat pb_arista_basic_config.yml --- - name: "Configure Basic Configuration on Arista Fabric"
 hosts: arista tasks: - name: "Conifgure Basic System config" eos_system: hostname: " {{global.site|lower}}-{{inventory_hostname}}" name_servers: "{{ global.dns }}" state: present
```

3.  使用以下任务更新`pb_arista_basic_config.yml`行动手册，在 Arista 设备上创建用户:

```
 - name: "Configure Users"
 eos_user:
 name: "{{ item.username }}"
 role: "{{ item.role | default('network-admin') }}"
 privilege: "{{ item.privilege | default(15)}}"
 configured_password: "{{ item.password }}"
 state: present
 loop: "{{ global.users }}"
```

# 它是如何工作的...

Ansible 提供了不同的声明性模块，以便管理 Arista 交换机上的不同资源。在本食谱中，我们概述了如何使用`eos_system`和`eos_user`可移植模块，以便在 Arista 设备上提供基本的系统属性。我们从定义我们将在`group_vars/all.yml`文件下使用的数据开始，我们包括了我们想要提供的域名系统和用户。我们创建了`pb_arista_basic_config.yml`行动手册，其中将包括在 Arista 交换机上设置基本设置所需的所有任务。

剧本中的第一个任务使用了`eos_system` Ansible 模块，该模块在所有 Arista 设备上设置 DNS 和主机名。第二个任务使用`eos_user` Ansible 模块在 Arista 交换机上设置系统用户。在最后一个任务中，我们遍历了在`group_vars/all.yml`文件中定义的`users`数据结构，以便为这个`list`数据结构中的每个用户提供资源。

运行剧本后，我们可以看到 Arista 交换机的配置已经更新，如下面的代码块所示:

```
!
hostname dc1-leaf01
ip name-server vrf default 172.20.1.1
ip name-server vrf default 172.20.1.15
!
```

# 还有更多...

我们在本节中概述的声明性 Ansible 模块提供了一种为 Arista 设备配置基本系统级参数的简单方法；然而，它们可能没有涵盖我们在 Arista 交换机上需要设置的所有参数。为了更好地控制和灵活配置系统级参数，我们可以使用 Jinja2 模板和`template` Ansible 模块来生成我们部署所需的特定系统级配置。在本节中，我们将概述这种方法，以实现这一目标。这将是我们在后续配方中为其他配置部分生成配置时使用的方法，这些部分没有内置的 Ansible 模块来满足我们的所有需求。

我们将重用这种方法来为我们的 Arista 设备生成不同部分的配置，例如系统、接口和 BGP。我们将创建一个 Ansible 角色，以便包含生成最终配置所需的所有 Jinja2 模板和任务，我们将把最终配置推送到我们的设备上。以下过程概述了创建角色所需的步骤，以及生成配置所需的行动手册:

我们将使用与我们在[第 3 章](03.html)、*中使用的相同的角色结构和任务，在使用 Ansible 的服务提供商中自动化 Juniper 设备，*来生成 Juniper 设备的配置。唯一的区别是我们将用来为 Arista 设备生成特定配置的 Jinja2 模板。

1.  新建`roles`目录，添加名为`dc_fabirc_config`的新角色，目录结构如下:

```
$ tree roles/
roles/
└── dc_fabric_config
 ├── tasks
 └── templates
```

2.  在`tasks`文件夹中，创建一个`build_config_dir.yml`文件，创建所需的文件夹来存储将要生成的配置，如下所示:

```
$ cat roles/dc_fabric_config/tasks/build_config_dir.yml

---
- name: Create Config Directory
 file: path={{config_dir}}   state=directory
 run_once: yes
- name: Create Temp Directory per Node
 file: path={{tmp_dir}}/{{inventory_hostname}}  state=directory
- name: SET FACT >> Build Directory
 set_fact:
 build_dir: "{{tmp_dir}}/{{inventory_hostname}}"
```

3.  在`templates`文件夹中，新建一个名为`eos`的文件夹，并在该文件夹中新建一个名为`mgmt.j2`的 Jinja2 模板，如下代码块所示:

```
 $ cat roles/dc_fabric_config/templates/eos/mgmt.j2

!
hostname {{global.site|lower}}-{{inventory_hostname}}
!
!
spanning-tree mode none
!
aaa authorization exec default local
!
{% for user in global.users%}
username {{user.name}} privilege {{user.privilege}} role
{{user.role|default('network-admin')}} secret {{user.password}}
{% endfor%}
!
{% for dns_server in global.dns%}
ip name-server vrf default {{ dns_server }}
{% endfor %}
!
```

4.  在`tasks`文件夹中，创建一个名为`build_device_config.yml`的新 YAML 文件来创建系统配置，如以下代码块所示:

```
$ cat roles/dc_fabric_config/tasks/build_device_config.yml

---
- name: "System Configuration"
 template:
 src: "{{ansible_network_os}}/mgmt.j2"
 dest: "{{build_dir}}/00_mgmt.cfg"
 tags: mgmt
```

5.  在`tasks`文件夹中创建`main.yml`文件，任务如下:

```
$ cat roles/build_router_config/tasks/main.yml

---
- name: Build Required Directories
 import_tasks: build_config_dir.yml
- name: Build Device Configuration
 import_tasks: build_device_config.yml

 - name: "Remove Old Assembled Config"
 file:
 path: "{{config_dir}}/{{ inventory_hostname }}.cfg"
 state: absent
- name: Build Final Device Configuration
 assemble:
 src: "{{ build_dir }}"
 dest: "{{config_dir}}/{{ inventory_hostname }}.cfg"
- name: Remove Build Directory
 file: path={{ tmp_dir }}  state=absent
 run_once: yes
```

6.  创建名为`pb_arista_dc_fabric.yml`的新行动手册，为我们库存中的所有`arista`设备生成配置:

```
$ cat pb_arista_dc_fabric.yml

---
- name: "Build Arista DC Fabric"
 hosts: arista
 tasks:
 - name: Generate DC Fabric Configuration
 import_role:
 name: dc_fabric_config
 delegate_to: localhost
```

使用这种方法，我们创建了一个名为`dc_fabric_config`的角色，并创建了一个名为`mgmt.j2`的新 Jinja2 模板，其中包括用于`arista`系统级配置的模板。我们使用`template` Ansible 模块来渲染带有在`group_vars/all.yml`文件下定义的 Ansible 变量的 Jinja2 模板。为了保存每个设备的配置，我们创建了`configs`文件夹目录，其中存储了每个设备的最终配置。

由于我们利用 Jinja2 方法来为每个部分(MGMT、接口、BGP 等)生成配置，我们将把每个部分分割成一个单独的 Jinja2 模板，并将每个部分生成在一个单独的文件中。我们使用`assemble`模块将所有这些不同的部分组合成一个配置文件，我们将存储在`configs`目录中，这是每个设备的最终和组装的配置文件。我们将临时组装的部分存储在每个设备的临时文件夹中，并在剧本运行结束时删除该临时文件夹。

在本剧本中，我们在`import_role`任务中使用`delegate_to`本地主机。因为在这个角色的所有任务中，我们不需要连接到远程设备，所以所有这些任务都应该在 Ansible 控制机器上运行，以便将文件本地存储在 Ansible 机器上。因此，我们使用`delegate_to`本地主机来运行 Ansible 控制机器上的所有任务。

运行`pb_junos_net_build.yml`剧本后，我们可以看到在`configs`目录中创建了以下配置文件，在这个阶段，它只有配置的管理部分:

```
$ tree configs/
configs/
├── leaf01.cfg
├── leaf02.cfg
├── leaf03.cfg
├── leaf04.cfg
├── spine01.cfg
└── spine02.cfg
```

我们可以检查为其中一个设备生成的配置(例如，`leaf01`)，如以下代码块所示:

```
!
hostname dc1-leaf01
!
snmp-server enable traps
!
spanning-tree mode none
!
aaa authorization exec default local
!
username ansible privilege 15 role network-admin secret ansible123
!
ip name-server vrf default 172.20.1.1
ip name-server vrf default 172.20.1.15
!
```

在这个阶段，我们已经生成了库存中所有`arista`开关的系统配置；然而，我们仍然没有将这种配置推向设备。在后面的食谱中，我们将概述如何将配置推送到`arista`设备。

# 在 Arista 设备上配置接口

在这个食谱中，我们将概述如何在 Arista 设备上配置不同的接口参数，例如接口描述和 IP 地址信息。我们将概述如何使用各种 Ansible 模块与 Arista 设备上的接口进行交互，以及如何在示例网络拓扑中设置 Arista 设备上的接口。

# 准备好

我们假设网络库存已经到位，并且按照之前的方法，eAPI 已经在 Arista 交换机上启用。

# 怎么做...

1.  将以下内容添加到`group_vars/all.yml`文件中，该文件描述了我们的示例 DC 结构网络上的接口:

```
p2p_ip:
 leaf01:
 - {port: Ethernet8, ip: 172.31.1.1 , peer: spine01, pport: Ethernet1, peer_ip: 172.31.1.0}
 - {port: Ethernet9, ip: 172.31.1.11 , peer: spine02, pport: Ethernet1, peer_ip: 172.31.1.10}
 leaf02:
 < -- Output Omitted for brevity -->
 leaf03:
 < -- Output Omitted for brevity -->
 leaf04:
 < -- Output Omitted for brevity -->
 spine01:
 < -- Output Omitted for brevity -->
 spine02:
 < -- Output Omitted for brevity -->

lo_ip:
 leaf01: 10.100.1.1/32
 leaf02: 10.100.1.2/32
 leaf03: 10.100.1.3/32
 leaf04: 10.100.1.4/32
 spine01: 10.100.1.254/32
 spine02: 10.100.1.253/32
```

2.  使用以下任务更新`pb_arista_basic_config.yml`行动手册，以启用接口并在所有结构接口上设置描述:

```
- name: "Configure the Physical Interfaces"
 eos_interface:
 name: "{{ item.port }}"
 enabled: true
 description: "{{global.site}} | Rpeer:{{item.peer}} | Rport:{{item.pport}}"
 with_items: "{{p2p_ip[inventory_hostname]}}"
```

3.  使用以下任务更新`pb_arista_basic_config.yml`行动手册，在所有**点对点** ( **P2P** )结构链接上设置 IPv4 地址:

```
- name: "Configure IP Addresses"
 eos_l3_interface:
 name: "{{ item.port }}"
 ipv4: "{{ item.ip }}/{{ global.p2p_prefix }}"
 state: present
 with_items: "{{ p2p_ip[inventory_hostname] }}"
```

# 它是如何工作的...

我们在`group_vars/all.yml`文件的两个主要数据结构中定义了示例网络拓扑中所有接口的所有数据。我们使用`p2p_ip`字典对样本网络中的所有 P2P IP 地址进行建模，并使用`lo_ip`字典为我们的节点指定环回 IP 地址。

我们使用`eos_interface` Ansible 模块来启用接口，并设置接口的基本参数，如接口描述。我们遍历了每个设备的`p2p_ip`数据结构，并为网络清单中所有设备的每个接口设置了正确的参数。我们使用`eos_l3_interface` Ansible 模块在所有设备的示例网络拓扑中的所有接口上设置正确的 IPv4 地址。

# 还有更多...

如果我们需要对接口配置有更多的控制，并设置我们在本节中概述的声明性 Ansible 模块没有涉及的参数，我们可以使用 Jinja2 模板来实现这个目标。使用与我们在前面的系统配置方法中概述的完全相同的方法，我们可以生成 Juniper 设备所需的接口配置。
使用我们在前面的配方中创建的同一个 Ansible 角色，我们可以扩展它来为我们的 Arista 设备生成接口配置。我们使用以下步骤来完成此任务:

1.  在`templates`文件夹中创建新的 Jinja2 模板文件`intf.js`，包含以下数据:

```
$ cat roles/dc_fabric_config/templates/eos/intf.j2

{% set node_intfs = p2p_ip[inventory_hostname] %}
{% for p in node_intfs| sort(attribute='port') %}
!
interface {{p.port}}
 description "{{global.site}} | Rpeer: {{p.peer}} | Rport: {{p.pport}}"
 no switchport
 ip address {{p.ip}}/{{global.p2p_prefix}}
{% endfor %}
!
!
interface Loopback0
 ip address {{lo_ip[inventory_hostname]}}
!
```

2.  用新任务更新`tasks`目录中的`build_device_config.yml`文件，生成界面配置:

```
$ cat roles/dc_fabric_config/tasks/build_device_config.yml

<-- Output Trimmed for brevity ------>

- name: "Interface Configuration"
 template:
 src: "{{ansible_network_os}}/intf.j2"
 dest: "{{build_dir}}/01_intf.cfg"
 tags: intf
```

3.  运行`pb_arista_dc_fabric.yml`行动手册后，我们将为我们的设备生成配置，例如为`leaf01`更新的`interface`部分:

```
$ cat configs/leaf01.cfg

< -- Output Omitted for brevity -->

!
interface Ethernet8
 description "DC1 | Rpeer: spine01 | Rport: Ethernet1"
 no switchport
 ip address 172.31.1.1/31
!
interface Ethernet9
 description "DC1 | Rpeer: spine02 | Rport: Ethernet1"
 no switchport
 ip address 172.31.1.11/31
!
!
interface Loopback0
 ip address 10.100.1.1/32
!
```

# 请参见...

欲了解更多关于`eos_interface`模块和该模块支持的不同参数的信息，请访问以下网址:[https://docs . ansi ble . com/ansi ble/latest/modules/EOS _ interface _ module . html](https://docs.ansible.com/ansible/latest/modules/eos_interface_module.html)。

有关`eos_l3_interface`模块以及该模块支持的不同参数的更多信息，请参考以下网址:[https://docs . ansi ble . com/ansi ble/latest/modules/EOS _ L3 _ interface _ module . html](https://docs.ansible.com/ansible/latest/modules/eos_l3_interface_module.html)。

# 在 Arista 设备上配置底层 BGP

在本食谱中，我们将概述如何将 eBGP 配置为示例叶/脊 DC 结构的底层路由协议。我们将使用叶交换机和主干交换机之间的 P2P IP 地址来构建 eBGP 对等设置。BGP **自治系统号** ( **ASN** )分配如下表所示:

| **节点** | **BGP ASN** |
| Spine01 | Sixty-five thousand one hundred |
| Spine02 | Sixty-five thousand one hundred |
| 活页 01 | Sixty-five thousand and one |
| 活页 02 | Sixty-five thousand and two |
| 活页 03 | Sixty-five thousand and three |
| 活页 04 | Sixty-five thousand and four |

# 准备好

在这个配方中，我们假设接口和 IP 地址信息已经按照前面的配方进行了配置。

# 怎么做...

1.  创建一个`host_vars`目录，并为我们清单中的每个设备创建一个文件夹。在每个文件夹中，创建一个新的 YAML 文件`underlay_bgp.yml`，其中包含 BGP 对等细节。以下是我们库存中的`leaf01`设备示例:

```
## Leaf01 BGP Data ###
bgp_asn: 65001
bgp_peers:
 - peer: spine01
 peer_ip: 172.31.1.0
 remote_as: 65100
 - peer: spine02
 peer_ip: 172.31.1.10
 remote_as: 65100
```

2.  用以下数据在`templates/eos`目录中创建新的 Jinja2 文件`underlay_bgp.j2`。此模板用于`prefix-list`，我们将使用它来控制我们的 DC 结构中的 BGP 广告:

```
$ cat roles/dc_fabric_config/templates/eos/underlay_bgp.j2 {% set bgp_grp = 'LEAF' if 'spine' in inventory_hostname else 'SPINE' %}
!
route-map loopback permit 10
 match ip address prefix-list loopback
!
{% if 'spine' in inventory_hostname %}
!
ip prefix-list loopback
{% for node,ip in lo_ip.items() | sort %}
{% if 'leaf' in node or inventory_hostname in node %}
 seq {{loop.index + 10 }} permit {{ip}}
{% endif %}
{% endfor %}
!
{% else %}
!
ip prefix-list loopback
 seq 10 permit {{lo_ip[inventory_hostname]}}
!
{% endif %}
```

3.  用 BGP 模板更新`templates/eos`目录下的`underlay_bgp.j2` Jinja2 文件，如下代码块所示:

```
$ cat roles/dc_fabric_config/templates/eos/underlay_bgp.j2

!
router bgp {{bgp_asn}}
 router-id {{lo_ip[inventory_hostname].split('/')[0]}}
 maximum-paths 2
 bgp bestpath tie-break router-id
 neighbor {{ bgp_grp }} peer-group
 neighbor {{ bgp_grp }} description "Peer Group for All {{bgp_grp}} Nodes"
 neighbor {{ bgp_grp }} graceful-restart-helper
 neighbor {{ bgp_grp }} send-community standard extended
 neighbor {{ bgp_grp }} maximum-routes 100000 warning-only
{% for p in bgp_peers %}
 neighbor {{ p.peer_ip}} peer-group {{ bgp_grp }}
 neighbor {{ p.peer_ip}} remote-as {{p.remote_as}}
{% endfor %}
 redistribute connected route-map loopback
 !
 address-family ipv4
 neighbor {{ bgp_grp }} activate
 neighbor {{ bgp_grp }} route-map loopback out
!
```

4.  在`tasks`文件夹内的`build_config.yml`文件中，添加以下任务来渲染底层 BGP 配置:

```
$ cat roles/dc_fabric_config/tasks/build_device_config.yml

< -- Output Omitted for brevity -->

- name: "Underlay BGP Configuration"
 template:
 src: "{{ansible_network_os}}/underlay_bgp.j2"
 dest: "{{config_dir}}/{{ inventory_hostname }}/03_bgp.cfg"
```

# 它是如何工作的...

根据我们的设计，我们将在叶节点和主干节点之间运行 eBGP，并且我们拓扑中的每个叶交换机都将有自己的 BGP ASN。描述这种设置的最佳方法是使用`host_vars`文件夹在每台主机的基础上包含所有这些数据。我们为每个节点创建了一个文件夹，以包含该文件夹下的所有相关主机数据。我们创建了一个 YAML 文件来保存每个设备的 BGP 信息，因此，如果我们需要为另一个协议添加更多特定于主机的数据，我们可以轻松地添加一个新文件:

```
$ tree host_vars
 host_vars
 ├── leaf01
 │ └── underlay_bgp.yml
 ├── leaf02
 │ └── underlay_bgp.yml
 ├── leaf03
 │ └── underlay_bgp.yml
 ├── leaf04
 │ └── underlay_bgp.yml
 ├── spine01
 │ └── underlay_bgp.yml
 └── spine02
 └── underlay_bgp.yml
```

在`tasks/build_device_config.yml`文件中，我们添加了一个新任务，该任务使用`underlay_bgp.j2` Jinja2 模板来渲染 Jinja2 模板，并输出 Ansible 清单中概述的每个设备的底层 BGP 配置部分。

对于每台设备，我们生成了一个`prefix-list`来匹配将向其 eBGP 对等方通告的所有前缀，符合以下标准:

*   对于主干交换机，我们会公布所有叶环回 IP 地址以及主干环回接口。
*   对于叶交换机，我们只通告环回 IP 地址。

以下片段概述了在运行带有新任务的剧本后为`leaf01`设备生成的 BGP 配置:

```
$ cat configs/leaf01/04_bgp.cfg

!
route-map loopback permit 10
 match ip address prefix-list loopback
!
ip prefix-list loopback
 seq 10 permit 10.100.1.1/32
!
router bgp 65001
 router-id 10.100.1.1
 maximum-paths 2
 bgp bestpath tie-break router-id
 neighbor SPINE peer-group
 neighbor SPINE description "Peer Group for All SPINE Nodes"
 neighbor SPINE graceful-restart-helper
 neighbor SPINE send-community standard extended
 neighbor SPINE maximum-routes 100000 warning-only
 neighbor 172.31.1.0 peer-group SPINE
 neighbor 172.31.1.0 remote-as 65100
 neighbor 172.31.1.10 peer-group SPINE
 neighbor 172.31.1.10 remote-as 65100
 redistribute connected route-map loopback
 !
 address-family ipv4
 neighbor SPINE activate
 neighbor SPINE route-map loopback out
! 
```

# 在 Arista 设备上配置覆盖 BGP EVPN

在本食谱中，我们将概述如何在示例拓扑中使用 Ansible 将覆盖的 BGP EVPN 配置为横跨叶脊 DC 结构的 VXLAN 隧道的控制平面。

# 准备好

本方案假设 P2P IP 地址和环回接口已经按照之前的方案进行了配置。此外，底层 BGP 配置应该已经按照之前的配方生成。

# 怎么做...

1.  在`templates/eos`目录中创建新的 Jinja2 文件`overlay_bgp.j2`，包含以下数据:

```
$ cat roles/dc_fabric_config/templates/eos/overlay_bgp.j2

{% set bgp_evpn_grp = 'LEAF_EVPN' if 'spine' in inventory_hostname else 'SPINE_EVPN' %}

service routing protocols model multi-agent
!
router bgp {{bgp_asn}}

 neighbor {{ bgp_evpn_grp }} peer-group
 neighbor {{ bgp_evpn_grp }} description "Peer Group for All {{bgp_evpn_grp}} EVPN Nodes"
 neighbor {{ bgp_evpn_grp }} graceful-restart-helper
 neighbor {{ bgp_evpn_grp }} send-community extended
 neighbor {{ bgp_evpn_grp }} maximum-routes 100000 warning-only
 neighbor {{ bgp_evpn_grp }} ebgp-multihop 2
 neighbor {{ bgp_evpn_grp }} update-source Loopback0
{% for p in bgp_peers %}
 neighbor {{ lo_ip[p.peer].split('/')[0]}} peer-group {{ bgp_evpn_grp }}
 neighbor {{ lo_ip[p.peer].split('/')[0]}} remote-as {{p.remote_as}}
{% endfor %}
 !
 address-family evpn
 neighbor {{ bgp_evpn_grp }} activate
 !
 address-family ipv4
 no neighbor {{ bgp_evpn_grp }} activate
!
```

2.  在`tasks`文件夹内的`build_config.yml`文件中，添加以下突出显示的任务:

```
$ cat tasks/build_config.yml

< -- Output Omitted for brevity -->

- name: "Overlay BGP EVPN Configuration"
 template:
 src: "{{ansible_network_os}}/overlay_bgp.j2"
 dest: "{{config_dir}}/{{ inventory_hostname }}/04_evpn.cfg"
```

# 它是如何工作的...

在这个配方中，我们使用了类似于配置底层 eBGP 的方法。我们构建了一个 Jinja2 模板，为库存中的 Arista 设备生成所需的 BGP EVPN 配置。以下代码块显示了`leaf01`开关的 BGP EVPN 配置示例:

```
service routing protocols model multi-agent
!
router bgp 65001

 neighbor SPINE_EVPN peer-group
 neighbor SPINE_EVPN description "Peer Group for All SPINE_EVPN EVPN Nodes"
 neighbor SPINE_EVPN graceful-restart-helper
 neighbor SPINE_EVPN send-community extended
 neighbor SPINE_EVPN maximum-routes 100000 warning-only
 neighbor SPINE_EVPN ebgp-multihop 2
 neighbor SPINE_EVPN update-source Loopback0
 neighbor 10.100.1.254 peer-group SPINE_EVPN
 neighbor 10.100.1.254 remote-as 65100
 neighbor 10.100.1.253 peer-group SPINE_EVPN
 neighbor 10.100.1.253 remote-as 65100
 !
 address-family evpn
 neighbor SPINE_EVPN activate
 !
 address-family ipv4
 no neighbor SPINE_EVPN activate
! 
```

# 在 Arista 设备上部署配置

在本食谱中，我们将概述如何将配置推送到 Arista 设备。我们将使用在前面的方案中生成的配置来配置拓扑中的设备。我们将学习如何使用合适的 Ansible 模块与 Arista 配置交互，以便根据预期的网络设计正确配置设备。

# 准备好

该配方要求在 Arista 设备上启用 eAPI。

# 怎么做...

在`pb_arista_dc_fabric.yml`文件中，添加以下任务将配置部署到 Arista 交换机:

```
- name: "Deploy Configuration"
 eos_config:
 src: "{{config_dir}}/{{ inventory_hostname }}.cfg"
 replace: config
 save_when: changed
 tags: deploy
```

# 它是如何工作的...

在前面的方案中，我们为 Arista 交换机生成了不同的配置部分，例如接口和底层/覆盖 BGP。我们使用了`assemble` Ansible 模块，以便将配置的不同部分组合成一个保存所有设备配置的配置文件。在本食谱中，我们使用`eos_config`模块将配置文件推送到 Arista 开关。

在`eos_config`模块中，我们使用`src`参数来指定要加载到设备中的配置文件的位置。我们将`replace`指令与`config`选项一起使用，以便用我们在`src`选项中指定的新配置替换目标设备上的所有配置。因此，设备上的配置完全由 Ansible 管理和控制。这也意味着，如果有任何配置是在我们的 Ansible 行动手册之外实施的，则在我们运行行动手册并将新配置推送到设备后，该配置将被删除。

最后，我们使用`save_when`参数并将其设置为`changed`，以便将运行配置复制到`startup-config`并保存配置。我们只在任务更改了设备配置的情况下执行此操作。

# 请参见...

欲了解更多关于`eos_config`模块和该模块支持的不同参数的信息，请访问以下网址:[https://docs . ansi ble . com/ansi ble/latest/modules/EOS _ config _ module . html](https://docs.ansible.com/ansible/latest/modules/eos_config_module.html)。

# 在 Arista 设备上配置虚拟局域网

在本食谱中，我们将概述如何在 Arista 交换机上配置虚拟局域网。下表显示了我们将通过 DC 结构构建的虚拟局域网:

| **节点** | **界面** | **接口类型** | VLAN |
| 活页 01 | 以太网 1 | 接近 | Ten |
| 活页 02 | 以太网 1 | 接近 | Twenty |
| 活页 03 | 以太网 1 | 接近 | Ten |
| 活页 03 | 以太网 2 | 接近 | Twenty |
| 活页 04 | 以太网 1 | 接近 | Ten |
| 活页 04 | 以太网 2 | 接近 | Twenty |

# 准备好

根据前面的配方，本配方假设底层和上层 BGP 配置已经生成。

# 怎么做...

1.  创建一个名为`vlan_design.yml`的新 YAML 文件，该文件将保存我们 DC 面料的 VLAN 设计，如以下代码块所示:

```
$ cat vlan_design.yml
vlan_data:
 leaf01:
 - id: 10
 description: DB
 ports:
 - Ethernet1
 leaf02:
 - id: 20
 description: web
 ports:
 - Ethernet1
 < -- Output Omitted for brevity -->
```

2.  在`roles`文件夹中创建新角色`provision_vlans`，其结构如下:

```
$ tree roles/provision_vlans/
 roles/provision_vlans/
 ├── tasks
 │ └── main.yml
 ├── templates
 └── vars
 └── main.yml
```

3.  在`tasks/main.yml`文件中，包括以下任务来配置我们的 DC 结构上的虚拟局域网:

```
$ cat roles/provision_vlans/tasks/main.yml

---
- name: Deploy VLANs on DC Fabric
 eos_vlan:
 name: "VLAN_{{vlan.id}}_{{ vlan.description }}"
 vlan_id: "{{ vlan.id }}"
 state: present
 interfaces: "{{ vlan.ports }}"
 loop: "{{ vlan_data[inventory_hostname] }}"
 loop_control:
 loop_var: vlan
 tags: vlans
```

4.  创建一个新的行动手册`pb_deploy_vlans.yml`，使用该角色在我们的 DC 结构上订购虚拟局域网，如以下代码块所示:

```

 $ cat pb_deploy_vlans.yml

---
- name: Provision VLANs on DC Fabric
 hosts: arista
 vars_files: vlan_design.yml
 tasks:
 - name: Deploy Vlans on DC Fabric
 import_role:
 name: provision_vlans
 when: inventory_hostname in vlan_data.keys()
```

# 它是如何工作的...

为了在我们的 DC 结构上提供虚拟局域网，我们在一个名为`vlan_design.yml`的 YAML 文件中建模并定义了我们的 VLAN 成员资格。该文件在`vlan_data`字典中对我们结构中所有交换机的所有虚拟局域网进行建模。该字典中的每个键都是设备，值是字典列表，每个字典对应一个 VLAN 定义。

我们创建了一个特定的角色`provision_vlans`，在我们的结构上配置虚拟局域网，这个角色的初始任务使用`eos_vlan` Ansible 模块来配置虚拟局域网。我们绕过了每个节点特有的`vlan_data`并提供了这些虚拟局域网。

我们创建了一个`pb_deploy_vlans.yml`剧本，使用这个角色来部署虚拟局域网。我们使用`vars_files`参数读取`vlan_design.yml`文件，使用`import_roles`导入`provision_vlans`角色。我们使用`when`指令只是为了在我们的 VLAN `design`文件中定义的设备上调用这个角色。

运行我们的行动手册后，我们可以看到虚拟局域网部署在我们的结构中，如这里为`leaf03`所述，例如:

```
dc1-leaf03#sh vlan
 VLAN Name Status Ports
 ----- -------------------------------- --------- -------------------------
 1 default active Et3, Et4, Et5, Et6, Et7
 10 VLAN_10_DB active Et1
 20 VLAN_20_web active Et2

```

# 请参见...

有关`eos_vlan`模块和该模块支持的不同参数的更多信息，请参考以下网址:[https://docs . ansi ble . com/ansi ble/latest/modules/EOS _ VLAN _ module . html](https://docs.ansible.com/ansible/latest/modules/eos_vlan_module.html)。[](https://docs.ansible.com/ansible/latest/modules/eos_vlan_module.html)

# 在 Arista 设备上配置 VXLANs 隧道

在本食谱中，我们将概述如何使用跨越叶脊结构的 BGP EVPN 来配置 VXLAN 隧道。在类似于示例拓扑的 IP 结构中，我们需要有 VXLAN 隧道，以便通过我们的结构传输 L2 虚拟局域网。下表概述了我们将在整个结构中使用的从 VLAN 到**虚拟网络标识符** ( **VNI** )的映射:

| **VLAN** | **格**格 |
| Ten | One thousand and ten |
| Twenty | One thousand and twenty |

# 准备好

该方法假设 BGP EVPN 已经在我们的结构中部署，并且所有虚拟局域网都已配置。

# 怎么做...

1.  用以下变量更新`provision_vlans`角色中的`vars/main.yml`文件夹。这将定义存储 VXLAN 配置的目录:

```
$ cat roles/provision_vlans/vars/main.yml
 ---
 config_dir: ./vxlan_configs
```

2.  在我们的`provision_vlans`角色中，创建一个`templates`文件夹。然后，在其中创建一个`eos`文件夹。之后，创建一个包含以下内容的 Jinja2 `vxlan.j2`文件:

```
$ cat roles/provision_vlans/templates/eos/vxlan.j2

{% set vlans = vlan_data[inventory_hostname] %}
{% set all_vlans = vlans | map(attribute='id') | list %}
!
interface Vxlan1
 vxlan source-interface Loopback0
{% for vlan in all_vlans %}
 vxlan vlan {{ vlan }} vni 10{{vlan}}
{% endfor %}
!
router bgp {{bgp_asn}}
!
{% for vlan in all_vlans %}
 vlan {{ vlan }}
 rd {{lo_ip[inventory_hostname].split('/')[0]}}:10{{vlan}}
 route-target both 10{{vlan}}:10{{vlan}}
 redistribute learned
{% endfor %}
 !
```

3.  使用以下任务更新`provision_vlans`角色中的`tasks/main.yml`文件，以生成 VXLAN 配置:

```
- name: Create VXLAN Configs Folder
 file: path={{config_dir}} state=directory
 run_once: yes
 delegate_to: localhost
 tags: vxlan

- name: "Create VXLAN Configuration"
 template:
 src: "{{ansible_network_os}}/vxlan.j2"
 dest: "{{config_dir}}/{{ inventory_hostname }}.cfg"
 delegate_to: localhost
 tags: vxlan
```

4.  使用以下任务更新`tasks/main.yml`文件，以便在我们的 DC 结构交换机上部署 VXLAN 配置:

```
- name: "Deploy Configuration"
 eos_config:
 src: "{{config_dir}}/{{ inventory_hostname }}.cfg"
 save_when: changed
 tags: vxlan
```

# 它是如何工作的...

在上一个食谱中，我们概述了如何在我们的 DC 结构中配置虚拟局域网。然而，在一个 IP 结构中，我们需要有隧道，以便通过 DC 结构传输 L2 虚拟局域网。在本食谱中，我们概述了如何使用 BGP EVPN 构建 VXLAN 隧道，以传输 L2 虚拟局域网，并通过我们的 DC 结构完成 VLAN 配置任务。

由于 VXLAN 隧道与各自的 VLAN 紧密耦合，我们将 VXLAN 隧道的设置包含在我们的`provision_vlans`角色中。我们使用 Jinja2 模板和`template` Ansible 模块来生成部署我们的 VXLAN 隧道所需的每台交换机上的 VXLAN 和 BGP 配置。我们创建了一个新文件夹来存放我们将为每个交换机生成的 VXLAN 配置。我们利用`template` Ansible 模块将我们的`vlan_design.yml`文件中定义的 VLAN 数据与 Jinja2 模板一起渲染，以生成每个交换机的 VXLAN 配置。

运行更新后的行动手册后，我们可以看到创建了新文件夹，并生成了所有交换机的配置:

```
$ tree vxlan_configs/
vxlan_configs/
├── leaf01.cfg
├── leaf02.cfg
├── leaf03.cfg
└── leaf04.cfg
```

以下代码块显示了为`leaf01`开关生成的 VXLAN 配置示例:

```
$ cat vxlan_configs/leaf01.cfg

interface Vxlan1
 vxlan source-interface Loopback0
 vxlan udp-port 4789
 vxlan vlan 10 vni 1010
!
router bgp 65001
!
 vlan 10
 rd 10.100.1.1:1010
 route-target both 1010:1010
 redistribute learned
 !
```

# 收集 Arista 设备事实

在本食谱中，我们将概述如何检索 Ansible 为运行 Arista EOS 软件的 Arista 设备收集的基础系统事实。这些基本的系统事实为我们提供了关于 Arista 设备的基本健康检查，我们可以用它来验证其运行状态。

# 准备好

必须在 Arista 设备上启用 eAPI，以便您可以使用本食谱中的 Ansible 模块。

# 怎么做...

1.  创建一个新的行动手册`pb_arista_facts.yml`，任务如下收集事实:

```
$ cat pb_jnpr_facts.yml

---
- name: Collect and Validate Arista DC Fabric Facts
 hosts: arista
 tasks:
 - name: Collect Arista Device Facts
 eos_facts:
```

2.  使用以下任务更新`pb_arista_facts.yml`行动手册，以验证我们所有结构接口的运行状态:

```
 - name: Validate all DC Fabric Interface are Operational
 assert:
 that:
 - ansible_net_interfaces[item.port].lineprotocol == 'up'
 fail_msg: "Interface {{item.port}} is not Operational "
 loop: "{{ p2p_ip[inventory_hostname] }}"
```

3.  使用以下任务更新行动手册，以验证所有结构接口的正确 IP 地址分配:

```
- name: Validate all DC Fabric Interface are has Correct IP
 assert:
 that:
 - ansible_net_interfaces[item.port].ipv4.address == item.ip
 fail_msg: "Interface {{item.port}} has Wrong IP Address"
 loop: "{{ p2p_ip[inventory_hostname] }}"
```

# 它是如何工作的...

Ansible 提供了一个事实收集模块来收集 Arista 设备的基本系统属性，并以一致和结构化的数据结构返回这些事实。我们可以使用本模块收集的事实来验证设备的基本属性和运行状态。

在这个配方中，我们使用`eos_facts`模块来收集我们所有 Arista 设备的设备事实。该模块返回了 Ansible 为多个变量中的每个设备收集的基本事实。我们感兴趣的主要变量是`ansible_net_interfaces`变量，它保存了设备上所有接口的所有操作状态。以下代码片段概述了存储在此变量中的数据示例:

```
"ansible_net_interfaces": {
 "Ethernet8": {
 "bandwidth": 0,
 "description": "DC1 | Rpeer: spine01 | Rport: Ethernet1",
 "duplex": "duplexFull",
 "ipv4": {
 "address": "172.31.1.1",
 "masklen": 31
 },
 "lineprotocol": "up",
 "macaddress": "50:00:00:03:37:66",
 "mtu": 1500,
 "operstatus": "connected",
 "type": "routed"
 }
}
```

我们使用 Ansible 检索并存储在`ansible_net_interfaces`变量中的数据，以验证所有结构接口都是可操作的，并且它们具有根据我们的设计分配的正确的 IP 地址。我们使用`assert`模块来执行该验证，并循环每个设备的`p2p_ip`数据结构，以便仅验证我们的结构接口的状态。

# 请参见...

欲了解更多关于`eos_facts`模块和该模块支持的不同参数的信息，请访问以下网址:[https://docs . ansi ble . com/ansi ble/latest/modules/EOS _ facts _ module . html](https://docs.ansible.com/ansible/latest/modules/eos_facts_module.html)。

# 从 Arista 设备检索操作数据

在本食谱中，我们将概述如何在 Arista 设备上执行操作命令，并使用输出来验证设备的状态。

# 准备好

必须在 Arista 设备上启用 eAPI，才能遵循该配方。

# 怎么做...

1.  创建一个名为`pb_get_vlans.yml`的新剧本，并填充它以在所有叶交换机上执行`show vlan`命令，并将输出存储在一个变量中:

```
---
- name: " Play 1: Retrieve All VLANs from Arista Switches"
 hosts: leaf
 vars_files: vlan_design.yml
 tasks:
 - name: "Get All VLANs"
 eos_command:
 commands: show vlan | json
 register: show_vlan
```

2.  更新`pb_get_vlans.yml`行动手册，并使用以下任务填充它，以比较和验证设备上配置了正确的虚拟局域网:

```
 - name: "Validate VLANs are Present"
 assert:
 that: (item.vlan | string) in show_vlan.stdout[0].vlans.keys()
 fail_msg: "VLAN:{{ item.vlan }} is NOT configured "
 success_msg: "VLAN:{{ item.vlan }} is configured "
 loop: "{{ access_interfaces[inventory_hostname] }}"
 delegate_to: localhost
```

# 它是如何工作的...

我们使用`eos_command` Ansible 模块在 Arista 开关上执行操作命令，为了返回结构化输出，我们在命令中使用`json`关键字返回操作命令的 JSON 输出(如果支持)。在本例中，我们发送了`show vlan`命令来获取设备上配置的虚拟局域网列表，并在`show_vlan`变量中收集了输出。下面的代码片段概述了我们从设备获得的输出，它存储在这个变量中:

```
ok: [leaf01] => {
 "show_vlan": {
 < -- Output Omitted for brevity -->
 "stdout": [
 {
 "vlans": {
 "1": {
 "dynamic": false,
 "interfaces": {
 < -- Output Omitted for brevity -->
 },
 "name": "default",
 "status": "active"
 },
 "10": {
 "dynamic": false,
 "interfaces": {
 "Ethernet1": {
 "privatePromoted": false
 },
 "Vxlan1": {
 "privatePromoted": false
 }
 },
 "name": "VLAN_10",
 "status": "active"
 }
 }
 }
 ] 
```

我们使用`assert`模块来验证我们设计中定义的虚拟局域网(在`vlans_design.yml`文件中)对于每台交换机都进行了配置和操作。我们将此文件中定义的虚拟局域网与我们使用`eos_command`模块(存储在`show_vlan`变量中)从设备中检索到的输出进行了比较，以确保每个 VLAN 在交换机上都处于活动状态。

We are using the `string` Jinja2 filter in our `assert` statement since the VLANs are defined as integers in our `vlan_design.yml` file. However, the VLANs stored in the `show_vlan` variable are strings. Thus, in order for the `assert` statement to succeed, we need to make sure that the type is similar.

# 请参见...

更多关于`eos_command`模块以及该模块支持的不同参数的信息，请参考以下网址:[https://docs . ansi ble . com/ansi ble/latest/modules/EOS _ command _ module . html](https://docs.ansible.com/ansible/latest/modules/eos_command_module.html)