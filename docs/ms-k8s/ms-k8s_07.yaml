- en: Handling Kubernetes Storage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理Kubernetes存储
- en: In this chapter, we'll look at how Kubernetes manages storage. Storage is very
    different from compute, but at a high level they are both resources. Kubernetes,
    as a generic platform, takes the approach of abstracting storage behind a programming
    model and a set of plugins for storage providers. First, we'll go into detail
    about the storage conceptual model and how storage is made available to containers
    in the cluster. Then, we'll cover the common cloud platform storage providers,
    such as AWS, GCE, and Azure. Then we'll look at a prominent open source storage
    provider (GlusterFS from Red Hat), which provides a distributed filesystem. We'll
    also look into an alternative solution–Flocker–that manages your data in containers
    as part of the Kubernetes cluster. Finally, we'll see how Kubernetes supports
    the integration of existing enterprise storage solutions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看一下Kubernetes如何管理存储。存储与计算非常不同，但在高层次上它们都是资源。作为一个通用平台，Kubernetes采取了在编程模型和一组存储提供者插件后面抽象存储的方法。首先，我们将详细介绍存储的概念模型以及如何将存储提供给集群中的容器。然后，我们将介绍常见的云平台存储提供者，如AWS、GCE和Azure。然后我们将看一下著名的开源存储提供者（来自红帽的GlusterFS），它提供了一个分布式文件系统。我们还将研究一种替代方案——Flocker——它将您的数据作为Kubernetes集群的一部分进行管理。最后，我们将看看Kubernetes如何支持现有企业存储解决方案的集成。
- en: At the end of this chapter, you'll have a solid understanding of how storage
    is represented in Kubernetes, the various storage options in each deployment environment
    (local testing, public cloud, and enterprise), and how to choose the best option
    for your use case.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将对Kubernetes中存储的表示有扎实的了解，了解每个部署环境（本地测试、公共云和企业）中的各种存储选项，并了解如何为您的用例选择最佳选项。
- en: Persistent volumes walk-through
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久卷演练
- en: In this section, we will look at the Kubernetes storage conceptual model and
    see how to map persistent storage into containers so they can read and write.
    Let's start by looking at the problem of storage. Containers and pods are ephemeral.
    Anything a container writes to its own filesystem gets wiped out when the container
    dies. Containers can also mount directories from their host node and read or write.
    That will survive container restarts, but the nodes themselves are not immortal.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看一下Kubernetes存储的概念模型，并了解如何将持久存储映射到容器中，以便它们可以读写。让我们先来看看存储的问题。容器和Pod是短暂的。当容器死亡时，容器写入自己文件系统的任何内容都会被清除。容器也可以挂载宿主节点的目录并进行读写。这样可以在容器重新启动时保留，但节点本身并不是不朽的。
- en: There are other problems, such as ownership for mounted hosted directories when
    the container dies. Just imagine a bunch of containers writing important data
    to various data directories on their host and then go away leaving all that data
    all over the nodes with no direct way to tell what container wrote what data.
    You can try to record this information, but where would you record it? It's pretty
    clear that, for a large-scale system, you need persistent storage accessible from
    any node to reliably manage the data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他问题，比如当容器死亡时，挂载的宿主目录的所有权。想象一下，一堆容器将重要数据写入它们的宿主机上的各个数据目录，然后离开，留下所有这些数据散落在节点上，没有直接的方法告诉哪个容器写入了哪些数据。您可以尝试记录这些信息，但您会在哪里记录呢？很明显，对于大规模系统，您需要从任何节点访问持久存储以可靠地管理数据。
- en: Volumes
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷
- en: The basic Kubernetes storage abstraction is the volume. Containers mount volumes
    that bind to their pod and they access the storage, wherever it may be, as if
    it's in their local filesystem. This is nothing new, and it is great because,
    as a developer who writes applications that need access to data, you don't have
    to worry about where and how the data is stored.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的Kubernetes存储抽象是卷。容器挂载绑定到其Pod的卷，并访问存储，无论它在哪里，都好像它在它们的本地文件系统中一样。这并不新鲜，但很棒，因为作为一个需要访问数据的应用程序开发人员，您不必担心数据存储在何处以及如何存储。
- en: Using emptyDir for intra-pod communication
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用emptyDir进行Pod内通信
- en: It is very simple to share data between containers in the same pod using a shared
    volume. Container 1 and container 2 simply mount the same volume and can communicate
    by reading and writing to this shared space. The most basic volume is the `emptyDir`.
    An `emptyDir` volume is an `empty` directory on the host. Note that it is not
    persistent because when the pod is removed from the node, the contents are erased.
    If a container just crashes, the pod will stick around and you can access it later.
    Another very interesting option is to use a RAM disk, by specifying the medium
    as `Memory`. Now, your containers communicate through shared memory, which is
    much faster but more volatile, of course. If the node is restarted, the `emptyDir`
    volume's contents are lost.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享卷在同一Pod中的容器之间共享数据非常简单。容器1和容器2只需挂载相同的卷，就可以通过读写到这个共享空间进行通信。最基本的卷是`emptyDir`。`emptyDir`卷是主机上的`empty`目录。请注意，它不是持久的，因为当Pod从节点中移除时，内容会被擦除。如果容器崩溃，Pod将继续存在，稍后可以访问它。另一个非常有趣的选项是使用RAM磁盘，通过指定介质为`Memory`。现在，您的容器通过共享内存进行通信，这样做速度更快，但当然更易失。如果节点重新启动，`emptyDir`卷的内容将丢失。
- en: 'Here is a `pod` configuration file that has two containers that mount the same
    volume called `shared-volume`. The containers mount it in different paths, but
    when the `hue-global-listener` container is writing a file to `/notifications`,
    `hue-job-scheduler` will see that file under `/incoming`:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个`pod`配置文件，其中有两个容器挂载名为`shared-volume`的相同卷。这些容器在不同的路径上挂载它，但当`hue-global-listener`容器将文件写入`/notifications`时，`hue-job-scheduler`将在`/incoming`下看到该文件。
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To use the shared memory option, we just need to add `medium`: `Memory` to
    the `emptyDir` section:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用共享内存选项，我们只需要在`emptyDir`部分添加`medium`:`Memory`：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using HostPath for intra-node communication
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用HostPath进行节点内通信
- en: 'Sometimes you want your pods to get access to some host information (for example,
    the Docker Daemon) or you want pods on the same node to communicate with each
    other. This is useful if the pods know they are on the same host. Since Kubernetes
    schedules pods based on available resources, pods usually don''t know what other
    pods they share the node with. There are two cases where a pod can rely on other
    pods being scheduled with it on the same node:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您希望您的Pod可以访问一些主机信息（例如Docker守护程序），或者您希望同一节点上的Pod可以相互通信。如果Pod知道它们在同一主机上，这将非常有用。由于Kubernetes根据可用资源调度Pod，Pod通常不知道它们与哪些其他Pod共享节点。有两种情况下，Pod可以依赖于其他Pod与其一起在同一节点上调度：
- en: In a single-node cluster all pods obviously share the same node
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单节点集群中，所有Pod显然共享同一节点
- en: DaemonSet pods always share a node with any other pod that matches their selector
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DaemonSet Pod始终与与其选择器匹配的任何其他Pod共享节点
- en: For example, in [Chapter 6](6502d8f5-5418-4f9e-9b61-ec3c38d2f018.xhtml), *Using
    Critical Kubernetes Resources*, we discussed a DaemonSet pod that serves as an
    aggregating proxy to other pods. Another way to implement this behavior is for
    the pods to simply write their data to a mounted volume that is bound to a `host`
    directory and the DaemonSet pod can directly read it and act on it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[第6章](6502d8f5-5418-4f9e-9b61-ec3c38d2f018.xhtml)中，*使用关键的Kubernetes资源*，我们讨论了一个作为聚合代理的DaemonSet
    pod到其他pod的。实现此行为的另一种方法是让pod将其数据简单地写入绑定到`host`目录的挂载卷，然后DaemonSet pod可以直接读取并对其进行操作。
- en: 'Before you decide to use the HostPath volume, make sure you understand the
    limitations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定使用HostPath卷之前，请确保您了解限制：
- en: The behavior of pods with the same configuration might be different if they
    are data-driven and the files on their host are different
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有相同配置的pod的行为可能会有所不同，如果它们是数据驱动的，并且它们主机上的文件不同
- en: It can violate resource-based scheduling (coming soon to Kubernetes) because
    Kubernetes can't monitor HostPath resources
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可能会违反基于资源的调度（即将推出到Kubernetes），因为Kubernetes无法监视HostPath资源
- en: The containers that access host directories must have a security context with
    `privileged` set to `true` or, on the host side, you need to change the permissions
    to allow writing
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问主机目录的容器必须具有`privileged`设置为`true`的安全上下文，或者在主机端，您需要更改权限以允许写入
- en: 'Here is a configuration file that mounts the `/coupons` directory into the
    `hue-coupon-hunter` container, which is mapped to the host''s `/etc/hue/data/coupons`
    directory:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置文件，将`/coupons`目录挂载到`hue-coupon-hunter`容器中，该容器映射到主机的`/etc/hue/data/coupons`目录：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since the pod doesn''t have a `privileged` security context, it will not be
    able to write to the `host` directory. Let''s change the container spec to enable
    it by adding a security context:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于pod没有`privileged`安全上下文，它将无法写入`host`目录。让我们改变容器规范以通过添加安全上下文来启用它：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the following diagram, you can see that each container has its own local
    storage area inaccessible to other containers or pods, and the host''s `/data`
    directory is mounted as a volume into both container 1 and container 2:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您可以看到每个容器都有自己的本地存储区，其他容器或pod无法访问，并且主机的`/data`目录被挂载为卷到容器1和容器2：
- en: '![](Images/51e8d5df-c170-469b-8284-f1c2e952e87a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/51e8d5df-c170-469b-8284-f1c2e952e87a.png)'
- en: Using local volumes for durable node storage
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用本地卷进行持久节点存储
- en: 'Local volumes are similar to HostPath, but they persist across pod restarts
    and node restarts. In that sense, they are considered persistent volumes. They
    were added in Kubernetes 1.7\. As of Kubernetes 1.10 require a feature gate to
    enable. The purpose of local volumes is to support StatefulSet, where specific
    pods need to be scheduled on nodes that contain specific storage volumes. Local
    volumes have node affinity annotations that simplify the binding of pods to the
    storage they need to access:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本地卷类似于HostPath，但它们在pod重新启动和节点重新启动时保持不变。在这种意义上，它们被视为持久卷。它们在Kubernetes 1.7中添加。截至Kubernetes
    1.10需要启用功能门。本地卷的目的是支持StatefulSet，其中特定的pod需要被调度到包含特定存储卷的节点上。本地卷具有节点亲和性注释，简化了将pod绑定到它们需要访问的存储的过程：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Provisioning persistent volumes
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供持久卷
- en: While `emptyDir` volumes can be mounted and used by containers, they are not
    persistent and don't require any special provisioning because they use existing
    storage on the node. `HostPath` volumes persist on the original node, but if a
    pod is restarted on a different node, it can't access the `HostPath` volume from
    its previous node. `Local` volumes persist on the node and can survive pod restarts
    and rescheduling and even node restarts. Real persistent volumes use external
    storage (not a disk physically attached to the node) provisioned ahead of time
    by administrators. In cloud environments, the provisioning may be very streamlined
    but it is still required, and, as a Kubernetes cluster administrator, you have
    to at least make sure your storage quota is adequate and monitor usage versus
    quota diligently.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`emptyDir` 卷可以被挂载和容器使用，但它们不是持久的，也不需要任何特殊的配置，因为它们使用节点上的现有存储。`HostPath` 卷在原始节点上持久存在，但如果
    pod 在不同的节点上重新启动，它无法访问先前节点上的 `HostPath` 卷。`Local` 卷在节点上持久存在，可以在 pod 重新启动、重新调度甚至节点重新启动时幸存下来。真正的持久卷使用提前由管理员配置的外部存储（不是物理连接到节点的磁盘）。在云环境中，配置可能非常简化，但仍然是必需的，作为
    Kubernetes 集群管理员，您至少要确保您的存储配额是充足的，并且要认真监控使用情况与配额的对比。'
- en: Remember that persistent volumes are resources that the Kubernetes cluster is
    using in a similar way to nodes. As such, they are not managed by the Kubernetes
    API server. You can provision resources statically or dynamically.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，持久卷是 Kubernetes 集群类似于节点使用的资源。因此，它们不受 Kubernetes API 服务器的管理。您可以静态或动态地配置资源。
- en: '**Provisioning persistent volumes statically**: Static provisioning is straightforward.
    The cluster administrator creates persistent volumes backed up by some storage
    media ahead of time, and these persistent volumes can be claimed by containers.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**静态配置持久卷**：静态配置很简单。集群管理员提前创建由某些存储介质支持的持久卷，这些持久卷可以被容器声明。'
- en: '**Provisioning persistent volumes dynamically**: Dynamic provisioning may happen
    when a persistent volume claim doesn''t match any of the statically provisioned
    persistent volumes. If the claim specified a storage class and the administrator
    configured that class for dynamic provisioning, then a persistent volume may be
    provisioned on the fly. We will see examples later when we discuss persistent
    volume claims and storage classes.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态配置持久卷**：当持久卷声明与静态配置的持久卷不匹配时，动态配置可能会发生。如果声明指定了存储类，并且管理员为该类配置了动态配置，那么持久卷可能会被即时配置。当我们讨论持久卷声明和存储类时，我们将在后面看到示例。'
- en: '**Provisioning persistent volumes externally**: One of the recent trends is
    to move storage provisioners out of the Kubernetes core into volume plugins (also
    known as out-of-tree). External provisioners work just like in-tree dynamic provisioners
    but can be deployed and updated independently. More and more in-tree storage provisioners
    migrate out-of-tree. Check out this Kubernetes incubator project: [https://github.com/kubernetes-incubator/external-storage](https://github.com/kubernetes-incubator/external-storage).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部配置持久卷**：最近的一个趋势是将存储配置器从 Kubernetes 核心移出到卷插件（也称为 out-of-tree）。外部配置器的工作方式与
    in-tree 动态配置器相同，但可以独立部署和更新。越来越多的 in-tree 存储配置器迁移到 out-of-tree。查看这个 Kubernetes
    孵化器项目：[https://github.com/kubernetes-incubator/external-storage](https://github.com/kubernetes-incubator/external-storage)。'
- en: Creating persistent volumes
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建持久卷
- en: 'Here is the configuration file for an NFS persistent volume:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 NFS 持久卷的配置文件：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A persistent volume has a spec and metadata that includes the name. Let''s
    focus on the spec here. There are several sections: capacity, volume mode, access
    modes, reclaim policy, storage class, and the volume type (`nfs` in the example).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 持久卷具有包括名称在内的规范和元数据。让我们在这里关注规范。有几个部分：容量、卷模式、访问模式、回收策略、存储类和卷类型（例如示例中的`nfs`）。
- en: Capacity
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容量
- en: 'Each volume has a designated amount of storage. Storage claims may be satisfied
    by persistent volumes that have at least that amount of storage. In the example,
    the persistent volume has a capacity of `100` Gibibytes (2^(30) bytes). It is
    important when allocating static persistent volumes to understand the storage
    request patterns. For example, if you provision 20 persistent volumes with 100
    GiB capacity and a container claims a persistent volume with 150 GiB, then this
    claim will not be satisfied even though there is enough capacity overall:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷都有指定的存储量。存储索赔可以由至少具有该存储量的持久卷满足。例如，持久卷的容量为`100` Gibibytes（2^(30)字节）。在分配静态持久卷时，了解存储请求模式非常重要。例如，如果您配置了100
    GiB容量的20个持久卷，并且容器索赔了150 GiB的持久卷，则即使总体容量足够，该索赔也不会得到满足：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Volume mode
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷模式
- en: The optional volume mode was added in Kubernetes 1.9 as an Alpha feature for
    static provisioning (even though you specify it as a field on the spec and not
    in an annotation). It lets you specify if you want a file system (`"Filesystem"`)
    or raw storage (`"Block"`). If you don't specify volume mode then the default
    is `"Filesystem"` just like it was pre-1.9.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的卷模式在Kubernetes 1.9中作为静态配置的Alpha功能添加（即使您在规范中指定它作为字段，而不是在注释中）。它允许您指定是否需要文件系统（`"Filesystem"`）或原始存储（`"Block"`）。如果不指定卷模式，则默认值是`"Filesystem"`，就像在1.9之前一样。
- en: Access modes
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问模式
- en: 'There are three access modes:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种访问模式：
- en: '`ReadOnlyMany`: Can be mounted as read-only by many nodes'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadOnlyMany`：可以由多个节点挂载为只读'
- en: '`ReadWriteOnce`: Can be mounted as read-write by a single node'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadWriteOnce`：可以由单个节点挂载为读写'
- en: '`ReadWriteMany`: Can be mounted as read-write by many nodes'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadWriteMany`：可以由多个节点挂载为读写'
- en: The storage is mounted to nodes, so even with `ReadWriteOnce`, multiple containers
    on the same node can mount the volume and write to it. If that causes a problem,
    you need to handle it though some other mechanism (for example, you could claim
    the volume only in DaemonSet pods that you know will have just one per node).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 存储被挂载到节点，所以即使使用`ReadWriteOnce`，同一节点上的多个容器也可以挂载该卷并对其进行写入。如果这造成问题，您需要通过其他机制来处理（例如，您可以只在您知道每个节点只有一个的DaemonSet
    pods中索赔该卷）。
- en: 'Different storage providers support some subset of these modes. When you provision
    a persistent volume, you can specify which modes it will support. For example,
    NFS supports all modes, but in the example, only these modes were enabled:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的存储提供程序支持这些模式的一些子集。当您配置持久卷时，可以指定它将支持哪些模式。例如，NFS支持所有模式，但在示例中，只启用了这些模式：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Reclaim policy
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回收策略
- en: 'The reclaim policy determines what happens when a persistent volume claim is
    deleted. There are three different policies:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 回收策略确定持久卷索赔被删除时会发生什么。有三种不同的策略：
- en: '`Retain`: The volume will need to be reclaimed manually'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Retain`：需要手动回收卷'
- en: '`Delete`: The associated storage asset, such as AWS EBS, GCE PD, Azure disk,
    or OpenStack Cinder volume, is deleted'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Delete`：关联的存储资产，如AWS EBS、GCE PD、Azure磁盘或OpenStack Cinder卷，将被删除'
- en: '`Recycle`: Delete content only (`rm -rf /volume/*`)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Recycle`：仅删除内容（`rm -rf /volume/*`）'
- en: The `Retain` and `Delete` policies mean the persistent volume is not available
    anymore for future claims. The `recycle` policy allows the volume to be claimed
    again.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`Retain`和`Delete`策略意味着持久卷将不再对未来索赔可用。`recycle`策略允许再次索赔该卷。'
- en: Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure disk,
    and Cinder volumes support deletion. Dynamically provisioned volumes are always
    deleted.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，只有NFS和HostPath支持回收。AWS EBS、GCE PD、Azure磁盘和Cinder卷支持删除。动态配置的卷总是被删除。
- en: Storage class
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储类
- en: You can specify a storage class using the optional `storageClassName` field
    of the spec. If you do, then only persistent volume claims that specify the same
    storage class can be bound to the persistent volume. If you don't specify a storage
    class, then only persistence volume claims that don't specify a storage class
    can be bound to it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用规范的可选`storageClassName`字段指定存储类。如果这样做，那么只有指定相同存储类的持久卷要求才能绑定到持久卷。如果不指定存储类，则只有不指定存储类的持久卷要求才能绑定到它。
- en: Volume type
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷类型
- en: The volume type is specified by name in the spec. There is no `volumeType` section.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 卷类型在规范中通过名称指定。没有`volumeType`部分。
- en: 'In the preceding example, `nfs` is the volume type:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，`nfs`是卷类型：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Each volume type may have its own set of parameters. In this case, it's a `path`
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每种卷类型可能有自己的一组参数。在这种情况下，它是一个`path`
- en: and `server`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 和`server`。
- en: We will go over various volume types later in this chapter.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面讨论各种卷类型。
- en: Making persistent volume claims
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提出持久卷要求
- en: When containers want access to some persistent storage they make a claim (or
    rather, the developer and cluster administrator coordinate on necessary storage
    resources
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器需要访问某些持久存储时，它们会提出要求（或者说，开发人员和集群管理员会协调必要的存储资源
- en: 'to claim). Here is a sample claim that matches the persistent volume from the
    previous section:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要求）。以下是一个与上一节中的持久卷匹配的示例要求：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The name `storage-claim` will be important later when mounting the claim into
    a container.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 名称`storage-claim`在将要将要求挂载到容器中时将变得重要。
- en: The access mode in the spec is `ReadWriteOnce`, which means if the claim is
    satisfied no other claim with the `ReadWriteOnce` access mode can be satisfied,
    but claims for `ReadOnlyMany` can still be satisfied.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 规范中的访问模式为`ReadWriteOnce`，这意味着如果要求得到满足，则不能满足其他具有`ReadWriteOnce`访问模式的要求，但仍然可以满足`ReadOnlyMany`的要求。
- en: The resources section requests 80 GiB. This can be satisfied by our persistent
    volume, which has a capacity of 100 GiB. But this is a little bit of a waste because
    20 GiB will not be used.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 资源部分请求80 GiB。这可以通过我们的持久卷满足，它的容量为100 GiB。但这有点浪费，因为20 GiB将不会被使用。
- en: The storage class name is `"normal"`. As mentioned earlier, it must match the
    class name of the persistent volume. However, with **Persistent Volume Claim**
    (**PVC**) there is a difference between empty class name (`""`) and no class name
    at all. The former (empty class name) matches persistent volumes with no storage
    class name. The latter (no class name) will be able to bind to persistent volumes
    only if the `DefaultStorageClass` admission plugin is turned off, or if it's on
    and the default storage class is used.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 存储类名称为`"normal"`。如前所述，它必须与持久卷的类名匹配。但是，对于**持久卷要求**（**PVC**），空类名（`""`）和没有类名之间存在差异。前者（空类名）与没有存储类名的持久卷匹配。后者（没有类名）只有在关闭`DefaultStorageClass`准入插件或者打开并且使用默认存储类时才能绑定到持久卷。
- en: 'The `Selector` section allows you to filter available volumes further. For
    example, here the volume must match the label `release: "stable"` and also have
    a label with either `capacity: 80 Gi` or `capacity: 100 Gi`. Imagine that we have
    several other volumes provisioned with capacities of 200 Gi and 500 Gi. We don''t
    want to claim a 500 Gi volume when we only need 80 Gi.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`Selector`部分允许您进一步过滤可用的卷。例如，在这里，卷必须匹配标签`release: "stable"`，并且还必须具有标签`capacity:
    80 Gi`或`capacity: 100 Gi`。假设我们还有其他几个容量为200 Gi和500 Gi的卷。当我们只需要80 Gi时，我们不希望索赔500
    Gi的卷。'
- en: Kubernetes always tries to match the smallest volume that can satisfy a claim,
    but if there are no 80 Gi or 100 Gi volumes then the labels will prevent assigning
    a 200 Gi or 500 Gi volume and use dynamic provisioning instead.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes始终尝试匹配可以满足索赔的最小卷，但如果没有80 Gi或100 Gi的卷，那么标签将阻止分配200 Gi或500 Gi的卷，并使用动态配置。
- en: It's important to realize that claims don't mention volumes by name. The matching
    is done by Kubernetes based on storage class, capacity, and labels.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要意识到索赔不会按名称提及卷。匹配是由基于存储类、容量和标签的Kubernetes完成的。
- en: Finally, persistent volume claims belong to a namespace. Binding a persistent
    volume to a claim is exclusive. That means that a persistent volume will be bound
    to a namespace. Even if the access mode is `ReadOnlyMany` or `ReadWriteMany`,
    all the pods that mount the persistent volume claim must be from that claim's
    namespace.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，持久卷索赔属于命名空间。将持久卷绑定到索赔是排他的。这意味着持久卷将绑定到一个命名空间。即使访问模式是`ReadOnlyMany`或`ReadWriteMany`，所有挂载持久卷索赔的Pod必须来自该索赔的命名空间。
- en: Mounting claims as volumes
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将索赔作为卷
- en: 'OK. We have provisioned a volume and claimed it. It''s time to use the claimed
    storage in a container. This turns out to be pretty simple. First, the persistent
    volume claim must be used as a volume in the pod and then the containers in the
    pod can mount it, just like any other volume. Here is a `pod` configuration file
    that specifies the persistent volume claim we created earlier (bound to the NFS
    persistent volume we provisioned):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我们已经配置了一个卷并对其进行了索赔。现在是时候在容器中使用索赔的存储了。这其实非常简单。首先，持久卷索赔必须在Pod中用作卷，然后Pod中的容器可以像任何其他卷一样挂载它。这是一个`pod`配置文件，指定了我们之前创建的持久卷索赔（绑定到我们配置的NFS持久卷）。
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The key is in the `persistentVolumeClaim` section under `volumes`. The claim
    name (`storage-claim` here) uniquely identifies within the current namespace the
    specific claim and makes it available as a volume named `persistent-volume` here.
    Then, the container can refer to it by its name and mount it to `/mnt/data`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在`volumes`下的`persistentVolumeClaim`部分。索赔名称（这里是`storage-claim`）在当前命名空间内唯一标识特定索赔，并使其作为卷命名为`persistent-volume`。然后，容器可以通过名称引用它，并将其挂载到`/mnt/data`。
- en: Raw block volumes
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始块卷
- en: 'Kubernetes 1.9 added this capability as an alpha feature. You must enable it
    with a feature gate: `--feature-gates=BlockVolume=true`.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 1.9将此功能作为alpha功能添加。您必须使用功能门控来启用它：`--feature-gates=BlockVolume=true`。
- en: 'Raw block volumes provide direct access to the underlying storage, which is
    not mediated through a filesystem abstraction. This is very useful for applications
    that require high-storage performance, such as databases, or when consistent I/O
    performance and low latency are needed. Fiber Channel, iSCSI, and local SSD are
    all suitable for use as raw block storage. At the moment (Kubernetes 1.10), only
    the `Local Volume` and `FiberChannel` storage providers supports raw block volumes.
    Here is how to define a raw block volume:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 原始块卷提供对底层存储的直接访问，不经过文件系统抽象。这对需要高存储性能的应用程序非常有用，比如数据库，或者需要一致的I/O性能和低延迟。光纤通道、iSCSI和本地SSD都适用于用作原始块存储。目前（Kubernetes
    1.10），只有`Local Volume`和`FiberChannel`存储提供程序支持原始块卷。以下是如何定义原始块卷：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A matching PVC must specify `volumeMode: Block`, as well. Here is what it looks
    like:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '匹配的PVC必须指定`volumeMode: Block`。这是它的样子：'
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Pods consume raw block volumes as devices under `/dev` and not as mounted filesystems.
    Containers can then access this device and read/write to it. In practice this
    means that I/O requests to block storage go straight to the underlying block storage
    and don't pass though the file system drivers. This is faster, in theory, but
    in practice it can actually decrease performance if your applications benefit
    from filesystem buffering.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Pods将原始块卷作为`/dev`下的设备而不是挂载的文件系统来消耗。容器可以访问这个设备并对其进行读/写。实际上，这意味着对块存储的I/O请求直接传递到底层块存储，而不经过文件系统驱动程序。理论上这更快，但实际上如果您的应用程序受益于文件系统缓冲，它实际上可能会降低性能。
- en: 'Here is a pod with a container that binds the `block-pvc` with the raw block
    storage as a device named `/dev/xdva`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个带有容器的Pod，它将`block-pvc`与原始块存储绑定为名为`/dev/xdva`的设备：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Storage classes
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储类
- en: 'Storage classes let an administrator configure your cluster with custom persistent
    storage (as long as there is a proper plugin to support it). A storage class has
    a `name` in the `metadata`, a `provisioner`, and `parameters`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 存储类允许管理员使用自定义持久存储配置集群（只要有适当的插件支持）。存储类在`metadata`中有一个`name`，一个`provisioner`和`parameters`：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You may create multiple storage classes for the same provisioner with different
    parameters. Each provisioner has its own parameters.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为同一个提供程序创建多个存储类，每个提供程序都有自己的参数。
- en: 'The currently supported volume types are as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 目前支持的卷类型如下：
- en: '`AwsElasticBlockStore`'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AwsElasticBlockStore`'
- en: '`AzureFile`'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AzureFile`'
- en: '`AzureDisk`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AzureDisk`'
- en: '`CephFS`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CephFS`'
- en: '`Cinder`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cinder`'
- en: '`FC`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FC`'
- en: '`FlexVolume`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FlexVolume`'
- en: '`Flocker`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Flocker`'
- en: '`GcePersistentDisk`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GcePersistentDisk`'
- en: '`GlusterFS`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GlusterFS`'
- en: '`ISCSI`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ISCSI`'
- en: '`PhotonPersistentDisk`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PhotonPersistentDisk`'
- en: '`Quobyte`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Quobyte`'
- en: '`NFS`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NFS`'
- en: '`RBD`'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RBD`'
- en: '`VsphereVolume`'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VsphereVolume`'
- en: '`PortworxVolume`'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PortworxVolume`'
- en: '`ScaleIO`'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ScaleIO`'
- en: '`StorageOS`'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StorageOS`'
- en: '`Local`'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Local`'
- en: This list doesn't contain other volume types, such as `gitRepo` or `secret`,
    that are not backed by your typical network storage. This area of Kubernetes is
    still in flux and, in the future, it will be decoupled further and the design
    will be cleaner so that the plugins will not be a part of Kubernetes itself. Utilizing
    volume types intelligently is a major part of architecting and managing your cluster.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表不包含其他卷类型，比如`gitRepo`或`secret`，这些类型不是由典型的网络存储支持的。Kubernetes的这个领域仍然在变化中，将来它会进一步解耦，设计会更清晰，插件将不再是Kubernetes本身的一部分。智能地利用卷类型是架构和管理集群的重要部分。
- en: Default storage class
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 默认存储类
- en: The cluster administrator can also assign a default `storage` class. When a
    default storage class is assigned and the `DefaultStorageClass` admission plugin
    is turned on, then claims with no storage class will be dynamically provisioned
    using the default `storage` class. If the default `storage` class is not defined
    or the admission plugin is not turned on, then claims with no `storage` class
    can only match volumes with no `storage` class.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理员还可以分配一个默认的`storage`类。当分配了默认的存储类并且打开了`DefaultStorageClass`准入插件时，那么没有存储类的声明将使用默认的`storage`类进行动态配置。如果默认的`storage`类没有定义或者准入插件没有打开，那么没有存储类的声明只能匹配没有`storage`类的卷。
- en: Demonstrating persistent volume storage end to end
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演示持久卷存储的端到端
- en: To illustrate all the concepts, let's do a mini-demonstration where we create
    a HostPath volume, claim it, mount it, and have containers write to it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明所有的概念，让我们进行一个小型演示，创建一个HostPath卷，声明它，挂载它，并让容器写入它。
- en: 'Let''s start by creating a `hostPath` volume. Save the following in `persistent-volume.yaml`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个`hostPath`卷。将以下内容保存在`persistent-volume.yaml`中：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To check out the available volumes, you can use the `persistentvolumes` resource
    type, or `pv` for short:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看可用的卷，可以使用`persistentvolumes`资源类型，或者简称为`pv`：
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'I edited the output a little bit so that it''s easier to see. The capacity
    is 1 GiB as requested. The reclaim policy is `Retain` because `HostPath` volumes
    are retained. The status is `Available` because the volume has not been claimed
    yet. The access mode is specified as `RWX`, which means `ReadWriteMany`. All access
    modes have a shorthand version:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我稍微编辑了一下输出，以便更容易看到。容量为1 GiB，符合要求。回收策略是`Retain`，因为`HostPath`卷是保留的。状态为`Available`，因为卷尚未被声明。访问模式被指定为`RWX`，表示`ReadWriteMany`。所有访问模式都有一个简写版本：
- en: '`RWO` : `ReadWriteOnce`'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RWO`：`ReadWriteOnce`'
- en: '`ROX`: `ReadOnlyMany`'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ROX`：`ReadOnlyMany`'
- en: '`RWX` : `ReadWriteMany`'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RWX`：`ReadWriteMany`'
- en: 'We have a persistent volume. Let''s create a claim. Save the following to `persistent-volume-claim.yaml`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个持久卷。让我们创建一个声明。将以下内容保存到`persistent-volume-claim.yaml`中：
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, run the following command:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行以下命令：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s check the `claim` and the `volume`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下`claim`和`volume`：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As you can see, the `claim` and the `volume` are bound to each other. The final
    step is to create a `pod` and assign the `claim` as a `volume`. Save the following
    to `shell-pod.yaml`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`claim`和`volume`已经绑定在一起。最后一步是创建一个`pod`并将`claim`分配为`volume`。将以下内容保存到`shell-pod.yaml`中：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This pod has two containers that use the Ubuntu image and both run a `shell`
    command that just sleeps in an infinite loop. The idea is that the containers
    will keep running, so we can connect to them later and check their filesystems.
    The pod mounts our persistent volume claim with a volume name of `pv`. Both containers
    mount it into their `/data` directory.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个pod有两个容器，它们使用Ubuntu镜像，并且都运行一个`shell`命令，只是在无限循环中睡眠。这样做的目的是让容器保持运行，这样我们以后可以连接到它们并检查它们的文件系统。该pod将我们的持久卷声明挂载为`pv`的卷名。两个容器都将其挂载到它们的`/data`目录中。
- en: 'Let''s create the `pod` and verify that both containers are running:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建`pod`并验证两个容器都在运行：
- en: '[PRE21]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, `ssh` to the node. This is the host whose `/tmp/data` is the pod''s volume
    that mounted as `/data` into each of the running containers:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`ssh`到节点。这是主机，其`/tmp/data`是pod的卷，挂载为每个正在运行的容器的`/data`：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Inside the node, we can communicate with the containers using Docker commands.
    Let''s look at the last two running containers:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点内部，我们可以使用Docker命令与容器进行通信。让我们看一下最后两个正在运行的容器：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, let''s create a file in the `/tmp/data` directory on the host. It should
    be visible by both containers through the mounted volume:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在主机的`/tmp/data`目录中创建一个文件。它应该通过挂载的卷对两个容器都可见：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s execute a `shell` on one of the containers, verify that the file `1.txt`
    is indeed visible, and create another file, `2.txt`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在其中一个容器上执行一个`shell`，验证文件`1.txt`确实可见，并创建另一个文件`2.txt`：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Public storage volume types – GCE, AWS, and Azure
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公共存储卷类型 - GCE，AWS和Azure
- en: In this section, we'll look at some of the common volume types available in
    the leading public cloud platforms. Managing storage at scale is a difficult task
    that eventually involves physical resources, similar to nodes. If you choose to
    run your Kubernetes cluster on a public cloud platform, you can let your cloud
    provider deal with all these challenges and focus on your system. But it's important
    to understand the various options, constraints, and limitations of each volume
    type.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些主要公共云平台中可用的常见卷类型。在规模上管理存储是一项困难的任务，最终涉及物理资源，类似于节点。如果您选择在公共云平台上运行您的Kubernetes集群，您可以让您的云提供商处理所有这些挑战，并专注于您的系统。但重要的是要了解每种卷类型的各种选项、约束和限制。
- en: AWS Elastic Block Store (EBS)
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS弹性块存储（EBS）
- en: 'AWS provides EBS as persistent storage for EC2 instances. An AWS Kubernetes
    cluster can use AWS EBS as persistent storage with the following limitations:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: AWS为EC2实例提供EBS作为持久存储。AWS Kubernetes集群可以使用AWS EBS作为持久存储，但有以下限制：
- en: The pods must run on AWS EC2 instances as nodes
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pod必须在AWS EC2实例上作为节点运行
- en: Pods can only access EBS volumes provisioned in their availability zone
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod只能访问其可用区中配置的EBS卷
- en: An EBS volume can be mounted on a single EC2 instance
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EBS卷可以挂载到单个EC2实例
- en: These are severe limitations. The restriction for a single availability zone,
    while great for performance, eliminates the ability to share storage at scale
    or across a geographically distributed system without custom replication and synchronization.
    The limit of a single EBS volume to a single EC2 instance means that even within
    the same availability zone, pods can't share storage (even for reading) unless
    you make sure they run on the same node.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是严重的限制。单个可用区的限制，虽然对性能有很大帮助，但消除了在规模或地理分布系统中共享存储的能力，除非进行自定义复制和同步。单个EBS卷限制为单个EC2实例意味着即使在同一可用区内，pod也无法共享存储（甚至是读取），除非您确保它们在同一节点上运行。
- en: 'With all the disclaimers out of the way, let''s see how to mount an EBS volume:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释所有免责声明之后，让我们看看如何挂载EBS卷：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You must create the EBS volume in AWS and then you just mount it into the pod.
    There is no need for a claim or storage class because you mount the volume directly
    by ID. The `awsElasticBlockStore` volume type is known to Kubernetes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须在AWS中创建EBS卷，然后将其挂载到pod中。不需要声明或存储类，因为您通过ID直接挂载卷。`awsElasticBlockStore`卷类型为Kubernetes所知。
- en: AWS Elastic File System
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS弹性文件系统
- en: 'AWS recently released a new service called the **Elastic File System** (**EFS**).
    This is really a managed NFS service. It''s using NFS 4.1 protocol and it has
    many benefits over EBS:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: AWS最近推出了一项名为**弹性文件系统**（**EFS**）的新服务。这实际上是一个托管的NFS服务。它使用NFS 4.1协议，并且与EBS相比有许多优点：
- en: Multiple EC2 instances can access the same files across multiple availability
    zones (but within the same region)
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个EC2实例可以跨多个可用区（但在同一区域内）访问相同的文件
- en: Capacity is automatically scaled up and down based on actual usage
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容量根据实际使用情况自动扩展和缩减
- en: You pay only for what you use
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您只支付您使用的部分
- en: You can connect on-premise servers to EFS over VPN
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过VPN将本地服务器连接到EFS
- en: EFS runs off SSD drives that are automatically replicated across availability
    zones
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EFS运行在自动在可用区之间复制的SSD驱动器上
- en: 'That said, EFS is more expansive than EBS even when you consider the automatic
    replication to multiple availability zones (assuming you fully utilize your EBS
    volumes). It is using an external provisioner and it is not trivial to deploy.
    Follow the instructions here:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，即使考虑到自动复制到多个可用区（假设您充分利用了EBS卷），EFS比EBS更加广泛。它正在使用外部供应商，部署起来并不是微不足道的。请按照这里的说明进行操作：
- en: '[https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs](https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs](https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs)'
- en: 'Once everything is set up and you have defined your storage class and the persistent
    volume exists, you can create a claim and mount it into as many pods as you like
    in `ReadWriteMany` mode. Here is the persistent claim:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦一切都设置好了，并且您已经定义了存储类，并且持久卷存在，您可以创建一个声明，并将其以`ReadWriteMany`模式挂载到尽可能多的`pod`中。这是持久声明：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is a pod that consumes it:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用它的`pod`：
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: GCE persistent disk
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GCE持久磁盘
- en: The `gcePersistentDisk` volume type is very similar to `awsElasticBlockStore`.
    You must provision the disk ahead of time. It can only be used by GCE instances
    in the same project and zone. But the same volume can be used as read-only on
    multiple instances. This means it supports `ReadWriteOnce` and `ReadOnlyMany`.
    You can use a GCE persistent disk to share data as read-only between multiple
    pods in the same zone.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`gcePersistentDisk`卷类型与`awsElasticBlockStore`非常相似。您必须提前规划磁盘。它只能被同一项目和区域中的GCE实例使用。但是同一卷可以在多个实例上以只读方式使用。这意味着它支持`ReadWriteOnce`和`ReadOnlyMany`。您可以使用GCE持久磁盘在同一区域的多个`pod`之间共享数据。'
- en: 'The pod that''s using a persistent disk in `ReadWriteOnce` mode must be controlled
    by a replication controller, a replica set, or a deployment with a replica count
    of `0` or `1`. Trying to scale beyond `1` will fail for obvious reasons:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`ReadWriteOnce`模式中的持久磁盘的`pod`必须由复制控制器、副本集或具有`0`或`1`个副本计数的部署控制。尝试扩展到`1`之外的数量将因明显原因而失败：
- en: '[PRE29]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Azure data disk
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure数据磁盘
- en: 'The Azure data disk is a virtual hard disk stored in Azure storage. It''s similar
    in capabilities to AWS EBS. Here is a sample `pod` configuration file:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Azure数据磁盘是存储在Azure存储中的虚拟硬盘。它的功能类似于AWS EBS。这是一个示例`pod`配置文件：
- en: '[PRE30]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In addition to the mandatory `diskName` and `diskURI` parameters, it also has
    a few optional parameters:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 除了强制的`diskName`和`diskURI`参数之外，它还有一些可选参数：
- en: '`cachingMode`: The disk caching mode. This must be one of `None`, `ReadOnly`,
    or `ReadWrite`. The default is `None`.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cachingMode`：磁盘缓存模式。必须是`None`、`ReadOnly`或`ReadWrite`之一。默认值为`None`。'
- en: '`fsType`: The filesystem type is set to `mount`. The default is `ext4`.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsType`：文件系统类型设置为`mount`。默认值为`ext4`。'
- en: '`readOnly`: Whether the filesystem is used as `readOnly`. The default is `false`.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readOnly`：文件系统是否以`readOnly`模式使用。默认值为`false`。'
- en: Azure data disks are limited to 1,023 GB. Each Azure VM can have up to 16 data
    disks. You can attach an Azure data disk to a single Azure VM.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Azure数据磁盘的限制为1,023 GB。每个Azure VM最多可以有16个数据磁盘。您可以将Azure数据磁盘附加到单个Azure VM上。
- en: Azure file storage
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure文件存储
- en: In addition to the data disk, Azure has also a shared filesystem similar to
    AWS EFS. However, Azure file storage uses the SMB/CIFS protocol (it supports SMB
    2.1 and SMB 3.0). It is based on the Azure storage platform and has the same availability,
    durability, scalability, and geo-redundancy capabilities as Azure Blob, Table,
    or Queue.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据磁盘，Azure还有一个类似于AWS EFS的共享文件系统。但是，Azure文件存储使用SMB/CIFS协议（支持SMB 2.1和SMB 3.0）。它基于Azure存储平台，具有与Azure
    Blob、Table或Queue相同的可用性、耐用性、可扩展性和地理冗余能力。
- en: 'In order to use Azure file storage, you need to install the `cifs-utils` package
    on each client VM. You also need to create a `secret`, which is a required parameter:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用Azure文件存储，您需要在每个客户端VM上安装`cifs-utils`软件包。您还需要创建一个`secret`，这是一个必需的参数：
- en: '[PRE31]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here is a configuration file for Azure file storage:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个Azure文件存储的配置文件：
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Azure file storage supports sharing within the same region as well as connecting
    on-premise clients. Here is a diagram that illustrates the workflow:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Azure文件存储支持在同一地区内共享以及连接本地客户端。以下是说明工作流程的图表：
- en: GlusterFS and Ceph volumes in Kubernetes
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的GlusterFS和Ceph卷
- en: 'GlusterFS and Ceph are two distributed persistent storage systems. GlusterFS
    is, at its core, a network filesystem. Ceph is, at the core, an object store.
    Both expose block, object, and filesystem interfaces. Both use the `xfs` filesystem
    under the covers to store data and metadata as `xattr` attributes. There are several
    reasons why you may want to use GlusterFS or Ceph as persistent volumes in your
    Kubernetes cluster:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS和Ceph是两个分布式持久存储系统。GlusterFS在其核心是一个网络文件系统。Ceph在核心是一个对象存储。两者都公开块、对象和文件系统接口。两者都在底层使用`xfs`文件系统来存储数据和元数据作为`xattr`属性。您可能希望在Kubernetes集群中使用GlusterFS或Ceph作为持久卷的几个原因：
- en: You may have a lot of data and applications that access the data in GlusterFS
    or Ceph
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能有很多数据和应用程序访问GlusterFS或Ceph中的数据
- en: You have administrative and operational expertise managing GlusterFS
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您具有管理和操作GlusterFS的专业知识
- en: or Ceph
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 或Ceph
- en: You run in the cloud, but the limitations of the cloud platform persistent storage
    are a non-starter
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您在云中运行，但云平台持久存储的限制是一个非起点。
- en: Using GlusterFS
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GlusterFS
- en: 'GlusterFS is intentionally simple, exposing the underlying directories as they
    are and leaving it to clients (or middleware) to handle high availability, replication,
    and distribution. GlusterFS organizes data into logical volumes, which encompass
    multiple nodes (machines) that contain bricks, which store files. Files are allocated
    to bricks according to DHT (distributed hash table). If files are renamed or the
    GlusterFS cluster is expanded or rebalanced, files may be moved between bricks.
    The following diagram shows the GlusterFS building blocks:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS故意简单，将底层目录公开，并留给客户端（或中间件）处理高可用性、复制和分发。GlusterFS将数据组织成逻辑卷，其中包括包含文件的多个节点（机器）的砖块。文件根据DHT（分布式哈希表）分配给砖块。如果文件被重命名或GlusterFS集群被扩展或重新平衡，文件可能会在砖块之间移动。以下图表显示了GlusterFS的构建模块：
- en: '![](Images/2cce08b3-be8d-480f-a250-4fa10b656553.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/2cce08b3-be8d-480f-a250-4fa10b656553.png)'
- en: To use a GlusterFS cluster as persistent storage for Kubernetes (assuming you
    have an up-and-running GlusterFS cluster), you need to follow several steps. In
    particular, the GlusterFS nodes are managed by the plugin as a Kubernetes service
    (although, as an application developer, it doesn't concern you).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 要将GlusterFS集群用作Kubernetes的持久存储（假设您已经运行了GlusterFS集群），您需要遵循几个步骤。特别是，GlusterFS节点由插件作为Kubernetes服务进行管理（尽管作为应用程序开发人员，这与您无关）。
- en: Creating endpoints
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建端点
- en: 'Here is an example of an endpoints resource that you can create as a normal
    Kubernetes resource using `kubectl create`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个端点资源的示例，您可以使用`kubectl create`创建为普通的Kubernetes资源：
- en: '[PRE33]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Adding a GlusterFS Kubernetes service
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加GlusterFS Kubernetes服务
- en: 'To make the endpoints persistent, you use a Kubernetes service with no selector
    to indicate the endpoints are managed manually:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使端点持久，您可以使用一个没有选择器的Kubernetes服务来指示端点是手动管理的：
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Creating pods
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建Pods
- en: 'Finally, in the pod spec''s `volumes` section, provide the following information:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在pod规范的`volumes`部分中，提供以下信息：
- en: '[PRE35]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The containers can then mount `glusterfsvol` by name.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后容器可以按名称挂载`glusterfsvol`。
- en: The `endpoints` tell the GlusterFS volume plugin how to find the storage nodes
    of the GlusterFS cluster.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`endpoints`告诉GlusterFS卷插件如何找到GlusterFS集群的存储节点。'
- en: Using Ceph
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ceph
- en: 'Ceph''s object store can be accessed using multiple interfaces. Kubernetes
    supports the **RBD** (block) and **CEPHFS** (filesystem) interfaces. The following
    diagram shows how RADOS – the underlying object store – can be accessed in multiple
    days. Unlike GlusterFS, Ceph does a lot of work automatically. It does distribution,
    replication, and self-healing all on its own:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph的对象存储可以使用多个接口访问。Kubernetes支持**RBD**（块）和**CEPHFS**（文件系统）接口。以下图表显示了RADOS -
    底层对象存储 - 如何在多天内访问。与GlusterFS不同，Ceph会自动完成大量工作。它自行进行分发、复制和自我修复：
- en: '![](Images/04ef7c1d-6584-455b-b84a-8d5cd92a0375.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04ef7c1d-6584-455b-b84a-8d5cd92a0375.png)'
- en: Connecting to Ceph using RBD
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RBD连接到Ceph
- en: 'Kubernetes supports Ceph through the **Rados****Block****Device** (**RBD**)
    interface. You must install `ceph-common` on each node in the Kubernetes cluster.
    Once you have your Ceph cluster up and running, you need to provide some information
    required by the Ceph RBD volume plugin in the `pod` configuration file:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes通过**Rados****Block****Device**（**RBD**）接口支持Ceph。您必须在Kubernetes集群中的每个节点上安装`ceph-common`。一旦您的Ceph集群正常运行，您需要在`pod`配置文件中提供Ceph
    RBD卷插件所需的一些信息：
- en: '`monitors`: Ceph monitors.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`monitors`：Ceph监视器。'
- en: '`pool`: The name of the RADOS pool. If one is not provided, the default RBD
    pool is used.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool`：RADOS池的名称。如果未提供，则使用默认的RBD池。'
- en: '`image`: The image name that RBD has created.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`：RBD创建的镜像名称。'
- en: '`user`: The RADOS username. If one is not provided, the default `admin` is
    used.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user`：RADOS用户名。如果未提供，则使用默认的`admin`。'
- en: '`keyring`: The path to the `keyring` file. If one is not provided, the default
    `/etc/ceph/keyring` is used.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keyring`：`keyring`文件的路径。如果未提供，则使用默认的`/etc/ceph/keyring`。'
- en: '`*` `secretName`: The name of the authentication secrets. If one is provided,
    `secretName` overrides `keyring`. Note: see the following paragraph about how
    to create a `secret`.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*` `secretName`：认证密钥的名称。如果提供了一个，则`secretName`会覆盖`keyring`。注意：请参阅下一段关于如何创建`secret`的内容。'
- en: '`fsType`: The filesystem type (`ext4`, `xfs`, and so on) that is formatted
    on'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsType`：在其上格式化的文件系统类型（`ext4`、`xfs`等）。'
- en: the device.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 设备。
- en: '`readOnly`: Whether the filesystem is used as `readOnly`.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readOnly`：文件系统是否以`readOnly`方式使用。'
- en: 'If the Ceph authentication `secret` is used, you need to create a `secret`
    object:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用了Ceph认证`secret`，则需要创建一个`secret`对象：
- en: '[PRE36]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `secret` type is `kubernetes.io/rbd`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`secret`类型为`kubernetes.io/rbd`。'
- en: 'The pod spec''s `volumes` section looks the same as this:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: pod规范的`volumes`部分看起来与此相同：
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Ceph RBD supports `ReadWriteOnce` and `ReadOnlyMany` access modes.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph RBD支持`ReadWriteOnce`和`ReadOnlyMany`访问模式。
- en: Connecting to Ceph using CephFS
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CephFS连接到Ceph
- en: If your Ceph cluster is already configured with CephFS, then you can assign
    it very easily to pods. Also, CephFS supports `ReadWriteMany` access modes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的Ceph集群已经配置了CephFS，则可以非常轻松地将其分配给pod。此外，CephFS支持`ReadWriteMany`访问模式。
- en: 'The configuration is similar to Ceph RBD, except you don''t have a pool, image,
    or filesystem type. The secret can be a reference to a Kubernetes `secret` object
    (preferred) or a `secret` file:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 配置类似于Ceph RBD，只是没有池、镜像或文件系统类型。密钥可以是对Kubernetes `secret`对象的引用（首选）或`secret`文件：
- en: '[PRE38]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You can also provide a path as a parameter in the `cephfs` system. The default
    is `/`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在`cephfs`系统中提供路径作为参数。默认为`/`。
- en: The in-tree RBD provisioner has an out-of-tree copy in the external-storage
    Kubernetes incubator project.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的RBD供应程序在外部存储Kubernetes孵化器项目中有一个独立的副本。
- en: Flocker as a clustered container data volume manager
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Flocker作为集群容器数据卷管理器
- en: So far, we have discussed storage solutions that store data outside the Kubernetes
    cluster (except for `emptyDir` and HostPath, which are not persistent). Flocker
    is a little different. It is Docker-aware. It was designed to let Docker data
    volumes transfer with their container when the container is moved between nodes.
    You may want to use the Flocker volume plugin if you're migrating a Docker-based
    system that use a different orchestration platform, such as Docker compose or
    Mesos, to Kubernetes, and you use Flocker for orchestrating storage. Personally,
    I feel that there is a lot of duplication between what Flocker does and what Kubernetes
    does to abstract storage.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了将数据存储在Kubernetes集群之外的存储解决方案（除了`emptyDir`和HostPath，它们不是持久的）。Flocker有点不同。它是Docker感知的。它旨在让Docker数据卷在容器在节点之间移动时一起传输。如果你正在将基于Docker的系统从不同的编排平台（如Docker
    compose或Mesos）迁移到Kubernetes，并且你使用Flocker来编排存储，你可能想使用Flocker卷插件。就个人而言，我觉得Flocker所做的事情和Kubernetes为抽象存储所做的事情之间存在很多重复。
- en: Flocker has a control service and agents on each node. Its architecture is very
    similar to Kubernetes with its API server and Kubelet running on each node. The
    Flocker control service exposes a REST API and manages the configuration of the
    state across the cluster. The agents are responsible for ensuring that the state
    of their node matches the current configuration. For example, if a dataset needs
    to be on node X, then the Flocker agent on node X will create it.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Flocker有一个控制服务和每个节点上的代理。它的架构与Kubernetes非常相似，其API服务器和每个节点上运行的Kubelet。Flocker控制服务公开了一个REST
    API，并管理着整个集群的状态配置。代理负责确保其节点的状态与当前配置匹配。例如，如果一个数据集需要在节点X上，那么节点X上的Flocker代理将创建它。
- en: 'The following diagram showcases the Flocker architecture:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了Flocker架构：
- en: '![](Images/795d9283-b014-4464-a7b0-4732d4d83736.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/795d9283-b014-4464-a7b0-4732d4d83736.png)'
- en: In order to use Flocker as persistent volumes in Kubernetes, you first must
    have a properly configured Flocker cluster. Flocker can work with many backing
    stores (again, very similar to Kubernetes persistent volumes).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Kubernetes中使用Flocker作为持久卷，你首先必须有一个正确配置的Flocker集群。Flocker可以与许多后备存储一起工作（再次，与Kubernetes持久卷非常相似）。
- en: 'Then you need to create Flocker datasets and at that point you''re ready to
    hook it up as a persistent volume. After all your hard work, this part is easy
    and you just need to specify the Flocker dataset''s name:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你需要创建Flocker数据集，这时你就可以将其连接为持久卷了。经过你的辛勤工作，这部分很容易，你只需要指定Flocker数据集的名称：
- en: '[PRE39]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Integrating enterprise storage into Kubernetes
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将企业存储集成到Kubernetes中
- en: 'If you have an existing **Storage ****Area ****Network** (**SAN**) exposed
    over the iSCSI interface, Kubernetes has a volume plugin for you. It follows the
    same model as other shared persistent storage plugins we''ve seen earlier. You
    must configure the iSCSI initiator, but you don''t have to provide any initiator
    information. All you need to provide is the following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个通过iSCSI接口公开的现有**存储区域网络**（**SAN**），Kubernetes为你提供了一个卷插件。它遵循了我们之前看到的其他共享持久存储插件的相同模型。你必须配置iSCSI启动器，但你不必提供任何启动器信息。你只需要提供以下内容：
- en: IP address of the iSCSI target and port (if not the default `3260`)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI目标的IP地址和端口（如果不是默认的`3260`）
- en: The target's `iqn` (iSCSI qualified name)—typically a reversed domain name
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标的`iqn`（iSCSI合格名称）—通常是反向域名
- en: '**LUN**—logical unit number'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LUN**—逻辑单元号'
- en: Filesystem type
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件系统类型
- en: '`readonly` Boolean flag'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readonly`布尔标志'
- en: 'The iSCSI plugin supports `ReadWriteOnce` and `ReadonlyMany`. Note that you
    can''t partition your device at this time. Here is the volume spec:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: iSCSI插件支持`ReadWriteOnce`和`ReadonlyMany`。请注意，目前无法对设备进行分区。以下是卷规范：
- en: '[PRE40]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Projecting volumes
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投影卷
- en: 'It''s possible to project multiple volumes into a single directory so that
    they appear as a single volume. The supported volume types are: `secret`, `downwardAPI`,
    and `configMap`. This is useful if you want to mount multiple sources of configuration
    into a pod. Instead of having to create a separate volume for each source, you
    can bundle all of them into a single projected volume. Here is an example:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将多个卷投影到单个目录中，使其显示为单个卷。支持的卷类型有：`secret`，`downwardAPI`和`configMap`。如果您想将多个配置源挂载到一个pod中，这将非常有用。您可以将它们全部捆绑到一个投影卷中，而不必为每个源创建单独的卷。这是一个例子：
- en: '[PRE41]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Using out-of-tree volume plugins with FlexVolume
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用FlexVolume的外部卷插件
- en: 'FlexVolume became generally available in Kubernetes 1.8\. It allows you to
    consume out-of-tree storage through a uniform API. Storage providers write a driver
    that you install on all nodes. The FlexVolume plugin can dynamically discover
    existing drivers. Here is an example of using FlexVolume to bind to an external
    NFS volume:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: FlexVolume在Kubernetes 1.8中已经普遍可用。它允许您通过统一API消耗外部存储。存储提供商编写一个驱动程序，您可以在所有节点上安装。FlexVolume插件可以动态发现现有的驱动程序。以下是使用FlexVolume绑定到外部NFS卷的示例：
- en: '[PRE42]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The Container Storage Interface
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器存储接口
- en: 'The **Container Storage Interface** (**CSI**) is an initiative to standardize
    the interaction between container orchestrators and storage providers. It is driven
    by Kubernetes, Docker, Mesos, and Cloud Foundry. The idea is that storage providers
    will need to implement just one plugin and container orchestrators will only need
    to support CSI. It is the equivalent of CNI for storage. There are several advantages
    over FlexVolume:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器存储接口**（**CSI**）是标准化容器编排器和存储提供商之间交互的一个倡议。它由Kubernetes、Docker、Mesos和Cloud
    Foundry推动。其想法是存储提供商只需要实现一个插件，容器编排器只需要支持CSI。这相当于存储的CNI。与FlexVolume相比，有几个优点：'
- en: CSI is an industry-wide standard
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSI是一个行业标准
- en: FlexVolume plugins require access to the node and master root filesystem to
    deploy derivers
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FlexVolume插件需要访问节点和主节点根文件系统来部署驱动程序
- en: FlexVolume storage drivers often require many external dependencies
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FlexVolume存储驱动程序通常需要许多外部依赖项
- en: FlexVolume's EXEC style interface is clunky
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FlexVolume的EXEC风格接口很笨拙
- en: A CSI volume plugin was added in Kubernetes 1.9 as an alpha feature and already
    moved to beta status in Kubernetes 1.10\. FlexVolume will remain for backwards
    compatibility, at least for a while. But as CSI gathers momentum and more storage
    providers implement CSI volume drivers, I can definitely see Kubernetes providing
    only in-tree CSI volume plugins and communicating with any storage provider though
    a CSI driver.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 1.9中添加了一个CSI卷插件作为alpha功能，并在Kubernetes 1.10中已经升级为beta状态。FlexVolume将保持向后兼容，至少一段时间。但随着CSI的发展和更多存储提供商实现CSI卷驱动程序，我确实可以看到Kubernetes只提供内部CSI卷插件，并通过CSI驱动程序与任何存储提供商进行通信。
- en: 'Here is a diagram that demonstrates how CSI works within Kubernetes:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个演示CSI在Kubernetes中如何工作的图表：
- en: '![](Images/a2dc5686-d962-4436-b8f5-b89c4d503f95.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/a2dc5686-d962-4436-b8f5-b89c4d503f95.png)'
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we took a deep look into storage in Kubernetes. We've looked
    at the generic conceptual model based on volumes, claims, and storage classes,
    as well as the implementation of volume plugins. Kubernetes eventually maps all
    storage systems into mounted filesystems in containers or raw block storage. This
    straightforward model allows administrators to configure and hook up any storage
    system from local `host` directories through cloud-based shared storage all the
    way to enterprise storage systems. The transition of storage provisioners from
    in-tree to out-of-tree bodes well for the storage ecosystem. You should now have
    a clear understanding of how storage is modeled and implemented in Kubernetes
    and be able to make intelligent choices of how to implement storage in your Kubernetes
    cluster.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入研究了Kubernetes中的存储。我们看了基于卷、声明和存储类的通用概念模型，以及卷插件的实现。Kubernetes最终将所有存储系统映射到容器中的挂载文件系统或原始块存储中。这种直接的模型允许管理员配置和连接任何存储系统，从本地的`host`目录到基于云的共享存储，再到企业存储系统。存储供应商从内部到外部的过渡对存储生态系统是一个好兆头。现在，您应该清楚地了解了存储在Kubernetes中的建模和实现，并能够在您的Kubernetes集群中做出明智的存储实现选择。
- en: In [Chapter 8](54092869-8ccc-42a9-bd39-b6f0616f2bd0.xhtml), *Running Stateful
    Applications with Kubernetes*, we'll see how Kubernetes can raise the level of
    abstraction and, on top of storage, help in developing, deploying, and operating
    stateful applications using concepts such as StatefulSets.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](54092869-8ccc-42a9-bd39-b6f0616f2bd0.xhtml)中，《使用Kubernetes运行有状态应用程序》，我们将看到Kubernetes如何提高抽象级别，并在存储之上，利用StatefulSets等概念来开发、部署和操作有状态的应用程序。
