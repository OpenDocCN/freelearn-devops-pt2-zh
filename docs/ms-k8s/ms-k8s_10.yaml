- en: Advanced Kubernetes Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级Kubernetes网络
- en: 'In this chapter, we will examine the important topic of networking. Kubernetes,
    as an orchestration platform, manages containers/pods running on different machines
    (physical or virtual) and requires an explicit networking model. We will look
    at the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究网络这一重要主题。作为一个编排平台，Kubernetes管理在不同机器（物理或虚拟）上运行的容器/Pod，并需要一个明确的网络模型。我们将讨论以下主题：
- en: The Kubernetes networking model
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes网络模型
- en: Standard interfaces that Kubernetes supports, such as EXEC, Kubenet, and, in
    particular, CNI
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes支持的标准接口，如EXEC、Kubenet，特别是CNI
- en: Various networking solutions that satisfy the requirements of Kubernetes networking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 满足Kubernetes网络要求的各种网络解决方案
- en: Network policies and load balancing options
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络策略和负载均衡选项
- en: Writing a custom CNI plugin
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写自定义CNI插件
- en: At the end of this chapter, you will understand the Kubernetes approach to networking
    and be familiar with the solution space for aspects such as standard interfaces,
    networking implementations, and load balancing. You will even be able to write
    your very own CNI plugin if you wish.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将了解Kubernetes对网络的处理方式，并熟悉标准接口、网络实现和负载均衡等方面的解决方案空间。甚至可以自己编写自己的CNI插件。
- en: Understanding the Kubernetes networking model
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Kubernetes网络模型
- en: The Kubernetes networking model is based on a flat address space. All pods in
    a cluster can directly see each other. Each pod has its own IP address. There
    is no need to configure any NAT. In addition, containers in the same pod share
    their pod's IP address and can communicate with each other through localhost.
    This model is pretty opinionated, but, once set up, it simplifies life considerably
    both for developers and administrators. It makes it particularly easy to migrate
    traditional network applications to Kubernetes. A pod represents a traditional
    node and each container represents a traditional process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes网络模型基于一个扁平的地址空间。集群中的所有Pod都可以直接相互通信。每个Pod都有自己的IP地址。无需配置任何NAT。此外，同一Pod中的容器共享其Pod的IP地址，并且可以通过localhost相互通信。这个模型非常有见地，一旦设置好，就会极大地简化开发人员和管理员的生活。它特别容易将传统网络应用迁移到Kubernetes。一个Pod代表一个传统节点，每个容器代表一个传统进程。
- en: Intra-pod communication (container to container)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod内通信（容器到容器）
- en: A running pod is always scheduled on one (physical or virtual) node. That means
    that all the containers run on the same node and can talk to each other in various
    ways, such as the local filesystem, any IPC mechanism, or using localhost and
    well-known ports. There is no danger of port collision between different pods
    because each pod has its own IP address, and when a container in the pod uses
    localhost, it applies to the pod's IP address only. So, if container 1 in pod
    1 connects to port `1234`, which container 2 listens to on pod 1, it will not
    conflict with another container in pod 2 running on the same node that also listens
    on port `1234`. The only caveat is that if you're exposing ports to the host then
    you should be careful about pod-to-node affinity. This can be handled using several
    mechanisms, such as DaemonSet and pod anti-affinity.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 运行中的Pod始终被调度到一个（物理或虚拟）节点上。这意味着所有的容器都在同一个节点上运行，并且可以以各种方式相互通信，比如本地文件系统、任何IPC机制，或者使用localhost和众所周知的端口。不同的Pod之间不会发生端口冲突，因为每个Pod都有自己的IP地址，当Pod中的容器使用localhost时，它只适用于Pod的IP地址。因此，如果Pod
    1中的容器1连接到Pod 1上的端口`1234`，而Pod 1上的容器2监听该端口，它不会与同一节点上运行的Pod 2中的另一个容器监听的端口`1234`发生冲突。唯一需要注意的是，如果要将端口暴露给主机，那么应该注意Pod到节点的亲和性。这可以通过多种机制来处理，比如DaemonSet和Pod反亲和性。
- en: Inter-pod communication (pod to pod)
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod间通信（Pod到Pod）
- en: Pods in Kubernetes are allocated a network-visible IP address (not private to
    the node). Pods can communicate directly without the aid of network address translation,
    tunnels, proxies, or any other obfuscating layer. Well-known port numbers can
    be used for a configuration-free communication scheme. The pod's internal IP address
    is the same as its external IP address that other pods see (within the cluster
    network; not exposed to the outside world). This means that standard naming and
    discovery mechanisms such as DNS work out of the box.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，Pod被分配了一个网络可见的IP地址（不是私有的节点）。Pod可以直接通信，无需网络地址转换、隧道、代理或任何其他混淆层的帮助。可以使用众所周知的端口号来进行无需配置的通信方案。Pod的内部IP地址与其他Pod看到的外部IP地址相同（在集群网络内；不暴露给外部世界）。这意味着标准的命名和发现机制，如DNS，可以直接使用。
- en: Pod-to-service communication
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod与服务之间的通信
- en: 'Pods can talk to each other directly using their IP addresses and well-known
    ports, but that requires the pods to know each other''s IP addresses. In a Kubernetes
    cluster, pods can be destroyed and created constantly. The service provides a
    layer of indirection that is very useful because the service is stable even if
    the set of actual pods that respond to requests is ever-changing. In addition,
    you get automatic, highly-available load balancing because the Kube-proxy on each
    node takes care of redirecting traffic to the correct pod:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Pod可以使用它们的IP地址和众所周知的端口直接相互通信，但这需要Pod知道彼此的IP地址。在Kubernetes集群中，Pod可能会不断被销毁和创建。服务提供了一个非常有用的间接层，因为即使实际响应请求的Pod集合不断变化，服务也是稳定的。此外，您会获得自动的高可用负载均衡，因为每个节点上的Kube-proxy负责将流量重定向到正确的Pod：
- en: '![](Images/640c8105-160e-4829-80a6-2c1381eddd02.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/640c8105-160e-4829-80a6-2c1381eddd02.png)'
- en: External access
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部访问
- en: Eventually, some containers need to be accessible from the outside world. The
    pod IP addresses are not visible externally. The service is the right vehicle,
    but external access typically requires two redirects. For example, cloud provider
    load balancers are Kubernetes-aware, so they can't direct traffic to a particular
    service directly to a node that runs a pod that can process the request. Instead,
    the public load balancer just directs traffic to any node in the cluster and the
    Kube-proxy on that node will redirect again to an appropriate pod if the current
    node doesn't run the necessary pod.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，一些容器需要从外部世界访问。Pod IP地址在外部不可见。服务是正确的载体，但外部访问通常需要两次重定向。例如，云服务提供商负载均衡器是Kubernetes感知的，因此它不能直接将流量定向到运行可以处理请求的Pod的节点。相反，公共负载均衡器只是将流量定向到集群中的任何节点，该节点上的Kube-proxy将再次重定向到适当的Pod，如果当前节点不运行必要的Pod。
- en: 'The following diagram shows how all that the external load balancer on the
    right side does is send traffic to all nodes that reach the proxy, which takes
    care of further routing, if it''s needed:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了右侧的外部负载均衡器所做的一切只是将流量发送到达到代理的所有节点，代理负责进一步路由，如果需要的话。
- en: '![](Images/44ed300f-4643-4e4c-93c5-db61ee64e1c7.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/44ed300f-4643-4e4c-93c5-db61ee64e1c7.png)'
- en: Kubernetes networking versus Docker networking
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes网络与Docker网络的对比
- en: Docker networking follows a different model, although over time it has gravitated
    towards the Kubernetes model. In Docker networking, each container has its own
    private IP address from the `172.xxx.xxx.xxx` address space confined to its own
    node. It can talk to other containers on the same node via their own `172.xxx.xxx.xxx`
    IP addresses. This makes sense for Docker because it doesn't have the notion of
    a pod with multiple interacting containers, so it models every container as a
    lightweight VM that has its own network identity. Note that with Kubernetes, containers
    from different pods that run on the same node can't connect over localhost (except
    by exposing host ports, which is discouraged). The whole idea is that, in general,
    Kubernetes can kill and create pods anywhere, so different pods shouldn't rely,
    in general, on other pods available on the node. Daemon sets are a notable exception,
    but the Kubernetes networking model is designed to work for all use cases and
    doesn't add special cases for direct communication between different pods on the
    same node.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Docker网络遵循不同的模型，尽管随着时间的推移，它已经趋向于Kubernetes模型。在Docker网络中，每个容器都有自己的私有IP地址，来自`172.xxx.xxx.xxx`地址空间，限定在自己的节点上。它可以通过它们自己的`172.xxx.xxx.xxx`
    IP地址与同一节点上的其他容器进行通信。这对Docker来说是有意义的，因为它没有多个交互容器的pod的概念，所以它将每个容器建模为一个具有自己网络身份的轻量级VM。请注意，使用Kubernetes，运行在同一节点上的不同pod的容器不能通过localhost连接（除非暴露主机端口，这是不鼓励的）。整个想法是，一般来说，Kubernetes可以在任何地方杀死和创建pod，因此不同的pod一般不应该依赖于节点上可用的其他pod。守护进程集是一个值得注意的例外，但Kubernetes网络模型旨在适用于所有用例，并且不为同一节点上不同pod之间的直接通信添加特殊情况。
- en: How do Docker containers communicate across nodes? The container must publish
    ports to the host. This obviously requires port coordination because if two containers
    try to publish the same host port, they'll conflict with each other. Then containers
    (or other processes) connect to the host's port that get channeled into the container.
    A big downside is that containers can't self-register with external services because
    they don't know what their host's IP address is. You could work around it by passing
    the host's IP address as an environment variable when you run the container, but
    that requires external coordination and complicates the process.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Docker容器如何跨节点通信？容器必须将端口发布到主机。这显然需要端口协调，因为如果两个容器尝试发布相同的主机端口，它们将互相冲突。然后容器（或其他进程）连接到被通道化到容器中的主机端口。一个很大的缺点是，容器无法自我注册到外部服务，因为它们不知道它们所在主机的IP地址。您可以通过在运行容器时将主机的IP地址作为环境变量传递来解决这个问题，但这需要外部协调并且使过程复杂化。
- en: 'The following diagram shows the networking setup with Docker. Each container
    has its own IP address; Docker creates the `docker0` bridge on every node:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Docker的网络设置。每个容器都有自己的IP地址；Docker在每个节点上创建了`docker0`桥接：
- en: '![](Images/f8e87edb-e287-404f-a56b-4bcee0767eed.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f8e87edb-e287-404f-a56b-4bcee0767eed.png)'
- en: Lookup and discovery
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找和发现
- en: In order for pods and containers to communicate with each other, they need to
    find each other. There are several ways for containers to locate other containers
    or announce themselves. There are also some architectural patterns that allow
    containers to interact indirectly. Each approach has its own pros and cons.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使pod和容器能够相互通信，它们需要找到彼此。容器定位其他容器或宣布自己有几种方法。还有一些架构模式允许容器间间接交互。每种方法都有其优缺点。
- en: Self-registration
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自注册
- en: We've mentioned self-registration several times. Let's understand exactly what
    it means. When a container runs, it knows its pod's IP address. Each container
    that wants to be accessible to other containers in the cluster can connect to
    some registration service and register its IP address and port. Other containers
    can query the registration service for the IP addresses and port of all registered
    containers and connect to them. When a container is destroyed (gracefully), it
    will unregister itself. If a container dies ungracefully then some mechanism needs
    to be established to detect that. For example, the registration service can periodically
    ping all registered containers, or the containers are required periodically to
    send a keepalive message to the registration service.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次提到自注册。让我们确切地理解它的含义。当一个容器运行时，它知道其pod的IP地址。每个希望对集群中的其他容器可访问的容器都可以连接到某个注册服务并注册其IP地址和端口。其他容器可以查询注册服务以获取所有已注册容器的IP地址和端口，并连接到它们。当一个容器被销毁（正常情况下），它将取消注册。如果一个容器非正常死亡，那么需要建立一些机制来检测。例如，注册服务可以定期ping所有已注册的容器，或者要求容器定期向注册服务发送保持活动的消息。
- en: The benefit of self-registration is that once the generic registration service
    is in place (no need to customize it for different purposes), there is no need
    to worry about keeping track of containers. Another huge benefit is that containers
    can employ sophisticated policies and decide to unregister temporarily if they
    are unavailable because of local conditions, such as if a container is busy and
    doesn't want to receive any more requests at the moment. This sort of smart and
    decentralized dynamic load balancing can be very difficult to achieve globally.
    The downside is that the registration service is yet another non-standard component
    that containers need to know about in order to locate other containers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自注册的好处在于一旦通用注册服务就位（无需为不同目的定制），就无需担心跟踪容器。另一个巨大的好处是，容器可以采用复杂的策略，并决定在本地条件下暂时取消注册，比如如果一个容器很忙，不想在这一刻接收更多请求。这种智能和分散的动态负载平衡在全球范围内很难实现。缺点是注册服务是另一个非标准组件，容器需要了解它以便定位其他容器。
- en: Services and endpoints
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务和端点
- en: Kubernetes services can be considered as a registration service. Pods that belong
    to a service are registered automatically based on their labels. Other pods can
    look up the endpoints to find all the service pods or take advantage of the service
    itself and directly send a message to the service that will get routed to one
    of the backend pods. Although most of the time, pods will just send their message
    to the service itself, which will forward it to one of the backing pods.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes服务可以被视为注册服务。属于服务的pod会根据其标签自动注册。其他pod可以查找端点以找到所有服务pod，或者利用服务本身直接发送消息到服务，消息将被路由到其中一个后端pod。尽管大多数情况下，pod将消息直接发送到服务本身，由服务转发到其中一个后端pod。
- en: Loosely coupled connectivity with queues
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与队列松散耦合的连接
- en: 'What if containers can talk to each other without knowing their IP addresses
    and ports or even service IP addresses or network names? What if most of the communication
    can be asynchronous and decoupled? In many cases, systems can be composed of loosely
    coupled components that are not only unaware of the identities of other components,
    but they are unaware that other components even exist. Queues facilitate such
    loosely coupled systems. Components (containers) listen to messages from the queue,
    respond to messages, perform their jobs, and post messages to the queue about
    progress, completion status, and errors. Queues have many benefits:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器可以相互通信，而不知道它们的IP地址和端口，甚至不知道服务IP地址或网络名称呢？如果大部分通信可以是异步和解耦的呢？在许多情况下，系统可以由松散耦合的组件组成，这些组件不仅不知道其他组件的身份，甚至不知道其他组件的存在。队列有助于这种松散耦合的系统。组件（容器）监听来自队列的消息，响应消息，执行它们的工作，并在队列中发布有关进度、完成状态和错误的消息。队列有许多好处：
- en: Easy to add processing capacity without coordination; just add more containers
    that listen to the queue
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需协调即可添加处理能力；只需添加更多监听队列的容器
- en: Easy to keep track of overall load by queue depth
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过队列深度轻松跟踪整体负载
- en: Easy to have multiple versions of components running side by side by versioning
    messages and/or topics
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对消息和/或主题进行版本控制，轻松同时运行多个组件的不同版本
- en: Easy to implement load balancing as well as redundancy by having multiple consumers
    process requests in different modes
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使多个消费者以不同模式处理请求，轻松实现负载均衡以及冗余
- en: 'The downsides of queues are the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 队列的缺点包括：
- en: Need to make sure that the queue provides appropriate durability and high availability
    so it doesn't become a critical SPOF
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要确保队列提供适当的耐用性和高可用性，以免成为关键的单点故障。
- en: Containers need to work with the async queue API (could be abstracted away)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器需要使用异步队列API（可以抽象化）
- en: Implementing request-response requires the somewhat cumbersome listening on
    response queues
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现请求-响应需要在响应队列上进行有些繁琐的监听
- en: Overall, queues are an excellent mechanism for large-scale systems and they
    can be utilized in large Kubernetes clusters to ease coordination.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，队列是大规模系统的一个很好的机制，可以在大型Kubernetes集群中使用，以简化协调工作。
- en: Loosely coupled connectivity with data stores
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与数据存储松散耦合的连接
- en: Another loosely coupled method is to use a data store (for example, Redis) to
    store messages and then other containers can read them. While possible, this is
    not the design objective of data stores and the result is often cumbersome, fragile,
    and doesn't have the best performance. Data stores are optimized for data storage
    and not for communication. That being said, data stores can be used in conjunction
    with queues, where a component stores some data in a data store and then sends
    a message to the queue that data is ready for processing. Multiple components
    listen to the message and all start processing the data in parallel.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种松散耦合的方法是使用数据存储（例如Redis）存储消息，然后其他容器可以读取它们。虽然可能，但这不是数据存储的设计目标，结果通常是繁琐、脆弱，并且性能不佳。数据存储针对数据存储进行了优化，而不是用于通信。也就是说，数据存储可以与队列一起使用，其中一个组件将一些数据存储在数据存储中，然后发送一条消息到队列，表示数据已准备好进行处理。多个组件监听该消息，并且都开始并行处理数据。
- en: Kubernetes ingress
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes入口
- en: Kubernetes offers an ingress resource and controller that is designed to expose
    Kubernetes services to the outside world. You can do it yourself, of course, but
    many tasks involved in defining ingress are common across most applications for
    a particular type of ingress, such as a web application, CDN, or DDoS protector.
    You can also write your own ingress objects.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了一个入口资源和控制器，旨在将Kubernetes服务暴露给外部世界。当然，您也可以自己做，但定义入口所涉及的许多任务在特定类型的入口（如Web应用程序、CDN或DDoS保护器）的大多数应用程序中是常见的。您还可以编写自己的入口对象。
- en: The `ingress` object is often used for smart load balancing and TLS termination.
    Instead of configuring and deploying your own NGINX server, you can benefit from
    the built-in ingress. If you need a refresher, hop on to [Chapter 6](6502d8f5-5418-4f9e-9b61-ec3c38d2f018.xhtml),
    *Using Critical Kubernetes Resources*, where we discussed the ingress resource
    with examples.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: “入口”对象通常用于智能负载平衡和TLS终止。您可以从内置入口中受益，而不是配置和部署自己的NGINX服务器。如果您需要复习，请转到[第6章](6502d8f5-5418-4f9e-9b61-ec3c38d2f018.xhtml)，*使用关键的Kubernetes资源*，在那里我们讨论了带有示例的入口资源。
- en: Kubernetes network plugins
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes网络插件
- en: Kubernetes has a network plugin system, because networking is so diverse and
    different people would like to implement it in different ways. Kubernetes is flexible
    enough to support any scenario. The primary network plugin is CNI, which we will
    discuss in depth. But Kubernetes also comes with a simpler network plugin called
    Kubenet. Before we go over the details, let's get on the same page with the basics
    of Linux networking (just the tip of the iceberg).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes具有网络插件系统，因为网络如此多样化，不同的人希望以不同的方式实现它。Kubernetes足够灵活，可以支持任何场景。主要的网络插件是CNI，我们将深入讨论。但Kubernetes还配备了一个更简单的网络插件，称为Kubenet。在我们详细讨论之前，让我们就Linux网络的基础知识达成一致（只是冰山一角）。
- en: Basic Linux networking
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的Linux网络
- en: Linux, by default, has a single shared network space. The physical network interfaces
    are all accessible in this namespace, but the physical namespace can be divided
    into multiple logical namespaces, which is very relevant to container networking.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Linux具有单个共享网络空间。物理网络接口都可以在此命名空间中访问，但物理命名空间可以分成多个逻辑命名空间，这与容器网络非常相关。
- en: IP addresses and ports
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IP地址和端口
- en: Network entities are identified by their IP address. Servers can listen to incoming
    connections on multiple ports. Clients can connect (TCP) or send data (UDP) to
    servers within their network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 网络实体通过其IP地址进行标识。服务器可以在多个端口上监听传入连接。客户端可以连接（TCP）或向其网络内的服务器发送数据（UDP）。
- en: Network namespaces
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络命名空间
- en: Namespaces group a bunch of network devices such that they can reach other servers
    in the same namespace, but not other servers even if they are physically on the
    same network. Linking networks or network segments can be done through bridges,
    switches, gateways, and routing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间将一堆网络设备分组在一起，以便它们可以在同一命名空间中到达其他服务器，但即使它们在物理上位于同一网络上，也不能到达其他服务器。通过桥接、交换机、网关和路由可以连接网络或网络段。
- en: Subnets, netmasks, and CIDRs
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子网、网络掩码和CIDR
- en: Granular division of network segments is very useful when designing and maintaining
    networks. Dividing networks in to smaller subnets with a common prefix is a common
    practice. These subnets can be defined by bitmasks that represent the size of
    the subnet (how many hosts it can contain). For example, a netmask of `255.255.255.0`
    means that the first three octets are used for routing and only 256 (actually
    254) individual hosts are available. The Classless Inter-Domain Routing (CIDR)
    notation is often used for this purpose because it is more concise, encodes more
    information, and also allows combining hosts from multiple legacy classes (A,
    B, C, D, E). For example, `172.27.15.0/24` means that the first 24 bits (three
    octets) are used for routing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计和维护网络时，网络段的细分非常有用。将网络划分为具有共同前缀的较小子网是一种常见做法。这些子网可以由表示子网大小（可以包含多少主机）的位掩码来定义。例如，`255.255.255.0`的子网掩码意味着前三个八位字节用于路由，只有256（实际上是254）个单独的主机可用。无类别域间路由（CIDR）表示法经常用于此目的，因为它更简洁，编码更多信息，并且还允许将来自多个传统类别（A、B、C、D、E）的主机组合在一起。例如，`172.27.15.0/24`表示前24位（三个八位字节）用于路由。
- en: Virtual Ethernet devices
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟以太网设备
- en: '**Virtual Ethernet** (**veth**) devices represent physical network devices.
    When you create a `veth` that''s linked to a physical device, you can assign that
    `veth` (and by extension the physical device) into a namespace in which devices
    from other namespaces can''t reach it directly, even if physically they are on
    the same local network.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚拟以太网**（**veth**）设备代表物理网络设备。当您创建一个与物理设备连接的`veth`时，您可以将该`veth`（以及物理设备）分配到一个命名空间中，其他命名空间的设备无法直接访问它，即使它们在物理上位于同一个本地网络上。'
- en: Bridges
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 桥接器
- en: Bridges connect multiple network segments to an aggregate network, so all the
    nodes can communicate with each other. Bridging is done at the L1 (physical) and
    L2 (data link) layers of the OSI network model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接器将多个网络段连接到一个聚合网络，以便所有节点可以彼此通信。桥接是在OSI网络模型的L1（物理）和L2（数据链路）层进行的。
- en: Routing
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 路由
- en: Routing connects separate networks, typically based on routing tables that instruct
    network devices how to forward packets to their destination. Routing is done through
    various network devices, such as routers, bridges, gateways, switches, and firewalls,
    including regular Linux boxes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 路由连接不同的网络，通常基于路由表，指示网络设备如何将数据包转发到其目的地。路由是通过各种网络设备进行的，如路由器、桥接器、网关、交换机和防火墙，包括常规的Linux框。
- en: Maximum transmission unit
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大传输单元
- en: The **maximum transmission unit** (**MTU**) determines how big packets can be.
    On Ethernet networks, for example, the MTU is 1,500 bytes. The bigger the MTU,
    the better the ratio between payload and headers, which is a good thing. The downside
    is that minimum latency is reduced because you have to wait for the entire packet
    to arrive and, furthermore, if there's a failure, you have to retransmit the entire
    packet.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大传输单元**（**MTU**）确定数据包的大小限制。例如，在以太网网络上，MTU为1500字节。MTU越大，有效载荷和标头之间的比率就越好，这是一件好事。缺点是最小延迟减少，因为您必须等待整个数据包到达，而且如果出现故障，您必须重新传输整个数据包。'
- en: Pod networking
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod网络
- en: 'Here is a diagram that describes the relationship between pod, host, and the
    global internet at the networking level through `veth0`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个描述通过`veth0`在网络层面上描述pod、主机和全局互联网之间关系的图表：
- en: '![](Images/5cf26bd3-81f9-42df-b4ad-f004f50d5e6d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/5cf26bd3-81f9-42df-b4ad-f004f50d5e6d.png)'
- en: Kubenet
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubenet
- en: Back to Kubernetes. Kubenet is a network plugin; it's very rudimentary and just
    creates a Linux bridge called `cbr0` and a `veth` for each pod. Cloud providers
    typically use it to set up routing rules for communication between nodes, or in
    single-node environments. The `veth` pair connects each pod to its host node using
    an IP address from the host's IP address range.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 回到Kubernetes。Kubenet是一个网络插件；它非常基础，只是创建一个名为`cbr0`的Linux桥接和为每个pod创建一个`veth`。云服务提供商通常使用它来设置节点之间的通信路由规则，或者在单节点环境中使用。`veth`对将每个pod连接到其主机节点，使用来自主机IP地址范围的IP地址。
- en: Requirements
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要求
- en: 'The Kubenet plugin has the following requirements:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Kubenet插件有以下要求：
- en: The node must be assigned a subnet to allocate IP addresses for its pods
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须为节点分配一个子网，以为其pod分配IP地址
- en: The standard CNI bridge, `lo`, and host-local plugins are required for version
    0.2.0 or greater
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本0.2.0或更高版本需要标准的CNI桥接、`lo`和host-local插件
- en: The Kubelet must be run with the `--network-plugin=kubenet` argument
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubelet必须使用`--network-plugin=kubenet`参数运行
- en: The Kubelet must be run with the `--non-masquerade-cidr=<clusterCidr>` argument
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubelet必须使用`--non-masquerade-cidr=<clusterCidr>`参数运行
- en: Setting the MTU
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置MTU
- en: The MTU is critical for network performance. Kubernetes network plugins such
    as Kubenet make their best efforts to deduce optimal MTU, but sometimes they need
    help. If an existing network interface (for example, the Docker `docker0` bridge)
    sets a small MTU, then Kubenet will reuse it. Another example is IPSEC, which
    requires lowering the MTU due to the extra overhead from IPSEC encapsulation overhead,
    but the Kubenet network plugin doesn't take it into consideration. The solution
    is to avoid relying on the automatic calculation of the MTU and just tell the
    Kubelet what MTU should be used for network plugins through the `--network-plugin-mtu`
    command-line switch, which is provided to all network plugins. However, at the
    moment, only the Kubenet network plugin accounts for this command-line switch.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: MTU对于网络性能至关重要。Kubernetes网络插件（如Kubenet）会尽最大努力推断最佳MTU，但有时它们需要帮助。如果现有的网络接口（例如Docker的`docker0`桥接）设置了较小的MTU，则Kubenet将重用它。另一个例子是IPSEC，由于IPSEC封装开销增加，需要降低MTU，但Kubenet网络插件没有考虑到这一点。解决方案是避免依赖MTU的自动计算，只需通过`--network-plugin-mtu`命令行开关告诉Kubelet应该为网络插件使用什么MTU，这个开关提供给所有网络插件。然而，目前只有Kubenet网络插件考虑了这个命令行开关。
- en: Container Networking Interface (CNI)
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器网络接口（CNI）
- en: 'CNI is a specification as well as a set of libraries for writing network plugins
    to configure network interfaces in Linux containers (not just Docker). The specification
    actually evolved from the rkt network proposal. There is a lot of momentum behind
    CNI and it''s on a fast track to become the established industry standard. Some
    of the organizations that use CNI are:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: CNI既是一个规范，也是一组用于编写网络插件以配置Linux容器中的网络接口的库（不仅仅是Docker）。该规范实际上是从rkt网络提案演变而来的。CNI背后有很多动力，正在快速成为行业标准。一些使用CNI的组织有：
- en: Kubernetes
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kurma
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurma
- en: Cloud foundry
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云原生
- en: Nuage
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nuage
- en: RedHat
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红帽
- en: Mesos
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesos
- en: 'The CNI team maintains some core plugins, but there are a lot of third-party
    plugins too that contribute to the success of CNI:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: CNI团队维护一些核心插件，但也有很多第三方插件对CNI的成功做出了贡献：
- en: '**Project Calico**: A layer 3 virtual network'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Project Calico**：三层虚拟网络'
- en: '**Weave**: A multi-host Docker network'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Weave**：多主机Docker网络'
- en: '**Contiv networking**: Policy-based networking'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Contiv网络**：基于策略的网络'
- en: '**Cilium**: BPF and XDP for containers'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cilium**：用于容器的BPF和XDP'
- en: '**Multus**: A Multi plugin'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Multus**：一个多插件'
- en: '**CNI-Genie**: A generic CNI network plugin'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CNI-Genie**：通用CNI网络插件'
- en: '**Flannel**: A network fabric for containers, designed for Kubernetes'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flannel**：为Kubernetes设计的容器网络布局'
- en: '**Infoblox**: Enterprise IP address management for containers'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Infoblox**：企业级容器IP地址管理'
- en: Container runtime
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器运行时
- en: CNI defines a plugin spec for networking application containers, but the plugin
    must be plugged into a container runtime that provides some services. In the context
    of CNI, an application container is a network-addressable entity (has its own
    IP address). For Docker, each container has its own IP address. For Kubernetes,
    each pod has its own IP address and the pod is the CNI container and not the containers
    within the pod.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: CNI为网络应用容器定义了插件规范，但插件必须插入提供一些服务的容器运行时中。在CNI的上下文中，应用容器是一个可寻址的网络实体（具有自己的IP地址）。对于Docker，每个容器都有自己的IP地址。对于Kubernetes，每个pod都有自己的IP地址，而pod是CNI容器，而不是pod内的容器。
- en: Likewise, rkt's app containers are similar to Kubernetes pods in that they may
    contain multiple Linux containers. If in doubt, just remember that a CNI container
    must have its own IP address. The runtime's job is to configure a network and
    then execute one or more CNI plugins, passing them the network configuration in
    JSON format.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，rkt的应用容器类似于Kubernetes中的pod，因为它们可能包含多个Linux容器。如果有疑问，只需记住CNI容器必须有自己的IP地址。运行时的工作是配置网络，然后执行一个或多个CNI插件，以JSON格式将网络配置传递给它们。
- en: 'The following diagram shows a container runtime using the CNI plugin interface
    to communicate with multiple CNI plugins:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了一个容器运行时使用CNI插件接口与多个CNI插件进行通信：
- en: '![](Images/9298924f-10d1-4fdc-b16f-749c5674e1b2.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/9298924f-10d1-4fdc-b16f-749c5674e1b2.png)'
- en: CNI plugin
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNI插件
- en: The CNI plugin's job is to add a network interface into the container network
    namespace and bridge the container to the host via a `veth` pair. It should then
    assign an IP address through an IPAM (IP address management) plugin and set up
    routes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: CNI插件的工作是将网络接口添加到容器网络命名空间，并通过`veth`对将容器桥接到主机。然后，它应通过IPAM（IP地址管理）插件分配IP地址并设置路由。
- en: 'The container runtime (Docker, rkt, or any other CRI-compliant runtime) invokes
    the CNI plugin as an executable. The plugin needs to support the following operations:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时（Docker，rkt或任何其他符合CRI标准的运行时）将CNI插件作为可执行文件调用。插件需要支持以下操作：
- en: Add a container to the network
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将容器添加到网络
- en: Remove a container from the network
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网络中删除容器
- en: Report version
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告版本
- en: 'The plugin uses a simple command-line interface, standard input/output, and
    environment variables. The network configuration in JSON format is passed to the
    plugin through standard input. The other arguments are defined as environment
    variables:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 插件使用简单的命令行界面，标准输入/输出和环境变量。以JSON格式的网络配置通过标准输入传递给插件。其他参数被定义为环境变量：
- en: '`CNI_COMMAND`: Indicates the desired operation; `ADD`, `DEL`, or `VERSION`.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNI_COMMAND`：指示所需操作的命令；`ADD`，`DEL`或`VERSION`。'
- en: '`CNI_CONTAINERID`: Container ID.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNI_CONTAINERID`：容器ID。'
- en: '`CNI_NETNS`: Path to network namespace file.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNI_NETNS`：网络命名空间文件的路径。'
- en: '`*` `CNI_IFNAME`: Interface name to set up; the plugin must honor this interface
    name or return an `error`.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*` `CNI_IFNAME`：要设置的接口名称；插件必须遵守此接口名称或返回一个`error`。'
- en: '`*` `CNI_ARGS`: Extra arguments passed in by the user at invocation time. Alphanumeric
    key-value pairs are separated by semicolons, for example, `FOO=BAR;ABC=123`.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*` `CNI_ARGS`：用户在调用时传入的额外参数。字母数字键值对由分号分隔，例如，`FOO=BAR;ABC=123`。'
- en: '`CNI_PATH`: List of paths to search for CNI plugin executables. Paths are separated
    by an OS-specific list separator, for example, `:` on Linux and `;` on Windows.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNI_PATH`：要搜索CNI插件可执行文件的路径列表。路径由操作系统特定的列表分隔符分隔，例如，在Linux上是`:`，在Windows上是`；`。'
- en: If the command succeeds, the plugin returns a zero exit code and the generated
    interfaces (in the case of the `ADD` command) are streamed to standard output
    as JSON. This low-tech interface is smart in the sense that it doesn't require
    any specific programming language, or component technology, or binary API. CNI
    plugin writers can use their favorite programming language too.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令成功，插件将返回零退出代码，并且生成的接口（在`ADD`命令的情况下）将作为JSON流式传输到标准输出。这种低技术接口很聪明，因为它不需要任何特定的编程语言、组件技术或二进制API。CNI插件编写者也可以使用他们喜欢的编程语言。
- en: 'The result of invoking the CNI plugin with the `ADD` command is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`ADD`命令调用CNI插件的结果如下：
- en: '[PRE0]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The input network configuration contains a lot of information: `cniVersion`,
    name, type, `args` (optional), `ipMasq` (optional), `ipam`, and `dns`. The `ipam`
    and `dns` parameters are dictionaries with their own specified keys. Here is an
    example of a network configuration:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输入网络配置包含大量信息：`cniVersion`、名称、类型、`args`（可选）、`ipMasq`（可选）、`ipam`和`dns`。`ipam`和`dns`参数是具有自己指定键的字典。以下是网络配置的示例：
- en: '[PRE1]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that additional plugin-specific elements can be added. In this case, the
    `bridge: cni0` element is a custom one that the specific `bridge` plugin understands.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，可以添加额外的特定于插件的元素。在这种情况下，`bridge: cni0`元素是特定的`bridge`插件理解的自定义元素。'
- en: The `CNI spec` also supports network configuration lists where multiple CNI
    plugins can be invoked in order. Later, we will dig into a fully-fledged implementation
    of a CNI plugin.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`CNI规范`还支持网络配置列表，其中可以按顺序调用多个CNI插件。稍后，我们将深入研究一个完全成熟的CNI插件实现。'
- en: Kubernetes networking solutions
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes网络解决方案
- en: Networking is a vast topic. There are many ways to set up networks and connect
    devices, pods, and containers. Kubernetes can't be opinionated about it. The high-level
    networking model of a flat address space for pods is all that Kubernetes prescribes.
    Within that space, many valid solutions are possible, with various capabilities
    and policies for different environments. In this section, we'll examine some of
    the available solutions and understand how they map to the Kubernetes networking
    model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是一个广阔的话题。有许多设置网络和连接设备、pod和容器的方法。Kubernetes对此不能有意见。对于pod的高级网络模型是Kubernetes规定的。在这个空间内，有许多有效的解决方案是可能的，具有不同环境的各种功能和策略。在本节中，我们将研究一些可用的解决方案，并了解它们如何映射到Kubernetes网络模型。
- en: Bridging on bare metal clusters
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 裸金属集群上的桥接
- en: 'The most basic environment is a raw bare-metal cluster with just an L2 physical
    network. You can connect your containers to the physical network with a Linux
    bridge device. The procedure is quite involved and requires familiarity with low-level
    Linux network commands such as `brctl`, `ip addr`, `ip route`, `ip link`, `nsenter`,
    and so on. If you plan to implement it, this guide can serve as a good start (search
    for the *With Linux Bridge devices* section): [http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/](http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的环境是一个只有L2物理网络的原始裸金属集群。您可以使用Linux桥设备将容器连接到物理网络。该过程非常复杂，需要熟悉低级Linux网络命令，如`brctl`、`ip
    addr`、`ip route`、`ip link`、`nsenter`等。如果您打算实施它，这篇指南可以作为一个很好的起点（搜索*使用Linux桥设备*部分）：[http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/](http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/)。
- en: Contiv
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Contiv
- en: 'Contiv is a general-purpose network plugin for container networking and it
    can be used with Docker directly, Mesos, Docker Swarm, and of course Kubernetes,
    through a CNI plugin. Contiv is focused on network policies that overlap somewhat
    with Kubernetes'' own network policy object. Here are some of the capabilities
    of the Contiv net plugin:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Contiv是一个通用的容器网络插件，可以直接与Docker、Mesos、Docker Swarm以及当然Kubernetes一起使用，通过一个CNI插件。Contiv专注于与Kubernetes自身网络策略对象有些重叠的网络策略。以下是Contiv
    net插件的一些功能：
- en: Supports both libnetwork's CNM and the CNI specification
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持libnetwork的CNM和CNI规范
- en: A feature-rich policy model to provide secure, predictable application deployment
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能丰富的策略模型，提供安全、可预测的应用部署
- en: Best-in-class throughput for container workloads
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于容器工作负载的最佳吞吐量
- en: Multi-tenancy, isolation, and overlapping subnets
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多租户、隔离和重叠子网
- en: Integrated IPAM and service discovery
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成IPAM和服务发现
- en: 'A variety of physical topologies:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种物理拓扑：
- en: Layer2 (VLAN)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Layer2（VLAN）
- en: Layer3 (BGP)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Layer3（BGP）
- en: Overlay (VXLAN)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖（VXLAN）
- en: Cisco SDN solution (ACI)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思科SDN解决方案（ACI）
- en: IPv6 support
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPv6支持
- en: Scalable policy and route distribution
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的策略和路由分发
- en: 'Integration with application blueprints, including the following:'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与应用蓝图的集成，包括以下内容：
- en: Docker-compose
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker-compose
- en: Kubernetes deployment manager
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes部署管理器
- en: Service load balancing is built in east-west microservice load balancing
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务负载平衡内置东西向微服务负载平衡
- en: Traffic isolation for storage, control (for example, `etcd`/`consul`), network,
    and management traffic
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储、控制（例如，`etcd`/`consul`）、网络和管理流量的流量隔离
- en: Contiv has many features and capabilities. I'm not sure if it's the best choice
    for Kubernetes due to its broad surface area and the fact that it caters to multiple
    platforms.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Contiv具有许多功能和能力。由于其广泛的适用范围以及它适用于多个平台，我不确定它是否是Kubernetes的最佳选择。
- en: Open vSwitch
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Open vSwitch
- en: 'Open vSwitch is a mature software-based virtual switch solution endorsed by
    many big players. The **Open Virtualization Network** (**OVN**) solution lets
    you build various virtual networking topologies. It has a dedicated Kubernetes
    plugin, but it is not trivial to set up, as demonstrated by this guide: [https://github.com/openvswitch/ovn-kubernetes](https://github.com/openvswitch/ovn-kubernetes).
    The Linen CNI plugin may be easier to set up, although it doesn''t support all
    the features of OVN: [https://github.com/John-Lin/linen-cni](https://github.com/John-Lin/linen-cni).
    Here is a diagram of the Linen CNI plugin:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch是一个成熟的基于软件的虚拟交换解决方案，得到许多大公司的认可。**Open Virtualization Network**（**OVN**）解决方案可以让您构建各种虚拟网络拓扑。它有一个专门的Kubernetes插件，但设置起来并不简单，正如这个指南所示：[https://github.com/openvswitch/ovn-kubernetes](https://github.com/openvswitch/ovn-kubernetes)。Linen
    CNI插件可能更容易设置，尽管它不支持OVN的所有功能：[https://github.com/John-Lin/linen-cni](https://github.com/John-Lin/linen-cni)。这是Linen
    CNI插件的图表：
- en: '![](Images/24da1447-6f6c-4ab2-90a3-38cd607be551.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/24da1447-6f6c-4ab2-90a3-38cd607be551.png)'
- en: Open vSwitch can connect bare-metal servers, VMs, and pods/containers using
    the same logical network. It actually supports both overlay and underlay modes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch可以连接裸机服务器、虚拟机和pod/容器，使用相同的逻辑网络。它实际上支持覆盖和底层模式。
- en: 'Here are some of its key features:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些其关键特性：
- en: Standard 802.1Q VLAN model with trunk and access ports
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准的802.1Q VLAN模型，带有干线和接入端口
- en: NIC bonding with or without LACP on upstream switch
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上游交换机上带或不带LACP的NIC绑定
- en: NetFlow, sFlow(R), and mirroring for increased visibility
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NetFlow、sFlow(R)和镜像，以增加可见性
- en: QoS (Quality of Service) configuration, plus policing
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QoS（服务质量）配置，以及流量控制
- en: Geneve, GRE, VXLAN, STT, and LISP tunneling
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geneve、GRE、VXLAN、STT和LISP隧道
- en: 802.1ag connectivity fault management
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 802.1ag连接故障管理
- en: OpenFlow 1.0 plus numerous extensions
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenFlow 1.0加上许多扩展
- en: Transactional configuration database with C and Python bindings
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有C和Python绑定的事务配置数据库
- en: High-performance forwarding using a Linux kernel module
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Linux内核模块进行高性能转发
- en: Nuage networks VCS
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Nuage网络VCS
- en: The **Virtualized Cloud Services** (**VCS**) product from Nuage networks provides
    a highly scalable policy-based **Software-Defined Networking** (**SDN**) platform.
    It is an enterprise-grade offering that builds on top of the open source Open
    vSwitch for the data plane along with a feature-rich SDN controller built on open
    standards.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Nuage网络的**虚拟化云服务**（**VCS**）产品提供了一个高度可扩展的基于策略的**软件定义网络**（**SDN**）平台。这是一个建立在开源Open
    vSwitch数据平面之上的企业级产品，配备了基于开放标准构建的功能丰富的SDN控制器。
- en: The Nuage platform uses overlays to provide seamless policy-based networking
    between Kubernetes Pods and non-Kubernetes environments (VMs and bare metal servers).
    Nuage's policy abstraction model is designed with applications in mind and makes
    it easy to declare fine-grained policies for applications. The platform's real-time
    analytics engine enables visibility and security monitoring for Kubernetes applications.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Nuage平台使用覆盖层在Kubernetes Pods和非Kubernetes环境（VM和裸金属服务器）之间提供无缝的基于策略的网络。Nuage的策略抽象模型是针对应用程序设计的，使得声明应用程序的细粒度策略变得容易。该平台的实时分析引擎实现了对Kubernetes应用程序的可见性和安全监控。
- en: In addition, all VCS components can be installed in containers. There are no
    special hardware requirements.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，所有VCS组件都可以安装在容器中。没有特殊的硬件要求。
- en: Canal
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Canal
- en: 'Canal is a mix of two open source projects: Calico and Flannel. The name **Canal**
    is a portmanteau of the project names. Flannel, by CoreOS, is focused on container
    networking, and **Calico** is focused on network policy. Originally, they were
    developed independently, but users wanted to use them together. The open source
    Canal project is currently a deployment pattern to install both projects as separate
    CNI plugins. **Tigera**—a company formed by Calico''s founders—is shepherding
    both projects now and had plans for tighter integration, but since they released
    their secure application connectivity solution for Kubernetes the focus seemed
    to shift to contribute back to Flannel and Calico to ease configuration and integration
    rather than providing a unified solution. The following diagram demonstrates the
    present status of Canal and how it relates to container orchestrators such as
    Kubernetes and Mesos:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Canal是两个开源项目的混合体：Calico和Flannel。**Canal**这个名字是这两个项目名称的混成词。由CoreOS开发的Flannel专注于容器网络，**Calico**专注于网络策略。最初，它们是独立开发的，但用户希望将它们一起使用。目前，开源的Canal项目是一个部署模式，可以将这两个项目作为独立的CNI插件进行安装。由Calico创始人组建的Tigera现在正在引导这两个项目，并计划更紧密地集成，但自从他们发布了用于Kubernetes的安全应用连接解决方案后，重点似乎转向了为Flannel和Calico做出贡献，以简化配置和集成，而不是提供统一的解决方案。以下图表展示了Canal的当前状态以及它与Kubernetes和Mesos等容器编排器的关系：
- en: '![](Images/4e064c65-c47a-4e1f-ab9a-52c758784c94.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/4e064c65-c47a-4e1f-ab9a-52c758784c94.png)'
- en: Note that when integrating with Kubernetes, Canal doesn't use `etcd` directly
    anymore, instead it relies on the Kubernetes API server.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与Kubernetes集成时，Canal不再直接使用`etcd`，而是依赖于Kubernetes API服务器。
- en: Flannel
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 法兰绒
- en: Flannel is a virtual network that gives a subnet to each host for use with container
    runtimes. It runs a `flaneld` agent on each host, which allocates a subnet to
    the node from a reserved address space stored in `etcd`. Forwarding packets between
    containers and, ultimately, hosts is done by one of multiple backends. The most
    common backend uses UDP over a TUN device that tunnels through port `8285` by
    default (make sure it's open in your firewall).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel是一个虚拟网络，为每个主机提供一个子网，用于容器运行时。它在每个主机上运行一个`flaneld`代理，该代理从存储在`etcd`中的保留地址空间中为节点分配子网。容器之间以及最终主机之间的数据包转发由多个后端之一完成。最常见的后端使用默认情况下通过端口`8285`进行的TUN设备上的UDP进行隧道传输（确保防火墙中已打开）。
- en: 'The following diagram describes in detail the various components of Flannel,
    the virtual network devices it creates, and how they interact with the host and
    the pod through the `docker0` bridge. It also shows the UDP encapsulation of packets
    and how they are transmitted between hosts:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表详细描述了Flannel的各个组件、它创建的虚拟网络设备以及它们如何通过`docker0`桥与主机和pod进行交互。它还显示了数据包的UDP封装以及它们在主机之间的传输：
- en: '![](Images/a1fc953d-4572-4b46-a1b4-231d0251b7ca.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/a1fc953d-4572-4b46-a1b4-231d0251b7ca.png)'
- en: 'Other backends include the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 其他后端包括以下内容：
- en: '`vxlan`: Uses in-kernel VXLAN to encapsulate the packets.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vxlan`：使用内核VXLAN封装数据包。'
- en: '`host-gw`: Creates IP routes to subnets via remote machine IPs. Note that this
    requires direct layer2 connectivity between hosts running Flannel.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host-gw`：通过远程机器IP创建到子网的IP路由。请注意，这需要在运行Flannel的主机之间直接的二层连接。'
- en: '`aws-vpc`: Creates IP routes in an Amazon VPC route table.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aws-vpc`：在Amazon VPC路由表中创建IP路由。'
- en: '`gce`: Creates IP routes in a Google compute engine network.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gce`：在Google计算引擎网络中创建IP路由。'
- en: '`alloc`: Only performs subnet allocation (no forwarding of data packets).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alloc`：仅执行子网分配（不转发数据包）。'
- en: '`ali-vpc`: Creates IP routes in an alicloud VPC route table.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ali-vpc`：在阿里云VPC路由表中创建IP路由。'
- en: Calico project
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Calico项目
- en: Calico is a versatile virtual networking and network security solution for containers.
    Calico can integrate with all the primary container orchestration frameworks
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Calico是一个多功能的容器虚拟网络和网络安全解决方案。Calico可以与所有主要的容器编排框架集成
- en: 'and runtimes:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 和运行时：
- en: Kubernetes (CNI plugin)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes（CNI插件）
- en: Mesos (CNI plugin)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesos（CNI插件）
- en: Docker (libnework plugin)
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker（libnework插件）
- en: OpenStack (Neutron plugin)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack（Neutron插件）
- en: Calico can also be deployed on-premises or on public clouds with its full feature
    set. Calico's network policy enforcement can be specialized for each workload
    and make sures that traffic is controlled precisely and packets always go from
    their source to vetted destinations. Calico can automatically map network policy
    concepts from orchestration platforms to its own network policy. The reference
    implementation of Kubernetes' network policy is Calico.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Calico还可以在本地部署或在公共云上部署，具有完整的功能集。Calico的网络策略执行可以针对每个工作负载进行专门化，并确保流量被精确控制，数据包始终从其源头到经过审查的目的地。Calico可以自动将编排平台的网络策略概念映射到自己的网络策略。Kubernetes网络策略的参考实现是Calico。
- en: Romana
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Romana
- en: Romana is a modern cloud-native container networking solution. It operates at
    layer 3, taking advantage of standard IP address management techniques. Whole
    networks can become the unit of isolation as Romana uses Linux hosts to create
    gateways and routes to the networks. Operating at layer 3 level means that no
    encapsulation is needed. Network policy is enforced as a distributed firewall
    across all endpoints and services. Hybrid deployments across cloud platforms and
    on-premises deployments are easier as there is no need to configure virtual overlay
    networks.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Romana是一个现代的云原生容器网络解决方案。它在第3层操作，利用标准IP地址管理技术。整个网络可以成为隔离单元，因为Romana使用Linux主机创建网关和网络的路由。在第3层操作意味着不需要封装。网络策略作为分布式防火墙在所有端点和服务上执行。跨云平台和本地部署的混合部署更容易，因为无需配置虚拟覆盖网络。
- en: New Romana virtual IPs allow on-premise users to expose services on layer 2
    LANs through external IPs and service specs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 新的Romana虚拟IP允许本地用户通过外部IP和服务规范在第2层LAN上公开服务。
- en: 'Romana claims that their approach brings significant performance improvements.
    The following diagram shows how Romana eliminates a lot of the overhead associated
    with VXLAN encapsulation:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Romana声称他们的方法带来了显著的性能改进。以下图表显示了Romana如何消除与VXLAN封装相关的大量开销。
- en: '![](Images/8688357c-538c-4144-be52-aee50bb2e67c.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/8688357c-538c-4144-be52-aee50bb2e67c.png)'
- en: Weave net
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Weave网络
- en: 'Weave net is all about ease of use and zero configuration. It uses VXLAN encapsulation
    under the covers and micro DNS on each node. As a developer, you operate at a
    high abstraction level. You name your containers, and Weave net lets you connect
    to and use standard ports for services. This helps you to migrate existing applications
    into containerized applications and microservices. Weave net has a CNI plugin
    for interfacing with Kubernetes (and Mesos). On Kubernetes 1.4 and higher, you
    can integrate Weave net with Kubernetes by running a single command that deploys
    a DaemonSet:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Weave网络主要关注易用性和零配置。它在底层使用VXLAN封装和每个节点上的微型DNS。作为开发人员，您在高抽象级别上操作。您为容器命名，Weave网络让您连接并使用标准端口进行服务。这有助于您将现有应用程序迁移到容器化应用程序和微服务中。Weave网络具有用于与Kubernetes（和Mesos）接口的CNI插件。在Kubernetes
    1.4及更高版本上，您可以通过运行一个部署DaemonSet的单个命令将Weave网络集成到Kubernetes中。
- en: '[PRE2]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The Weave net pods on every node will take care of attaching any new pod you
    create to the Weave network. Weave net supports the network policy API as well
    providing a complete yet easy-to-set-up solution.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点上的Weave网络pod将负责将您创建的任何新pod连接到Weave网络。Weave网络支持网络策略API，提供了一个完整而易于设置的解决方案。
- en: Using network policies effectively
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效使用网络策略
- en: The Kubernetes network policy is about managing network traffic to selected
    pods and namespaces. In a world of hundreds of deployed and orchestrated microservices,
    as is often the case with Kubernetes, managing networking and connectivity between
    pods is essential. It's important to understand that it is not primarily a security
    mechanism. If an attacker can reach the internal network, they will probably be
    able to create their own pods that comply with the network policy in place and
    communicate freely with other pods. In the previous section, we looked at different
    Kubernetes networking solutions and focused on the container networking interface.
    In this section, the focus is on network policy, although there are strong connections
    between the networking solution and how network policy is implemented on top of
    it.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes网络策略是关于管理流向选定的pod和命名空间的网络流量。在部署和编排了数百个微服务的世界中，通常情况下是Kubernetes，管理pod之间的网络和连接至关重要。重要的是要理解，它并不是主要的安全机制。如果攻击者可以访问内部网络，他们可能能够创建符合现有网络策略并与其他pod自由通信的自己的pod。在前一节中，我们看了不同的Kubernetes网络解决方案，并侧重于容器网络接口。在本节中，重点是网络策略，尽管网络解决方案与如何在其之上实现网络策略之间存在着紧密的联系。
- en: Understanding the Kubernetes network policy design
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解Kubernetes网络策略设计
- en: A network policy is a specification of how selections of pods can communicate
    with each other and other network endpoints. `NetworkPolicy` resources use labels
    to select pods and define whitelist rules that allow traffic to the selected pods
    in addition to what is allowed by the isolation policy for a given namespace.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略是选择的pod之间以及其他网络端点之间如何通信的规范。`NetworkPolicy`资源使用标签选择pod，并定义白名单规则，允许流量到达选定的pod，除了给定命名空间的隔离策略允许的流量之外。
- en: Network policies and CNI plugins
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络策略和CNI插件
- en: There is an intricate relationship between network policies and CNI plugins.
    Some CNI plugins implement both network connectivity and network policy, while
    others implement just one aspect, but they can collaborate with another CNI plugin
    that implements the other aspect (for example, Calico and Flannel).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略和CNI插件之间存在复杂的关系。一些CNI插件同时实现了网络连接和网络策略，而其他一些只实现了其中一个方面，但它们可以与另一个实现了另一个方面的CNI插件合作（例如，Calico和Flannel）。
- en: Configuring network policies
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置网络策略
- en: 'Network policies are configured through the `NetworkPolicy` resource. Here
    is a sample network policy:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略是通过`NetworkPolicy`资源进行配置的。以下是一个示例网络策略：
- en: '[PRE3]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Implementing network policies
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施网络策略
- en: 'While the network policy API itself is generic and is part of the Kubernetes
    API, the implementation is tightly coupled to the networking solution. That means
    that on each node, there is a special agent or gatekeeper that does the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然网络策略API本身是通用的，并且是Kubernetes API的一部分，但实现与网络解决方案紧密耦合。这意味着在每个节点上都有一个特殊的代理或守门人，执行以下操作：
- en: Intercepts all traffic coming into the node
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拦截进入节点的所有流量
- en: Verifies that it adheres to the network policy
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证其是否符合网络策略
- en: Forwards or rejects each request
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转发或拒绝每个请求
- en: 'Kubernetes provides the facility to define and store network policies through
    the API. Enforcing the network policy is left to the networking solution or a
    dedicated network policy solution that is tightly integrated with the specific
    networking solution. Calico and Canal are good examples of this approach. Calico
    has its own networking solution and a network policy solution that work together,
    but it can also provide network policy enforcement on top of Flannel as part of
    Canal. In both cases, there is tight integration between the two pieces. The following
    diagram shows how the Kubernetes policy controller manages the network policies
    and how agents on the nodes execute it:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了通过API定义和存储网络策略的功能。执行网络策略由网络解决方案或与特定网络解决方案紧密集成的专用网络策略解决方案来完成。Calico和Canal是这种方法的很好的例子。Calico有自己的网络解决方案和网络策略解决方案，它们可以一起工作，但也可以作为Canal的一部分在Flannel之上提供网络策略执行。在这两种情况下，这两个部分之间有紧密的集成。以下图表显示了Kubernetes策略控制器如何管理网络策略以及节点上的代理如何执行它：
- en: '![](Images/fe40a1b6-1aa2-4d01-9d93-eebb58eb6d8e.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fe40a1b6-1aa2-4d01-9d93-eebb58eb6d8e.png)'
- en: Load balancing options
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡选项
- en: Load balancing is a critical capability in dynamic systems such as a Kubernetes
    cluster. Nodes, VMs, and pods come and go, but the clients can't keep track of
    which individual entities can service their requests. Even if they could, it would
    require a complicated dance of managing a dynamic map of the cluster, refreshing
    it frequently, and handling disconnected, unresponsive, or just slow nodes. Load
    balancing is a battle-tested and well-understood mechanism that adds a layer of
    indirection that hides the internal turmoil from the clients or consumers outside
    the cluster. There are options for external as well as internal load balancers.
    You can also mix and match and use both. The hybrid approach has its own particular
    pros and cons, such as performance versus flexibility.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡是动态系统中的关键能力，比如Kubernetes集群。节点、虚拟机和Pod会不断变化，但客户端无法跟踪哪个个体可以处理他们的请求。即使他们可以，也需要管理集群的动态映射，频繁刷新它，并处理断开连接、无响应或者慢速节点的复杂操作。负载均衡是一个经过验证和深入理解的机制，它增加了一层间接性，将内部动荡隐藏在集群外部的客户端或消费者之外。外部和内部负载均衡器都有选项。您也可以混合使用两者。混合方法有其特定的优缺点，比如性能与灵活性。
- en: External load balancer
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部负载均衡器
- en: An external load balancer is a load balancer that runs outside the Kubernetes
    cluster. There must be an external load balancer provider that Kubernetes can
    interact with to configure the external load balancer with health checks, firewall
    rules, and to get the external IP address of the load balancer.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 外部负载均衡器是在Kubernetes集群之外运行的负载均衡器。必须有一个外部负载均衡器提供商，Kubernetes可以与其交互，以配置外部负载均衡器的健康检查、防火墙规则，并获取负载均衡器的外部IP地址。
- en: 'The following diagram shows the connection between the load balancer (in the
    cloud), the Kubernetes API server, and the cluster nodes. The external load balancer
    has an up-to-date picture of which pods run on which nodes, and it can direct
    external service traffic to the right pods:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了负载均衡器（在云中）、Kubernetes API服务器和集群节点之间的连接。外部负载均衡器有关于哪些Pod运行在哪些节点上的最新信息，并且可以将外部服务流量引导到正确的Pod。
- en: '![](Images/d05c30c7-b23a-45ce-a923-c55debcda637.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/d05c30c7-b23a-45ce-a923-c55debcda637.png)'
- en: Configuring an external load balancer
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置外部负载均衡器
- en: An external load balancer is configured via the service configuration file or
    directly through Kubectl. We use a service type of `LoadBalancer` instead of using
    a service type of `ClusterIP`, which directly exposes a Kubernetes node as a load
    balancer. This depends on an external load balancer provider being properly installed
    and configured in the cluster. Google's GKE is the most well-tested provider,
    but other cloud platforms provide their integrated solution on top of their cloud
    load balancer.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过服务配置文件或直接通过Kubectl配置外部负载均衡器。我们使用`LoadBalancer`服务类型，而不是使用`ClusterIP`服务类型，后者直接将Kubernetes节点公开为负载均衡器。这取决于外部负载均衡器提供程序在集群中是否已正确安装和配置。Google的GKE是最经过充分测试的提供程序，但其他云平台在其云负载均衡器之上提供了集成解决方案。
- en: Via configuration file
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过配置文件
- en: 'Here is an example service configuration file that accomplishes this goal:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个实现此目标的示例服务配置文件：
- en: '[PRE4]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Via Kubectl
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Kubectl
- en: 'You can also accomplish the same result using a direct `kubectl` command:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用直接的`kubectl`命令来实现相同的结果：
- en: '[PRE5]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The decision whether to use a `service` configuration file or `kubectl` command
    is usually determined by the way you set up the rest of your infrastructure and
    deploy your system. Configuration files are more declarative and arguably more
    appropriate for production usage, where you want a versioned, auditable, and repeatable
    way to manage your infrastructure.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`service`配置文件还是`kubectl`命令的决定通常取决于您设置其余基础设施和部署系统的方式。配置文件更具声明性，可以说更适合生产使用，因为您希望以一种有版本控制、可审计和可重复的方式来管理基础设施。
- en: Finding the load balancer IP addresses
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找负载均衡器IP地址
- en: 'The load balancer will have two IP addresses of interest. The internal IP address
    can be used inside the cluster to access the service. Clients outside the cluster
    will use the external IP address. It''s a good practice to create a DNS entry
    for the external IP address. To get both addresses, use the `kubectl describe`
    command. The `IP` will denote the internal IP address. `LoadBalancer ingress`
    will denote the external IP address:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器将有两个感兴趣的IP地址。内部IP地址可在集群内部用于访问服务。集群外部的客户端将使用外部IP地址。为外部IP地址创建DNS条目是一个良好的做法。要获取这两个地址，请使用`kubectl
    describe`命令。`IP`将表示内部IP地址。`LoadBalancer ingress`将表示外部IP地址：
- en: '[PRE6]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Preserving client IP addresses
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保留客户端IP地址
- en: Sometimes, the service may be interested in the source IP address of the clients.
    Up until Kubernetes 1.5, this information wasn't available. In Kubernetes 1.5,
    there is a beta feature available only on GKE through an annotation to get the
    source IP address. In Kubernetes 1.7, the capability to preserve the original
    client IP was added to the API.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，服务可能对客户端的源IP地址感兴趣。直到Kubernetes 1.5版本，这些信息是不可用的。在Kubernetes 1.5中，通过注释仅在GKE上可用的beta功能可以获取源IP地址。在Kubernetes
    1.7中，API添加了保留原始客户端IP的功能。
- en: Specifying original client IP address preservation
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指定原始客户端IP地址保留
- en: 'You need to configure the following two fields of the service spec:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要配置服务规范的以下两个字段：
- en: '`service.spec.externalTrafficPolicy`: This field determines whether the service
    should route external traffic to a node-local endpoint or a cluster-wide endpoint,
    which is the default. The cluster option doesn''t reveal the client source IP
    and might add a hop to a different node, but spreads the load well. The Local
    option keeps the client source IP and doesn''t add an extra hop as long as the
    service type is `LoadBalancer` or `NodePort`. Its downside is it might not balance
    the load very well.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`service.spec.externalTrafficPolicy`：此字段确定服务是否应将外部流量路由到节点本地端点或集群范围的端点，这是默认设置。集群选项不会显示客户端源IP，并可能将跳转到不同节点，但会很好地分散负载。本地选项保留客户端源IP，并且只要服务类型为`LoadBalancer`或`NodePort`，就不会添加额外的跳转。其缺点是可能无法很好地平衡负载。'
- en: '`service.spec.healthCheckNodePort`: This field is optional. If used, then the
    service health check will use this port number. The default is the allocate node
    port. It has an effect for services of type `LoadBalancer` whose `externalTrafficPolicy`
    is set to `Local`.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`service.spec.healthCheckNodePort`：此字段是可选的。如果使用，则服务健康检查将使用此端口号。默认值为分配节点端口。对于`LoadBalancer`类型的服务，如果其`externalTrafficPolicy`设置为`Local`，则会产生影响。'
- en: 'Here is an example:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: '[PRE7]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Understanding potential in even external load balancing
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 即使在外部负载均衡中理解潜力
- en: External load balancers operate at the node level; while they direct traffic
    to a particular pod, the load distribution is done at the node level. That means
    that if your service has four pods, and three of them are on node A and the last
    one is on node B, then an external load balancer is likely to divide the load
    evenly between node A and node B. This will have the three pods on node A handle
    half of the load (1/6 each) and the single pod on node B handle the other half
    of the load on its own. Weights may be added in the future to address this issue.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 外部负载均衡器在节点级别运行；虽然它们将流量引导到特定的pod，但负载分配是在节点级别完成的。这意味着如果您的服务有四个pod，其中三个在节点A上，最后一个在节点B上，那么外部负载均衡器很可能会在节点A和节点B之间均匀分配负载。这将使节点A上的三个pod处理一半的负载（每个1/6），而节点B上的单个pod将独自处理另一半的负载。未来可能会添加权重来解决这个问题。
- en: Service load balancer
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务负载均衡器
- en: Service load balancing is designed for funneling internal traffic within the
    Kubernetes cluster and not for external load balancing. This is done by using
    a service type of `clusterIP`. It is possible to expose a service load balancer
    directly via a pre-allocated port by using service type of `NodePort` and use
    it as an external load balancer, but it wasn't designed for that use case. For
    example, desirable features such as SSL termination and HTTP caching will not
    be readily available.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 服务负载平衡旨在在Kubernetes集群内部传输内部流量，而不是用于外部负载平衡。这是通过使用`clusterIP`服务类型来实现的。可以通过使用`NodePort`服务类型直接公开服务负载均衡器，并将其用作外部负载均衡器，但它并不是为此用例而设计的。例如，诸如SSL终止和HTTP缓存之类的理想功能将不会很容易地可用。
- en: 'The following diagram shows how the service load balancer (the yellow cloud)
    can route traffic to one of the backend pods it manages (through labels, of course):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了服务负载均衡器（黄色云）如何将流量路由到其管理的后端pod之一（通过标签，当然）：
- en: '![](Images/6bee68dc-a3fc-4698-8b1a-07df51a7a14a.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/6bee68dc-a3fc-4698-8b1a-07df51a7a14a.png)'
- en: Ingress
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入口
- en: 'Ingress in Kubernetes is, at its core, a set of rules that allow inbound connections
    to reach cluster services. In addition, some ingress controllers support the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的入口在其核心是一组规则，允许入站连接到达集群服务。此外，一些入口控制器支持以下功能：
- en: Connection algorithms
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接算法
- en: Request limits
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求限制
- en: URL rewrites and redirects
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: URL重写和重定向
- en: TCP/UDP load balancing
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TCP/UDP负载平衡
- en: SSL termination
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSL终止
- en: Access control and authorization
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问控制和授权
- en: 'Ingress is specified using an ingress resource and is serviced by an ingress
    controller. It''s important to note that ingress is still in beta and it doesn''t
    yet cover all of the necessary capabilities. Here is an example of an ingress
    resource that manages traffic into two services. The rules map the externally
    visible `http:// foo.bar.com/foo` to the `s1` service and `http://foo.bar.com/bar`
    to the `s2` service:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 入口是使用入口资源指定的，并由入口控制器提供服务。重要的是要注意，入口仍处于测试阶段，尚未涵盖所有必要的功能。以下是一个管理流量进入两个服务的入口资源示例。规则将外部可见的`http://
    foo.bar.com/foo`映射到`s1`服务，将`http://foo.bar.com/bar`映射到`s2`服务：
- en: '[PRE8]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There are two official ingress controllers right now. One of them is an L7 ingress
    controller for GCE only, the other is a more general-purpose NGINX ingress controller
    that lets you configure NGINX through a ConfigMap. The NGNIX ingress controller
    is very sophisticated and brings to bear a lot of features that are not available
    yet through the ingress resource directly. It uses the endpoints API to directly
    forward traffic to pods. It supports Minikube, GCE, AWS, Azure, and bare-metal
    clusters. For a detailed review, check out [https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有两个官方的入口控制器。其中一个是专门为GCE设计的L7入口控制器，另一个是更通用的NGINX入口控制器，可以通过ConfigMap配置NGINX。NGNIX入口控制器非常复杂，并且提供了许多目前通过入口资源直接不可用的功能。它使用端点API直接将流量转发到pod。它支持Minikube、GCE、AWS、Azure和裸机集群。有关详细审查，请查看[https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx)。
- en: HAProxy
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HAProxy
- en: 'We discussed using a cloud provider external load balancer using service type
    of `LoadBalancer` and using the internal service load balancer inside the cluster
    using `ClusterIP`. If we want a custom external load balancer, we can create a
    custom external load balancer provider and use `LoadBalancer` or use the third
    service type, `NodePort`. **High Availability** (**HA**) Proxy is a mature and
    battle-tested load balancing solution. It is considered the best choice for implementing
    external load balancing with on-premises clusters. This can be done in several
    ways:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了使用云提供商外部负载均衡器，使用`LoadBalancer`服务类型以及在集群内部使用`ClusterIP`的内部服务负载均衡器。如果我们想要一个自定义的外部负载均衡器，我们可以创建一个自定义的外部负载均衡器提供程序，并使用`LoadBalancer`或使用第三种服务类型`NodePort`。**高可用性**（**HA**）代理是一个成熟且经过实战考验的负载均衡解决方案。它被认为是在本地集群中实现外部负载均衡的最佳选择。这可以通过几种方式实现：
- en: Utilize `NodePort` and carefully manage port allocations
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用`NodePort`并仔细管理端口分配
- en: Implement custom load balancer provider interface
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现自定义负载均衡器提供程序接口
- en: Run HAProxy inside your cluster as the only target of your frontend servers
    at the edge of the cluster (load balanced or not)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群内部运行HAProxy作为集群边缘前端服务器的唯一目标（无论是否经过负载平衡）
- en: 'You can use all approaches with HAProxy. Regardless, it is still recommended
    to use ingress objects. The `service-loadbalancer` project is a community project
    that implemented a load balancing solution on top of HAProxy. You can find it
    at: [https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用所有方法与HAProxy。不过，仍建议使用入口对象。`service-loadbalancer`项目是一个社区项目，它在HAProxy之上实现了一个负载均衡解决方案。您可以在[https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer)找到它。
- en: Utilizing the NodePort
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用NodePort
- en: Each service will be allocated a dedicated port from a predefined range. This
    usually is a high range, such as 30,000 and above, to avoid clashing with other
    applications using low known ports. HAProxy will run outside the cluster in this
    case, and it will be configured with the correct port for each service. Then it
    can just forward any traffic to any nodes and Kubernetes through the internal
    service, and the load balancer will route it to a proper pod (double load balancing).
    This is, of course, sub-optimal because it introduces another hop. The way to
    circumvent it is to query the Endpoints API and dynamically manage for each service
    the list of its backend pods and directly forward traffic to the pods.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务将从预定义范围中分配一个专用端口。通常这是一个较高的范围，例如30,000及以上，以避免与使用低已知端口的其他应用程序发生冲突。在这种情况下，HAProxy将在集群外部运行，并且将为每个服务配置正确的端口。然后它可以将任何流量转发到任何节点和Kubernetes通过内部服务，并且负载均衡器将其路由到适当的pod（双重负载均衡）。当然，这是次优的，因为它引入了另一个跳跃。规避它的方法是查询Endpoints
    API，并动态管理每个服务的后端pod列表，并直接将流量转发到pod。
- en: Custom load balancer provider using HAProxy
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用HAProxy自定义负载均衡器提供程序
- en: This approach is a little more complicated, but the benefit is that it is better
    integrated with Kubernetes and can make the transition to/from on-premises from/to
    the cloud easier.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法稍微复杂一些，但好处是它与Kubernetes更好地集成，可以更容易地在本地和云端之间进行过渡。
- en: Running HAProxy Inside the Kubernetes cluster
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes集群内运行HAProxy
- en: 'In this approach, we use the internal HAProxy load balancer inside the cluster.
    There may be multiple nodes running HAProxy, and they will share the same configuration
    to map incoming requests and load balance them across the backend servers (the
    Apache servers in the following diagram):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们在集群内部使用HAProxy负载均衡器。可能有多个运行HAProxy的节点，它们将共享相同的配置来映射传入请求并在后端服务器（以下图表中的Apache服务器）之间进行负载均衡。
- en: '![](Images/0585274f-5df9-40ea-923b-788ba4f6168f.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/0585274f-5df9-40ea-923b-788ba4f6168f.png)'
- en: Keepalived VIP
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keepalived VIP
- en: Keepalived **Virtual****IP** (**VIP**) is not necessarily a load balancing solution
    of its own. It can be a complement to the NGINX ingress controller or the HAProxy-based
    service `LoadBalancer`. The main motivation is that pods move around in Kubernetes,
    including your load balancer(s). That creates a problem for clients outside the
    network that require a stable endpoint. DNS is often not good enough due to performance
    issues. Keepalived provides a high-performance virtual IP address that can serve
    as the address to the NGINX ingress controller or the HAProxy load balancer. Keepalived
    utilizes core Linux networking facilities such as IPVS (IP virtual server) and
    implements high availability through **Virtual Redundancy Router Protocol** (**VRRP**).
    Everything runs at layer 4 (TCP/UDP). It takes some effort and attention to detail
    to configure it. Luckily, there is a Kubernetes `contrib` project that can get
    you started, at [https://github.com/kubernetes/contrib/tree/master/keepalived-vip](https://github.com/kubernetes/contrib/tree/master/keepalived-vip).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Keepalived **虚拟**IP（**VIP**）并不一定是一个独立的负载均衡解决方案。它可以作为NGINX入口控制器或基于HAProxy的服务`LoadBalancer`的补充。主要动机是Kubernetes中的pod会移动，包括您的负载均衡器。这对需要稳定端点的网络外客户端造成了问题。由于性能问题，DNS通常不够好。Keepalived提供了一个高性能的虚拟IP地址，可以作为NGINX入口控制器或HAProxy负载均衡器的地址。Keepalived利用核心Linux网络设施，如IPVS（IP虚拟服务器），并通过**虚拟冗余路由协议**（**VRRP**）实现高可用性。一切都在第4层（TCP/UDP）运行。配置它需要一些努力和细节的关注。幸运的是，Kubernetes有一个`contrib`项目可以帮助您入门，网址为[https://github.com/kubernetes/contrib/tree/master/keepalived-vip](https://github.com/kubernetes/contrib/tree/master/keepalived-vip)。
- en: Træfic
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Træfic
- en: 'Træfic is a modern HTTP reverse proxy and load balancer. It was designed to
    support microservices. It works with many backends, including Kubernetes, to manage
    its configuration automatically and dynamically. This is a game changer compared
    to traditional load balancers. It has an impressive list of features:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Træfic是一个现代的HTTP反向代理和负载均衡器。它旨在支持微服务。它可以与许多后端一起工作，包括Kubernetes，以自动和动态地管理其配置。与传统的负载均衡器相比，这是一个改变游戏规则的产品。它具有令人印象深刻的功能列表：
- en: It's fast
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它很快
- en: Single Go executable
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个Go可执行文件
- en: Tiny official Docker image
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微型官方Docker镜像
- en: Rest API
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rest API
- en: Hot-reloading of configuration; no need to restart the process
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 热重新加载配置；无需重新启动进程
- en: Circuit breakers, retry
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 断路器，重试
- en: Round Robin, rebalancer load-balancers
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮询，重新平衡负载均衡器
- en: Metrics (Rest, Prometheus, Datadog, Statsd, InfluxDB)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标（Rest，Prometheus，Datadog，Statsd，InfluxDB）
- en: Clean AngularJS Web UI
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 干净的AngularJS Web UI
- en: Websocket, HTTP/2, GRPC ready
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Websocket，HTTP/2，GRPC准备就绪
- en: Access Logs (JSON, CLF)
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问日志（JSON，CLF）
- en: Let's Encrypt support (Automatic HTTPS with renewal)
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持Let's Encrypt（自动HTTPS与更新）
- en: High availability with cluster mode
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有集群模式的高可用性
- en: Writing your own CNI plugin
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写自己的CNI插件
- en: 'In this section, we will look at what it takes to actually write your own CNI
    plugin. First, we will look at the simplest plugin possible – the loopback plugin.
    Then, we will examine the plugin skeleton that implements most of the boilerplate
    associated with writing a CNI plugin. Finally, we will review the implementation
    of the bridge plugin. Before we dive in, here is a quick reminder of what a CNI
    plugin is:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看看实际编写自己的CNI插件需要什么。首先，我们将看看可能的最简单的插件——环回插件。然后，我们将检查实现大部分样板与编写CNI插件相关的插件框架。最后，我们将回顾桥接插件的实现。在我们深入之前，这里是一个快速提醒CNI插件是什么：
- en: A CNI plugin is an executable
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNI插件是可执行的
- en: It is responsible for connecting new containers to the network, assigning unique
    IP addresses to CNI containers, and taking care of routing
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它负责将新容器连接到网络，为CNI容器分配唯一的IP地址，并负责路由
- en: A container is a network namespace (in Kubernetes, a pod is a CNI container)
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器是一个网络命名空间（在Kubernetes中，一个pod是一个CNI容器）
- en: Network definitions are managed as JSON files, but stream to the plugin through
    standard input (no files are being read by the plugin)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络定义以JSON文件的形式进行管理，但通过标准输入流传输到插件（插件不会读取任何文件）
- en: Auxiliary information can be provided via environment variables
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辅助信息可以通过环境变量提供
- en: First look at the loopback plugin
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 首先看看环回插件
- en: 'The loopback plugin simply adds the loopback interface. It is so simple that
    it doesn''t require any network configuration information. Most CNI plugins are
    implemented in Golang, and the loopback CNI plugin is no exception. The full source
    code is available at:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 环回插件只是添加环回接口。它非常简单，不需要任何网络配置信息。大多数CNI插件都是用Golang实现的，环回CNI插件也不例外。完整的源代码可在以下链接找到：
- en: '[https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback](https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback](https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback)'
- en: 'Let''s look at the imports first. There are multiple packages from the container
    networking project on GitHub that provide many of the building blocks necessary
    to implement CNI plugins and the `netlink` package for adding and removing interfaces,
    as well as setting IP addresses and routes. We will look at the `skel` package
    soon:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看一下导入。来自GitHub上的容器网络项目的多个软件包提供了实现CNI插件和`netlink`软件包所需的许多构建块，用于添加和删除接口，以及设置IP地址和路由。我们很快将看到`skel`软件包：
- en: '[PRE9]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, the plugin implements two commands, `cmdAdd` and `cmdDel`, which are
    called when a `container` is added to or removed from the network. Here is the
    `cmdAdd` command:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，插件实现了两个命令，`cmdAdd`和`cmdDel`，当`container`被添加到或从网络中移除时调用。以下是`cmdAdd`命令：
- en: '[PRE10]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The core of this function is setting the interface name to `lo` (for loopback)
    and adding the link to the container''s network namespace. The `del` command does
    the opposite:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 该功能的核心是将接口名称设置为`lo`（用于环回），并将链接添加到容器的网络命名空间中。`del`命令则相反：
- en: '[PRE11]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `main` function simply calls the `skel` package, passing the command functions.
    The `skel` package will take care of running the CNI plugin executable and will
    invoke the `addCmd` and `delCmd` functions at the right time:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`main`函数只是简单地调用`skel`包，传递命令函数。`skel`包将负责运行CNI插件可执行文件，并在适当的时候调用`addCmd`和`delCmd`函数：'
- en: '[PRE12]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Building on the CNI plugin skeleton
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建CNI插件骨架
- en: 'Let''s explore the `skel` package and see what it does under the covers. Starting
    with the `PluginMain()` entry point, it is responsible for invoking `PluginMainWithError()`,
    catching errors, printing them to standard output, and exiting:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索`skel`包，并了解其在内部的工作原理。从`PluginMain()`入口点开始，它负责调用`PluginMainWithError()`，捕获错误，将其打印到标准输出并退出：
- en: '[PRE13]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `PluginErrorWithMain()` instantiates a dispatcher, sets it up with all
    the I/O streams and the environment, and invokes its `PluginMain()` method:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '`PluginErrorWithMain()`实例化一个分发器，设置它与所有I/O流和环境，并调用其`PluginMain()`方法：'
- en: '[PRE14]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is, finally, the main logic of the skeleton. It gets the `cmd` arguments
    from the environment (which includes the configuration from standard input), detects
    which `cmd` is invoked, and calls the appropriate `plugin` function (`cmdAdd`
    or `cmdDel`). It can also return version information:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是骨架的主要逻辑。它从环境中获取`cmd`参数（其中包括来自标准输入的配置），检测调用了哪个`cmd`，并调用适当的`plugin`函数（`cmdAdd`或`cmdDel`）。它还可以返回版本信息：
- en: '[PRE15]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Reviewing the bridge plugin
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查桥接插件
- en: 'The bridge plugin is more substantial. Let''s look at some of the key parts
    of its implementation. The full source code is available at:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接插件更为重要。让我们看一下其实现的一些关键部分。完整的源代码可在以下链接找到：
- en: '[https://github.com/containernetworking/plugins/blob/master/plugins/main/bridge](https://github.com/containernetworking/plugins/blob/master/plugins/main/bridge).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/containernetworking/plugins/blob/master/plugins/main/bridge](https://github.com/containernetworking/plugins/blob/master/plugins/main/bridge)。'
- en: 'It defines a network configuration `struct` with the following fields:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 它定义了一个网络配置`struct`，具有以下字段：
- en: '[PRE16]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will not cover what each parameter does and how it interacts with the other
    parameters due to space limitations. The goal is to understand the flow and have
    a starting point if you want to implement your own CNI plugin. The configuration
    is loaded from JSON through the `loadNetConf()` function. It is called at the
    beginning of the `cmdAdd()` and `cmdDel()` functions:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 由于空间限制，我们将不会涵盖每个参数的作用以及它如何与其他参数交互。目标是理解流程，并且如果您想要实现自己的CNI插件，这将是一个起点。配置通过`loadNetConf()`函数从JSON加载。它在`cmdAdd()`和`cmdDel()`函数的开头被调用：
- en: '[PRE17]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the core of the `cmdAdd()` function. It uses information from network
    configuration, sets up a `veth`, interacts with the IPAM plugin to add a proper
    IP address, and returns the results:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`cmdAdd()`函数的核心。它使用来自网络配置的信息，设置了一个`veth`，与IPAM插件交互以添加适当的IP地址，并返回结果：
- en: '[PRE18]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is just part of the full implementation. There is also route setting and
    hardware IP allocation. I encourage you to pursue the full source code, which
    is quite extensive, to get the full picture.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是完整实现的一部分。还有路由设置和硬件IP分配。我鼓励您追求完整的源代码，这是相当广泛的，以获得全貌。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered a lot of ground. Networking is such a vast topic
    and there are so many combinations of hardware, software, operating environments,
    and user skills that coming up with a comprehensive networking solution that is
    robust, secure, performs well, and is easy to maintain, is a very complicated
    endeavor. For Kubernetes clusters, the cloud providers mostly solve these issues.
    But if you run on-premise clusters or need a tailor-made solution, you get a lot
    of options to choose from. Kubernetes is a very flexible platform, designed for
    extension. Networking in particular is totally pluggable. The main topics we discussed
    were the Kubernetes networking model (flat address space where pods can reach
    others and shared localhost between all containers inside a pod), how lookup and
    discovery work, the Kubernetes network plugins, various networking solutions at
    different levels of abstraction (a lot of interesting variations), using network
    policies effectively to control the traffic inside the cluster, the spectrum of
    load balancing solutions, and finally we looked at how to write a CNI plugin by
    dissecting a real-world implementation.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了很多内容。网络是一个如此广泛的主题，有如此多的硬件、软件、操作环境和用户技能的组合，要想提出一个全面的网络解决方案，既稳健、安全、性能良好又易于维护，是一项非常复杂的工作。对于Kubernetes集群，云提供商大多解决了这些问题。但如果您在本地运行集群或需要定制解决方案，您有很多选择。Kubernetes是一个非常灵活的平台，设计用于扩展。特别是网络是完全可插拔的。我们讨论的主要主题是Kubernetes网络模型（平面地址空间，其中pod可以访问其他pod，并且在pod内部所有容器之间共享本地主机），查找和发现的工作原理，Kubernetes网络插件，不同抽象级别的各种网络解决方案（许多有趣的变体），有效使用网络策略来控制集群内部的流量，负载均衡解决方案的范围，最后我们看了如何通过剖析真实实现来编写CNI插件。
- en: At this point, you are probably overwhelmed, especially if you're not a subject-matter
    expert. You should have a good grasp of the internals of Kubernetes networking,
    be aware of all the interlocking pieces required to implement a fully-fledged
    solution, and be able to craft your own solution based on trade-offs that make
    sense for your system.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您可能会感到不知所措，特别是如果您不是专家。您应该对Kubernetes网络的内部有很好的理解，了解实现完整解决方案所需的所有相互关联的部分，并能够根据对系统有意义的权衡来制定自己的解决方案。
- en: In [Chapter 11](11dc34c4-beb4-4ec9-bc2a-fcd0f50bf74c.xhtml), *Running Kubernetes
    on Multiple Clouds and Cluster Federation*, we will go even bigger and look at
    running Kubernetes on multiple clusters, cloud providers, and federation. This
    is an important part of the Kubernetes story for geo-distributed deployments and
    ultimate scalability. Federated Kubernetes clusters can exceed local limitations,
    but they bring a whole slew of challenges too.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](11dc34c4-beb4-4ec9-bc2a-fcd0f50bf74c.xhtml)中，*在多个云和集群联合上运行Kubernetes*，我们将更进一步，看看如何在多个集群、云提供商和联合上运行Kubernetes。这是Kubernetes故事中的一个重要部分，用于地理分布式部署和最终可扩展性。联合的Kubernetes集群可以超越本地限制，但它们也带来了一系列挑战。
