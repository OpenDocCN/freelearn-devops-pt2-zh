["```\nsystemctl enable docker \nsystemctl enable kublet \n```", "```\n> helm init \nCreating /Users/gigi.sayfan/.helm \nCreating /Users/gigi.sayfan/.helm/repository \nCreating /Users/gigi.sayfan/.helm/repository/cache \nCreating /Users/gigi.sayfan/.helm/repository/local \nCreating /Users/gigi.sayfan/.helm/plugins \nCreating /Users/gigi.sayfan/.helm/starters \nCreating /Users/gigi.sayfan/.helm/cache/archive \nCreating /Users/gigi.sayfan/.helm/repository/repositories.yaml \nAdding stable repo with URL: https://kubernetes-charts.storage.googleapis.com \nAdding local repo with URL: http://127.0.0.1:8879/charts \n$HELM_HOME has been configured at /Users/gigi.sayfan/.helm. \n\nTiller (the Helm server-side component) has been installed into your Kubernetes Cluster. \nHappy Helming! \n```", "```\n# Wide open access to the cluster (mostly for kubelet) \nkind: ClusterRole \napiVersion: rbac.authorization.k8s.io/v1beta1 \nmetadata: \n  name: cluster-writer \nrules: \n  - apiGroups: [\"*\"] \n    resources: [\"*\"] \n    verbs: [\"*\"] \n  - nonResourceURLs: [\"*\"] \n    verbs: [\"*\"] \n\n--- \n\n# Full read access to the api and resources \nkind: ClusterRole \napiVersion: rbac.authorization.k8s.io/v1beta1metadata: \n  name: cluster-reader \nrules: \n  - apiGroups: [\"*\"] \n    resources: [\"*\"] \n    verbs: [\"get\", \"list\", \"watch\"] \n  - nonResourceURLs: [\"*\"] \n    verbs: [\"*\"] \n--- \n# Give admin, kubelet, kube-system, kube-proxy god access \nkind: ClusterRoleBinding \napiVersion: rbac.authorization.k8s.io/v1beta1metadata: \n  name: cluster-write \nsubjects: \n  - kind: User \n    name: admin \n  - kind: User \n    name: kubelet \n  - kind: ServiceAccount \n    name: default \n    namespace: kube-system \n  - kind: User \n    name: kube-proxy \nroleRef: \n  kind: ClusterRole \n  name: cluster-writer \n  apiGroup: rbac.authorization.k8s.io \n```", "```\nkubectl apply -f rbac.yaml.  \n```", "```\n> helm install stable/etcd-operator --name x \nNAME:   x \nLAST DEPLOYED: Sun Jan  7 19:29:17 2018 \nNAMESPACE: default \nSTATUS: DEPLOYED \n\nRESOURCES: \n==> v1beta1/ClusterRole \nNAME                           AGE \nx-etcd-operator-etcd-operator  1s \n\n==> v1beta1/ClusterRoleBinding \nNAME                                   AGE \nx-etcd-operator-etcd-backup-operator   1s \nx-etcd-operator-etcd-operator          1s \nx-etcd-operator-etcd-restore-operator  1s \n\n==> v1/Service \nNAME                   TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)    AGE \netcd-restore-operator  ClusterIP  10.96.236.40  <none>       19999/TCP  1s \n\n==> v1beta1/Deployment \nNAME                                   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE \nx-etcd-operator-etcd-backup-operator   1        1        1         0          1s \nx-etcd-operator-etcd-operator          1        1        1         0          1s \nx-etcd-operator-etcd-restore-operator  1        1        1         0          1s \n\n==> v1/ServiceAccount \nNAME                                   SECRETS  AGE \nx-etcd-operator-etcd-backup-operator   1        1s \nx-etcd-operator-etcd-operator          1        1s \nx-etcd-operator-etcd-restore-operator  1        1s \n\nNOTES: \n1\\. etcd-operator deployed. \n  If you would like to deploy an etcd-cluster set cluster.enabled to true in values.yaml \n  Check the etcd-operator logs \n    export POD=$(kubectl get pods -l app=x-etcd-operator-etcd-operator --namespace default --output name) \n    kubectl logs $POD --namespace=default \n```", "```\napiVersion: \"etcd.database.coreos.com/v1beta2\" \nkind: \"EtcdCluster\" \nmetadata: \n  name: \"etcd-cluster\" \nspec: \n  size: 3 \n  version: \"3.2.13\" \n```", "```\n> k create -f etcd-cluster.yaml\netcdcluster \"etcd-cluster\" created\n\nLet's verify the cluster pods were created properly:\n> k get pods | grep etcd-cluster\netcd-cluster-0000                         1/1       Running   0          4m\netcd-cluster-0001                         1/1       Running   0          4m\netcd-cluster-0002                         1/1       Running   0          4m\n\n```", "```\n> k exec etcd-cluster-0000 etcdctl cluster-health\nmember 898a228a043c6ef0 is healthy: got healthy result from http://etcd-cluster-0001.etcd-cluster.default.svc:2379\nmember 89e2f85069640541 is healthy: got healthy result from http://etcd-cluster-0002.etcd-cluster.default.svc:2379\nmember 963265fbd20597c6 is healthy: got healthy result from http://etcd-cluster-0000.etcd-cluster.default.svc:2379\ncluster is healthy  \n```", "```\n> k exec etcd-cluster-0000 etcdctl set test \"Yeah, it works\"\nYeah, it works\n> k exec etcd-cluster-0000 etcdctl get test  \n```", "```\ncommand:\n- /bin/sh\n- -c\n- /usr/local/bin/kube-scheduler --master=127.0.0.1:8080 --v=2 --leader-elect=true 1>>/var/log/kube-scheduler.log\n2>&1\n```", "```\n- command:\n- /bin/sh\n- -c\n- /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns\n--cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce  --service-account-private-key-file=/srv/kubernetes/server.key\n--v=2 --leader-elect=true 1>>/var/log/kube-controller-manager.log 2>&1\nimage: gcr.io/google_containers/kube-controller-manager:fda24638d51a48baa13c35337fcd4793 \n```", "```\n> kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.5 --replicas=3 -- --election=election -http=0.0.0.0:4040\n```", "```\n> kubectl get pods | grep elect\nleader-elector-57746fd798-7s886                1/1       Running   0          39s\nleader-elector-57746fd798-d94zx                1/1       Running   0          39s\nleader-elector-57746fd798-xcljl                1/1       Running   0          39s \n```", "```\n    > kubectl get endpoints election -o json\n    {\n        \"apiVersion\": \"v1\",\n        \"kind\": \"Endpoints\",\n        \"metadata\": {\n            \"annotations\": {\n                \"control-plane.alpha.kubernetes.io/leader\": \"{\\\"holderIdentity\\\":\\\"leader-elector-57746fd798-xcljl\\\",\\\"leaseDurationSeconds\\\":10,\\\"acquireTime\\\":\\\"2018-01-08T04:16:40Z\\\",\\\"renewTime\\\":\\\"2018-01-08T04:18:26Z\\\",\\\"leaderTransitions\\\":0}\"\n            },\n            \"creationTimestamp\": \"2018-01-08T04:16:40Z\",\n            \"name\": \"election\",\n            \"namespace\": \"default\",\n            \"resourceVersion\": \"1090942\",\n            \"selfLink\": \"/api/v1/namespaces/default/endpoints/election\",\n            \"uid\": \"ba42f436-f42a-11e7-abf8-080027c94384\"\n        },\n        \"subsets\": null\n    }  \n```", "```\n> kubectl get endpoints election -o json | jq -r .metadata.annotations[] | jq .holderIdentity\n\"leader-elector-57746fd798-xcljl\"\n```", "```\n> kubectl delete pod leader-elector-916043122-10wjj\npod \"leader-elector-57746fd798-xcljl\" deleted \n```", "```\n> kubectl get endpoints election -o json | jq -r .metadata.annotations[] | jq .holderIdentity\n\"leader-elector-57746fd798-d94zx\"  \n```", "```\n> kubectl proxy \nIn a separate console:\n\n> curl http://localhost:8001/api/v1/proxy/namespaces/default/pods/leader-elector-57746fd798-d94zx:4040/ | jq .name\n\"leader-elector-57746fd798-d94zx\"  \n```", "```\napiVersion: extensions/v1beta1 \nkind: Deployment \nmetadata: \n  name: nginx-deployment \nspec: \n  replicas: 3 \n  template: \n    metadata: \n      labels: \n        app: nginx \n    spec: \n      containers: \n      - name: nginx \n        image: nginx:1.7.9 \n        ports: \n        - containerPort: 80 \n```", "```\n$ kubectl create -f nginx-deployment.yaml --record  \n```", "```\n$ kubectl rollout status deployment/nginx-deployment  \n```"]