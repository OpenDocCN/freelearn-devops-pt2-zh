["```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd src/chapter9/rbac\n```", "```\n$ kubectl get clusterroles\n$ kubectl get clusterrolebindings\n```", "```\n$ kubectl get clusterroles system:node -oyaml\n```", "```\n$ kubectl get clusterroles | grep -v '^system'\nNAME AGE\nadmin 8d #gives read-write access\n to all resources\ncluster-admin 8d #super-user, gives read-write access\n to all resources\nedit 8d #allows create/update/delete on resources except RBAC permissions\nkops:dns-controller 8d\nkube-dns-autoscaler 8d\nview 8d #read-only access to resources\n```", "```\n$ kubectl get clusterrolebindings/cluster-admin -o yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n...\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: ClusterRole\n name: cluster-admin\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n kind: Group\n name: system:masters\n```", "```\n$ openssl genrsa -out user3445.key 2048\n```", "```\n$ openssl req -new -key user3445.key \\\n-out user3445.csr \\\n-subj \"/CN=john.geek/O=development\"\n```", "```\n$ openssl x509 -req -in user3445.csr \\\n-CA CERT_LOCATION/ca.crt \\\n-CAkey CERT_LOCATION/ca.key \\\n-CAcreateserial -out user3445.crt \\\n-days 500\n```", "```\nSignature ok\nsubject=CN = john.geek, O = development\nGetting CA Private Key\n```", "```\n$ kubectl config set-credentials user3445 --client-certificate=user3445.crt --client-key=user3445.key\n$ kubectl config set-context user3445-context --cluster=local --namespace=secureapp --user=user3445\n```", "```\n$ kubectl config get-contexts\nCURRENT NAME                    CLUSTER AUTHINFO NAMESPACE\n*       service-account-context local   kubecfg\n user3445-context        local   user3445 secureapp\n```", "```\n$ kubectl --context=user3445-context get pods\nError from server (Forbidden): pods is forbidden: User \"john.geek\" cannot list resource \"pods\" in API group \"\" in the namespace \"secureapps\"\n```", "```\n$ cat config-user3445.yaml\n```", "```\n$ kubectl create ns secureapp\n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n namespace: secureapp\n name: deployer\nrules:\n- apiGroups: [\"\", \"extensions\", \"apps\"]\n resources: [\"deployments\", \"replicasets\", \"pods\"]\n verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\nEOF\n```", "```\n$ cat <<EOF | kubectl apply -f -\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n name: deployer-binding\n namespace: secureapp\nsubjects:\n- kind: User\n name: john.geek\n apiGroup: \"\"\nroleRef:\n kind: Role\n name: deployer\n apiGroup: \"\"\nEOF\n```", "```\n$ cat <<EOF | kubectl --context=user3445-context apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n name: busybox\n namespace: secureapp\nspec:\n containers:\n - image: busybox\n command:\n - sleep\n - \"3600\"\n imagePullPolicy: IfNotPresent\n name: busybox\n restartPolicy: Always\nEOF\n```", "```\n$ kubectl --context=user3445-context get pods \nNAME    READY STATUS  RESTARTS AGE\nbusybox 1/1   Running 1        2m\n```", "```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd src/chapter9/psp\n```", "```\n$ kubectl get psp eks.privileged\nNAME           PRIV CAPS SELINUX  RUNASUSER FSGROUP  SUPGROUP READONLYROOTFS VOLUMES\neks.privileged true *    RunAsAny RunAsAny  RunAsAny RunAsAny false          *\n```", "```\n$ kubectl describe psp eks.privileged\n```", "```\n$ gcloud beta container clusters update k8s-devops-cookbook-1 --enable-pod-security-policy\n```", "```\n$ kubectl get psp\nNAME                         PRIV  CAPS SELINUX  RUNASUSER FSGROUP  SUPGROUP READONLYROOTFS VOLUMES\ngce.event-exporter           false      RunAsAny RunAsAny  RunAsAny RunAsAny false          hostPath,secret\ngce.fluentd-gcp              false      RunAsAny RunAsAny  RunAsAny RunAsAny false          configMap,hostPath,secret\ngce.persistent-volume-binder false      RunAsAny RunAsAny  RunAsAny RunAsAny false          nfs,secret,projected\ngce.privileged               true  *    RunAsAny RunAsAny  RunAsAny RunAsAny false          *\ngce.unprivileged-addon       false SETPCAP,MKNOD,AUDIT_WRITE,CHOWN,NET_RAW,DAC_OVERRIDE,FOWNER,FSETID,KILL,SETGID,SETUID,NET_BIND_SERVICE,SYS_CHROOT,SETFCAP RunAsAny RunAsAny RunAsAny RunAsAny false emptyDir,configMap,secret,projected\n```", "```\n$ kubectl describe psp gce.privileged\n```", "```\n$ az aks create --resource-group k8sdevopscookbook \\\n--name AKSCluster \\\n--enable-pod-security-policy\n```", "```\n$ kubectl get psp\nNAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP READONLYROOTFS VOLUMES\nprivileged true * RunAsAny RunAsAny RunAsAny RunAsAny false * configMap,emptyDir,projected,secret,downwardAPI,persistentVolumeClaim\n```", "```\n$ kubectl describe psp privileged\n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: extensions/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n name: restricted-psp\nspec:\n privileged: false\n runAsUser:\n rule: MustRunAsNonRoot\n seLinux:\n rule: RunAsAny\n fsGroup:\n rule: RunAsAny\n supplementalGroups:\n rule: RunAsAny\n volumes:\n - '*'\nEOF\n```", "```\n$ kubectl get psp restricted-psp\nNAME           PRIV  CAPS     SELINUX  RUNASUSER        FSGROUP   SUPGROUP READONLYROOTFS VOLUMES\nrestricted-psp false          RunAsAny MustRunAsNonRoot RunAsAny  RunAsAny false          *\n```", "```\n$ kubectl run --image=mariadb:10.4.8 mariadb --port=3306 --env=\"MYSQL_ROOT_PASSWORD=my-secret-pw\"\n$ kubectl get pods\nNAME                     READY STATUS                                                RESTARTS AGE\nmariadb-5584b4f9d8-q6whd 0/1   container has runAsNonRoot and image will run as root 0        46s\n```", "```\n$ cat <<EOF | kubectl apply -f -\nkind: PodSecurityPolicy\nmetadata:\n name: restricted-vol-psp\nspec:\n privileged: false\n runAsUser:\n rule: RunAsAny\n seLinux:\n rule: RunAsAny\n fsGroup:\n rule: RunAsAny\n supplementalGroups:\n rule: RunAsAny\n volumes:\n - 'nfs'\nEOF\n```", "```\n$ kubectl create -f \\\nhttps://raw.githubusercontent.com/k8sdevopscookbook/src/master/chapter6/minio/minio.yaml\n```", "```\n$ kubectl delete psp restricted-vol-psp\n$ kubectl delete -f \\\nhttps://raw.githubusercontent.com/k8sdevopscookbook/src/master/chapter6/minio/minio.yaml\n```", "```\n$ cat <<EOF | kubectl apply -f -\nkind: PodSecurityPolicy\nmetadata:\n name: permit-pvc-psp\nspec:\n privileged: false\n runAsUser:\n rule: RunAsAny\n seLinux:\n rule: RunAsAny\n fsGroup:\n rule: RunAsAny\n supplementalGroups:\n rule: RunAsAny\n volumes:\n - 'persistentVolumeClaim'\nEOF\n```", "```\n$ git clone https://github.com/sysdiglabs/kube-psp-advisor\n$ cd kube-psp-advisor && make build\n```", "```\n$ ./kube-psp-advisor --namespace=secureapp > psp-advisor.yaml\n```", "```\n$ cat psp-advisor.yaml\n$ kubectl apply -f psp-advisor.yaml\n```", "```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd src/chapter9/cis\n```", "```\n$ curl --silent --location \"https://github.com/aquasecurity/kube-bench/releases/download/v0.1.0/kube-bench_0.1.0_linux_amd64.tar.gz\" | tar xz -C /tmp\n$ sudo mv /tmp/kube-bench /usr/local/bin\n```", "```\n$ kube-bench master\n...\n== Summary ==\n31 checks PASS\n36 checks FAIL\n24 checks WARN\n1 checks INFO\n```", "```\n$ kube-bench master > kube-bench-master.txt\n```", "```\n[INFO] 1 Master Node Security Configuration\n[INFO] 1.1 API Server\n[PASS] 1.1.1 Ensure that the --anonymous-auth argument is set to false (Not Scored)\n[FAIL] 1.1.2 Ensure that the --basic-auth-file argument is not set (Scored)\n[PASS] 1.1.3 Ensure that the --insecure-allow-any-token argument is not set (Not Scored)\n[PASS] 1.1.4 Ensure that the --kubelet-https argument is set to true (Scored)\n[FAIL] 1.1.5 Ensure that the --insecure-bind-address argument is not set (Scored)\n[FAIL] 1.1.6 Ensure that the --insecure-port argument is set to 0 (Scored)\n[PASS] 1.1.7 Ensure that the --secure-port argument is not set to 0 (Scored)\n[FAIL] 1.1.8 Ensure that the --profiling argument is set to false (Scored)\n[FAIL] 1.1.9 Ensure that the --repair-malformed-updates argument is set to false (Scored)\n[PASS] 1.1.10 Ensure that the admission control plugin AlwaysAdmit is not set (Scored)\n...\n```", "```\n== Remediations ==\n1.1.2 Follow the documentation and configure alternate mechanisms for authentication. Then,\nedit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.manifest\non the master node and remove the --basic-auth-file=<filename>\nparameter.\n\n1.1.5 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.manife$\non the master node and remove the --insecure-bind-address\nparameter.\n\n1.1.6 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.manife$\napiserver.yaml on the master node and set the below parameter.\n--insecure-port=0\n\n1.1.8 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.manife$\non the master node and set the below parameter.\n--profiling=false\n\n1.2.1 Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube-scheduler.manifest\nfile on the master node and set the below parameter.\n--profiling=false\n...\n```", "```\n...\nspec:\n containers:\n - command:\n - /bin/sh\n - -c\n - mkfifo /tmp/pipe; (tee -a /var/log/kube-scheduler.log < /tmp/pipe & ) ; exec\n /usr/local/bin/kube-scheduler --profiling=False --kubeconfig=/var/lib/kube-scheduler/kubeconfig\n --leader-elect=true --v=2 > /tmp/pipe 2>&1\n...\n```", "```\n$ kube-bench master\n...\n== Summary ==\n32 checks PASS\n35 checks FAIL\n24 checks WARN\n1 checks INFO\n```", "```\n$ kube-bench node\n...\n== Summary ==\n9 checks PASS\n12 checks FAIL\n2 checks WARN\n1 checks INFO\n```", "```\n$ kube-bench node > kube-bench-worker.txt\n```", "```\n[INFO] 2 Worker Node Security Configuration\n[INFO] 2.1 Kubelet\n[PASS] 2.1.1 Ensure that the --anonymous-auth argument is set to false (Scored)\n[FAIL] 2.1.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Scored)\n[PASS] 2.1.3 Ensure that the --client-ca-file argument is set as appropriate (Scored)\n...\n```", "```\n$ aws ecr create-repository --repository-name k8sdevopscookbook/kube-bench --image-tag-mutability MUTABLE\n```", "```\n$ git clone https://github.com/aquasecurity/kube-bench.git\n```", "```\n$ $(aws ecr get-login --no-include-email --region us-west-2)\n```", "```\n$ docker build -t k8sdevopscookbook/kube-bench \n```", "```\n$ docker tag k8sdevopscookbook/kube-bench:latest <AWS_ACCT_NUMBER>.dkr.ecr.us-west-2.amazonaws.com/k8s/kube-bench:latest\n# docker push <AWS_ACCT_NUMBER>.dkr.ecr.us-west-2.amazonaws.com/k8s/kube-bench:latest\n```", "```\napiVersion: batch/v1\nkind: Job\nmetadata:\n name: kube-bench\nspec:\n template:\n spec:\n hostPID: true\n containers:\n - name: kube-bench\n # Push the image to your ECR and then refer to it here\n image: 316621595343.dkr.ecr.us-west-2.amazonaws.com/k8sdevopscookbook/kube-bench:latest\n...\n```", "```\n$ kubectl apply -f job-eks.yaml\n```", "```\n$ kubectl get pods |grep kube-bench\nkube-bench-7lxzn 0/1 Completed 0 5m\n```", "```\n$ kubectl logs kube-bench-7lxzn\n```", "```\n$ kube-bench master --version ocp-3.11\n```", "```\n$ kube-bench master --version ocp-3.11 > kube-bench-master.txt\n```", "```\n$ kube-bench node --version ocp-3.11 > kube-bench-node.txt\n```", "```\n$ kube-bench master --version 1.14\n```", "```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd src/chapter9\n```", "```\n$ VERSION=$(curl --silent \"https://api.github.com/repos/aquasecurity/trivy/releases/latest\" | \\\ngrep '\"tag_name\":' | \\\nsed -E 's/.*\"v([^\"]+)\".*/\\1/')\n```", "```\n$ curl --silent --location \"https://github.com/aquasecurity/trivy/releases/download/v${VERSION}/trivy_${VERSION}_Linux-64bit.tar.gz\" | tar xz -C /tmp\n$ sudo mv /trivy /usr/local/bin\n```", "```\n$ trivy --version\ntrivy version 0.1.7\n```", "```\n$ trivy postgres:12.0\n2019-11-12T04:08:02.013Z INFO Updating vulnerability database...\n2019-11-12T04:08:07.088Z INFO Detecting Debian vulnerabilities...\n\npostgres:12.0 (debian 10.1)\n===========================\nTotal: 164 (UNKNOWN: 1, LOW: 26, MEDIUM: 122, HIGH: 14, CRITICAL: 1)\n...\n```", "```\n+-----------+---------------+----------+----------+-----------+------+\n| LIBRARY   | V ID          | SEVERITY | INST VER | FIXED VER | TITLE|\n+-----------+------------------+-------+----------+-----------+------+\n| apt       | CVE-2011-3374 | LOW      | 1.8.2    |           |      |\n+-----------+---------------+          +----------+-----------+------+\n| bash      | TEMP-0841856  |          | 5.0-4    |           |      |\n+-----------+---------------+          +----------+-----------+------+\n| coreutils | CVE-2016-2781 |          | 8.30-3   |           |      |\n+           +---------------+          +          +-----------+------+\n|           | CVE-2017-18018|          |          |           |      |\n+-----------+---------------+----------+----------+-----------+------+\n| file      | CVE-2019-18218| HIGH     | 1:5.35-4 | 1:5.35-4+d| file:|\n...\n```", "```\n$ vim .gitlab-ci.yml\n```", "```\nstages:\n - build\n - vulTest\n - staging\n - production\n#Add the Step 3 here\n```", "```\ntrivy:\n stage: vulTest\n image: docker:stable-git\n before_script:\n - docker build -t trivy-ci-test:${CI_COMMIT_REF_NAME} .\n - export VERSION=$(curl --silent \"https://api.github.com/repos/aquasecurity/trivy/releases/latest\" | grep '\"tag_name\":' | sed -E 's/.*\"v([^\"]+)\".*/\\1/')\n - wget https://github.com/aquasecurity/trivy/releases/download/v${VERSION}/trivy_${VERSION}_Linux-64bit.tar.gz\n - tar zxvf trivy_${VERSION}_Linux-64bit.tar.gz\n variables:\n DOCKER_DRIVER: overlay2\n allow_failure: true\n services:\n - docker:stable-dind\n#Add the Step 4 here\n```", "```\n script:\n - ./trivy --exit-code 0 --severity HIGH --no-progress --auto-refresh trivy-ci-test:${CI_COMMIT_REF_NAME}\n - ./trivy --exit-code 1 --severity CRITICAL --no-progress --auto-refresh trivy-ci-test:${CI_COMMIT_REF_NAME}\n cache:\n directories:\n - $HOME/.cache/trivy\n```", "```\n$ vim .circleci/config.yml\n```", "```\njobs:\n build:\n docker:\n - image: docker:18.09-git\n#Add the Step 3 here\n```", "```\n steps:\n - checkout\n - setup_remote_docker\n - restore_cache:\n key: vulnerability-db\n - run:\n name: Build image\n command: docker build -t trivy-ci-test:${CIRCLE_SHA1} .\n#Add the Step 4 here\n```", "```\n - run:\n name: Install trivy\n command: |\n apk add --update curl\n VERSION=$(\n curl --silent \"https://api.github.com/repos/aquasecurity/trivy/releases/latest\" | \\\n grep '\"tag_name\":' | \\\n sed -E 's/.*\"v([^\"]+)\".*/\\1/'\n )\n\n wget https://github.com/aquasecurity/trivy/releases/download/v${VERSION}/trivy_${VERSION}_Linux-64bit.tar.gz\n tar zxvf trivy_${VERSION}_Linux-64bit.tar.gz\n mv trivy /usr/local/bin\n#Add the Step 5 here\n```", "```\n - run:\n name: Scan the local image with trivy\n command: trivy --exit-code 1 --severity CRITICAL --no-progress --auto-refresh trivy-ci-test:${CIRCLE_SHA1}\n - save_cache:\n key: vulnerability-db\n paths:\n - $HOME/.cache/trivy\n#Add the Step 6 here\n```", "```\nworkflows:\n version: 2\n release:\n jobs:\n - build\n```", "```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd src/chapter9\n```", "```\n$ git clone https://github.com/falcosecurity/falco.git\n$ cd falco/integrations/k8s-using-daemonset/k8s-with-rbac\n```", "```\n$ kubectl create -f falco-account.yaml\n```", "```\n$ kubectl create -f falco-service.yaml\n```", "```\n$ mkdir config\n$ cp ../../../falco.yaml config/\n$ cp ../../../rules/falco_rules.* config/\n$ cp ../../../rules/k8s_audit_rules.yaml config/\n```", "```\n$ kubectl create configmap falco-config --from-file=config\n```", "```\n$ kubectl create -f falco-daemonset-configmap.yaml\n```", "```\n$ kubectl get pods | grep falco-daemonset\nfalco-daemonset-94p8w 1/1 Running 0 2m34s\nfalco-daemonset-c49v5 1/1 Running 0 2m34s\nfalco-daemonset-htrxw 1/1 Running 0 2m34s\nfalco-daemonset-kwms5 1/1 Running 0 2m34s\n```", "```\n$ cat config/falco_rules.yaml\n$ cat config/falco_rules.local.yaml\n```", "```\n- rule: Disallowed SSH Connection\n- rule: Launch Disallowed Container\n- rule: Contact K8S API Server From Container\n- rule: Unexpected K8s NodePort Connection\n- rule: Launch Suspicious Network Tool in Container\n- rule: Create Symlink Over Sensitive Files\n- rule: Detect crypto miners using the Stratum protocol\n```", "```\n$ kubectl get pods | grep falco-daemonset\nfalco-daemonset-94p8w 1/1 Running 0 2m34s\nfalco-daemonset-c49v5 1/1 Running 0 2m34s\nfalco-daemonset-htrxw 1/1 Running 0 2m34s\nfalco-daemonset-kwms5 1/1 Running 0 2m34s\n```", "```\n$ kubectl exec -it falco-daemonset-94p8w bash\n$ kubectl logs falco-daemonset-94p8w  \n```", "```\n{\"output\":\"00:58:23.798345403: Notice A shell was spawned in a container with an attached terminal (user=root k8s.ns=default k8s.pod=falco-daemonset-94p8w container=0fcbc74d1b4c shell=bash parent=docker-runc cmdline=bash terminal=34816 container_id=0fcbc74d1b4c image=falcosecurity/falco) k8s.ns=default k8s.pod=falco-daemonset-94p8w container=0fcbc74d1b4c k8s.ns=default k8s.pod=falco-daemonset-94p8w container=0fcbc74d1b4c\",\"priority\":\"Notice\",\"rule\":\"Terminal shell in container\",\"time\":\"2019-11-13T00:58:23.798345403Z\", \"output_fields\": {\"container.id\":\"0fcbc74d1b4c\",\"container.image.repository\":\"falcosecurity/falco\",\"evt.time\":1573606703798345403,\"k8s.ns.name\":\"default\",\"k8s.pod.name\":\"falco-daemonset-94p8w\",\"proc.cmdline\":\"bash\",\"proc.name\":\"bash\",\"proc.pname\":\"docker-runc\",\"proc.tty\":34816,\"user.name\":\"root\"}}\n```", "```\n$ cd src/chapter9/falco\n```", "```\n$ kubectl create ns falcotest\n```", "```\n$ kubectl create -f mysql.yaml\n$ kubectl create -f ping.yaml\n$ kubectl create -f client.yaml\n```", "```\n$ kubectl exec client -n falcotest -- curl -F \"s=OK\" -F \"user=bob\" -F \"passwd=foobar\" -F \"ipaddr=localhost\" -X POST http://ping/ping.php\n```", "```\n$ vim config/falco_rules.local.yaml\n```", "```\n - rule: Unauthorized process\n desc: There is a running process not described in the base template\n condition: spawned_process and container and k8s.ns.name=falcotest and k8s.deployment.name=ping and not proc.name in (apache2, sh, ping)\n output: Unauthorized process (%proc.cmdline) running in (%container.id)\n priority: ERROR\n tags: [process]\n```", "```\n$ kubectl delete -f falco-daemonset-configmap.yaml\n$ kubectl create configmap falco-config --from-file=config --dry-run --save-config -o yaml | kubectl apply -f -\n$ kubectl apply -f falco-daemonset-configmap.yaml\n```", "```\n$ kubectl exec client -n falcotest -- curl -F \"s=OK\" -F \"user=bad\" -F \"passwd=wrongpasswd' OR 'a'='a\" -F \"ipaddr=localhost; cat /var/www/html/ping.php\" -X POST http://ping/ping.php\n```", "```\n3 packets transmitted, 3 received, 0% packet loss, time 2044ms\nrtt min/avg/max/mdev = 0.028/0.035/0.045/0.007 ms\n<?php\n$link = mysqli_connect(\"mysql\", \"root\", \"foobar\", \"employees\");\n?>\n```", "```\n$ kubectl get pods | grep falco-daemonset\nfalco-daemonset-5785b 1/1 Running 0 9m52s\nfalco-daemonset-brjs7 1/1 Running 0 9m52s\nfalco-daemonset-mqcjq 1/1 Running 0 9m52s\nfalco-daemonset-pdx45 1/1 Running 0 9m52s\n```", "```\n$ kubectl exec -it falco-daemonset-94p8w bash\n**$ kubectl logs falco-daemonset-94p8w** \n```", "```\n05:41:59.9275580001: Error Unauthorized process (cat /var/www/html/ping.php) running in (5f1b6d304f99) k8s.ns=falcotest k8s.pod=ping-74dbb488b6-6hwp6 container=5f1b6d304f99\n```", "```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd src/chapter9\n```", "```\n$ git clone https://github.com/hashicorp/vault-helm.git\n$ cd vault-helm\n```", "```\n$ git checkout v$(curl --silent \"https://api.github.com/repos/hashicorp/vault-helm/releases/latest\" | \\\ngrep '\"tag_name\":' | \\\nsed -E 's/.*\"v([^\"]+)\".*/\\1/')\n```", "```\n$ helm install --name vault --namespace vault ./\n```", "```\n$ helm install --name vault --namespace vault --set='server.ha.enabled=true' ./\n```", "```\n$ $ kubectl get pods -nvault\nNAME                                  READY STATUS  RESTARTS AGE\nvault-0                               0/1   Running 0        83s\nvault-agent-injector-5fb898d6cd-rct82 1/1   Running 0        84s\n```", "```\n$ kubectl exec -it vault-0 -nvault -- vault status\nKey             Value\n---             -----\nSeal Type       shamir\nInitialized     false\nSealed          true\nTotal Shares    0\nThreshold       0\nUnseal Progress 0/0\nUnseal Nonce    n/a\nVersion         n/a\nHA Enabled      false\n```", "```\n$ kubectl exec -it vault-0 -nvault -- vault operator init -n 1 -t 1\n\nUnseal Key 1: lhLeU6SRdUNQgfpWAqWknwSxns1tfWP57iZQbbYtFSE=\nInitial Root Token: s.CzcefEkOYmCt70fGSbHgSZl4\nVault initialized with 1 key shares and a key threshold of 1\\. Please securely\ndistribute the key shares printed above. When the Vault is re-sealed,\nrestarted, or stopped, you must supply at least 1 of these keys to unseal it\nbefore it can start servicing requests.\n```", "```\n$ kubectl exec -it vault-0 -nvault -- vault operator unseal lhLeU6SRdUNQgfpWAqWknwSxns1tfWP57iZQbbYtFSE=\nKey          Value\n---          -----\nSeal Type    shamir\nInitialized  true\nSealed       false\nTotal Shares 1\nThreshold    1\nVersion      1.3.1\nCluster Name vault-cluster-6970c528\nCluster ID   dd88cca8-20bb-326c-acb3-2d924bb1805c\nHA Enabled   false\n```", "```\n$ kubectl get pods -nvault\nNAME                                  READY STATUS  RESTARTS AGE\nvault-0                               1/1   Running 0        6m29s\nvault-agent-injector-5fb898d6cd-rct82 1/1   Running 0        6m30s\n```", "```\n$ kubectl port-forward vault-0 -nvault 8200:8200\n```", "```\n$ vault login <root-token-here>\n```", "```\n$ vault write secret/foo value=bar\nSuccess! Data written to: secret/foo\n$ vault read secret/foo\nKey              Value\n---              -----\nrefresh_interval 768h\nvalue            bar\n```", "```\n$ kubectl -n vault create serviceaccount vault-k8s\n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n name: role-tokenreview-binding\n namespace: default\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: ClusterRole\n name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n name: vault-k8s\n namespace: default\nEOF\n```", "```\n$ SECRET_NAME=$(kubectl -n vault get serviceaccount vault-k8s -o jsonpath={.secrets[0].name})\n$ ACCOUNT_TOKEN=$(kubectl -n vault get secret ${SECRET_NAME} -o jsonpath={.data.token} | base64 --decode; echo)\n$ export VAULT_SA_NAME=$(kubectl get sa -n vault vault-k8s -o jsonpath=\u201d{.secrets[*].name}\u201d)\n$ export SA_CA_CRT=$(kubectl get secret $VAULT_SA_NAME -n vault -o jsonpath={.data.'ca\\.crt'} | base64 --decode; echo)\n```", "```\n$ vault auth enable kubernetes\n$ vault write auth/kubernetes/config kubernetes_host=\u201dhttps://MASTER_IP:6443\" kubernetes_ca_cert=\u201d$SA_CA_CRT\u201d token_reviewer_jwt=$TR_ACCOUNT_TOKEN\n```", "```\n$ vault write sys/policy/vault-policy policy=@policy.hcl\n```", "```\n$ vault write auth/kubernetes/role/demo-role \\\n bound_service_account_names=vault-coreos-test \\\n bound_service_account_namespaces=default \\\n policies=demo-policy \\\n ttl=1h\n```", "```\n$ DEFAULT_ACCOUNT_TOKEN=$(kubectl get secret $VAULT_SA_NAME -n vault -o jsonpath={.data.token} | base64 \u2014 decode; echo )\n```", "```\n$ vault write auth/kubernetes/login role=demo-role jwt=${DEFAULT_ACCOUNT_TOKEN}\n```", "```\n$ vault write secret/demo/foo value=bar\n```"]