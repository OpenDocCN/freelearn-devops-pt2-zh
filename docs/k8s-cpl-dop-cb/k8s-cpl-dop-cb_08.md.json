["```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd /src/chapter8\n```", "```\n$ git clone https://github.com/kubernetes-incubator/metrics-        server.git\n```", "```\n$ kubectl apply -f metrics-server/deploy/1.8+\n```", "```\n$ kubectl top nodes\nNAME                          CPU(cores) CPU% MEMORY(bytes) MEMORY%\nip-172-20-32-169.ec2.internal 259m       12%  1492Mi        19%\nip-172-20-37-106.ec2.internal 190m       9%   1450Mi        18%\nip-172-20-48-49.ec2.internal  262m       13%  2166Mi        27%\nip-172-20-58-155.ec2.internal 745m       37%  1130Mi        14%\n```", "```\n$ kubectl top pods -n openebs\nNAME                                         CPU(cores) MEMORY(bytes)\nmaya-apiserver-6ff5bc7bdd-l5gmt              2m         10Mi\nopenebs-admission-server-76dbdf97d9-swjw9    0m         3Mi\nopenebs-localpv-provisioner-6777f78966-f6lzp 2m         8Mi\nopenebs-ndm-operator-797495544c-hblxv        5m         12Mi\nopenebs-ndm-prvcr                            1m         6Mi\nopenebs-ndm-qmr66                            1m         6Mi\nopenebs-ndm-xbc2q                            1m         6Mi\nopenebs-provisioner-58bbbb8575-jzch2         3m         7Mi\nopenebs-snapshot-operator-6d7545dc69-b2zr7   4m         15Mi\n```", "```\n$ kubectl get pods -n kubernetes-dashboard\nNAME                                       READY STATUS  RESTARTS AGE\ndashboard-metrics-scraper-69fcc6d9df-hhkkw 1/1   Running 0        177m\nkubernetes-dashboard-566c79c67d-xqc6h      1/1   Running 0        177m\n```", "```\n$ cat debug/node-problem-detector.yaml\n$ kubectl apply -f debug/node-problem-detector.yaml\n```", "```\n$ kubectl get nodes\nNAME                          STATUS ROLES  AGE   VERSION\nip-172-20-32-169.ec2.internal Ready  node   6d23h v1.14.6\nip-172-20-37-106.ec2.internal Ready  node   6d23h v1.14.6\nip-172-20-48-49.ec2.internal  Ready  master 6d23h v1.14.6\nip-172-20-58-155.ec2.internal Ready  node   6d23h v1.14.6\n```", "```\n$ kubectl describe node ip-172-20-32-169.ec2.internal | grep -i condition -A 20 | grep Ready -B 20\nConditions:\n Type Status LastHeartbeatTime LastTransitionTime Reason Message\n ---- ------ ----------------- ------------------ ------ -------\n NetworkUnavailable False Sat, 12 Oct 2019 00:06:46 +0000 Sat, 12 Oct 2019 00:06:46 +0000 RouteCreated RouteController created a route\n MemoryPressure False Fri, 18 Oct 2019 23:43:37 +0000 Sat, 12 Oct 2019 00:06:37 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available\n DiskPressure False Fri, 18 Oct 2019 23:43:37 +0000 Sat, 12 Oct 2019 00:06:37 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure\n PIDPressure False Fri, 18 Oct 2019 23:43:37 +0000 Sat, 12 Oct 2019 00:06:37 +0000 KubeletHasSufficientPID kubelet has sufficient PID available\n Ready True Fri, 18 Oct 2019 23:43:37 +0000 Sat, 12 Oct 2019 00:06:37 +0000 KubeletReady kubelet is posting ready status\n```", "```\n$ kubectl get node ip-172-20-32-169.ec2.internal -o yaml | grep -B5 KernelDeadlock\n - lastHeartbeatTime: \"2019-10-18T23:58:53Z\"\n lastTransitionTime: \"2019-10-18T23:49:46Z\"\n message: kernel has no deadlock\n reason: KernelHasNoDeadlock\n status: \"False\"\n type: KernelDeadlock\n```", "```\n$ cat debug/mongo-sc.yaml\n$ kubectl apply -f debug/mongo-sc.yaml\n```", "```\n$ kubectl get pods\nNAME    READY STATUS  RESTARTS AGE\nmongo-0 0/2   Pending 0        3m\n```", "```\n$ kubectl describe pod mongo-0\n...\nEvents:\n Type    Reason           Age    From          Message\n ----    ------           ----   ----          -------\n Warning FailedScheduling 2m34s (x34 over 48m) default-scheduler pod has unbound immediate PersistentVolumeClaims (repeated 3 times)\n\n```", "```\n$ kubectl get pvc\nNAME              STATUS  VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE\nmongo-pvc-mongo-0 Pending                              storageclass 53m\n```", "```\n$ kubectl describe pvc mongo-pvc-mongo-0\n...\nEvents:\n Type    Reason             Age  From           Message\n ----    ------             ---- ----           -------\n Warning ProvisioningFailed 70s  (x33 over 58m) persistentvolume-controller storageclass.storage.k8s.io \"storageclass\" not found\n\n```", "```\n$ kubectl get sc\nNAME                            PROVISIONER AGE\ndefault                         kubernetes.io/aws-ebs 16d\ngp2                             kubernetes.io/aws-ebs 16d\nopenebs-cstor-default (default) openebs.io/provisioner-iscsi 8d\nopenebs-device                  openebs.io/local 15d\nopenebs-hostpath                openebs.io/local 15d\nopenebs-jiva-default            openebs.io/provisioner-iscsi 15d\nopenebs-snapshot-promoter       volumesnapshot.external-storage.k8s.io/snapshot-promoter 15d\n```", "```\n$ kubectl create -f sc-gp2.yaml\n```", "```\n$ kubectl get pods\nNAME    READY STATUS  RESTARTS AGE\nmongo-0 2/2   Running 0        2m18s\nmongo-1 2/2   Running 0        88s\nmongo-2 2/2   Running 0        50s\n```", "```\n$ cat debug/mongo-image.yaml\n$ kubectl apply -f debug/mongo-image.yaml\n```", "```\n$ kubectl get pods\nNAME    READY STATUS           RESTARTS AGE\nmongo-0 0/2   ImagePullBackOff 0        32s\n```", "```\n$ kubectl describe pod mongo-0\n...\nEvents:\n Type    Reason           Age    From          Message\n ----    ------           ----   ----          -------\n Warning Failed 25s (x3 over 68s) kubelet, ip-172-20-32-169.ec2.internal Error: ErrImagePull\n Warning Failed 25s (x3 over 68s) kubelet, ip-172-20-32-169.ec2.internal Failed to pull image \"mongi\": rpc error: code = Unknown desc = Error response from daemon: pull access denied for mongi, repository does not exist or may require 'docker login'\n Normal Pulling 25s (x3 over 68s) kubelet, ip-172-20-32-169.ec2.internal Pulling image \"mongi\"\n Normal BackOff 14s (x4 over 67s) kubelet, ip-172-20-32-169.ec2.internal Back-off pulling image \"mongi\"\n Warning Failed 14s (x4 over 67s) kubelet, ip-172-20-32-169.ec2.internal Error: ImagePullBackOff\n```", "```\n... \nspec:\n terminationGracePeriodSeconds: 10\n containers:\n - name: mongo\n image: mongo\n command:\n...\n```", "```\n$ kubectl delete -f mongo-image.yaml\n$ kubectl apply -f mongo-image.yaml\n```", "```\n$ kubectl get pods\nNAME    READY STATUS  RESTARTS AGE\nmongo-0 2/2   Running 0        4m55s\nmongo-1 2/2   Running 0        4m55s\nmongo-2 2/2   Running 0        4m55s\n```", "```\n$ cat debug/mongo-config.yaml\n$ kubectl apply -f debug/mongo-config.yaml\n```", "```\n$ kubectl get pods\nNAME READY STATUS RESTARTS AGE\nmongo-0 1/2 CrashLoopBackOff 3 58s\n```", "```\n$ kubectl describe pod mongo-0\n...\nEvents:\n Type    Reason           Age    From          Message\n ----    ------           ----   ----          -------\n...\n Normal Pulled 44s (x4 over 89s) kubelet, ip-172-20-32-169.ec2.internal Successfully pulled image \"mongo\"\n Warning BackOff 43s (x5 over 87s) kubelet, ip-172-20-32-169.ec2.internal Back-off restarting failed container\n\n```", "```\n$ kubectl logs mongo-0 mongo\n/bin/sh: 1: cannot open : No such file\n```", "```\n...\n spec:\n terminationGracePeriodSeconds: 10\n containers:\n - name: mongo\n image: mongo\n command: [\"/bin/sh\"]\n args: [\"-c\", \"sed \\\"s/foo/bar/\\\" < $MYFILE\"]\n...\n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n name: app-env\ndata:\n MYFILE: \"/etc/profile\"\nEOF\n```", "```\n$ kubectl delete -f mongo-image.yaml\n$ kubectl apply -f mongo-image.yaml\n```", "```\n$ kubectl get pods\nNAME    READY STATUS  RESTARTS AGE\nmongo-0 2/2   Running 0        4m15s\nmongo-1 2/2   Running 0        4m15s\nmongo-2 2/2   Running 0        4m15s\n```", "```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd src/chapter8/\n```", "```\n--authentication-token-webhook=true \n--authorization-mode=Webhook \n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n name: amazon-cloudwatch\n labels:\n name: amazon-cloudwatch\nEOF\n```", "```\n$ kubectl apply -f cloudwatch/cwagent-serviceaccount.yaml\n```", "```\n$ eksctl get cluster\nNAME                        REGION\nadorable-rainbow-1571556654 us-west-2\n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n name: cwagentconfig\n namespace: amazon-cloudwatch\ndata:\n cwagentconfig.json: |\n {\n \"logs\": {\n \"metrics_collected\": {\n \"kubernetes\": {\n \"cluster_name\": \"{{cluster_name}}\",\n \"metrics_collection_interval\": 60\n }\n },\n \"force_flush_interval\": 5\n }\n }\nEOF\n```", "```\n$ kubectl apply -f cloudwatch/cwagent.yaml\n```", "```\n$ kubectl get pods -n amazon-cloudwatch\nNAME                   READY STATUS  RESTARTS AGE\ncloudwatch-agent-dtpxt 1/1   Running 0        67s\ncloudwatch-agent-j7frt 1/1   Running 0        67s\n```", "```\n$ az aks enable-addons -a monitoring \\\n--name AKSCluster --resource-group k8sdevopscookbook\n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1 \nkind: ClusterRole \nmetadata: \n name: containerHealth-log-reader \nrules: \n - apiGroups: [\"\"] \n resources: [\"pods/log\", \"events\"] \n verbs: [\"get\", \"list\"] \nEOF\n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1 \nkind: ClusterRoleBinding \nmetadata: \n name: containerHealth-read-logs-global \nroleRef: \n kind: ClusterRole \n name: containerHealth-log-reader \n apiGroup: rbac.authorization.k8s.io \nsubjects: \n - kind: User \n name: clusterUser \n apiGroup: rbac.authorization.k8s.io\nEOF\n```", "```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd /src/chapter8\n```", "```\n$ helm repo update\n```", "```\n$ helm install stable/prometheus-operator --name prometheus \\\n --namespace monitoring\n```", "```\n$ kubectl get pods -n monitoring\nNAME READY STATUS RESTARTS AGE\nalertmanager-prometheus-prometheus-oper-alertmanager-0 2/2 Running 0 88s\nprometheus-grafana-6c6f7586b6-f9jbr 2/2 Running 0 98s\nprometheus-kube-state-metrics-57d6c55b56-wf4mc 1/1 Running 0 98s\nprometheus-prometheus-node-exporter-8drg7 1/1 Running 0 98s\nprometheus-prometheus-node-exporter-lb7l5 1/1 Running 0 98s\nprometheus-prometheus-node-exporter-vx7w2 1/1 Running 0 98s\nprometheus-prometheus-oper-operator-86c9c956dd-88p82 2/2 Running 0 98s\nprometheus-prometheus-prometheus-oper-prometheus-0 3/3 Running 1 78s\n```", "```\n$ kubectl get svc -n monitoring\nNAME                                    TYPE      CLUSTER-IP   EXTERNAL-IP PORT(S)                    AGE\nalertmanager-operated                   ClusterIP None         <none>      9093/TCP,9094/TCP,9094/UDP 33m\nprometheus-grafana                      ClusterIP 10.0.1.132   <none>      80/TCP                     33m\nprometheus-kube-state-metrics           ClusterIP 10.0.69.144  <none>      8080/TCP                   33m\nprometheus-operated                     ClusterIP None         <none>      9090/TCP                   33m\nprometheus-prometheus-node-exporter     ClusterIP 10.0.100.183 <none>      9100/TCP                   33m\nprometheus-prometheus-oper-alertmanager ClusterIP 10.0.202.140 <none>      9093/TCP                   33m\nprometheus-prometheus-oper-operator     ClusterIP 10.0.174.214 <none>      8080/TCP,443/TCP           33m\nprometheus-prometheus-oper-prometheus   ClusterIP 10.0.243.177 <none>      9090/TCP                   33m\n```", "```\n$ kubectl port-forward -n monitoring prometheus-grafana 8000:80\n```", "```\n$ helm install --name sysdig-agent --set sysdig.accessKey=YourAccessKey, \\ sysdig.settings.tags='linux:ubuntu, dept:dev,local:ca' \\\n--set sysdig.settings.k8s_cluster_name='my_cluster' stable/sysdig\n```", "```\n$ helm repo add kubecost https://kubecost.github.io/cost-analyzer/\n```", "```\n$ helm install kubecost/cost-analyzer --namespace kubecost --name kubecost --set kubecostToken=\"dGVzdEB0ZXN0LmNvbQ==xm343yadf98\"\n```", "```\n$ kubectl get pods -nkubecost\nNAME READY STATUS RESTARTS AGE\ncost-analyzer-checks-1571781600-6mhwh 0/1 Completed 0 7m1s\nkubecost-cost-analyzer-54bc969689-8rznl 3/3 Running 0 9m7s\nkubecost-grafana-844d4b9844-dkdvn 3/3 Running 0 9m7s\nkubecost-prometheus-alertmanager-85bbbd6b7b-fpmqr 2/2 Running 0 9m7s\nkubecost-prometheus-kube-state-metrics-857c5d4b4f-gxmgj 1/1 Running 0 9m7s\nkubecost-prometheus-node-exporter-6bsp2 1/1 Running 0 9m7s\nkubecost-prometheus-node-exporter-jtw2h 1/1 Running 0 9m7s\nkubecost-prometheus-node-exporter-k69fh 1/1 Running 0 9m7s\nkubecost-prometheus-pushgateway-7689458dc9-rx5jj 1/1 Running 0 9m7s\nkubecost-prometheus-server-7b8b759d74-vww8c 2/2 Running 0 9m7s\n```", "```\n$ kubectl get svc -nkubecost\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nkubecost-cost-analyzer ClusterIP 100.65.53.41 <none> 9001/TCP,9003/TCP,9090/TCP 13m\nkubecost-grafana ClusterIP 100.69.52.23 <none> 80/TCP 13m\nkubecost-prometheus-alertmanager ClusterIP 100.71.217.248 <none> 80/TCP 13m\nkubecost-prometheus-kube-state-metrics ClusterIP None <none> 80/TCP 13m\nkubecost-prometheus-node-exporter ClusterIP None <none> 9100/TCP 13m\nkubecost-prometheus-pushgateway ClusterIP 100.69.137.163 <none> 9091/TCP 13m\nkubecost-prometheus-server ClusterIP 100.64.7.82 <none> 80/TCP 13m\n```", "```\n$ kubectl port-forward --namespace kubecost deployment/kubecost-cost-analyzer 9090\n```"]