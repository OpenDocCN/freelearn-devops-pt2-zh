["```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd /src/chapter7/\n```", "```\n$ kubectl top node\nerror: metrics not available yet\n```", "```\n$ kubectl top nodes\nNAME                          CPU(cores) CPU% MEMORY(bytes) MEMORY%\nip-172-20-32-169.ec2.internal 259m       12%  1492Mi        19%\nip-172-20-37-106.ec2.internal 190m       9%   1450Mi        18%\nip-172-20-48-49.ec2.internal  262m       13%  2166Mi        27%\nip-172-20-58-155.ec2.internal 745m       37%  1130Mi        14%\n```", "```\n$ cd /charts/node/\n```", "```\n$ helm install . --name my-ch7-app\n```", "```\n$ export SERVICE_IP=$(kubectl get svc --namespace default my-ch7-app-node --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\n$ echo http://$SERVICE_IP/\nhttp://mytodoapp.us-east-1.elb.amazonaws.com/\n```", "```\n$ helm status my-ch7-app\nLAST DEPLOYED: Thu Oct 3 00:13:10 2019\nNAMESPACE: default\nSTATUS: DEPLOYED\nRESOURCES:\n==> v1/Deployment\nNAME               READY UP-TO-DATE AVAILABLE AGE\nmy-ch7-app-mongodb 1/1   1          1         9m9s\nmy-ch7-app-node    1/1   1          1         9m9s\n...\n```", "```\n$ kubectl scale --replicas 3 deployment/my-ch7-app-node\ndeployment.extensions/my-ch7-app-node scaled\n```", "```\n$ helm status my-ch7-app\n...\nRESOURCES:\n==> v1/Deployment\nNAME READY UP-TO-DATE AVAILABLE AGE\nmy-ch7-app-mongodb 1/1 1 1 26m\nmy-ch7-app-node 3/3 3 3 26m\n...\n==> v1/Pod(related)\nNAME READY STATUS RESTARTS AGE\nmy-ch7-app-mongodb-5499c954b8-lcw27 1/1 Running 0 26m\nmy-ch7-app-node-d8b94964f-94dsb 1/1 Running 0 91s\nmy-ch7-app-node-d8b94964f-h9w4l 1/1 Running 3 26m\nmy-ch7-app-node-d8b94964f-qpm77 1/1 Running 0 91s\n```", "```\n$ kubectl scale --replicas 2 deployment/my-ch7-app-node\ndeployment.extensions/my-ch7-app-node scaled\n```", "```\n$ kubectl get pods | grep my-ch7-app\nmy-ch7-app-mongodb-5499c954b8-lcw27 1/1 Running 0 4h41m\nmy-ch7-app-node-d8b94964f-94dsb     1/1 Running 0 4h16m\nmy-ch7-app-node-d8b94964f-h9w4l     1/1 Running 3 4h41m\n```", "```\ncat <<EOF | kubectl apply -f -\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n name: my-ch7-app-autoscaler\n namespace: default\nspec:\n scaleTargetRef:\n apiVersion: apps/v1\n kind: Deployment\n name: my-ch7-app-node\n minReplicas: 1\n maxReplicas: 5\n targetCPUUtilizationPercentage: 50\nEOF\n```", "```\n$ kubectl get hpa\nNAME                  REFERENCE                  TARGETS       MINPODS MAXPODS REPLICAS AGE\nmy-ch7-app-autoscaler Deployment/my-ch7-app-node 0%/50%        1       5       1        40s\n```", "```\n$ export SERVICE_IP=$(kubectl get svc --namespace default my-ch7-app-node --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\n$ echo http://$SERVICE_IP/\nhttp://mytodoapp.us-east-1.elb.amazonaws.com/\n```", "```\n$ kubectl run -i --tty load-generator --image=busybox /bin/sh\n\nwhile true; do wget -q -O- YOUR_SERVICE_IP; done\n```", "```\n$ kubectl get hpa\nNAME                  REFERENCE                  TARGETS       MINPODS MAXPODS REPLICAS AGE\nmy-ch7-app-autoscaler Deployment/my-ch7-app-node 210%/50%      1       5       1        23m\n```", "```\n$ kubectl get deployment my-ch7-app-node\nNAME            READY UP-TO-DATE AVAILABLE AGE\nmy-ch7-app-node 5/5   5          5         5h23m\n```", "```\n$ kubectl get hpa\nNAME                  REFERENCE                  TARGETS MINPODS MAXPODS REPLICAS AGE\nmy-ch7-app-autoscaler Deployment/my-ch7-app-node 0%/50%  1       5       1        34m\n```", "```\n$ kubectl get deployment my-ch7-app-node\nNAME            READY UP-TO-DATE AVAILABLE AGE\nmy-ch7-app-node 1/1   1          1         5h35m\n```", "```\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-ch7-app-node\n  minReplicas: 1\n  maxReplicas: 5\n  targetCPUUtilizationPercentage: 50\n```", "```\n$ kubectl get hpa.v2beta2.autoscaling my-ch7-app-node -o yaml\n```", "```\n$ kubectl get nodes --show-labels\nNAME                          STATUS ROLES AGE VERSION LABELS\nip-172-20-49-12.ec2.internal  Ready   node  23h v1.14.6 \nkubernetes.io/arch=amd64,kubernetes.io/instance-type=t3.large,\nkubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,\nfailure-domain.beta.kubernetes.io/zone=us-east-1a,\nkops.k8s.io/instancegroup=nodes,kubernetes.io/hostname=ip-172-20-49-12.ec2.internal,\nkubernetes.io/role=node,node-role.kubernetes.io/node=\n...\n```", "```\n$ kubectl get nodes\nNAME                           STATUS ROLES  AGE VERSION\nip-172-20-49-12.ec2.internal   Ready  node   23h v1.14.6\nip-172-20-50-171.ec2.internal  Ready  node   23h v1.14.6\nip-172-20-58-83.ec2.internal   Ready  node   23h v1.14.6\nip-172-20-59-8.ec2.internal    Ready  master 23h v1.14.6\n```", "```\n$ kubectl label nodes ip-172-20-49-12.ec2.internal environment=production\n$ kubectl label nodes ip-172-20-50-171.ec2.internal environment=production\n$ kubectl label nodes ip-172-20-58-83.ec2.internal environment=development\n```", "```\n$ kubectl get nodes --show-labels\n```", "```\n$ cd src/chapter7/charts\n$ mkdir todo-dev\n$ cp -a node/* todo-dev/\n$ cd todo-dev\n```", "```\n$ vi templates/deployment.yaml\n```", "```\n...\n          mountPath: {{ .Values.persistence.path }}\n      {{- end }}\n# Start of the addition\n      nodeSelector:\n        environment: \"{{ .Values.environment }}\"\n# End of the addition\n      containers:\n      - name: {{ template \"node.fullname\" . }}\n\n...\n```", "```\n$ vi values.yaml\n```", "```\n...\n## Affinity for pod assignment\n## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n##\naffinity: {}\nenvironment: development\n```", "```\napiVersion: v1\nname: todo-dev\n...\n```", "```\n$ helm dep update & helm dep build\n```", "```\n$ helm lint .\n==> Linting .\nLint OK\n1 chart(s) linted, no failures\n```", "```\n$ helm install . --name my-app7-dev --set serviceType=LoadBalancer\n```", "```\n$ for n in $(kubectl get nodes -l environment=development --no-headers | cut -d \" \" -f1); do kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; done\n```", "```\n$ cd src/chapter7/charts\n$ mkdir todo-prod\n$ cp -a node/* todo-prod/\n$ cd todo-prod\n```", "```\n$ vi values.yaml\n```", "```\n## Affinity for pod assignment\n## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n# affinity: {}\n# Start of the affinity addition #1\naffinity:\n nodeAffinity:\n requiredDuringSchedulingIgnoredDuringExecution:\n nodeSelectorTerms:\n - matchExpressions:\n - key: environment\n operator: In\n values:\n - production\n# End of the affinity addition #1\n```", "```\n          - production\n# End of the affinity addition #1\n# Start of the affinity addition #2\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 1\n      preference:\n        matchExpressions:\n        - key: failure-domain.beta.kubernetes.io/zone\n          operator: In\n          values:\n          - us-east-1a\n          - us-east-1b\n# End of the affinity addition #2\n```", "```\n          - us-east-1b\n# End of the affinity addition #2\n# Start of the affinity addition #3a\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n    - labelSelector:\n        matchExpressions:\n        - key: app\n          operator: In\n          values:\n          - mongodb\n      topologyKey: failure-domain.beta.kubernetes.io/zone\n# End of the affinity addition #3a\n```", "```\n      topologyKey: failure-domain.beta.kubernetes.io/zone\n# End of the affinity addition #3a\n# Start of the affinity addition #3b\n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - weight: 100\n      podAffinityTerm:\n        labelSelector:\n          matchExpressions:\n          - key: app\n            operator: In\n            values:\n            - todo-dev\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n# End of the affinity addition #3b\n```", "```\napiVersion: v1\nname: todo-prod\n...\n```", "```\n$ helm dep update & helm dep build\n```", "```\n$ helm lint .\n==> Linting .\nLint OK\n1 chart(s) linted, no failures\n```", "```\n$ helm install . --name my-app7-prod --set serviceType=LoadBalancer\n```", "```\n$ for n in $(kubectl get nodes -l environment=production --no-headers | cut -d \" \" -f1); do kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; done\n```", "```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd src/chapter7/lb/\n```", "```\n$ kubectl apply -f minio.yaml\n```", "```\n$ kubectl get svc\nNAME       TYPE      CLUSTER-IP EXTERNAL-IP PORT(S)  AGE\nkubernetes ClusterIP 100.64.0.1 <none>      443/TCP  5d\nminio      ClusterIP None       <none>      9000/TCP 4m\n```", "```\ncat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n name: minio-service\nspec:\n type: LoadBalancer\n ports:\n - port: 9000\n targetPort: 9000\n protocol: TCP\n selector:\n app: minio\nEOF\n```", "```\n$ kubectl expose rc example --port=9000 --target-port=9000 --name=minio-service --type=LoadBalancer\n```", "```\n$ kubectl get svc |grep LoadBalancer\nNAME          TYPE         CLUSTER-IP    EXTERNAL-IP                                  PORT(S)        AGE\nminio-service LoadBalancer 100.69.15.120 containerized.me.us-east-1.elb.amazonaws.com 9000:30705/TCP 4h39m\n```", "```\n$ SERVICE_IP=http://$(kubectl get svc minio-service \\\n-o jsonpath='{.status.loadBalancer.ingress[0].hostname}:{.spec.ports[].targetPort}')\n$ echo $SERVICE_IP\n```", "```\n$ SERVICE_IP=http://$(kubectl get svc minio-service \\\n-o jsonpath='{.status.loadBalancer.ingress[0].ip}:{.spec.ports[].targetPort}')\n$ echo $SERVICE_IP\n```", "```\n$ git clone https://github.com/istio/istio.git \n$ cd istio\n```", "```\n$ helm install install/kubernetes/helm/istio-init --name istio-init \\\n--namespace istio-system\n```", "```\n$ helm install install/kubernetes/helm/istio --name istio \\\n--namespace istio-system\n```", "```\n$ kubectl label namespace default istio-injection=enabled\n```", "```\n$ kubectl get crds | grep 'istio.io' | wc -l\n23\n```", "```\n$ kubectl get svc -n istio-system\nNAME                   TYPE         CLUSTER-IP     EXTERNAL-IP PORT(S)             AGE\nistio-citadel          ClusterIP    100.66.235.211 <none>      8060/TCP,...        2m10s\nistio-galley           ClusterIP    100.69.206.64  <none>      443/TCP,...         2m11s\nistio-ingressgateway   LoadBalancer 100.67.29.143  domain.com  15020:31452/TCP,... 2m11s\nistio-pilot            ClusterIP    100.70.130.148 <none>      15010/TCP,...       2m11s\nistio-policy           ClusterIP    100.64.243.176 <none>      9091/TCP,...        2m11s\nistio-sidecar-injector ClusterIP    100.69.244.156 <none>      443/TCP,...         2m10s\nistio-telemetry        ClusterIP    100.68.146.30  <none>      9091/TCP,...        2m11s\nprometheus             ClusterIP    100.71.172.191 <none>      9090/TCP            2m11s\n```", "```\n$ kubectl get pods -n istio-system\n```", "```\n$ kubectl get namespace -L istio-injection\nNAME            STATUS AGE  ISTIO-INJECTION\ndefault         Active 5d8h enabled\nistio-system    Active 40m\nkube-node-lease Active 5d8h\nkube-public     Active 5d8h\nkube-system     Active 5d8h\n```", "```\n$ kubectl apply -f minio.yaml\n```", "```\n$ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].port}')\n$ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"https\")].port}')\n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n name: minio-gateway\nspec:\n selector:\n istio: ingressgateway \n servers:\n - port:\n number: 80\n name: http\n protocol: HTTP\n hosts:\n - \"*\"\nEOF\n```", "```\n$ cat <<EOF | kubectl apply -f -\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n name: minio\nspec:\n hosts:\n - \"*\"\n gateways:\n - minio-gateway.default\n http:\n - match:\n - uri:\n prefix: /\n route:\n - destination:\n port:\n number: 9000\n host: minio\nEOF\n```", "```\n$ helm delete istio\n$ helm delete istio-init\n```", "```\n$ git clone https://github.com/k8sdevopscookbook/src.git\n$ cd src/chapter7/linkerd/\n```", "```\n$ curl -sL https://run.linkerd.io/install | sh\n```", "```\n$ export PATH=$PATH:$HOME/.linkerd2/bin\n```", "```\n$ linkerd version\nClient version: stable-2.5.0\nServer version: unavailable\n```", "```\n$ linkerd check --pre\nStatus check results are \u221a\n```", "```\n$ linkerd install | kubectl apply -f -\n```", "```\n$ linkerd check\n...\n\ncontrol-plane-version\n---------------------\n\u221a control plane is up-to-date\n\u221a control plane and cli versions match\n\nStatus check results are \u221a\n```", "```\n$ cd /src/chapter7/linkerd\n```", "```\n$ kubectl apply -f emojivoto.yml\n```", "```\n$ SERVICE_IP=http://$(kubectl get svc web-svc -n emojivoto \\\n-o jsonpath='{.status.loadBalancer.ingress[0].hostname}:{.spec.ports[].targetPort}')\n$ echo $SERVICE_IP\n```", "```\n$ kubectl label namespace emojivoto linkerd.io/inject=enabled\n```", "```\n$ linkerd dashboard &\n```", "```\nhttp://127.0.0.1:50750\n```", "```\n$ wget https://raw.githubusercontent.com/k8sdevopscookbook/src/master/chapter7/linkerd/ingress-nginx.yaml\n```", "```\n$ kubectl apply -f ingress-nginx.yaml\n```", "```\n$ linkerd install --ignore-cluster | kubectl delete -f -\n```", "```\n$ cd src/chapter7/autoheal/minio\n$ kubectl apply -f minio.yaml\n```", "```\n$ kubectl get pods |grep minio\nminio-0 1/1 Running 0 4m38ms\nminio-1 1/1 Running 0 4m25s\nminio-2 1/1 Running 0 4m12s\nminio-3 1/1 Running 0 3m48s\n```", "```\n$ kubectl delete pod minio-0\npod \"minio-0\" deleted\n$ kubectl get pods |grep miniominio-0\nminio-0 0/1 ContainerCreating 0 2s\nminio-1 1/1 Running           0 8m9s\nminio-2 1/1 Running           0 7m56s\nminio-3 1/1 Running           0 7m32s\n```", "```\n...\n        volumeMounts:\n        - name: data\n          mountPath: /data\n#### Starts here \n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: 9000\n          initialDelaySeconds: 120\n          periodSeconds: 20\n#### Ends here \n  # These are converted to volume claims by the controller\n  # and mounted at the paths mentioned above.\n  volumeClaimTemplates:\n```", "```\n$ kubectl apply -f minio.yaml\n```", "```\n$ kubectl describe pod minio-0\n...\n Liveness: http-get http://:9000/minio/health/live delay=120s timeout=1s period=20s #success=1 #failure=3\n...\n```", "```\n$ kubectl describe pod minio-0\n```", "```\n$ kubectl get pods\nNAME    READY STATUS  RESTARTS AGE\nminio-0 1/1   Running 4        12m\nminio-1 1/1   Running 4        12m\nminio-2 1/1   Running 3        11m\nminio-3 1/1   Running 3        11m\n```", "```\n$ cd /src/chapter7/bluegreen\n```", "```\n$ kubectl create -f blue-percona.yaml\npod \"blue\" created\npersistentvolumeclaim \"demo-vol1-claim\" created\n```", "```\n$ kubectl create -f percona-svc.yaml\n```", "```\n$ kubectl get svc percona\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\npercona ClusterIP 10.3.0.75 <none> 3306/TCP 1m\n```", "```\n      containers:\n      - name: sql-loadgen\n        image: openebs/tests-mysql-client\n        command: [\"/bin/bash\"]\n        args: [\"-c\", \"timelimit -t 300 sh MySQLLoadGenerate.sh 10.3.0.75 > /dev/null 2>&1; exit 0\"]\n        tty: true\n```", "```\n$ kubectl create -f sql-loadgen.yaml\n```", "```\n$ kubectl create -f snapshot.yaml\nvolumesnapshot.volumesnapshot.external-storage.k8s.io \"snapshot-blue\" created\n```", "```\n$ kubectl create -f green-percona.yaml\npod \"green\" created\npersistentvolumeclaim \"demo-snap-vol-claim\" created\n```", "```\n$ kubectl edit svc percona\n```"]