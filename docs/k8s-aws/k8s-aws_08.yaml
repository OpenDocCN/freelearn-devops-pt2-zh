- en: Sorry My App Ate the Cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抱歉，我的应用程序吃掉了集群
- en: Using Kubernetes to run our applications allows us to achieve much higher utilization
    of resources on the machines in our clusters. The Kubernetes scheduler is very
    effective at packing different applications onto your cluster in a way that will
    maximize the use of the resources on each machine. You can schedule a mix of lower-priority
    jobs that can be restarted if needed, for example, batch jobs, and high-priority
    jobs, such as web servers or databases. Kubernetes will help you make use of the
    idle CPU cycles that occur when your web server is waiting for requests.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kubernetes运行我们的应用程序可以使我们在集群中的机器上实现更高的资源利用率。Kubernetes调度程序非常有效地将不同的应用程序打包到您的集群中，以最大程度地利用每台机器上的资源。您可以安排一些低优先级的作业，如果需要可以重新启动，例如批处理作业，以及高优先级的作业，例如Web服务器或数据库。Kubernetes将帮助您利用空闲的CPU周期，这些周期发生在您的Web服务器等待请求时。
- en: This is great news if you want to reduce the amount that you are paying AWS,
    for your EC2 instances to run your applications. It is important to learn how
    to configure your pods, so Kubernetes can account for the resource use of your
    applications. If you don't configure your pods correctly, then the reliability
    and performance of your application could be impacted as Kubernetes may need to
    evict your pods from a node because it is running out of resources.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想减少在AWS上支付的EC2实例的费用来运行您的应用程序，这是个好消息。学会如何配置您的Pod是很重要的，这样Kubernetes可以核算您的应用程序的资源使用情况。如果您没有正确配置您的Pod，那么您的应用程序的可靠性和性能可能会受到影响，因为Kubernetes可能需要从节点中驱逐您的Pod，因为资源不足。
- en: In this chapter, you are going to start by learning how to account for the memory
    and CPU that your pods will use. We will learn how to configure pods with a different
    quality of service so important workloads are guaranteed the resources they need,
    but less important workloads can make use of idle resources when they are available
    without needing dedicated resources. You will also learn how to make use of Kubernetes
    autoscaling facilities to add additional pods to your applications when they are
    under increased load, and to add additional nodes to your cluster when resources
    run low.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将首先学习如何核算Pod将使用的内存和CPU。我们将学习如何配置具有不同服务质量的Pod，以便重要的工作负载能够保证它们所需的资源，但不太重要的工作负载可以在有空闲资源时使用，而无需专用资源。您还将学习如何利用Kubernetes自动缩放功能，在负载增加时向您的应用程序添加额外的Pod，并在资源不足时向您的集群添加额外的节点。
- en: 'In this chapter, you will learn how to do the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何执行以下操作：
- en: Configure container resource requests and limits
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置容器资源请求和限制
- en: Configure your pods for a desired **Quality of Service** (**QoS**) class
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为所需的**服务质量**（**QoS**）类别配置您的Pod
- en: Set quotas on the use of resources per namespace
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置每个命名空间资源使用的配额
- en: Use the horizontal pod autoscaler to automatically scale your applications to
    match the demand for them
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用水平Pod自动缩放器自动调整您的应用程序，以满足对它们的需求
- en: Use the cluster autoscaler to automatically provision and terminate EC2 instances
    as the use of your cluster changes over time
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集群自动缩放器根据集群随时间变化的使用情况自动提供和终止EC2实例
- en: Resource requests and limits
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源请求和限制
- en: Kubernetes allows us to achieve high utilization of our cluster by scheduling
    multiple different workloads to a single pool of machines. Whenever we ask Kubernetes
    to schedule a pod, it needs to consider which node to place it on. The scheduler
    can make much better decisions about where to place a pod if we can give it some
    information about the resources that the pod will need; it then can calculate
    the current workload on each node and choose the node that fits the expected resource
    usage of our pod. We can optionally give Kubernetes this information with resource
    **requests**. Requests are considered at the time when a pod is scheduled to a
    node. Requests do not provide any limit to the amount of resources a pod may consume
    once it is running on a particular node, they just represent an accounting of
    the requests that we, the cluster operator, made when we asked for a particular
    pod to be scheduled to the cluster.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes允许我们通过将多个不同的工作负载调度到一组机器中来实现集群的高利用率。每当我们要求Kubernetes调度一个Pod时，它需要考虑将其放置在哪个节点上。如果我们可以给调度器一些关于Pod所需资源的信息，它就可以更好地决定在哪个节点上放置Pod。然后它可以计算每个节点上的当前工作负载，并选择符合我们Pod预期资源使用的节点。我们可以选择使用资源**请求**向Kubernetes提供这些信息。请求在将Pod调度到节点时考虑。请求不会对Pod在特定节点上运行时可能消耗的资源量提供任何限制，它们只是代表我们，集群操作员，在要求将特定Pod调度到集群时所做的请求的记录。
- en: In order to prevent pods from using more resources than they should, we can
    set resource **limits**. These limits can be enforced by the container runtime,
    to ensure that a pod doesn't use more of a particular resource than required.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止Pod使用超过其应该的资源，我们可以设置资源**限制**。这些限制可以由容器运行时强制执行，以确保Pod不会使用超过所需资源的数量。
- en: We can say that the CPU use of a container is compressible because if we limit
    it, it might result in our processes running more slowly, but typically won't
    cause any other ill effects, whereas the memory use of a container is uncompressible,
    because the only remedy available if a container uses more than its memory limit
    is to kill the container in question.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，容器的CPU使用是可压缩的，因为如果我们限制它，可能会导致我们的进程运行更慢，但通常不会造成其他不良影响，而容器的内存使用是不可压缩的，因为如果容器使用超过其内存限制，唯一的补救措施就是杀死相关的容器。
- en: 'It is very simple to add the configuration for resource limits and requests
    to a pod specification. In our manifests, each container specification can have
    a `resources` field that contains requests and limits. In this example, we request
    that an Nginx web server container is allocated 250 MiB of RAM and a quarter of
    a CPU core. Because the limit is set higher than the request, this allows the
    pod to use up to half a CPU core, and the container will only be killed if its
    memory use exceeds 128 Mi:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 向Pod规范添加资源限制和请求的配置非常简单。在我们的清单中，每个容器规范都可以有一个包含请求和限制的`resources`字段。在这个例子中，我们请求分配250
    MiB的RAM和四分之一的CPU核心给一个Nginx web服务器容器。因为限制设置得比请求高，这允许Pod使用高达半个CPU核心，并且只有在内存使用超过128
    Mi时才会杀死容器：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Resource units
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源单位
- en: Whenever we specify CPU requests or limits, we specify them in terms of CPU
    cores. Because often we want to request or limit the use of a pod to some fraction
    of a whole CPU core, we can either specify this fraction of a CPU as a decimal
    or as a millicore value. For example, a value of 0.5 represents half of a core.
    It is also possible to configure requests or limits with a millicore value. As
    there are 1,000 millicores to a single core, we could specify half a CPU as 500
    m. The smallest amount of CPU that can be specified is 1 m or 0.001.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们指定CPU请求或限制时，我们都是以CPU核心为单位进行指定。因为我们经常希望请求或限制Pod使用整个CPU核心的一部分，我们可以将这部分CPU指定为小数或毫核值。例如，值为0.5表示半个核心。还可以使用毫核值配置请求或限制。由于1,000毫核等于一个核心，我们可以将一半CPU指定为500
    m。可以指定的最小CPU量为1 m或0.001。
- en: I find that it can be more readable to use the millicore units in your manifests.
    When using `kubectl` or the Kubernetes dashboard, you will also notice that CPU
    limits and requests are formatted as millicore values. But if you are creating
    manifests with an automated process, you might use the floating point version.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现在清单中使用毫核单位更易读。当使用`kubectl`或Kubernetes仪表板时，您还会注意到CPU限制和请求以毫核值格式化。但是，如果您正在使用自动化流程创建清单，您可能会使用浮点版本。
- en: Limits and requests for memory are measured in bytes. But specifying them in
    this way in your manifests would be quite unwieldy and difficult to read. So,
    Kubernetes supports the standard prefixes for referring to multiples of bytes;
    you can choose to use either a decimal multiplier such as M or G, or one of the
    binary equivalents, such as Mi or Gi, which are more commonly used as they reflect
    the actual size of the physical RAM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 内存的限制和请求以字节为单位。但在清单中以这种方式指定它们会非常笨拙且难以阅读。因此，Kubernetes支持用于引用字节的倍数的标准前缀；您可以选择使用十进制乘数，如M或G，或者其中一个二进制等效项，如Mi或Gi，后者更常用，因为它们反映了物理RAM的实际大小。
- en: The binary versions of these units are actually what most people really mean
    when they are talking about megabytes or gigabytes, even though more correctly
    they are talking about mebibytes and gibibytes!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些单位的二进制版本实际上是大多数人在谈论兆字节或千兆字节时真正意味着的，尽管更正确的说法是他们在谈论兆比字节和吉比字节！
- en: In practice, you should just always remember to use the units with an **i**
    on the end, or you will end up with slightly less memory than you expected. This
    notation was introduced in the ISO/IEC 80000 standard in 1998, in order to avoid
    confusion between the decimal and binary units.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，您应该始终记住使用末尾带有**i**的单位，否则您将得到比预期少一些的内存。这种表示法是在1998年引入ISO/IEC 80000标准中的，以避免十进制和二进制单位之间的混淆。
- en: '| **Decimal** | **Binary** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **十进制** | **二进制** |'
- en: '| **Name** | **Bytes** | **Suffix** | **Name** | **Bytes** | **Suffix** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **字节** | **后缀** | **名称** | **字节** | **后缀** |'
- en: '| kilobyte | 1000 | K | kibibyte | 1024 | Ki |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 千字节 | 1000 | K | 基比字节 | 1024 | Ki |'
- en: '| megabyte | 1000² | M | mebibyte | 1024² | Mi |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 兆字节 | 1000² | M | 兆比字节 | 1024² | Mi |'
- en: '| gigabyte | 1000³ | G | gibibyte | 1024³ | Gi |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 吉字节 | 1000³ | G | 吉比字节 | 1024³ | Gi |'
- en: '| terabyte | 1000⁴ | T | tebibyte | 1024⁴ | Ti |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 太字节 | 1000⁴ | T | 泰比字节 | 1024⁴ | Ti |'
- en: '| petabyte | 1000⁵ | P | pebibyte | 1024⁵ | Pi |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 皮字节 | 1000⁵ | P | 皮比字节 | 1024⁵ | Pi |'
- en: '| exabyte | 1000⁶ | E | exbibyte | 1024⁶ | Ei |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 艾字节 | 1000⁶ | E | 艾比字节 | 1024⁶ | Ei |'
- en: The memory units supported by Kubernetes
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持的内存单位
- en: How pods with resource limits are managed
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何管理具有资源限制的Pods
- en: When the Kubelet starts a container, the CPU and memory limits are passed to
    the container runtime, which is then responsible for managing the resource usage
    of that container.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当Kubelet启动容器时，CPU和内存限制将传递给容器运行时，然后容器运行时负责管理该容器的资源使用。
- en: If you are using Docker, the CPU limit (in milicores) is multiplied by 100 to
    give the amount of CPU time the container will be allowed to use every 100 ms.
    If the CPU is under load, once a container has used its quota it will have to
    wait until the next 100 ms period before it can continue to use the CPU.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Docker，CPU限制（以毫核为单位）将乘以100，以确定容器每100毫秒可以使用的CPU时间。如果CPU负载过重，一旦容器使用完其配额，它将不得不等到下一个100毫秒周期才能继续使用CPU。
- en: The method used to share CPU resources between different processes running in
    cgroups is called the **Completely Fair Scheduler** or **CFS**; this works by
    dividing CPU time between the different cgroups. This typically means assigning
    a certain number of slices to a cgroup. If the processes in one cgroup are idle
    and don't use their allocated CPU time, these shares will become available to
    be used by processes in other cgroups.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在cgroups中运行的不同进程之间共享CPU资源的方法称为**完全公平调度器**或**CFS**；这通过在不同的cgroups之间分配CPU时间来实现。这通常意味着为一个cgroup分配一定数量的时间片。如果一个cgroup中的进程处于空闲状态，并且没有使用其分配的CPU时间，这些份额将可供其他cgroup中的进程使用。
- en: This means that a pod might perform well even if the limit is set too low, but
    could then grind to a halt only later, when another pod begins to take its fair
    share of allocated CPU. You may find that if you begin to set CPU limits on your
    pods on an empty cluster and add additional workloads, the performance of your
    pods begins to suffer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着一个pod即使限制设置得太低，可能仍然表现良好，但只有在另一个pod开始占用其公平份额的CPU后，它才可能突然停止。您可能会发现，如果您在空集群上开始为您的pod设置CPU限制，并添加额外的工作负载，您的pod的性能会开始受到影响。
- en: Later in this chapter, we discuss some basic tooling that can give us an idea
    of how much CPU each pod is using.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面，我们将讨论一些基本的工具，可以让我们了解每个pod使用了多少CPU。
- en: If memory limits are reached, the container runtime will kill the container
    (and it might be restarted). If a container is using more memory than the requested
    amount, it becomes a candidate for eviction if and when the node begins to run
    low on memory.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果内存限制达到，容器运行时将终止容器（并可能重新启动）。如果容器使用的内存超过请求的数量，那么当节点开始内存不足时，它就成为被驱逐的候选者。
- en: Quality of Service (QoS)
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务质量（QoS）
- en: 'When Kubernetes creates a pod, it is assigned one of three QoS classes. These
    classes are used to decide how Kubernetes schedules and evicts pods from nodes.
    Broadly, pods with a guaranteed QoS class will be subject to the least amount
    of disruption from evictions, and pods with the BestEffort QoS class are the most
    likely to be disrupted:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当Kubernetes创建一个pod时，它被分配为三个QoS类别之一。这些类别用于决定Kubernetes如何在节点上调度和驱逐pod。广义上讲，具有保证QoS类别的pod将受到最少的驱逐干扰，而具有BestEffort
    QoS类别的pod最有可能受到干扰：
- en: '**Guaranteed**: This is for high-priority workloads that benefit from avoiding
    being evicted from a node wherever possible and have priority over pods in the
    lower QoS classes for CPU resources, with the container runtime guaranteeing that
    the full amount of the CPU specified in the limit will be available when needed.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保证**：这适用于高优先级的工作负载，这些工作负载受益于尽可能避免从节点中被驱逐，并且对于CPU资源具有比较低QoS类别的pod的优先级，容器运行时保证在需要时将提供指定限制中的全部CPU数量。'
- en: '**Burstable**: This is for less important workloads, for example, background
    jobs that can take advantage of greater CPU when available but are only guaranteed
    the level specified in the CPU request. Burstable pods are more likely to be evicted
    from a node than those in the Guaranteed QoS class when the node is running low
    on resources, especially if they are using more than the requested amount of memory.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可突发**: 这适用于较不重要的工作负载，例如可以在可用时利用更多CPU的后台作业，但只保证CPU请求中指定的级别。当节点资源不足时，可突发的pod更有可能被从节点中驱逐，特别是如果它们使用的内存超过了请求的数量。'
- en: '**BestEffort**: Pods with this class are the most likely to be evicted if a
    node is running low on resources. Pods in this QoS class can also only use whatever
    CPU and memory are free on the node at that time, so if other pods running on
    the node are making heavy use of the CPU, these pods may end up completely starved
    of resources. If you schedule Pods in this class, you should ensure that your
    application behaves as expected when subject to resource starvation and frequent
    restarts.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BestEffort**: 具有此类别的pod在节点资源不足时最有可能被驱逐。此QoS类别中的pod也只能使用节点上空闲的CPU和内存，因此如果节点上运行的其他pod正在大量使用CPU，这些pod可能会完全被资源耗尽。如果您在此类别中调度Pods，您应确保您的应用在资源匮乏和频繁重启时表现如预期。'
- en: In practice, it is always best to avoid using pods with a BestEffort QoS class,
    as these pods will be subject to very unusual behavior when the cluster is under
    heavy load.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，最好避免使用具有BestEffort QoS类别的pod，因为当集群负载过重时，这些pod将受到非常不寻常的行为影响。
- en: When we set the resource and request limits on the containers in our pod, the
    combination of the values decides the QoS class the pod will be in.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在pod的容器中设置资源和请求限制时，这些值的组合决定了pod所在的QoS类别。
- en: 'To be given a QoS class of BestEffort, none of the containers in the pod should
    have any CPU or memory requests or limits set:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要被赋予BestEffort的QoS类别，pod中的任何容器都不应该设置任何CPU或内存请求或限制：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A pod with no resource limits or requests will be assigned the BestEffort QoS
    class.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个没有资源限制或请求的pod将被分配BestEffort的QoS类别。
- en: 'To be given a QoS class of Guaranteed, a pod needs to have both CPU and memory
    requests and limits set on each container in the pod. The limits and requests
    must match each other. As a shortcut, if a container only has its limits set,
    Kubernetes automatically assigns equal values to the resource requests:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要被赋予保证的QoS类别，pod需要在pod中的每个容器上都设置CPU和内存请求和限制。限制和请求必须相匹配。作为快捷方式，如果一个容器只设置了其限制，Kubernetes会自动为资源请求分配相等的值：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A pod that will be assigned the Guaranteed QoS class.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将被分配保证的QoS类别的pod。
- en: 'Anything that falls between these two cases will be given a QoS class of Burstable.
    This applies to any pod where any CPU or memory limits or requests have been set
    on any pods. But where they do not meet the criteria for the Guaranteed class,
    for example by not setting both limits on each container, or by having requests
    and limits that do not match:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 任何介于这两种情况之间的情况都将被赋予可突发的QoS类别。这适用于任何设置了任何pod的CPU或内存限制或请求的pod。但是如果它们不符合保证类别的标准，例如没有在每个容器上设置限制，或者请求和限制不匹配：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A pod that will be assigned the Burstable QoS class.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将被分配可突发的QoS类别的pod。
- en: Resource quotas
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源配额
- en: Resource quotas allow you to place limits on how many resources a particular
    namespace can use. Depending on how you have chosen to use namespaces in your
    organization, they can give you a powerful way to limit the resources that are
    used by a particular team, application, or group of applications, while still
    giving developers the freedom to tweak the resource limits of each individual
    container.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 资源配额允许您限制特定命名空间可以使用多少资源。根据您在组织中选择使用命名空间的方式，它们可以为您提供一种强大的方式，限制特定团队、应用程序或一组应用程序使用的资源，同时仍然让开发人员有自由调整每个单独容器的资源限制。
- en: Resource quotas are a useful tool when you want to control the resource costs
    of different teams or applications, but still want to achieve the utilization
    benefits of scheduling multiple workloads to the same cluster.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 资源配额是一种有用的工具，当您想要控制不同团队或应用程序的资源成本，但仍希望实现将多个工作负载调度到同一集群的利用率时。
- en: In Kubernetes, resource quotas are managed by an admission controller. This
    controller tracks the use of resources such as pods and services, and if a limit
    is exceeded, it prevents new resources from being created.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，资源配额由准入控制器管理。该控制器跟踪诸如 Pod 和服务之类的资源的使用，如果超出限制，它将阻止创建新资源。
- en: The resource quota admission controller is configured by one or more `ResourceQuota`
    objects created in the namespace. These objects would typically be created by
    a cluster administrator, but you could integrate creating them into a wider process
    in your organization for allocating resources.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 资源配额准入控制器由在命名空间中创建的一个或多个 `ResourceQuota` 对象配置。这些对象通常由集群管理员创建，但您可以将创建它们整合到您组织中用于分配资源的更广泛流程中。
- en: 'Let''s look at an example of how a quota can be used to limit the use of CPU
    resources in a cluster. As quotas will affect all the pods within a namespace,
    we will start by creating a new namespace using `kubectl`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个示例，说明配额如何限制集群中 CPU 资源的使用。由于配额将影响命名空间中的所有 Pod，因此我们将从使用 `kubectl` 创建一个新的命名空间开始：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will start by creating a simple example that will ensure that every new
    pod that is created has the CPU limit set, and that the total limits do not exceed
    two cores:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从创建一个简单的示例开始，确保每个新创建的 Pod 都设置了 CPU 限制，并且总限制不超过两个核心：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Create the `ResourceQuota` by submitting the manifest to the cluster using `kubectl`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `kubectl` 将清单提交到集群来创建 `ResourceQuota`。
- en: Once a `ResourceQuota` specifying resource requests or limits has been created
    in a namespace, it becomes mandatory for all pods to specify a corresponding request
    or limit before they can be created.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在命名空间中创建了指定资源请求或限制的 `ResourceQuota`，则在创建之前，所有 Pod 必须指定相应的请求或限制。
- en: 'To see this behavior in action, let''s create an example deployment in our
    namespace:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这种行为，让我们在我们的命名空间中创建一个示例部署：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once you have submitted the deployment manifest to Kubernetes with `kubectl`,
    check that the pod is running:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您使用 `kubectl` 将部署清单提交给 Kubernetes，请检查 Pod 是否正在运行：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, scale up the deployment and observe that additional pods are created:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，扩展部署并观察是否创建了额外的 Pod：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Because we specified a CPU limit of `500m`, there is no problem scaling our
    deployment to four replicas, which uses the two cores that we specified in our
    quota.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们指定了 `500m` 的 CPU 限制，所以将部署扩展到四个副本没有问题，这使用了我们在配额中指定的两个核心。
- en: 'But if you now try to scale the deployment so it uses more resources than specified
    in the quota, you will find that additional pods are not scheduled by Kubernetes:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您现在尝试扩展部署，使其使用的资源超出配额中指定的资源，您将发现 Kubernetes 不会安排额外的 Pod：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running `kubectl get events` will show you a message where the scheduler failed
    to create the additional pod required to meet the replica count:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`kubectl get events`将显示一个消息，其中调度程序未能创建满足副本计数所需的额外pod：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Default limits
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 默认限制
- en: When you are using quotas on a namespace, one requirement is that every container
    in the namespace must have resource limits and requests defined. Sometimes this
    requirement can cause complexity and make it more difficult to work quickly with
    Kubernetes. Specifying resource limits correctly, while an essential part of preparing
    an application for production, does add additional overhead when, for example,
    using Kubernetes as a platform for development or testing workloads.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在命名空间上使用配额时，一个要求是命名空间中的每个容器必须定义资源限制和请求。有时，这个要求可能会导致复杂性，并使在Kubernetes上快速工作变得更加困难。正确指定资源限制是准备应用程序投入生产的重要部分，但是，例如，在使用Kubernetes作为开发或测试工作负载的平台时，这确实增加了额外的开销。
- en: Kubernetes provides the facility for default requests and limits to be provided
    at the namespace level. You could use this to provide some sensible defaults to
    namespaces used by a particular application or team.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了在命名空间级别提供默认请求和限制的功能。您可以使用这个功能为特定应用程序或团队使用的命名空间提供一些合理的默认值。
- en: We can configure default limits and requests for the containers in a namespace
    using the `LimitRange` object. This object allows us to provide defaults for the
    CPU or memory, or both. If a `LimitRange` object exists in a namespace, then any
    container created without the resource requests or limits configured in `LimitRange`
    will inherit these values from the limit range.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`LimitRange`对象为命名空间中的容器配置默认限制和请求。这个对象允许我们为CPU或内存，或两者都提供默认值。如果一个命名空间中存在一个`LimitRange`对象，那么任何没有在`LimitRange`中配置资源请求或限制的容器将从限制范围中继承这些值。
- en: 'There are two situations where `LimitRange` will affect the resource limits
    or requests when a pod is created:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种情况下，当创建一个pod时，`LimitRange`会影响资源限制或请求：
- en: Containers that have no resource limits or requests will inherit the resource
    limit and requests from the `LimitRange` object
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有资源限制或请求的容器将从`LimitRange`对象继承资源限制和请求
- en: Containers that have no resource limits but do have requests specified will
    inherit the resource limit from the `LimitRange` object
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有资源限制但有指定请求的容器将从`LimitRange`对象中继承资源限制
- en: 'If a container already has limits and requests defined, then `LimitRange` will
    have no effect. Because containers that specify only limits default the request
    field to the same value, they will not inherit the request value from `LimitRange`.
    Let''s look at a quick example of this in action. We start by creating a new namespace:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器已经定义了限制和请求，那么`LimitRange`将不起作用。因为只指定了限制的容器会将请求字段默认为相同的值，它们不会从`LimitRange`继承请求值。让我们看一个快速示例。我们首先创建一个新的命名空间：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create a manifest for the limit range object, and submit it to the cluster
    with `kubectl`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 创建限制范围对象的清单，并使用`kubectl`将其提交到集群中：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you create a pod in this namespace without resource limits, it will inherit
    from the limit range object when they are created:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在此命名空间中创建一个没有资源限制的pod，它将在创建时从限制范围对象中继承：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`deployment.apps/` example created.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`deployment.apps/`示例已创建。'
- en: 'You can check the limits by running `kubectl describe`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行`kubectl describe`来检查限制：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Horizontal Pod Autoscaling
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水平Pod自动缩放
- en: Some applications can be scaled up to handle an increased load by adding additional
    replicas. Stateless web applications are a great example of this, as adding additional
    replicas provides the additional capacity required to handle increased requests
    to your application. Some other applications are also designed to operate in such
    a way that adding additional pods can handle increased loads; many systems that
    are architected around processing messages from a central queue can also handle
    an increased load in this way.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一些应用程序可以通过添加额外的副本来扩展以处理增加的负载。无状态的 Web 应用程序就是一个很好的例子，因为添加额外的副本提供了处理对应用程序的增加请求所需的额外容量。一些其他应用程序也设计成可以通过添加额外的
    pod 来处理增加的负载；许多围绕从中央队列处理消息的系统也可以以这种方式处理增加的负载。
- en: When we use Kubernetes deployments to deploy our pod workloads, it is simple
    to scale the number of replicas used by our applications up and down using the
    `kubectl scale` command. However, if we want our applications to automatically
    respond to changes in their workloads and scale to meet demand, then Kubernetes
    provides us with Horizontal Pod Autoscaling.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 Kubernetes 部署来部署我们的 pod 工作负载时，使用 `kubectl scale` 命令简单地扩展应用程序使用的副本数量。然而，如果我们希望我们的应用程序自动响应其工作负载的变化并根据需求进行扩展，那么
    Kubernetes 为我们提供了水平 Pod 自动缩放。
- en: Horizontal Pod Autoscaling allows us to define rules that will scale the numbers
    of replicas up or down in our deployments based on CPU utilization and optionally
    other custom metrics. Before we are able to use Horizontal Pod Autoscaling in
    our cluster, we need to deploy the Kubernetes metrics server; this server provides
    endpoints that are used to discover CPU utilization and other metrics generated
    by our applications.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 水平 Pod 自动缩放允许我们定义规则，根据 CPU 利用率和其他自定义指标，在我们的部署中扩展或缩减副本的数量。在我们的集群中使用水平 Pod 自动缩放之前，我们需要部署
    Kubernetes 度量服务器；该服务器提供了用于发现应用程序生成的 CPU 利用率和其他指标的端点。
- en: Deploying the metrics server
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署度量服务器
- en: Before we can make use of Horizontal Pod Autoscaling, we need to deploy the
    Kubernetes metrics server to our cluster. This is because the Horizontal Pod Autoscaling
    controller makes use of the metrics provided by the `metrics.k8s.io` API, which
    is provided by the metrics server.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以使用水平 Pod 自动缩放之前，我们需要将 Kubernetes 度量服务器部署到我们的集群中。这是因为水平 Pod 自动缩放控制器使用 `metrics.k8s.io`
    API 提供的指标，而这些指标是由度量服务器提供的。
- en: While some installations of Kubernetes may install this add-on by default, in
    our EKS cluster we will need to deploy it ourselves.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些 Kubernetes 的安装可能默认安装此附加组件，在我们的 EKS 集群中，我们需要自己部署它。
- en: 'There are a number of ways to deploy add-on components to your cluster:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以部署附加组件到您的集群中：
- en: If you have followed the advice in [Chapter 7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml), *A
    Production-Ready Cluster*, and are provisioning your cluster with Terraform, you
    could provision the required manifests with `kubectl` as we did in [Chapter 7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml), *A
    Production-Ready Cluster*, when we provisioned kube2iam.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您遵循了[第7章](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml)中的建议，*一个生产就绪的集群*，并且正在使用
    Terraform 为您的集群进行配置，您可以像我们在[第7章](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml)中配置
    kube2iam 时一样，使用 `kubectl` 部署所需的清单。
- en: If you are using helm to manage applications on your cluster, you could use
    the stable/metrics server chart.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您正在使用 helm 管理集群上的应用程序，您可以使用 stable/metrics server 图表。
- en: In this chapter, for simplicity we are just going to deploy the metrics server
    manifests using `kubectl.`
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这一章中，为了简单起见，我们将使用 `kubectl` 部署度量服务器清单。
- en: I like to integrate deploying add-ons such as the metrics server and kube2iam
    with the process that provisions the cluster, as I see them as integral parts
    of the cluster infrastructure. But if you are going to use a tool like helm to
    manage deploying applications to your cluster, then you might prefer to manage
    everything running on your cluster with the same tool. The decision you take really
    depends on the processes you and your team adopt for managing your cluster and
    the applications that run on it.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我喜欢将部署诸如指标服务器和kube2iam等附加组件集成到配置集群的过程中，因为我认为它们是集群基础设施的组成部分。但是，如果您要使用类似helm的工具来管理在集群上运行的应用程序的部署，那么您可能更喜欢使用相同的工具来管理在集群上运行的所有内容。您所做的决定取决于您和您的团队为管理集群及其上运行的应用程序采用的流程。
- en: The metrics server is developed in the GitHub repository found at [https://github.com/kubernetes-incubator/metrics-server](https://github.com/kubernetes-incubator/metrics-server)
    You will find the manifests required to deploy it in the deploy directory of that
    repository.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标服务器是在GitHub存储库中开发的，网址为[https://github.com/kubernetes-incubator/metrics-server](https://github.com/kubernetes-incubator/metrics-server)。您将在该存储库的deploy目录中找到部署所需的清单。
- en: Start by cloning the configuration from GitHub. The metrics server began supporting
    the authentication methods provided by EKS in version 0.0.3 so make sure the manifests
    you have use at least that version.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先从GitHub克隆配置。指标服务器从版本0.0.3开始支持EKS提供的身份验证方法，因此请确保您使用的清单至少使用该版本。
- en: You will find a number of manifests in the `deploy/1.8+` directory. The `auth-reader.yaml`
    and `auth-delegator.yaml` files configure the integration of the metrics server
    with the Kubernetes authorization infrastructure. The `resource-reader.yaml` file
    configures a role to give the metrics server the permissions to read resources
    from the API server, in order to discover the nodes that pods are running on.
    Basically, `metrics-server-deployment.yaml` and `metrics-server-service.yaml`
    define the deployment used to run the service itself and a service to be able
    to access it. Finally, the `metrics-apiservice.yaml` file defines an `APIService`
    resource that registers the metrics.k8s.io API group with the Kubernetes API server
    aggregation layer; this means that requests to the API server for the metrics.k8s.io
    group will be proxied to the metrics server service.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在`deploy/1.8+`目录中找到许多清单。`auth-reader.yaml`和`auth-delegator.yaml`文件配置了指标服务器与Kubernetes授权基础设施的集成。`resource-reader.yaml`文件配置了一个角色，以赋予指标服务器从API服务器读取资源的权限，以便发现Pod所在的节点。基本上，`metrics-server-deployment.yaml`和`metrics-server-service.yaml`定义了用于运行服务本身的部署，以及用于访问该服务的服务。最后，`metrics-apiservice.yaml`文件定义了一个`APIService`资源，将metrics.k8s.io
    API组注册到Kubernetes API服务器聚合层；这意味着对于metrics.k8s.io组的API服务器请求将被代理到指标服务器服务。
- en: 'Deploying these manifests with `kubectl` is simple, just submit all of the
    manifests to the cluster with `kubectl apply`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl`部署这些清单很简单，只需使用`kubectl apply`将所有清单提交到集群：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You should see a message about each of the resources being created on the cluster.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到关于在集群上创建的每个资源的消息。
- en: If you are using a tool like Terraform to provision your cluster, you might
    use it to submit the manifests for the metrics server when you create your cluster.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用类似Terraform的工具来配置集群，您可以在创建集群时使用它来提交指标服务器的清单。
- en: Verifying the metrics server and troubleshooting
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证指标服务器和故障排除
- en: Before we continue, we should take a moment to check that our cluster and the
    metrics server are correctly configured to work together.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们应该花一点时间检查我们的集群和指标服务器是否正确配置以共同工作。
- en: After the metrics server is running on your cluster and has had a chance to
    collect metrics from the cluster (give it a minute or so), you should be able
    to use the `kubectl top` command to see the resource usage of the pods and nodes
    in your cluster.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在度量服务器在您的集群上运行并有机会从集群中收集度量数据（给它一分钟左右的时间）之后，您应该能够使用`kubectl top`命令来查看集群中pod和节点的资源使用情况。
- en: 'Start by running `kubectl top nodes`. If you see output like this, then the
    metrics server is configured correctly and is collecting metrics from your nodes:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先运行`kubectl top nodes`。如果您看到像这样的输出，那么度量服务器已经正确配置，并且正在从您的节点收集度量数据：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If you see an error message, then there are a number of troubleshooting steps
    you can follow.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到错误消息，那么有一些故障排除步骤可以遵循。
- en: 'You should start by describing the metrics server deployment and checking that
    one replica is available:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该从描述度量服务器部署并检查一个副本是否可用开始：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If it is not, you should debug the created pod by running `kubectl -n kube-system
    describe pod`. Look at the events to see why the server is not available. Make
    sure that you are running at least version 0.0.3 of the metrics server, as previous
    versions didn't support authenticating with the EKS API server.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有配置正确，您应该通过运行`kubectl -n kube-system describe pod`来调试创建的pod。查看事件，看看服务器为什么不可用。确保您至少运行版本0.0.3的度量服务器，因为之前的版本不支持与EKS
    API服务器进行身份验证。
- en: If the metrics server is running correctly and you still see errors when running
    `kubectl top`, the issue is that the APIservice registered with the aggregation
    layer is not configured correctly. Check the events output at the bottom of the
    information returned when you run `kubectl describe apiservice v1beta1.metrics.k8s.io`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果度量服务器正在正确运行，但在运行`kubectl top`时仍然看到错误，问题可能是聚合层注册的APIservice没有正确配置。在运行`kubectl
    describe apiservice v1beta1.metrics.k8s.io`时，检查底部返回的信息中的事件输出。
- en: One common issue is that the EKS control plane cannot connect to the metrics
    server service on port `443`. If you followed the instructions in [Chapter 7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml),
    *A Production-Ready Cluster*, you should already have a security group rule allowing
    this traffic from the control plane to the worker nodes, but some other documentation
    can suggest more restrictive rules, which might not allow traffic on port `443`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的问题是，EKS控制平面无法连接到端口`443`上的度量服务器服务。如果您遵循了[第7章](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml)中的说明，*一个生产就绪的集群*，您应该已经有一个安全组规则允许控制平面到工作节点的流量，但一些其他文档可能建议更严格的规则，这可能不允许端口`443`上的流量。
- en: Autoscaling pods based on CPU usage
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根据CPU使用率自动扩展pod
- en: Once the metrics server has been installed into our cluster, we will be able
    to use the metrics API to retrieve information about CPU and memory usage of the
    pods and nodes in our cluster. Using the `kubectl top` command is a simple example
    of this.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦度量服务器安装到我们的集群中，我们将能够使用度量API来检索有关集群中pod和节点的CPU和内存使用情况的信息。使用`kubectl top`命令就是一个简单的例子。
- en: The Horizontal Pod Autoscaler can also use this same metrics API to gather information
    about the current resource usage of the pods that make up a deployment.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 水平Pod自动缩放器也可以使用相同的度量API来收集组成部署的pod的当前资源使用情况的信息。
- en: Let's look at an example of this; we are going to deploy a sample application
    that uses a lot of CPU under load, then configure a Horizontal Pod Autoscaler
    to scale up extra replicas of this pod to provide extra capacity when CPU utilization
    exceeds a target level.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子；我们将部署一个使用大量CPU的示例应用程序，然后配置一个水平Pod自动缩放器，在CPU利用率超过目标水平时扩展该pod的额外副本，以提供额外的容量。
- en: 'The application we will be deploying as an example is a simple Ruby web application
    that can calculate the nth number in the Fibonacci sequence, this application
    uses a simple recursive algorithm, and is not very efficient (perfect for us to
    experiment with autoscaling). The deployment for this application is very simple.
    It is important to set resource limits for CPU because the target CPU utilization
    is based on a percentage of this limit:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将部署的示例应用程序是一个简单的Ruby Web应用程序，可以计算斐波那契数列中的第n个数字，该应用程序使用简单的递归算法，效率不是很高（非常适合我们进行自动缩放实验）。该应用程序的部署非常简单。设置CPU资源限制非常重要，因为目标CPU利用率是基于此限制的百分比：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We are not specifying a number of replicas in the deployment spec; when we first
    submit this deployment to the cluster, the number of replicas will therefore default
    to 1\. This is good practice when creating a deployment where we intend the replicas
    to be adjusted by a Horizontal Pod Autoscaler, because it means that if we use
    `kubectl apply` to update the deployment later, we won't override the replica
    value the Horizonal Pod Autoscaler has set (inadvertently scaling the deployment
    down or up).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有在部署规范中指定副本的数量；因此，当我们首次将此部署提交到集群时，副本的数量将默认为1。这是创建部署的良好实践，我们打算通过Horizontal
    Pod Autoscaler调整副本的数量，因为这意味着如果我们稍后使用`kubectl apply`来更新部署，我们不会覆盖水平Pod Autoscaler设置的副本值（无意中缩减或增加部署）。
- en: 'Let''s deploy this application to the cluster:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个应用程序部署到集群中：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You could run `kubectl get pods -l app=fib` to check that the application started
    up correctly.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以运行`kubectl get pods -l app=fib`来检查应用程序是否正确启动。
- en: 'We will create a service, so we are able to access the pods in our deployment,
    requests will be proxied to each of the replicas, spreading the load:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个服务，以便能够访问部署中的Pod，请求将被代理到每个副本，分散负载：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Submit the service manifest to the cluster with `kubectl`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl`将服务清单提交到集群：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We are going to configure a Horizonal Pod Autoscaler to control the number of
    replicas in our deployment. The `spec` defines how we want the autoscaler to behave;
    we have defined here that we want the autoscaler to maintain between 1 and 10
    replicas of our application and achieve a target average CPU utilization of 60,
    across those replicas.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将配置一个Horizontal Pod Autoscaler来控制部署中副本的数量。`spec`定义了我们希望自动缩放器的行为；我们在这里定义了我们希望自动缩放器维护应用程序的1到10个副本，并实现60%的目标平均CPU利用率。
- en: 'When CPU utilization falls below 60%, then the autoscaler will adjust the replica
    count of the targeted deployment down; when it goes above 60%, replicas will be
    added:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当CPU利用率低于60%时，自动缩放器将调整目标部署的副本计数；当超过60%时，将添加副本：
- en: '[PRE23]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Create the autoscaler with `kubectl`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl`创建自动缩放器：
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `kubectl autoscale` command is a shortcut to create a `HorizontalPodAutoscaler`.
    Running `kubectl autoscale deployment fib --min=1 --max=10 --cpu-percent=60` would
    create an equivalent autoscaler.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl autoscale`命令是创建`HorizontalPodAutoscaler`的快捷方式。运行`kubectl autoscale
    deployment fib --min=1 --max=10 --cpu-percent=60`将创建一个等效的自动缩放器。'
- en: 'Once you have created the Horizontal Pod Autoscaler, you can see a lot of interesting
    information about its current state with `kubectl describe`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了Horizontal Pod Autoscaler后，您可以使用`kubectl describe`查看有关其当前状态的许多有趣信息：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now we have set up our Horizontal Pod Autoscaler, we should generate some load
    on the pods in our deployment to illustrate how it works. In this case, we are
    going to use the `ab` (Apache benchmark) tool to repeatedly ask our application
    to compute the thirtieth Fibonacci number:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了Horizontal Pod Autoscaler，我们应该在部署中的Pod上生成一些负载，以说明它的工作原理。在这种情况下，我们将使用`ab`（Apache
    benchmark）工具重复要求我们的应用程序计算第30个斐波那契数：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This job uses `ab` to make 1,000 requests to the endpoint (with a concurrency
    of 4). Submit the job to the cluster, then observe the state of the Horizontal
    Pod Autoscaler:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此作业使用`ab`向端点发出1,000个请求（并发数为4）。将作业提交到集群，然后观察水平Pod自动缩放器的状态：
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once the load job has started to make requests, the autoscaler will scale up
    the deployment in order to handle the load:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦负载作业开始发出请求，自动缩放器将扩展部署以处理负载：
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Autoscaling pods based on other metrics
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于其他指标自动缩放pod
- en: The metrics server provides APIs that the Horizontal Pod Autoscaler can use
    to gain information about the CPU and memory utilization of pods in the cluster.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 度量服务器提供了水平Pod自动缩放器可以使用的API，以获取有关集群中pod的CPU和内存利用率的信息。
- en: 'It is possible to target a utilization percentage like we did for the CPU metric,
    or to target the absolute value as we have here for the memory metric:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 可以针对利用率百分比进行目标设置，就像我们对CPU指标所做的那样，或者可以针对绝对值进行目标设置，就像我们对内存指标所做的那样：
- en: '[PRE29]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The Horizonal Pod Autoscaler also allows us to scale on other metrics provided
    by more comprehensive metrics systems. Kubernetes allows for metrics APIs to be
    aggregated for custom and external metrics.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 水平Pod自动缩放器还允许我们根据更全面的指标系统提供的其他指标进行缩放。Kubernetes允许聚合自定义和外部指标的指标API。
- en: Custom metrics are metrics other than CPU and memory that are associated with
    a pod. You might for example use an adapter that allows you to use metrics that
    a system like Prometheus has collected from your pods.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义指标是与pod相关的除CPU和内存之外的指标。例如，您可以使用适配器，使您能够使用像Prometheus这样的系统从您的pod中收集的指标。
- en: This can be very beneficial if you have more detailed metrics available about
    the utilization of your application, for example, a forking web server that exposes
    a count of busy worker processes, or a queue processing application that exposes
    metrics about the number of items currently enqueued.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有关于应用程序利用率的更详细的指标可用，这可能非常有益，例如，一个公开忙碌工作进程计数的分叉Web服务器，或者一个公开有关当前排队项目数量的指标的队列处理应用程序。
- en: External metrics adapters provide information about resources that are not associated
    with any object within Kubernetes, for example, if you were using an external
    queuing system, such as the AWS SQS service.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 外部指标适配器提供有关与Kubernetes中的任何对象不相关的资源的信息，例如，如果您正在使用外部排队系统，比如AWS SQS服务。
- en: On the whole, it is simpler if your applications can expose metrics about resources
    that they depend on that use an external metrics adapter, as it can be hard to
    limit access to particular metrics, whereas custom metrics are tied to a particular
    Pod, so Kubernetes can limit access to only those users and processes that need
    to use them.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，如果您的应用程序可以公开有关其所依赖的资源的指标，并使用外部指标适配器，那将更简单，因为很难限制对特定指标的访问，而自定义指标与特定的Pod相关联，因此Kubernetes可以限制只有那些需要使用它们的用户和进程才能访问它们。
- en: Autoscaling the cluster
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群自动缩放
- en: The capabilities of Kubernetes Horizontal Pod Autoscaler allow us to add and
    remove pod replicas from our applications as their resource usage changes over
    time. However, this makes no difference to the capacity of our cluster. If our
    pod autoscaler is adding pods to handle an increase in load, then eventually we
    might run out of space in our cluster, and additional pods would fail to be scheduled.
    If there is a decrease in the load on our application and the pod autoscaler removes
    pods, then we are paying AWS for EC2 instances that will sit idle.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Horizontal Pod Autoscaler的功能使我们能够根据应用程序随时间变化的资源使用情况添加和删除pod副本。然而，这对我们集群的容量没有影响。如果我们的pod自动缩放器正在添加pod来处理负载增加，那么最终我们的集群可能会用尽空间，额外的pod将无法被调度。如果我们的应用程序负载减少，pod自动缩放器删除pod，那么我们就需要为EC2实例支付费用，而这些实例将处于空闲状态。
- en: When we created our cluster in [Chapter 7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml),
    *A Production-Ready Cluster*, we deployed the cluster nodes using an autoscaling
    group, so we should be able to use this to grow and shrink the cluster as the
    needs of the applications deployed to it change over time.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第7章](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml)中创建了我们的集群，*一个生产就绪的集群*，我们使用自动缩放组部署了集群节点，因此我们应该能够利用它根据部署到集群上的应用程序的需求随时间变化而扩展和收缩集群。
- en: Autoscaling groups have built-in support for scaling the size of the cluster,
    based on the average CPU utilization of the instances. This, however, is not really
    suitable when dealing with a Kubernetes cluster because the workloads running
    on each node of our cluster might be quite different, so the average CPU utilization
    is not really a very good proxy for the free capacity of the cluster.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 自动缩放组内置支持根据实例的平均CPU利用率来调整集群的大小。然而，当处理Kubernetes集群时，这并不是很合适，因为运行在集群每个节点上的工作负载可能会有很大不同，因此平均CPU利用率并不是集群空闲容量的很好代理。
- en: Thankfully, in order to schedule pods to nodes effectively, Kubernetes keeps
    track of the capacity of each node and the resources requested by each pod. By
    utilizing this information, we can automate scaling the cluster to match the size
    of the workload.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 值得庆幸的是，为了有效地将pod调度到节点上，Kubernetes会跟踪每个节点的容量和每个pod请求的资源。通过利用这些信息，我们可以自动调整集群的大小以匹配工作负载的大小。
- en: The Kubernetes autoscaler project provides a cluster autoscaler component for
    some of the main cloud providers, including AWS. The cluster autoscaler can be
    deployed to our cluster quite simply. As well as being able to add instances to
    our cluster, the cluster autoscaler is also able to drain the pods from and then
    terminate instances when the capacity of the cluster can be reduced.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes自动缩放器项目为一些主要的云提供商提供了集群自动缩放器组件，包括AWS。集群自动缩放器可以很简单地部署到我们的集群。除了能够向我们的集群添加实例外，集群自动缩放器还能够从集群中清除pod，然后在集群容量可以减少时终止实例。
- en: Deploying the cluster autoscaler
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署集群自动缩放器
- en: Deploying the cluster autoscaler to our cluster is quite simple as it just requires
    a simple pod to be running. All we need for this is a simple Kubernetes deployment,
    just as we have used in previous chapters.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 将集群自动缩放器部署到我们的集群非常简单，因为它只需要一个简单的pod在运行。我们只需要一个简单的Kubernetes部署，就像我们在之前的章节中使用过的那样。
- en: 'In order for the cluster autoscaler to update the desired capacity of our autoscaling
    group, we need to give it permissions via an IAM role. If you are using kube2iam,
    as we discussed in [Chapter 7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml), *A
    Production-Ready Cluster*, we will be able to specify this role for the cluster
    autoscaler pod via an appropriate annotation:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让集群自动缩放器更新自动缩放组的期望容量，我们需要通过IAM角色授予权限。如果您正在使用kube2iam，正如我们在[第7章](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml)中讨论的那样，我们将能够通过适当的注释为集群自动缩放器pod指定这个角色：
- en: '[PRE30]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In order to deploy the cluster autoscaler to our cluster, we will submit a deployment
    manifest using `kubectl`, in a similar way to how we deployed kube2iam in [Chapter
    7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml), *A Production-Ready Cluster*.
    We will use Terraform's templating system to produce the manifest.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将集群自动缩放器部署到我们的集群，我们将使用`kubectl`提交一个部署清单，类似于我们在[第7章](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml)中部署kube2iam的方式，*一个生产就绪的集群*。我们将使用Terraform的模板系统来生成清单。
- en: 'We create a service account that is used by the autoscaler to connect to the
    Kubernetes API:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个服务账户，用于自动扩展器连接到Kubernetes API：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The cluster autoscaler needs to read information about the current resource
    usage of the cluster, and needs to be able to evict pods from nodes that need
    to be removed from the cluster and terminated. Basically, `cluster-autoscalerClusterRole`
    provides the required permissions for these actions. The following is the code
    continuation for `cluster_autoscaler.tpl`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放器需要读取有关集群当前资源使用情况的信息，并且需要能够从需要从集群中移除并终止的节点中驱逐Pod。基本上，`cluster-autoscalerClusterRole`为这些操作提供了所需的权限。以下是`cluster_autoscaler.tpl`的代码续写：
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Note that `cluster-autoscaler` stores state information in a config map, so
    needs permissions to be able to read and write from it. This role allows that.
    The following is the code continuation for `cluster_autoscaler.tpl`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`cluster-autoscaler`在配置映射中存储状态信息，因此需要有权限能够从中读取和写入。这个角色允许了这一点。以下是`cluster_autoscaler.tpl`的代码续写：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Finally, let's consider the manifest for the cluster autoscaler deployment itself.
    The cluster autoscaler pod contains a single container running the cluster autoscaler
    control loop. You will notice that we are passing some configuration to the cluster
    autoscaler as command-line arguments. Most importantly, the `--node-group-auto-discovery`
    flag allows the autoscaler to operate on autoscaling groups with the `kubernetes.io/cluster/<cluster_name>`
    tag that we set on our autoscaling group when we created the cluster in [Chapter
    7](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml), *A Production-Ready Cluster*.
    This is convenient because we don't have to explicitly configure the autoscaler
    with our cluster autoscaling group.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们考虑一下集群自动缩放器部署本身的清单。集群自动缩放器Pod包含一个运行集群自动缩放器控制循环的单个容器。您会注意到我们正在向集群自动缩放器传递一些配置作为命令行参数。最重要的是，`--node-group-auto-discovery`标志允许自动缩放器在具有我们在[第7章](a873e36a-237e-471d-ae10-d15d29fe47f6.xhtml)中创建集群时在我们的自动缩放组上设置的`kubernetes.io/cluster/<cluster_name>`标记的自动缩放组上操作。这很方便，因为我们不必显式地配置自动缩放器与我们的集群自动缩放组。
- en: If your Kubernetes cluster has nodes in more than one availability zone and
    you are running pods that rely on being scheduled to a particular zone (for example,
    pods that are making use of EBS volumes), it is recommended to create an autoscaling
    group for each availability zone that you plan to use. If you use one autoscaling
    group that spans several zones, then the cluster autoscaler will be unable to
    specify the availability zone of the instances that it launches.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的Kubernetes集群在多个可用区中有节点，并且正在运行依赖于被调度到特定区域的Pod（例如，正在使用EBS卷的Pod），建议为您计划使用的每个可用区创建一个自动缩放组。如果您使用跨多个区域的一个自动缩放组，那么集群自动缩放器将无法指定它启动的实例的可用区。
- en: 'Here is the code continuation for `cluster_autoscaler.tpl`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`cluster_autoscaler.tpl`的代码续写：
- en: '[PRE34]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, we render the templated manifest by passing in the variables for the
    AWS region, cluster name and IAM role, and submitting the file to Kubernetes using
    `kubectl`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过传递AWS区域、集群名称和IAM角色的变量来渲染模板化的清单，并使用`kubectl`将文件提交给Kubernetes：
- en: 'Here is the code continuation for `cluster_autoscaler.tpl`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`cluster_autoscaler.tpl`的代码续写：
- en: '[PRE35]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Kubernetes is a powerful tool; it is very effective at achieving much higher
    usage of compute resources than would ever be possible by manually scheduling
    applications to machines. It is important that you learn how to allocate resources
    to your pods by setting the correct resource limits and requests; if you don't,
    your applications can become unreliable or starved of resources.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个强大的工具；它非常有效地实现了比手动将应用程序调度到机器上更高的计算资源利用率。重要的是，您要学会通过设置正确的资源限制和请求来为您的Pod分配资源；如果不这样做，您的应用程序可能会变得不可靠或者资源匮乏。
- en: By understanding how Kubernetes assigns Quality of Service classes to your pods
    based on the resource requests and limits that you assign them, you can have precisely
    control how your pods are managed. By ensuring your critical applications, such
    as web servers and databases, run with the Guaranteed class, you can ensure that
    they will perform consistently and suffer minimal disruption when pods need to
    be rescheduled. You can improve the efficiency of your cluster by setting limits
    on lower-priority pods that will result in them being scheduled with the Burstable
    QoS class. Burstable pods can use extra resources when they are available but
    won't need extra capacity to be added to the cluster when load increases.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解Kubernetes如何根据您分配给它们的资源请求和限制为您的pod分配服务质量类别，您可以精确控制您的pod的管理方式。通过确保您的关键应用程序（如Web服务器和数据库）以保证类别运行，您可以确保它们将始终表现一致，并在需要重新安排pod时遭受最小的中断。您可以通过为较低优先级的pod设置限制来提高集群的效率，这将导致它们以可突发的QoS类别进行安排。可突发的pod可以在有空闲资源时使用额外的资源，但在负载增加时不需要向集群添加额外的容量。
- en: Resource quotas can be invaluable when managing a large cluster that is used
    to run several applications, and even by different teams in your organization,
    especially if you are trying to control the cost of non-production workloads,
    such as testing and staging environments.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 资源配额在管理大型集群时非常有价值，该集群用于运行多个应用程序，甚至由组织中的不同团队使用，特别是如果您试图控制非生产工作负载（如测试和分段环境）的成本。
- en: 'AWS calls its machines elastic for a reason: they can be scaled up or down
    in a matter of minutes to meet the demands of your applications. If you run workloads
    on a cluster where the load is variable, then you should make use of these properties
    and the tools that Kubernetes provides to scale your deployments to match the
    load that your applications are receiving, and your cluster to the size of the
    pods that need to be scheduled.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: AWS之所以称其机器为弹性，是有原因的：它们可以在几分钟内按需扩展或缩减，以满足应用程序的需求。如果您在负载变化的集群上运行工作负载，那么您应该利用这些特性和Kubernetes提供的工具来扩展部署，以匹配应用程序接收的负载以及需要安排的pod的大小。
