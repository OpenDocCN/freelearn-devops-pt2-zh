["```\naws.tf\nprovider \"aws\" { \n  version = \"~> 1.0\" \n\n  region = \"us-west-2\" \n} \n```", "```\nterraform.init\nInitializing provider plugins... \n- Checking for available provider plugins on https://releases.hashicorp.com... \n- Downloading plugin for provider \"aws\" (1.33.0)... \nTerraform has been successfully initialized! \n```", "```\nvariables.tf\nvariable \"cluster_name\" { \n  default = \"lovelace\" \n} \n\nvariable \"vpc_cidr\" { \n  default     = \"10.1.0.0/16\" \n  description = \"The CIDR of the VPC created for this cluster\" \n} \n\nvariable \"availability_zones\" { \n  default     = [\"us-west-2a\",\"us-west-2b\"] \n  description = \"The availability zones to run the cluster in\" \n} \n\nvariable \"k8s_version\" { \n  default     = \"1.10\" \n  description = \"The version of Kubernetes to use\" \n} \n```", "```\nnetworking.tf\n/*  Set up a VPC for our cluster. \n*/resource \"aws_vpc\" \"k8s\" { \n  cidr_block           = \"${var.vpc_cidr}\" \n  enable_dns_hostnames = true \n\n  tags = \"${ \n    map( \n     \"Name\", \"${var.cluster_name}\", \n     \"kubernetes.io/cluster/${var.cluster_name}\", \"shared\", \n    ) \n  }\" \n} \n\n/*  In order for our instances to connect to the internet \n  we provision an internet gateway.*/ \nresource \"aws_internet_gateway\" \"gateway\" { \n  vpc_id = \"${aws_vpc.k8s.id}\" \n\n  tags { \n    Name = \"${var.cluster_name}\" \n  } \n} \n\n/*  For instances without a Public IP address we will route traffic  \n  through a NAT Gateway. Setup an Elastic IP and attach it. \n\n  We are only setting up a single NAT gateway, for simplicity. \n  If the availability is important you might add another in a  \n  second availability zone. \n*/ \nresource \"aws_eip\" \"nat\" { \n  vpc        = true \n  depends_on = [\"aws_internet_gateway.gateway\"] \n} \n\nresource \"aws_nat_gateway\" \"nat_gateway\" { \n  allocation_id = \"${aws_eip.nat.id}\" \n  subnet_id     = \"${aws_subnet.public.*.id[0]}\" \n} \n```", "```\nterraform validate  \n```", "```\nterraform plan -out k8s.plan  \n```", "```\nTo perform exactly these actions, run the following command to apply:\nterraform apply \"k8s.plan\"\n```", "```\nkubeconfig.tpl \napiVersion: v1 \nkind: Config \nclusters: \n- name: ${cluster_name} \n  cluster: \n    certificate-authority-data: ${ca_data} \n    server: ${endpoint} \nusers: \n- name: ${cluster_name} \n  user: \n    exec: \n      apiVersion: client.authentication.k8s.io/v1alpha1 \n      command: aws-iam-authenticator \n      args: \n      - \"token\" \n      - \"-i\" \n      - \"${cluster_name}\" \ncontexts: \n- name: ${cluster_name} \n  context: \n    cluster: ${cluster_name} \n    user: ${cluster_name} \ncurrent-context: ${cluster_name} \n```", "```\ninstall.sh\n          #!/bin/bash\n          set -euxo pipefail  \n...\n          # Install aws-iam-authenticator\ncurl -Lo /usr/local/bin/heptio-authenticator-aws https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.3.0/heptio-authenticator-aws_0.3.0_linux_amd64\nchmod +x /usr/local/bin/heptio-authenticator-aws\n\napt-get install -y \\\n  docker-ce=$DOCKER_VERSION* \\\n  kubelet=$K8S_VERSION* \\\n  ekstrap=$EKSTRAP_VERSION*\n# Cleanup\napt-get clean\nrm -rf /tmp/*\n   # Cleanup\n   apt-get clean\n   rm -rf /tmp/*\n```", "```\npacker build node.json  \n```", "```\nnodes.tf \n/* \n  IAM policy for nodes \n*/ \ndata \"aws_iam_policy_document\" \"node\" { \n  statement { \n    actions = [\"sts:AssumeRole\"] \n\n    principals { \n      type        = \"Service\" \n      identifiers = [\"ec2.amazonaws.com\"] \n    } \n  } \n} \n... \n\nresource \"aws_iam_instance_profile\" \"node\" { \n  name = \"${aws_iam_role.node.name}\" \n  role = \"${aws_iam_role.node.name}\" \n} \n```", "```\n/* \n  This config map configures which IAM roles should be trusted by Kubernetes \n*/ \n\nresource \"local_file\" \"aws_auth\" { \n  content = <<YAML \napiVersion: v1 \nkind: ConfigMap \nmetadata: \n  name: aws-auth \n  namespace: kube-system \ndata: \n  mapRoles: | \n    - rolearn: ${aws_iam_role.node.arn} \n      username: system:node:{{EC2PrivateDNSName}} \n      groups: \n        - system:bootstrappers \n        - system:nodes \nYAML \n  filename = \"${path.module}/aws-auth-cm.yaml\" \n  depends_on = [\"local_file.kubeconfig\"] \n\n  provisioner \"local-exec\" { \n    command = \"kubectl --kubeconfig=${local_file.kubeconfig.filename} apply -f ${path.module}/aws-auth-cm.yaml\" \n  } \n} \n```", "```\n resource \"aws_security_group\" \"nodes\" { \n  name        = \"${var.cluster_name}-nodes\" \n  description = \"Security group for all nodes in the cluster\" \n  vpc_id      = \"${aws_vpc.k8s.id}\" \n\n  egress { \n    from_port   = 0 \n    to_port     = 0 \n    protocol    = \"-1\" \n    cidr_blocks = [\"0.0.0.0/0\"] \n  } \n\n... \nresource \"aws_security_group_rule\" \"nodes-control_plane-proxy\" { \n  description              = \"API (proxy) communication to pods\" \n  from_port                = 0 \n  to_port                  = 65535 \n  protocol                 = \"tcp\" \n  security_group_id        = \"${aws_security_group.nodes.id}\" \n  source_security_group_id = \\ \n                          \"${aws_security_group.control_plane.id}\" \n  type                     = \"ingress\" \n} \n```", "```\ndata \"aws_ami\" \"eks-worker\" { \n  filter { \n    name   = \"name\" \n    values = [\"eks-worker-${var.k8s_version}*\"] \n  } \n\n  most_recent = true \n  owners      = [\"self\"] \n} \n\n...                                                                    \n  resource \"aws_autoscaling_group\" \"node\" { \n  launch_configuration = \"${aws_launch_configuration.node.id}\" \n  max_size             = 2 \n  min_size             = 10 \n  name                 = \"eks-node-${var.cluster_name}\" \n  vpc_zone_identifier  = [\"${aws_subnet.private.*.id}\"] \n\n  tag { \n    key                 = \"Name\" \n    value               = \"eks-node-${var.cluster_name}\" \n    propagate_at_launch = true \n  } \n\n  tag { \n    key              = \"kubernetes.io/cluster/${var.cluster_name}\" \n    value               = \"owned\" \n    propagate_at_launch = true \n  } \n} \n\n```", "```\nkube2iam.yaml\n--- \napiVersion: v1 \nkind: ServiceAccount \nmetadata: \n  name: kube2iam \n  namespace: kube-system \n--- \napiVersion: v1 \nkind: List \nitems:         \n...                                                           kube2iam.tf \nresource \"null_resource\" \"kube2iam\" { \n  triggers = { \n    manifest_sha1 = \"${sha1(file(\"${path.module}/kube2iam.yaml\"))}\" \n  } \n\n  provisioner \"local-exec\" { \n    command = \" kubectl --kubeconfig=${local_file.kubeconfig.filename} apply -f \n${path.module}/kube2iam.yaml\" \n  } \n} \n```", "```\nUpdate K8s Node Security Groups \n\nOpen port 80 on the Node Security Group  \n```", "```\nAllow deveopers to access the guestbook app \n\nThe guestbook is served from port 80\\. We are allowing the control plane access to this port on the Node security groups, so developers can test the application using kubectl proxy. \n\nOnce the application is in production and we provision a LoadBalancer, we can remove these rules. \n```"]