# Eleven

# 在多个云和集群联合上运行 Kubernetes

在本章中，我们将通过在多个云、多个集群和集群联盟上运行 Kubernetes 来更进一步。Kubernetes 集群是一个紧密结合的单元，其中所有组件运行在相对接近的位置，并通过快速网络(通常是物理数据中心或云提供商可用性区域)连接在一起。这对于许多用例来说非常好，但是有几个重要的用例，系统需要扩展到单个集群之外。解决这个用例的一种方法是使用 Kubernetes 联邦，这是一种将多个 Kubernetes 集群组合在一起并作为单个实体与之交互的系统方法。园丁([https://gardener.cloud/](https://gardener.cloud/))项目采用的另一种方法是围绕管理多个独立的 Kubernetes 集群提供抽象。

我们将涵盖以下主题:

*   Kubernetes 集群联盟的历史
*   了解集群联盟
*   深入探究集群联盟的含义
*   如何准备、配置和管理集群联合
*   如何跨多个集群运行联合工作负载
*   园丁项目简介

# Kubernetes 集群联盟的历史

在进入集群联盟的细节之前，让我们先了解一些历史背景。谈论像 Kubernetes 这样 5 年前甚至不存在的项目的历史很有趣，但是发展的步伐和大量的贡献者通过加速进化带走了 Kubernetes。这与 Kubernetes 联盟尤其相关。

2015 年 3 月，Kubernetes 集群联盟([https://github . com/Kubernetes/community/blob/master/contributor/design-proposts/multi Cluster/Federation . MD](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multicluster/federation.md))提案初版发布。当时，它被亲切地昵称为“乌拜内斯”基本思想是重用现有的 Kubernetes APIs 来管理多个集群。这个提案，现在被称为联邦 V1，经历了几轮修订和实施，但它从未达到普遍可用性，并被认为在这一点上被否决。SIG 集群工作组意识到多集群问题比最初想象的要复杂。剥这只猫的皮有很多种方法，没有放之四海而皆准的解决方案。

集群联盟的新方向是联盟专用 API。在本章的其余部分，我们将考虑联邦 V2 的设计。请注意，当前状态被认为是 Alpha，因此我不建议在没有重大考虑的情况下将其投入生产使用。

# 了解集群联盟

集群联合在概念上很简单。您可以聚合多个 Kubernetes 集群，并将它们视为一个逻辑集群。有一个联合控制平面，向客户端呈现系统的单一统一视图。

下图展示了 Kubernetes 集群联盟的全貌。粉色方框是运行联盟应用编程接口的主机集群，绿色方框是成员集群:

![](img/B15559_11_01.png)

图 11.1:库本内特集群联盟

联合控制平面由相互协作的联合应用编程接口服务器和联合控制器管理器组成。联邦应用编程接口服务器将请求转发给联邦中的所有集群。此外，联邦控制器管理器通过将请求路由到各个联邦集群成员的变更来执行所有集群中控制器管理器的职责。在实践中，集群联盟并不是微不足道的，也不能完全抽象掉。跨吊舱通信和数据传输可能会突然产生巨大的延迟和成本开销。让我们先看一下集群联合的用例，了解联合组件和资源是如何工作的，然后检查硬部分:位置相似性、跨集群调度和联合数据访问。

## 集群联合的重要用例

有四类用例受益于集群联合。

### 容量溢出

AWS、GCE、Azure 等公有云平台很棒，提供了很多好处，但它们并不便宜。许多大型组织都在自己的数据中心投入了大量资金。其他组织与私营服务提供商合作，如 OVS、Rackspace 或数字海洋。如果您有独立管理和运营基础架构的运营能力，那么在您的基础架构上运行 Kubernetes 集群比在云中运行更经济。但是，如果您的一些工作负载出现波动，并且在相对较短的时间内需要大量容量，该怎么办？

例如，你的系统可能会在周末或节假日受到特别严重的冲击。传统方法是只提供额外的容量。但在许多动态情况下，这并不容易。有了容量溢出，您可以在运行于内部数据中心或私有服务提供商的 Kubernetes 集群中运行大部分工作，并在大型平台提供商之一上运行基于云的第二个 Kubernetes 集群。大多数情况下，基于云的集群将被关闭(停止的实例)，但是当需要时，您可以通过启动一些停止的实例来弹性地为您的系统增加容量。Kubernetes 集群联合可以使这种配置相对简单。它消除了容量规划和为大部分时间没有使用的硬件付费方面的许多麻烦。

这种方法有时被称为“云爆发”

### 敏感的工作负载

这几乎与容量溢出相反。也许你已经接受了云原生的生活方式，你的整个系统运行在云上，但是一些数据或工作负载处理敏感信息。法规遵从性或您组织的安全策略可能要求这些数据和工作负载必须在完全由您控制的环境中运行。您的敏感数据和工作负载可能会受到外部审计。确保没有任何信息从私有的 Kubernetes 集群泄露到基于云的 Kubernetes 集群可能是至关重要的。但是，可能希望在公共集群中具有可见性，并且能够将非敏感工作负载从私有集群启动到基于云的集群。如果工作负载的性质可以动态地从非敏感变为敏感，那么就需要通过制定适当的策略和实施过程来解决这个问题。

例如，您可能会阻止工作负载改变其性质。或者，您可以迁移突然变得敏感的工作负载，并确保它不再在基于云的集群上运行。另一个重要的例子是国家合规，法律要求某些数据必须保留，并且只能从指定的地理区域(通常是一个国家)访问。在这种情况下，必须在该地理区域创建集群。

### 避免供应商锁定

大型组织通常更喜欢有多种选择，而不是局限于单一的提供商。风险往往太大，因为提供商可能会关闭或无法提供相同级别的服务。拥有多个供应商通常也有利于价格谈判。Kubernetes 的设计与供应商无关。您可以在不同的云平台、私有服务提供商和内部数据中心运行它。

然而，这不是小事。如果您想确保能够快速切换提供者或将一些工作负载从一个提供者转移到下一个提供者，您应该已经在多个提供者上运行您的系统。你可以自己做，或者有一些公司提供在多个提供商上透明运行 Kubernetes 的服务。由于不同的提供商运行不同的数据中心，您可以自动从供应商范围的停机中获得一些冗余和保护。

### 地理分布高可用性

高可用性意味着即使系统的某些部分出现故障，服务仍然对用户可用。在联合 Kubernetes 集群的环境中，失败的范围是整个集群，这通常是由于托管集群的物理数据中心的问题，或者可能是平台提供商的更广泛的问题。高可用性的关键是冗余。地理分布式冗余意味着多个集群在不同的位置运行。可能是同一云提供商的不同可用性区域，也可能是同一云提供商的不同区域，甚至是完全不同的云提供商(参见*避免供应商锁定*部分)。在运行具有冗余的集群联盟时，有许多问题需要解决。我们稍后将讨论其中的一些问题。假设技术和组织问题已经解决，高可用性将允许您将流量从一个故障集群切换到另一个集群。在某种程度上，这对于用户来说应该是透明的(如果在切换期间有延迟，一些正在进行的请求或任务可能会消失或失败)。系统管理员可能需要采取额外的步骤来支持切换和处理原始集群故障。

现在我们已经理解了为什么多集群联盟是 Kubernetes 如此重要的一个方面，是时候深入研究一下了。

## 学习 Kubernetes 联盟的基础知识

Kubernetes 联邦是一个复杂的话题，我们将轻松进入其中。在本节中，我们将首先了解一些基本概念，然后了解 API 的构建模块，最后了解其支持的特性和功能。

### 定义基本概念

让我们从一些基本的概念和术语开始我们进入库本内特联盟的旅程。下表描述了最重要的概念和术语:

<colgroup><col> <col></colgroup> 
| **概念** | **描述** |
| 联合的 | 为集群池创建一个公共接口，以便在这些集群中部署 Kubernetes 应用程序。 |
| KubeFed | Kubernetes 集群联邦的 API 和控制平面。 |
| 主机集群 | 一个集群，它公开了库贝费应用编程接口并运行库贝费控制平面。 |
| 集群注册 | 向联盟添加新集群的过程。 |
| 成员群集 | 一个向库贝费应用编程接口注册的集群，并且库贝费控制器具有的身份验证凭据。主机群集也可以是成员群集。 |
| 服务记录 | 一种资源，它将一个或多个库本内特服务资源与一个为服务构建域名系统资源记录的方案相关联。 |
| IngressDNSRecord | 一种资源，它将一个或多个 Kubernetes 入口资源与为入口构建 DNS 资源记录的方案相关联。 |
| 端点 | 代表**域名系统** ( **域名系统**)资源记录的资源。 |
| DNSEndpoint | 端点资源的自定义资源包装。 |

除了这些概念，该架构还基于三个构建块。

### 联邦构件

联盟负责一组给定的 API 类型(Kubernetes 资源)，它管理这些 API 类型并将其分配到一组成员集群中。对于每种 API 类型，联邦控制平面都使用公共专用资源来保持它们的状态:

*   `FederatedTemplate`:存储被管理资源的基本规范
*   `FederatedPlacement`:该类型保存资源应该被分配到的集群的规范
*   `FederatedOverrides`:这个可选资源可以指定模板资源在特定集群上的行为

这些类型都是通过名称关联的。例如`ReplicaSet`资源，有`FederatedReplicaSetTemplate`、`FederatedReplicaSetPlacement`和`FederatedReplicaSetOverrides`。

除此之外，更高级别的 API 可以使用以下元素来定制和控制联盟的行为:

*   **状态**:收集 KubeFed 在所有联合集群中分发的资源的状态
*   **策略**:确定允许将资源分配到哪个集群子集
*   **调度**:决定工作负载应该如何分布在不同的集群中

下图说明了 Kubernetes 联盟所有元素的完整生命周期和交互:

![](img/B15559_11_02.png)

图 11.2:库本内特联盟的生命周期

### 联合功能

这些概念和基础构建模块用于实现以下功能:

*   将任意类型的传播推送到远程集群
*   一个名为`kubefedctl`的命令行界面程序，用于与库贝菲德应用编程接口进行交互
*   在不编写代码的情况下生成 KubeFed APIs
*   通过外部域名系统的多集群服务域名系统
*   通过外部域名系统的多集群入口域名系统
*   副本计划首选项

## 库贝德控制飞机

KubeFed 控制平面需要 Kubernetes 1.13 或更高版本。它由两个组件组成，这两个组件一起使 Kubernetes 集群的联盟能够作为一个统一的 Kubernetes 集群出现和运行。

### 联合应用编程接口服务器

联盟应用编程接口服务器管理库本内特集群，这些集群共同组成了联盟。它管理 etcd 数据库中的联盟状态(哪些集群是联盟的一部分)，方式与常规的 Kubernetes 集群相同，但它保持的状态只是那些属于联盟成员的集群。每个集群的状态存储在该集群的 etcd 数据库中。联盟应用编程接口服务器的主要目的是与联盟控制器管理器交互，并将请求路由到联盟的成员集群。联盟成员不需要知道他们是联盟的一部分:他们只是工作相同。

### 联盟控制器管理器

联盟控制器管理器确保联盟的期望状态与实际状态相匹配。它会将任何必要的更改转发到相关的一个或多个集群。联邦控制器管理器二进制文件包含用于所有不同联邦资源的多个控制器，我们将在本章后面介绍。然而，控制逻辑是相似的:它观察变化，并在它们偏离时将集群状态带到期望的状态。这是为集群联合中的每个成员完成的。

下图演示了这种永久控制循环:

![](img/B15559_11_03.png)

图 11.3:联邦控制器管理器的永久控制循环

## 最难的部分

到目前为止，联邦看起来几乎是直截了当的。库贝德为你做重担。你只需将一堆集群组合在一起，通过控制平面访问它们，所有的东西都会被复制到所有的集群中。但是有一些困难的因素和基本概念使这个简化的观点变得复杂。Kubernetes 的大部分力量来自于它在幕后做大量工作的能力。在完全部署在单个物理数据中心或可用性区域的单个集群中，所有组件都通过快速网络连接，Kubernetes 本身非常有效。在 Kubernetes 集群联盟中，情况就不同了。延迟、数据传输成本和在集群之间移动吊舱都有不同的权衡。根据不同的用例，让联邦工作可能需要系统设计者和操作者额外的关注、计划和维护。此外，一些联邦资源并不像本地资源那样成熟，这增加了更多的不确定性。参考[https://github.com/kubernetes-sigs/kubefed](https://github.com/kubernetes-sigs/kubefed)了解最新信息。

### 联合工作单位

Kubernetes 集群中的工作单位是豆荚。在库本内斯你不能打破一个豆荚。整个吊舱将始终部署在一起，并接受相同的生命周期处理。pod 应该仍然是集群联盟的工作单元吗？也许能够将一个更大的单元(例如整个复制集、部署或服务)与特定的集群相关联更有意义。如果群集出现故障，整个复制集、部署或服务将被调度到不同的群集。紧密耦合的复制集集合怎么样？这些问题的答案并不总是容易的，甚至可能随着系统的发展而动态变化。

### 位置相似性

位置关联性是一个主要问题。豆荚什么时候可以跨集群分布？那些豆荚之间有什么关系？对 pods 或 pods 与其他资源(如存储)之间的关联性有什么要求吗？有几个主要类别:

*   严格耦合
*   松散耦合
*   优先耦合
*   严格去耦
*   均匀分布

在设计系统以及如何在整个联盟中分配和调度服务和 pods 时，确保始终遵守位置相似性要求是非常重要的。

#### 严格耦合

严格的耦合要求适用于吊舱必须在同一集群中的应用。如果对 pods 进行分区，应用程序将会失败(可能是由于跨集群联网时无法满足实时要求)，或者成本太高(pods 访问大量本地数据)。将如此紧密耦合的应用程序移动到另一个群集的唯一方法是在另一个群集上启动一个完整的副本(包括数据)，然后关闭当前群集上的应用程序。如果数据太大，应用程序实际上可能无法移动并且对灾难性故障敏感。这是最难处理的情况，如果可能的话，您应该设计您的系统来避免严格耦合的需求。

#### 松散耦合

当工作负载令人尴尬地并行时，松散耦合的应用程序是最好的，并且每个 pod 不需要知道其他 pod 或访问大量数据。在这些情况下，可以仅根据整个联盟的容量和资源利用率将 pods 调度到集群。如有必要，可以毫无问题地将吊舱从一个集群移动到另一个集群。例如，考虑一个无状态验证服务，它执行一个计算，并在请求本身中获取其所有输入，并且不查询或写入任何联盟范围的数据。它只是验证它的输入，并向调用者返回一个有效/无效的结论。

#### 优先耦合

当所有吊舱都在同一个集群中或吊舱和数据位于同一位置时，优先耦合的应用程序性能更好，但这不是硬性要求。例如，这可以适用于只需要最终一致性的应用程序，其中一些联邦范围的应用程序定期同步所有集群中的应用程序状态。在这些情况下，分配会明确地分配给一个集群，但会为运行或迁移到处于压力下的其他集群留下一个安全通道。

#### 严格去耦

一些服务具有故障隔离或高可用性要求，这迫使跨集群划分。如果一个关键服务的三个副本可能最终都被安排到同一个群集，那么运行这三个副本是没有意义的，因为该群集只是一个临时 SPOF。

#### 均匀分布

统一分布是指服务、复制集或 pod 的实例必须在每个集群上运行。它类似于daemmonset，但不是确保每个节点上有一个实例，而是每个集群有一个实例。一个很好的例子是由一些外部持久存储备份的 Redis 缓存。每个集群中的 pod 应该有自己的集群本地 Redis 缓存，以避免访问中央存储，中央存储可能会变慢或成为瓶颈。另一方面，每个集群不需要一个以上的 Redis 服务(它可以分布在同一个集群中的几个 pod 上)。

### 跨集群调度

跨集群调度与位置相似性密切相关。当一个新的 pod 被创建或者一个现有的 pod 出现故障，需要安排替换时，它应该去哪里？当前的集群联盟并没有处理我们前面提到的位置相似性的所有场景和选项。

此时，集群联盟很好地处理松散耦合(包括加权分布)和严格耦合(通过确保副本数量与集群数量相匹配)的类别。其他任何事情都要求您不要使用集群联合。您将不得不添加自己的自定义联邦层，它考虑到了更专业的问题，并且可以适应更复杂的调度用例。

### 联合数据访问

这是一个棘手的问题。如果您有大量数据和吊舱运行在多个集群中(可能在不同的大陆上)，并且需要快速访问它们，那么您有几个不愉快的选择:

*   将数据复制到每个集群(复制速度慢、传输成本高、存储成本高、同步和处理错误复杂)
*   远程访问数据(这种方式访问速度慢，每次访问都很昂贵，而且可能是 SPOF 式的)
*   使用复杂的混合解决方案，按集群缓存一些最热门的数据(这很复杂，会导致数据过时，并且您仍然需要传输大量数据)

### 联合自动缩放

目前不支持联合自动调用。有两个尺度可以利用，也有两个尺度的组合:

*   按群集扩展
*   在联盟中添加/删除集群
*   混合工艺

考虑一个相对简单的场景，一个松散耦合的应用程序在三个集群上运行，每个集群中有五个 pod。从某种程度上来说，15 个吊舱已经无法承受这种负荷了。我们需要增加更多的容量。我们可以增加每个集群的豆荚数量，但是如果我们在联盟级别这样做，那么我们将在每个集群中运行六个豆荚。当只需要一个吊舱时，我们增加了三个吊舱的联邦能力。当然，如果你有更多的集群，问题会变得更糟。另一种选择是选择一个集群，只需更改其容量。有了注释，这是可能的，但是现在我们明确地管理整个联盟的容量。如果我们有许多集群运行数百个具有动态变化需求的服务，那么事情会变得非常复杂。

添加一个完整的新集群就更复杂了。我们应该在哪里添加新集群？不需要额外的可用性来指导决策。这只是额外的能力。创建新集群通常还需要复杂的首次设置，在公共云平台上批准各种配额可能需要几天时间。混合方法增加了联盟中现有集群的容量，直到达到阈值，然后开始添加新集群。这种方法的好处是，当您越来越接近每个群集的容量限制时，您就可以开始准备新的群集，以便在必要时投入使用。除此之外，它还需要大量的努力，并且随着复杂性的增加，您需要为灵活性和可扩展性付出代价。

# 管理库本内特集群联盟

管理 Kubernetes 集群联盟除了管理单个集群之外，还涉及许多活动。您需要考虑级联资源删除、跨集群的负载平衡、跨集群的故障转移、联合服务发现和联合发现。让我们详细检查一下各种活动。请注意，由于 KubeFed 的 Alpha 状态，这不应被视为一个需要遵循的分步指南。这里的目标是了解作为一个联盟管理多个 Kubernetes 集群涉及到什么。

## 正在安装 kubefedctl

与 KubeFed 交互的最佳方式是通过 kubefedctl 命令行界面。以下是为 macOS 安装最新版本的 kubefedctl 的说明:

```
VERSION=0.3.0
OS=Darwin
ARCH=amd64
curl -LO https://github.com/kubernetes-sigs/kubefed/releases/download/v${VERSION}/kubefedctl-${VERSION}-${OS}-${ARCH}.tgz
tar -zxvf kubefedctl-*.tgz
chmod u+x kubefedctl
sudo mv kubefedctl /usr/local/bin/ 
```

如果输入`kubefedctl`是一个太大的负担，那么你可以像我一样给它取别名:

```
alias kf='kubefedctl' 
```

要验证它的安装是否正确，只需运行它，您将看到以下内容:

```
$ kf
kubefedctl controls a Kubernetes Cluster Federation. Find more information at https://sigs.k8s.io/kubefed.
Usage:
  kubefedctl [flags]
  kubefedctl [command]
Available Commands:
  disable            Disables propagation of a Kubernetes API type
  enable             Enables propagation of a Kubernetes API type
  federate           Federate creates a federated resource from a kubernetes resource
  help               Help about any command
  join               Register a cluster with a KubeFed control plane
  orphaning-deletion Manage orphaning delete policy
  unjoin             Remove the registration of a cluster from a KubeFed control plane
  version            Print the version info
Flags:
      --alsologtostderr                  log to standard error as well as files
  -h, --help                             help for kubefedctl
      --log-backtrace-at traceLocation   when logging hits line file:N, emit a stack trace (default :0)
      --log-dir string                   If non-empty, write log files in this directory
      --log-file string                  If non-empty, use this log file
      --log-flush-frequency duration     Maximum number of seconds between log flushes (default 5s)
      --logtostderr                      log to standard error instead of files (default true)
      --skip-headers                     If true, avoid header prefixes in the log messages
      --stderrthreshold severity         logs at or above this threshold go to stderr
  -v, --v Level                          number for the log level verbosity
      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging
Use "kubefedctl [command] --help" for more information about a command. 
```

下一步是创建一些集群，形成我们的联盟。

## 创建集群

KubeFed 正式支持四种 Kubernetes 环境:

*   子项(坞站中的库)
*   迷你库比
*   GKE
*   IBM 云

KubeFed 端到端测试使用 KinD 环境。然而，minikube 是最容易设置的玩法。以下是创建两个 minikube 集群的说明:

```
minikube start -p cluster-1
minikube start -p cluster-2 
```

然后，在每个集群中，在继续下一步之前，验证所有吊舱都在运行。

## 配置主机集群

好的。是时候在您的主机集群中安装 KubeFed 控制平面了。KubeFed 项目为任务提供了一个方便的 Helm 图表。遗憾的是，KubeFed 还不支持 Helm 3，因为他们使用了一个过时的注释(`crd-install hook`)。您可能已经安装了 Helm 3，但是如果您想尝试使用 KubeFed，也应该安装 Helm 2。由于 Helm 2 在集群中使用 Tiller，因此您需要为 Tiller 创建一个服务帐户，并授予它管理员权限，以便它可以安全地安装 KubeFed 控制平面:

```
$ kubectl config use-context cluster-1
Switched to context "cluster-1".
$ cat << EOF | kubectl apply -f –
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF
serviceaccount/tiller created
clusterrolebinding.rbac.authorization.k8s.io/tiller created
$ helm2 init --service-account tiller 
```

接下来，我们需要添加 KubeFed 图表存储库:

```
$ helm2 repo add kubefed-charts https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts
"kubefed-charts" has been added to your repositories
$ helm2 repo list
NAME            URL
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts
kubefed-charts  https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts 
```

我们可以使用以下`helm` `search`命令来验证库贝菲图表是否可用:

```
$ helm2 search kubefed
NAME                            CHART VERSION   APP VERSION DESCRIPTION
kubefed-charts/federation-v2    0.0.10 
```

更新您的回购:

```
$ helm2 repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kubefed-charts" chart
repository
...Successfully got an update from the "stable" chart repository
Update Complete. ![](img/B15559_09_021.png) Happy Helming!![](img/B15559_09_021.png) 
```

那么的安装就简单到:

```
$  helm2 install kubefed-charts/kubefed --name kubefed --version=$VERSION --namespace kube-federation-system --devel 
```

## 向联盟注册集群

安装控制平面后，确保您的`~/.kube/config`文件具有主机集群和所有成员集群的上下文。然后，您可以使用`kubefedctl join`命令向联盟添加集群:

```
$ kf join cluster1 --cluster-context cluster-1 --host-cluster-context cluster-1 --v=2
$ kf join cluster2 --cluster-context cluster-2 --host-cluster-context cluster-1 --v=2 
```

注意这里的`cluster-1`是主机集群，但也注册为成员集群。这完全没问题。不需要专用的主机集群。

要查看联盟的状态，您可以获得`kubefedclusters` CRDs:

```
$ kubectl -n kube-federation-system get kubefedclusters

  NAME       READY   AGE
  cluster-1   True    1m
  cluster-2   True    1m 
```

前面的命令适用于头盔 2。未来，我们预计它将与 Helm 3 合作。你可以参考[https://github . com/kubernetes-sigs/kube fed/blob/master/charts/kube fed/readme . MD # installing-the-chart](https://github.com/kubernetes-sigs/kubefed/blob/master/charts/kubefed/README.md#installing-the-chart)来检查这个。

你在这里用`kubectl`而不是`kubefedctl`。

如果要从联盟中注销集群，请使用 kubefedctl `unjoin`命令。

```
$ kf unjoin cluster-2 --cluster-context cluster-2 --host-cluster-context cluster-1 --v=2 
```

## 使用联合应用编程接口类型

V1 库本内特联盟仅支持有限数量的库本内特 API 类型。使用库贝菲德 V2，任何应用编程接口类型都可以联合，包括您自己的客户关系管理。

然而，这并不是自动发生的。您需要首先启用您想要联合的任何类型:

```
$ kf enable <API Type> 
```

要启用的类型的规范非常灵活。它可以是类型、复数名称、组限定复数名称或简称。例如，对于部署，可以是`Deployment`(种类)、`deploy`(简称)、`deployments`(复数)或`deployment.apps`(组限定复数名称)。

当您启用一个类型时，kubefedctl 会生成一个与原始类型(部署)的`Federated<Type>` CRD(例如，`FederatedDeployment`)和`Federated<Type>Config`关联。

请注意，目标类型必须安装在所有成员集群上。理想情况下，所有集群都应该运行相同版本的 Kubernetes，并协同升级以避免版本问题。即使所有集群都运行相同版本的 Kubernetes，它们也可能没有安装相同的 CRD。请记住，您也可以联合 CRD，但前提是它们安装在所有集群上。

假设你在 API 组`example.com`中有一个叫牛逼的 CRD。您可以通过运行以下小脚本来验证它是否安装在集群 1 和集群 2 中:

```
CLUSTER_CONTEXTS=(cluster-1 cluster-2)
for c in ${CLUSTER_CONTEXTS}; do
    echo ----- ${c} -----
    kubectl --context=${c} api-resources --api-group=example.com
done 
```

结果应该是:

```
----- cluster1 -----
NAME   SHORTNAMES   APIGROUP      NAMESPACED   KIND
awesome                example.com   true      Awesome
----- cluster2 -----
NAME   SHORTNAMES   APIGROUP      NAMESPACED   KIND
awesome                                        Awesome 
```

## 联合资源

为您的联合启用 API 类型实际上并没有在集群之间分配任何资源。当您准备在您的联盟中传播资源时，您可以使用`federate`命令:

```
kubefedctl federate <TYPE NAME> <RESOURCE NAME> [flags] 
```

让我们看一下在没有实际联合 pod 的情况下联合 pod 的输出(类似于试运行)。下面是命令:

```
$ kf federate pod trouble --output yaml 
```

以下是生成的输出的一些选定部分。一、API 版本为`types.kubefed.io/v1beta1`，种类为 FederatedPod:

```
apiVersion: types.kubefed.io/v1beta1
kind: FederatedPod
metadata:
  name: trouble
  namespace: default 
```

然后是包含带有`clusterSelector`的位置的规范，以防您只想联合符合某些标准的集群:

```
spec:
  placement:
    clusterSelector:
      matchLabels: {} 
```

规范的其余部分是标准的 pod 模板:

```
 template:
    metadata:
      labels:
        run: trouble
    spec:
      containers:
      - args:
        - bash
        image: g1g1/py-kube:0.2
        imagePullPolicy: IfNotPresent
        name: trouble        
        resources: {}
        ... 
```

默认情况下，联邦资源将在与目标类型相同的命名空间中创建。当然，必须在所有联盟集群中启用并安装 API 类型。

### 联合整个命名空间

KubeFed 也支持整个命名空间联合。这非常有用，因为名称空间便于将资源组组织在一起，并且将名称空间中的所有资源一次性联合起来通常是有意义的。从概念上来说，名称空间也是一个 Kubernetes 资源，因此您可以将其视为只是联合一个资源，这就是名称空间。但是，在实践中，名称空间不同于其他资源，因为它们是唯一包含其他资源的资源。联合名称空间的关键是所需的`--contents`标志。您也可以使用带有逗号分隔的资源列表的`--skip-api-resources`标志从联盟中排除一些资源:

```
kubefedctl federate namespace awesome-namespace --contents --skip-api-resources "secrets,apps" 
```

### 检查联合资源的状态

联合资源的顶层`status`字段包含关于资源在联合成员集群间传播的信息。这里有一个例子:

```
apiVersion: types.kubefed.io/v1beta1
kind: FederatedNamespace
metadata:
  name: awesome-namespace
  namespace: awesome-namespace
spec:
  placement:     clusterSelector: {}
status:
  # The status True of the condition of type Propagation
  # indicates that the state of all member clusters is as
  # intended as of the last probe time.
  conditions:
  - type: Propagation
    status: True
    lastTransitionTime: "2019-12-08T14:33:45Z"
    lastUpdateTime: "2019-12-08T14:33:45Z"
  # The namespace 'awesome-namespace' has been verified to exist in the
  # following clusters as of the lastUpdateTime recorded
  # in the 'Propagation' condition. Since that time, no
  # change has been detected to this resource or the
  # resources it manages.
  clusters:
  - name: cluster-1
  - name: cluster-2 
```

## 使用覆盖

在现实世界中，并不是所有的集群都是一样的。您可能需要进行各种特定于集群的更改。`FederatedDeployment`的覆盖字段允许您完全这样做。您可以使用**JSON patch**([http://jsonpatch.com/](http://jsonpatch.com/))语法指定覆盖字段，类似于Kustomize。

对于每个覆盖，指定一个路径(例如`/spec/replicas`)，然后提供要设置的值(例如`value: 5`)或操作(例如`add`或`remove`)以及要应用的值。这里有一个例子:

```
kind: FederatedDeployment
...
spec:
  ...
  overrides:
  # Apply overrides to cluster1
    - clusterName: cluster1
      clusterOverrides:
        # Set the replicas field to 5
        - path: "/spec/replicas"
          value: 5
        # Set the image of the first container
        - path: "/spec/template/spec/containers/0/image"
          value: "nginx:1.17.0-alpine"
        # Ensure the annotation "foo: bar" exists
        - path: "/metadata/annotations"
          op: "add"
          value:
            foo: bar
        # Ensure an annotation with key "baz" does not exist
        - path: "/metadata/annotations/baz"
          op: "remove" 
```

## 使用位置控制联合

联合资源的放置字段控制资源将被联合到哪个成员集群。有两种方法可以放置联合资源:

*   集群名称的直接列表
*   基于标签的集群选择器

下面是一个直接的列表位置:

```
spec:
  placement:
    clusters: 
      - name: cluster-1
      - name: cluster-2 
```

这里是一个基于标签的集群选择器，它将资源部署到具有标签`federate: True`的集群:

```
spec:
  placement:
    clusterSelector:
      matchLabels:
        federate: True 
```

到此为止，所以不错；然而，还有更多。如果没有`placement`字段，或者如果有一个`placement`带有一个空的`clusterSelector`，那么该资源将被放置在所有成员集群中:

```
spec:
  placement:
    clusterSelector: {} 
```

但是，如果指定了一个空的集群列表，则该资源不会部署到任何集群！

```
spec:
  placement:
    clusters: [] 
```

通常，集群列表(如果指定的话)总是会覆盖集群选择器。在这种情况下，资源不会部署到任何集群，因为集群列表为空，甚至不会部署到具有匹配标签的集群:

```
spec:
  placement:
    clusters: []
    clusterSelector:
      matchLabels:
        federate: True 
```

## 调试传播失败

您可以使用 kubectl 和用于调试一般 Kubernetes 问题的相同技术。 `kubectl describe`命令将向您显示与联合资源相关的事件:

```
$ kubectl describe <federated CRD> <CR name> -n awesome-namespace 
```

如果不起作用，那么您可以查看联邦控制器日志:

```
$ kubectl logs deployment/kubefed-controller-manager -n kube-federation-system 
```

现在，您已经很好地理解了如何使用联邦资源，让我们看看一些建立在基础之上的更高的抽象。

## 使用高阶行为

KubeFed 开箱即用支持的多集群注意事项和模式有几个高级别。这些行为建立在模板、替代和放置的基础构建块之上。我们来复习一下。

### 利用多集群入口域名系统

单个集群中的入口在集群的边缘完成，并将流量转发到集群中。入口字面意思是入口。然而，在多集群的世界里，情况就不同了。可能需要群集将请求从接收群集发送到不同的群集。找到正确的目的地依赖于一个外部域名系统，它是在集群内核心域名系统之外使用的。主要思想是所有集群的端点都由一个域名系统端点控制器和一个**入口域名系统控制器**管理。他们关注所有的集群并更新多集群的域名。一个**外部域名系统控制器**与外部**域名系统提供商**交互，以分配在所有集群中有效的外部名称，并允许您跨集群定位端点。

下图说明了信息流和控制循环:

![](img/B15559_11_04.png)

图 11.4:入口域名系统

### 利用多集群服务域名系统

在 Kubernetes 联盟中，服务需要被联合，这意味着它们的支持荚可以跨多个集群联合。为了访问这些 pod 及其端点，联合服务需要一种与多集群入口 DNS 非常相似的机制。

典型的工作流程是:

1.  创建`FederatedDeployment`和`FederatedService`对象。
2.  创建一个`Domain`对象，该对象将一个域名系统区域和一个权威的名称服务器关联到库贝费控制平面。
3.  创建一个`ServiceDNSRecord`对象，用于标识多集群`Service`对象的预期域名。
4.  域名系统端点控制器将创建一个与`ServiceDNSRecord`关联的`DNSEndpoint`对象。它包含三个 A 记录:
    *   `<service>.<namespace>.<federation>.svc.<federation-domain>`
    *   `<service>.<namespace>.<federation>.svc.<region>.<federation-domain>`
    *   `<service>.<namespace>.<federation>.svc.<availability-zone>.<region>.<federation-domain>`
5.  外部域名系统监视并列出`DNSEndpoint`对象，并在外部域名系统提供商中创建域名系统资源记录。

下图说明了这一过程:

![](img/B15559_11_05.png)

图 11.5:服务域名系统

接下来，让我们看看多集群调度的关键主题。

### 利用多集群调度

考虑做多集群调度意味着什么。您需要为每个部署指定副本的总数，就像在单个集群上一样，但是您还需要提供一些约束和指导，以便在不同的集群之间分配 pod。进入`ReplicaSchedulingPreference`。该资源允许您指定所有首选项，并在所有联盟集群中实现工作负载的健康分布。

下面是一个简单的示例，它指定了 15 个副本的总数。KubeFed 将尝试在所有成员集群中平均分配 15 个副本:

```
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: awesome-deployment
  namespace: awesome-ns
spec:
  targetKind: FederatedDeployment
  totalReplicas: 15 
```

如果联盟中有三个集群，则每个集群中将运行 5 个副本。

下面的例子稍微复杂一点，使用加权分布。聚类 1 和聚类 2 的权重是 2:3。这意味着 15 个副本将以 2 比 3 的比例分布，这导致 6 个副本在群集-1 中运行，9 个副本在群集-2 中运行:

```
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: awesome-deployment
  namespace: awesome-ns
spec:
  targetKind: FederatedDeployment
  totalReplicas: 15
  clusters:
    cluster-1:
      weight: 2
    cluster-2:
      weight: 3 
```

加权分布很好，但是它会导致不希望的边缘情况，尤其是当成员集群之一变得不可达或不可用时。为了保持某些界限，您可以为每个集群指定允许在集群中运行的副本的最小和最大数量范围。KubeFed 将尽最大努力保持加权分布，而不违反副本最小和最大数量的约束。特别是，最大数量是 KubeFed 将始终遵守的硬性限制。在某些情况下，最低数量可能无法维持。

在以下示例中，保持了 15 个副本的 2:3 比例。但是，集群-1 的`maxReplicas`限制为 5，因此它将只运行 5 个副本，而不是像以前一样运行 6 个副本。另一方面，cluster-2 的`maxReplicas`限制为 12，因此可以力挽狂澜，运行 10 个副本，比之前多了一个副本。最终结果是所有 15 个副本都已计划，也就是说，cluster-1 运行 5 个副本，cluster-2 运行 10 个副本，这是 1:2 而不是 2:3 的比例。这是 KubeFed 在这一组特定的限制条件下所能做的最好的事情，同时仍然调度所有 15 个副本:

```
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: awesome-deployment
  namespace: awesome-ns
spec:
  targetKind: FederatedDeployment
  totalReplicas: 15
  clusters:
    cluster-1:
      weight: 2
      minReplicas: 4
      maxReplicas: 5
    cluster-2:
      weight: 3
      minReplicas: 4
      maxReplicas: 12 
```

除了例外，您还可以进行均匀分布。例如，您可以在所有集群中均匀分布，但有一些限制的特定集群除外。这里，100 个副本将平均分布到所有集群，但集群 3 除外，集群 3 必须至少有 5 个副本:

```
apiVersion: scheduling.kubefed.io/v1alpha1
kind: ReplicaSchedulingPreference
metadata:
  name: awesome-deployment
  namespace: awesome-ns
spec:
  targetKind: FederatedDeployment
  totalReplicas: 100
  clusters:
    "*":
      weight: 1
      cluster-3:
      minReplicas: 5
      weight: 1 
```

当你想把你的多集群系统当成一个大集群时，集群联盟就发光了。然而，在许多情况下，正确的抽象级别是独立集群的集合。这就是园丁项目的由来。

# 介绍园丁项目

**园丁**([https://gardener.cloud/](https://gardener.cloud/))项目是 SAP 开发的开源项目。它让你管理成千上万(是的，成千上万！)的库本内特集群高效地和经济地。园丁解决了一个非常复杂的问题，解决方案优雅但不简单。在本节中，我们将介绍 Gardener 的术语、概念模型、深入研究其体系结构，并了解其可扩展性特征。Gardener 的主要主题是使用 Kubernetes 来管理 Kubernetes 集群。思考 Gardener 的一个好方法是将其视为 Kubernetes-control-plane-as-service。

## 理解园丁的术语

你可能已经猜到，园丁项目使用植物学术语来描述世界。那里是一个花园，是一个 Kubernetes 集群，负责管理种子集群。种子是一个 Kubernetes 集群，负责管理一组芽集群。shoot 集群是运行实际工作负载的 Kubernetes 集群。园丁背后很酷的想法是，芽簇只包含工人节点。所有拍摄集群的控制平面作为种子集群中的 Kubernetes 豆荚和服务运行。

下图详细描述了 Gardener 的结构及其组件之间的关系:

![](img/B15559_11_06.png)

图 11.6:园丁及其组件的结构

别慌！所有这些复杂性的背后是一个清晰的概念模型。

## 理解园丁的概念模型

《园丁》的建筑图可以说是铺天盖地。让我们慢慢打开包装，考虑一下的基本原理。Gardener 真正拥抱了 Kubernetes 的精神，并将管理一大组 Kubernetes 集群的复杂性转移到了 Kubernetes 本身。从本质上来说，Gardener 是一个聚合的 API 服务器(扩展的 Kubernetes API 服务器)，它使用各种控制器管理一组定制资源。它包含并充分利用了 Kubernetes 的可扩展性。这种方法在 Kubernetes 社区中很常见。定义一组自定义资源，让 Kubernetes 为您管理它们。Gardener 的新颖之处在于它将这种方法发挥到了极致，并抽象出了 Kubernetes 基础设施本身的一部分。

在“正常”的 Kubernetes 集群中，控制平面与工作节点运行在同一个集群中。通常，在大型集群中，像 Kubernetes API 服务器和 etcd 这样的控制平面组件运行在专用节点上，不会与工作节点混合。Gardener 从许多集群的角度来思考，它拿走了所有芽集群的所有控制平面，并使用一个种子集群来管理它们。因此，拍摄集群的 Kubernetes 控制平面在种子集群中作为常规的 Kubernetes 部署进行管理，Kubernetes 会自动为其提供复制、监控、自我修复和滚动更新。

因此库本内特射击场的控制平面类似于部署。另一方面，种子集群映射到一个 Kubernetes 节点。它管理多个拍摄集群。我们建议每个云提供商都有一个种子集群。Gardener 开发人员实际上在为种子集群开发一个类似于节点上的 kubelet 的 Gardenlet 控制器。

如果种子集群像 Kubernetes 节点，那么管理这些种子集群的花园集群就像管理其工作节点的 Kubernetes 集群。

通过将 Kubernetes 模型推进到这一步，Gardener 项目利用了 Kubernetes 的优势，实现了很难从头构建的健壮性和性能。

让我们深入了解一下建筑。

## 潜入园丁建筑

园丁为每个芽簇在种子簇中创建一个 Kubernetes 命名空间。它将芽簇的证书作为种子簇中的 Kubernetes 秘密来管理。

## 管理群集状态

每个集群的 etcd 数据存储被部署为一个带有一个副本的状态集合。除此之外，事件存储在单独的 etcd 实例中。etcd 数据会定期拍摄快照并存储在远程存储中，用于备份和恢复。这使得失去控制平面的集群能够非常快速地恢复(例如，当整个种子集群变得不可达时)。请注意，当种子集群停止运行时，芽集群会像往常一样继续运行。

### 管理控制平面

如前所述，一个射击集群 X 的控制平面运行在一个单独的种子集群中，而工作节点运行在一个射击集群中。这意味着拍摄集群中的豆荚可以使用内部域名系统来定位彼此，但是与种子集群中运行的 Kubernetes API 服务器的通信必须通过外部域名系统来完成。这意味着 Kubernetes API 服务器作为负载平衡器类型的服务运行。

### 准备基础设施

创建新拍摄集群时，提供必要的基础设施很重要。园丁使用**地形**来完成这项任务。它基于拍摄集群规范生成地形脚本，并将其作为配置图存储在种子集群中。专用 Terraformer 组件作为作业运行，执行所有资源调配，然后将状态写入单独的配置映射。

### 使用机器控制器管理器

为了以提供者不可知的方式调配节点，这种方式也适用于私有云，Gardener 拥有多个自定义资源，如`MachineDeployment`、`MachineClass`、`MachineSet`和`Machine`。他们与 Kubernetes 集群生命周期组合作来统一他们的抽象，因为有很多重叠。此外，Gardener 利用集群自动缩放器来减轻上下缩放节点池的复杂性。

### 跨集群联网

种子集群和芽集群可以在不同的云提供商上运行。拍摄集群中的工作节点通常部署在专用网络中。由于控制平面需要与工作节点(主要是 kubelet)紧密交互，所以园丁创建了一个用于直接通信的 VPN。

### 监控集群

可观测性是操作复杂分布式系统的一个重要部分。Gardener 使用一流的开源项目提供了大量开箱即用的监控，如中央**普罗米修斯**([https://github.com/prometheus/prometheus](https://github.com/prometheus/prometheus))服务器，该服务器部署在收集所有种子集群信息的花园集群中。

此外，每个射击集群在种子集群中都有自己的普罗米修斯实例。为了收集度量，Gardener 为每个集群部署了两个 **kube 状态度量**([https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics))实例(一个用于种子中的控制平面，一个用于种子中的工作节点)。也部署了**节点导出器**([https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter))来提供关于节点的附加信息。普罗米修斯**警报管理器**([https://prometheus.io/docs/alerting/alertmanager/](https://prometheus.io/docs/alerting/alertmanager/))用于在出现问题时通知操作员。**Grafana**([https://github.com/grafana/grafana](https://github.com/grafana/grafana))用来显示仪表盘，里面有系统状态的相关数据。

### 花园命令行界面

您只能使用 kubectl 来管理 Gardener，但是当您探索不同的集群时，您将不得不频繁地切换概要文件和上下文。Gardener 提供了`gardenctl`命令行工具，提供了更高级别的抽象，可以同时在多个集群上运行。这里有一个例子:

```
$ gardenctl ls shoots
projects:
- project: team-a
  shoots:
  - dev-eu1
  - prod-eu1
$ gardenctl target shoot prod-eu1
[prod-eu1]
$ gardenctl show Prometheus
NAME           READY     STATUS    RESTARTS   AGE       IP              NODE
prometheus-0   3/3       Running   0          106d      10.241.241.42   ip-10-240-7-72.eu-central-1.compute.internal
URL: https://user:password@p.prod-eu1.team-a.seed.aws-eu1.example.com 
```

Gardener 最突出的特点之一是它的可扩展性。它表面积大，支持多种环境。让我们看看可扩展性是如何嵌入到它的设计中的。

## 延伸园丁

园丁支持以下环境:

*   自动警报系统
*   GCP
*   蔚蓝的
*   abicloud
*   小包裹
*   OpenStack

它就像 Kubernetes 本身一样，在主要的 Gardener 存储库中有许多特定于提供商的支持。随着时间的推移，它遵循了 Kubernetes 的例子，该例子将云提供商外部化，并将提供商迁移到一个单独的 Gardener 扩展中。可以使用`CloudProfile` CRD 指定提供商，例如:

```
apiVersion: gardener.cloud/v1alpha1
kind: CloudProfile
metadata:
  name: aws
spec:
  type: aws
# caBundle: |
#   -----BEGIN CERTIFICATE-----
#   ...
#   -----END CERTIFICATE-----
  dnsProviders:
  - type: aws-route53
  - type: unmanaged
  kubernetes:
    versions:
    - 1.12.1
    - 1.11.0
    - 1.10.5
  machineTypes:
  - name: m4.large
    cpu: "2"
    gpu: "0"
    memory: 8Gi
  # storage: 20Gi   # optional (not needed in every environment, may only be specified if no volumeTypes have been specified)
  ...
  volumeTypes:      # optional (not needed in every environment, may only be specified if no machineType has a 'storage' field)
  - name: gp2
    class: standard
  - name: io1
    class: premium
  providerConfig:
    apiVersion: aws.cloud.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    constraints:
      minimumVolumeSize: 20Gi
      machineImages:
      - name: coreos
        regions:
        - name: eu-west-1
          ami: ami-32d1474b
        - name: us-east-1
          ami: ami-e582d29f
      zones:
      - region: eu-west-1
        zones:
        - name: eu-west-1a
          unavailableMachineTypes: # list of machine types defined above that are not available in this zone
          - name: m4.large
          unavailableVolumeTypes:  # list of volume types defined above that are not available in this zone
          - name: gp2
        - name: eu-west-1b
        - name: eu-west-1c 
```

然后，拍摄集群将选择一个提供商，并使用必要的信息对其进行配置:

```
apiVersion: gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: johndoe-aws
  namespace: garden-dev
spec:
  cloudProfileName: aws
  secretBindingName: core-aws
  cloud:
    type: aws
    region: eu-west-1
    providerConfig:
      apiVersion: aws.cloud.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vpc: # specify either 'id' or 'cidr'
        # id: vpc-123456
          cidr: 10.250.0.0/16
        internal:
        - 10.250.112.0/22
        public:
        - 10.250.96.0/22
        workers:
        - 10.250.0.0/19
      zones:
      - eu-west-1a
    workerPools:
    - name: pool-01
    # Taints, labels, and annotations are not yet implemented. This requires interaction with the machine-controller-manager, see
    # https://github.com/gardener/machine-controller-manager/issues/174\. It is only mentioned here as future proposal.
    # taints:
    # - key: foo
    #   value: bar
    #   effect: PreferNoSchedule
    # labels:
    # - key: bar
    #   value: baz
    # annotations:
    # - key: foo
    #   value: hugo
      machineType: m4.large
      volume: # optional, not needed in every environment, may only be specified if the referenced CloudProfile contains the volumeTypes field
        type: gp2
        size: 20Gi
      providerConfig:
        apiVersion: aws.cloud.gardener.cloud/v1alpha1
        kind: WorkerPoolConfig
        machineImage:
          name: coreos
          ami: ami-d0dcef3
        zones:
        - eu-west-1a
      minimum: 2
      maximum: 2
      maxSurge: 1
      maxUnavailable: 0
  kubernetes:
    version: 1.11.0
    ...
  dns:
    provider: aws-route53
    domain: johndoe-aws.garden-dev.example.com
  maintenance:
    timeWindow:
      begin: 220000+0100
      end: 230000+0100
    autoUpdate:
      kubernetesVersion: true
  backup:
    schedule: "*/5 * * * *"
    maximum: 7
  addons:
    kube2iam:
      enabled: false
    kubernetes-dashboard:
      enabled: true
    cluster-autoscaler:
      enabled: true
    nginx-ingress:
      enabled: true
      loadBalancerSourceRanges: []
    kube-lego:
      enabled: true
      email: john.doe@example.com 
```

然而，Gardener 的可扩展性目标远不止是提供者不可知。建立库本内特星团的整个过程包括许多步骤。园丁项目旨在让操作员通过定义定制资源和网络钩子来定制每一步。这是一个通用流程图，其中包含与每个步骤相关的 CRD、变更/验证准入控制器和网络挂钩:

![](img/B15559_11_07.png)

图 11.7:通用流程图，包括变更/验证准入控制器和网络挂钩

以下是构成园丁扩展空间的 CRD 类别:

*   域名系统提供商(例如，路由 53 和云域名系统)
*   Blob 存储提供商(例如，S3、全球通信系统和澳大利亚统计局)
*   基础设施提供商(例如，AWS、GCP 和 Azure)
*   操作系统(例如，CoreOS 容器 Linux、Ubuntu 和 FlatCar Linux)
*   网络插件(例如，印花布、法兰绒和纤毛)
*   非必要的扩展(例如，让我们加密证书服务)

## 园丁戒指

另一个新颖的想法是创建一个至少有三个簇的簇环，其中芽簇作为环中下一个簇的种子簇。加上将控制平面迁移到其他集群的能力，该环提供了一个强大的解决方案，可以在任何集群变得不可用时自我修复。如果集群部署在不同的云提供商上，或者至少部署在不同的地区，这一点尤其强大。它有可能保护花园免受严重情况的影响，如整个地区停电，甚至整个云提供商停电。

以下是戒指的组织方式:

![](img/B15559_11_08.png)

图 11.8:园丁戒指

# 摘要

在本章中，我们已经讨论了 Kubernetes 集群联盟的重要方面，以及使用 Gardener 项目管理许多 Kubernetes 集群。集群联盟还在测试阶段，有点原始，但是已经可以使用了。部署不多，官方支持的目标平台目前是 AWS 和 GCE/GKE，但云联盟背后有很大的动力。这是在 Kubernetes 上构建大规模可扩展系统的一个非常重要的部分。我们已经讨论了 Kubernetes 集群联邦、联邦控制平面组件和联邦 Kubernetes 对象的动机和用例。我们还研究了联邦中不太受支持的方面，例如自定义调度、联邦数据访问和自动伸缩。然后，我们研究了如何运行多个 Kubernetes 集群，其中包括建立 Kubernetes 集群联盟、在联盟中添加和删除集群以及负载平衡、出现问题时的联合故障转移、服务发现和迁移。然后，我们深入研究了使用联合服务跨多个集群运行联合工作负载，以及与此场景相关的各种挑战。

园丁项目有一个非常有趣的方法和架构。它从不同的角度处理多个集群的问题，并专注于集群的大规模管理。它相对较新，但被 SAP 和其他合作伙伴大规模使用。

此时，您应该对联盟的当前状态、利用 Kubernetes 提供的现有功能需要什么，以及您必须自己实现哪些部分来增强不完整或不成熟的特性有一个清晰的了解。根据您的用例，您可能会认为现在还为时过早，或者您想冒险一试。致力于 Kubernetes 联盟的开发人员进展很快，所以很有可能在你需要做出决定的时候，它会更加成熟和经得起考验。

如果您的职位需要管理多个 Kubernetes 集群，那么 Gardener 项目可能适合您。

在下一章中，我们将探索 Kubernetes 上令人兴奋的无服务器计算世界。无服务器意味着两件不同的事情:当您不必为长期运行的工作负载管理服务器时，以及当在服务上运行功能时。Kubernetes 可以使用这两种形式的无服务器，而且都非常有用。