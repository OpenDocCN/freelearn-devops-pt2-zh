- en: 17\. Advanced Scheduling in Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17. Kubernetes中的高级调度
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter focuses on scheduling, which is the process by which Kubernetes
    selects a node for running a Pod. In this chapter, we will take a closer look
    at this process and the Kubernetes Scheduler, which is the default Kubernetes
    component responsible for this process.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍调度，即Kubernetes选择运行Pod的节点的过程。在本章中，我们将更仔细地研究这个过程和Kubernetes调度器，这是负责这个过程的默认Kubernetes组件。
- en: By the end of this chapter, you will be able to use different ways to control
    the behavior of the Kubernetes Scheduler to suit the requirements of an application.
    The chapter will equip you to be able to choose appropriate Pod scheduling methods
    to control which nodes you want to run your Pods on based on your business needs.
    You will learn about the different ways to control the scheduling of Pods on the
    Kubernetes cluster.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的学习，您将能够使用不同的方式来控制Kubernetes调度器的行为，以满足应用程序的要求。本章将使您能够选择适当的Pod调度方法，根据业务需求控制您想要在哪些节点上运行Pod。您将了解在Kubernetes集群上控制Pod调度的不同方式。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: We have seen that we package our applications as containers and deploy them
    as a Pod in Kubernetes, which is the minimal unit of Deployment. With the help
    of the advanced scheduling capabilities provided by Kubernetes, we can optimize
    the deployment of these Pods with respect to our hardware infrastructure to meet
    our needs and get the most out of the available resources.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，我们将我们的应用程序打包为容器，并将它们部署为Kubernetes中的Pod，这是部署的最小单位。借助Kubernetes提供的先进调度功能，我们可以优化这些Pod的部署，以满足我们的硬件基础设施的需求，并充分利用可用资源。
- en: Kubernetes clusters generally have more than a few nodes (or machines or hosts)
    where the Pod can be executed. Consider that you are managing a few of the machines
    and you have been assigned to execute an application on these machines. What would
    you do to decide which machine is the best fit for the given application? Until
    now in this workshop, whenever you wanted to run a Pod on a Kubernetes cluster,
    have you mentioned which node(s) the Pod should run on?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群通常有多个节点（或机器或主机），可以在其中执行Pod。假设您正在管理一些机器，并且已被指定在这些机器上执行应用程序。为了决定哪台机器最适合给定的应用程序，您会怎么做？在本次研讨会中，每当您想在Kubernetes集群上运行Pod时，您是否提到过Pod应该在哪个节点上运行？
- en: That's right – we don't need to; Kubernetes comes with a smart component that
    finds the best node to run your Pod. This component is the **Kubernetes Scheduler**.
    In this chapter, we will look a bit more deeply into how the Kubernetes Scheduler
    works, and how to adapt it to better control our cluster to suit different needs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 没错 - 我们不需要; Kubernetes配备了一个智能组件，可以找到最适合运行您的Pod的节点。这个组件就是**Kubernetes调度器**。在本章中，我们将更深入地了解Kubernetes调度器的工作原理，以及如何调整它以更好地控制我们的集群，以满足不同的需求。
- en: The Kubernetes Scheduler
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes调度器
- en: As mentioned in the introduction, a typical cluster has several nodes. When
    you create a Pod, Kubernetes has to choose a node and assign the Pod to it. This
    process is known as **Pod scheduling**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如介绍中所述，典型的集群有多个节点。当您创建一个Pod时，Kubernetes必须选择一个节点并将Pod分配给它。这个过程被称为**Pod调度**。
- en: The Kubernetes component that is responsible for deciding which node a Pod should
    be assigned to for execution is called a scheduler. Kubernetes comes with a default
    scheduler that suffices for most use cases. For example, the default Kubernetes
    Scheduler spreads the load evenly in the cluster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 负责决定将Pod分配给哪个节点以执行的Kubernetes组件称为调度器。Kubernetes配备了一个默认调度器，适用于大多数用例。例如，默认的Kubernetes调度器在集群中均匀分配负载。
- en: Now, consider a scenario in which two different Pods are expected to communicate
    with each other very often. As a system architect, you may want them to be on
    the same node to reduce latency and free up some internal networking bandwidth.
    The Scheduler does not know the relationship between different types of Pods,
    but Kubernetes provides ways to inform the Scheduler about this relationship and
    influence the scheduling behavior so that these two different Pods can be hosted
    on the same node. But first, let's take a closer look at the Pod **scheduling
    process**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑这样一个场景：两个不同的Pod预计经常需要相互通信。作为系统架构师，您可能希望它们位于同一节点上，以减少延迟并释放一些内部网络带宽。调度器不知道不同类型的Pod之间的关系，但Kubernetes提供了方法来告知调度器这种关系，并影响调度行为，以便这两个不同的Pod可以托管在同一节点上。但首先，让我们更仔细地看一下Pod的**调度过程**。
- en: The Pod Scheduling Process
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod调度过程
- en: 'The scheduler works in a three-step process: **filtering**, **scoring**, and
    **assigning**. Let''s take a look at what happens during the execution of each
    of these steps. An overview of the process is described in the following diagram:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器的工作分为三个步骤：**过滤**、**评分**和**分配**。让我们来看看在执行每个步骤时会发生什么。下图描述了该过程的概述：
- en: '![Figure 17.1: An overview of how the Kubernetes Scheduler selects a suitable
    node'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.1：Kubernetes Scheduler选择合适节点的概述'
- en: '](image/B14870_17_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_01.jpg)'
- en: 'Figure 17.1: An overview of how the Kubernetes Scheduler selects a suitable
    node'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：Kubernetes Scheduler选择合适节点的概述
- en: Filtering
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤
- en: Filtering is a process in which the **Kubernetes Scheduler** runs a series of
    checks or filters to see which nodes are not suitable to run the target Pod. An
    example of a filter is to see if the node has enough CPU and memory to host the
    Pod, or if the storage volume requested by the Pod can be mounted on the host.
    If the cluster has no node that's suitable to meet the requirements of the Pod,
    then the Pod is deemed un-schedulable and is not executed on the cluster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤是指**Kubernetes Scheduler**运行一系列检查或过滤器，以查看哪些节点不适合运行目标Pod的过程。过滤器的一个例子是查看节点是否有足够的CPU和内存来托管Pod，或者Pod请求的存储卷是否可以挂载在主机上。如果集群中没有适合满足Pod要求的节点，那么Pod被视为不可调度，并且不会在集群上执行。
- en: Scoring
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评分
- en: Once the **Kubernetes Scheduler** has a list of feasible nodes, the second step
    is to score the nodes and find the best node(s) to host the target Pod. The node
    is passed through several priority functions and assigned a priority score. Each
    function assigns a score between 0 and 10, where 0 is the lowest and 10 is the
    highest.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦**Kubernetes Scheduler**有了可行节点的列表，第二步是对节点进行评分，并找到最适合托管目标Pod的节点。节点会经过几个优先函数，并分配一个优先级分数。每个函数都会分配一个介于0和10之间的分数，其中0是最低的，10是最高的。
- en: To understand priority functions, let's take `SelectorSpreadPriority` as an
    example. This priority function uses label selectors to find the Pods that are
    associated together. Let's say, for example, that a bunch of Pods is created by
    the same Deployment. As the name SpreadPriority suggests, this function tries
    to spread the Pods across different nodes so that in case of a node failure, we
    will still have replicas running on other nodes. Under this priority function,
    the Kubernetes Scheduler selects the nodes that have the fewest Pods running using
    the same label selectors as the requested Pod. These nodes will be assigned the
    highest score and vice versa.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解优先级函数，让我们以`SelectorSpreadPriority`为例。这个优先级函数使用标签选择器来找到相关的Pod。比如说，一堆Pod是由同一个部署创建的。正如SpreadPriority这个名字所暗示的，这个函数试图将Pod分布在不同的节点上，这样在节点故障的情况下，我们仍然会在其他节点上运行副本。在这个优先级函数下，Kubernetes
    Scheduler选择使用与请求的Pod相同的标签选择器运行最少的节点。这些节点将被分配最高的分数，反之亦然。
- en: Another example of a priority function is `LeastRequestedPriority`. This tries
    to spread the workload on the nodes that have the most resources available. The
    scheduler gets the nodes that have the lowest amount of memory and CPU allocated
    to existing Pods. These nodes are assigned the highest scores. In other words,
    this priority function will assign a higher score for a larger amount of free
    resources.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优先级函数的示例是`LeastRequestedPriority`。这试图在具有最多资源可用的节点上分配工作负载。调度程序获取已分配给现有Pod的内存和CPU最少的节点。这些节点被分配最高的分数。换句话说，这个优先级函数将为更多的空闲资源分配更高的分数。
- en: Note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'There are far too many priority functions to cover within the limited scope
    of this chapter. The full list of priority functions can be found at the following
    link: [https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/#scoring](https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/#scoring).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的有限范围内，有太多的优先级函数需要涵盖。完整的优先级函数列表可以在以下链接找到：[https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/#scoring](https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/#scoring)。
- en: Assigning
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分配
- en: Lastly, the Scheduler informs the API server about the node that has been selected
    based on the highest score. If there are multiple nodes with the same score, the
    Scheduler picks a random node and effectively applies a tiebreaker.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，调度程序通知API服务器已基于最高分数选择的节点。如果有多个具有相同分数的节点，调度程序会选择一个随机节点，并有效地应用决胜局。
- en: 'The default Kubernetes Scheduler runs as a Pod in the `kube-system` namespace.
    You can see it running by listing all the Pods in the `kube-system` namespace:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的Kubernetes Scheduler作为一个Pod在`kube-system`命名空间中运行。您可以通过列出`kube-system`命名空间中的所有Pod来查看它的运行情况：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You should see the following list of Pods:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下Pod列表：
- en: '![Figure 17.2: Listing Pods in the kube-system namespace'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.2：在kube-system命名空间中列出Pod'
- en: '](image/B14870_17_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_02.jpg)'
- en: 'Figure 17.2: Listing Pods in the kube-system namespace'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2：在kube-system命名空间中列出Pod
- en: In our Minikube environment, the Kubernetes Scheduler Pod is named `kube-scheduler-minikube`,
    as you can see in this screenshot.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的Minikube环境中，Kubernetes Scheduler Pod的名称为`kube-scheduler-minikube`，正如您在此截图中所看到的。
- en: Timeline of Pod Scheduling
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod调度时间表
- en: 'Let''s dig into the timeline of the **Pod scheduling** process. When you request
    a Pod to be created, different Kubernetes components get invoked to assign the
    Pod to the right node. There are three steps involved, from requesting a Pod to
    assigning a node. The following diagram gives an overview of this process, and
    we will elaborate and break down the process into more detailed steps after the
    diagram:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解**Pod调度**过程的时间线。当您请求创建一个Pod时，不同的Kubernetes组件会被调用来将Pod分配给正确的节点。从请求Pod到分配节点，涉及三个步骤。以下图表概述了这个过程，我们将在图表之后详细阐述和分解这个过程：
- en: '![Figure 17.3: Timeline of the Pod scheduling process'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2**：**Kubernetes调度器**通过API服务器不断监视Kubernetes数据存储。一旦有Pod创建请求可用（或Pod处于挂起状态），调度器会尝试调度它。重要的是要注意，调度器不负责运行Pod。它只是计算最适合托管Pod的节点，并通知Kubernetes
    API服务器，然后将这些信息存储在etcd中。在这一步中，Pod被分配到最佳节点，并且关联关系被存储在etcd中。'
- en: '](image/B14870_17_03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 管理Kubernetes调度器
- en: 'Figure 17.3: Timeline of the Pod scheduling process'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.3：Pod调度过程的时间线
- en: '**Step 1**: When a request is raised for creating and running a Pod, for instance,
    through a kubectl command or by a Kubernetes Deployment, the API server responds
    to this request. It updates the Kubernetes internal database (etcd) with a Pod
    pending entry to be executed. Note that at this stage, there is no guarantee that
    Pod will be scheduled.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1**：当提出创建和运行Pod的请求时，例如通过kubectl命令或Kubernetes部署，API服务器会响应此请求。它会更新Kubernetes内部数据库（etcd），并将一个待执行的Pod条目添加到其中。请注意，在这个阶段，不能保证Pod将被调度。'
- en: '**Step 2**: The **Kubernetes Scheduler** constantly watches the Kubernetes
    data store through the API server. As soon as a Pod creation request is available
    (or a Pod is in the pending state), the Scheduler tries to schedule it. It is
    important to note that the Scheduler is not responsible for running the Pod. It
    simply calculates the best node for hosting the Pod and informs the Kubernetes
    API server, which then stores this information in etcd. In this step, the Pod
    is assigned to the optimal node, and the association is stored in etcd.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 污点和容忍
- en: '**Step 3**: The Kubernetes agent (kubelet) constantly watches the Kubernetes
    data store through the API server. As soon as a new Pod is assigned to a node,
    it tries to execute the Pod on the node. When the Pod is successfully up and running,
    it is marked as running in etcd through the API server, and now the process is
    complete.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3**：Kubernetes代理（kubelet）通过API服务器不断监视Kubernetes数据存储。一旦一个新的Pod被分配到一个节点，它会尝试在节点上执行Pod。当Pod成功启动并运行时，通过API服务器将其标记为在etcd中运行，此时过程完成。'
- en: Now that we have an idea of the scheduling process, let's see how we can tweak
    it to suit our needs in the following topic.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对调度过程有了一定的了解，让我们看看如何调整它以满足我们的需求。
- en: Managing the Kubernetes Scheduler
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '](image/B14870_17_03.jpg)'
- en: 'Kubernetes provides many parameters and objects through which we can manage
    the behavior of the **Kubernetes Scheduler**. We will look into the following
    ways of managing the scheduling process:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了许多参数和对象，通过它们我们可以管理**Kubernetes调度器**的行为。我们将研究以下管理调度过程的方式：
- en: Node affinity and anti-affinity
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图17.3：Pod调度过程的时间线'
- en: Pod affinity and anti-affinity
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod亲和性和反亲和性
- en: Pod priority and preemption
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点亲和性和反亲和性
- en: Taints and tolerations
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod优先级和抢占
- en: Node Affinity and Anti-Affinity
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点亲和性和反亲和性
- en: Using node affinity rules, a Kubernetes cluster administrator can control the
    placement of Pods on specific sets of nodes. Node affinity or anti-affinity allows
    you to constrain which nodes a Pod can run on based on the labels of the nodes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用节点亲和规则，Kubernetes集群管理员可以控制Pod在特定节点集上的放置。节点亲和性或反亲和性允许您根据节点的标签来限制Pod可以运行的节点。
- en: Imagine that you are an administrator of the shared Kubernetes cluster in a
    bank. Multiple teams are running their applications on the same cluster. Your
    organization's security group has identified nodes that can run data-sensitive
    applications and would like you to make sure that no other applications run on
    those nodes. Node affinity or anti-affinity rules provide a solution to this requirement
    to only associate specific Pods to a set of nodes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您是银行共享Kubernetes集群的管理员。多个团队在同一集群上运行其应用程序。您的组织安全组已经确定了可以运行数据敏感应用程序的节点，并希望您确保没有其他应用程序在这些节点上运行。节点亲和性或反亲和性规则为满足此要求提供了解决方案，只将特定的Pod关联到一组节点。
- en: Node affinity rules are defined through two components. First, you assign a
    label to a set of nodes. The second part is to configure the Pods to associate
    them only with the nodes with certain labels. Another way to think about this
    is that the Pod defines where it should be placed, and the Scheduler matches the
    labels in this definition with the node labels.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 节点亲和规则由两个组件定义。首先，您为一组节点分配一个标签。第二部分是配置Pod，使它们只与具有特定标签的节点相关联。另一种思考方式是，Pod定义了它应该放置在哪里，调度程序将此定义中的标签与节点标签进行匹配。
- en: 'There are two types of node affinity/anti-affinity rules:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的节点亲和性/反亲和性规则：
- en: '**Required rules** are hard rules. If these rules are not met, the Pod cannot
    be scheduled on a node. It is defined as the `requiredDuringSchedulingIgnoredDuringExecution`
    section in the Pod specification. Please see *Exercise 17.01*, *Running a Pod
    with Node Affinity* as an example of this.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必需规则是硬性规则。如果不满足这些规则，Pod将无法在节点上调度。它在Pod规范的`requiredDuringSchedulingIgnoredDuringExecution`部分中定义。请参阅*练习17.01*，*使用节点亲和性运行Pod*，作为此规则的示例。
- en: '**Preferred rules** are soft rules. The Scheduler tries to enforce preferred
    rules whenever possible, but it goes ahead to ignore them when the rules cannot
    be enforced, that is, the Pod would be rendered unschedulable if these rules were
    followed as rigidly. Preferred rules are defined as the `preferredDuringSchedulingIgnoredDuringExecution`
    section in the Pod specification.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首选规则是软规则。调度程序尽量在可能的情况下执行首选规则，但如果规则无法执行，它会忽略这些规则，也就是说，如果严格遵循这些规则，Pod将无法被调度。首选规则在Pod规范的`preferredDuringSchedulingIgnoredDuringExecution`部分中定义。
- en: Preferred rules have weights associated with each criterion. The Scheduler will
    create a score based on these weights to schedule a Pod at the right node. The
    value of the weight field ranges from 1 to 100\. The Scheduler calculates the
    priority score for all the suitable nodes to find the optimal one. Note that the
    score can be impacted by other priority functions, such as `LeastRequestedPriority`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首选规则与每个标准相关联的权重。调度程序将根据这些权重创建一个分数，以在正确的节点上调度Pod。权重字段的值范围从1到100。调度程序计算所有合适节点的优先级分数，以找到最佳节点。请注意，分数可能会受到其他优先级函数的影响，例如`LeastRequestedPriority`。
- en: If you define a weight that is too low (compared to the other weights), then
    the overall score will be most affected by other priority functions, and our preferred
    rule may have little effect on the scheduling process. If you have multiple rules
    defined, then you can alter the weights of the rules that are the most important
    to you.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您定义的权重太低（与其他权重相比），则整体分数将受到其他优先级函数的最大影响，我们的首选规则可能对调度过程产生很少影响。如果定义了多个规则，则可以更改对您最重要的规则的权重。
- en: Affinity rules are defined in the Pod specification. Based on the labels of
    our desired/undesired nodes, we would provide the first part of the selection
    criteria in the Pod spec. It consists of the set of labels and, optionally, their
    values.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和规则是在Pod规范中定义的。基于我们期望/不期望的节点的标签，我们将在Pod规范中提供选择标准的第一部分。它包括一组标签，以及可选的标签值。
- en: 'The other part of the criteria is to provide the way we want to match the labels.
    We define these matching criteria as the **operator** in the affinity definition.
    This operator can have the following values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的另一部分是提供我们想要匹配标签的方式。我们将这些匹配标准定义为亲和性定义中的**运算符**。此运算符可以具有以下值：
- en: The `In` operator instructs the Scheduler to schedule the Pods on the nodes
    that match the label and one of the specified values.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`In`运算符指示调度程序在匹配标签和指定值之一的节点上调度Pod。'
- en: The `NotIn` operator instructs the Scheduler to not schedule the Pods on the
    nodes that do not match the label and any of the specified values. This is a negative
    operator and denotes the anti-affinity configuration.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NotIn`运算符指示调度程序不要在不匹配标签和任何指定值的节点上调度Pod。这是一个否定运算符，表示反亲和性配置。'
- en: The `Exists` operator instructs the Scheduler to schedule the Pods on the nodes
    that match the label. The value of the label does not matter in this case. Thus,
    this operator is satisfied even if the specified label exists and the value of
    the label does not match.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Exists`运算符指示调度程序在匹配标签的节点上调度Pod。在这种情况下，标签的值并不重要。因此，即使指定的标签存在且标签的值不匹配，此运算符也是满足的。'
- en: The `DoesNotExist` operator instructs the Scheduler to not schedule the Pods
    on the nodes that do not match the label. The value of the label does not matter
    in this case. This is a negative operator and denotes the anti-affinity configuration.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DoesNotExist`运算符指示调度程序不要在不匹配标签的节点上调度Pod。在这种情况下，标签的值并不重要。这是一个否定运算符，表示反亲和性配置。'
- en: Note that affinity and anti-affinity rules are defined based on the labels on
    the nodes. If the labels on a node are changed, it is possible that a node affinity
    rule may no longer be applied. In this case, the Pods that are running will continue
    to run on the node. If a Pod is restarted, or if it dies and a new Pod is created,
    Kubernetes considers this a new Pod. In this case, if the node labels have been
    modified, the Scheduler may not put the Pod on the same node. This is something
    that you would want to be mindful of when you modify node labels. Let's implement
    these rules for a Pod in the following exercise.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，亲和性和反亲和性规则是基于节点上的标签定义的。如果节点上的标签发生更改，可能会导致节点亲和性规则不再适用。在这种情况下，正在运行的Pod将继续在节点上运行。如果重新启动Pod，或者Pod死亡并创建了一个新的Pod，Kubernetes将视其为新的Pod。在这种情况下，如果节点标签已被修改，调度程序可能不会将Pod放在同一节点上。当您修改节点标签时，这是您需要注意的事项。让我们在以下练习中为一个Pod实现这些规则。
- en: 'Exercise 17.01: Running a Pod with Node Affinity'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习17.01：运行具有节点亲和性的Pod
- en: 'In this exercise, we will configure a Pod to be scheduled on the node available
    in our Minikube environment. We will also see, if the labels do not match, the
    Pod will be in the `Pending` state. Think of this state in which the scheduler
    is unable to find the right node to assign to the Pod:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将配置一个Pod，以便在我们的Minikube环境中可用的节点上进行调度。我们还将看到，如果标签不匹配，Pod将处于`Pending`状态。想象一下这种状态，在这种状态下，调度程序无法找到合适的节点分配给Pod：
- en: 'Create a new namespace called `schedulerdemo` using the following command:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建一个名为`schedulerdemo`的新命名空间：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should see the following response:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we need to create a Pod with node affinity defined. Create a file named
    `pod-with-node-affinity.yaml` with the following specification:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要创建一个具有节点亲和性定义的Pod。创建一个名为`pod-with-node-affinity.yaml`的文件，其中包含以下规范：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that in the Pod specification, we have added the new `affinity` section.
    This rule is configured as `requiredDuringSchedulingIgnoredDuringExecution`. This
    means if the node with a matching label does not exist, this Pod will not get
    scheduled. Also note that as per the `In` operator, the expressions mentioned
    here are to be matched with the node labels. In this example, a matching node
    would have the label `data-center=sydney`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在Pod规范中，我们已经添加了新的`affinity`部分。这个规则被配置为`requiredDuringSchedulingIgnoredDuringExecution`。这意味着如果没有具有匹配标签的节点，这个Pod将不会被调度。还要注意，根据`In`运算符，这里提到的表达式将与节点标签匹配。在这个例子中，匹配的节点将具有标签`data-center=sydney`。
- en: 'Try to create this Pod and see if it gets scheduled and executed:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试创建这个Pod，看看它是否被调度和执行：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should see the following response:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that the response you see here does not necessarily imply that the Pod
    has successfully been executed on a node. Let's check that in the following step.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里看到的响应并不一定意味着Pod已成功在节点上执行。让我们在下一步中检查一下。
- en: 'Check the status of the Pod using this command:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个命令检查Pod的状态：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You will see the following response:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到以下响应：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From this output, you can see that the Pod is in the `Pending` state and it
    is not being executed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中，你可以看到Pod处于`Pending`状态，没有被执行。
- en: 'Check the `events` to see why the Pod is not being executed:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查`events`以查看为什么Pod没有被执行：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You will see the following response:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到以下响应：
- en: '![Figure 17.4: Getting the list of events'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.4：获取事件列表'
- en: '](image/B14870_17_04.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_04.jpg)'
- en: 'Figure 17.4: Getting the list of events'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4：获取事件列表
- en: You can see that Kubernetes is saying that there is no node to match the selector
    for this Pod.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到Kubernetes说没有节点与此Pod的选择器匹配。
- en: 'Let''s delete the Pod before proceeding further:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在继续之前，让我们删除Pod：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should see the following response:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s see what nodes are available in our cluster:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们集群中有哪些节点可用：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You will see the following response:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到以下响应：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Since we are using Minikube, there is only one node available called `minikube`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是Minikube，只有一个名为`minikube`的节点可用。
- en: 'Check the label for the `minikube` node. Use the `describe` command as shown
    here:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查`minikube`节点的标签。使用如下所示的`describe`命令：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You should see the following response:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 17.5: Describing the minikube node'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.5：描述minikube节点'
- en: '](image/B14870_17_05.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_05.jpg)'
- en: 'Figure 17.5: Describing the minikube node'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.5：描述minikube节点
- en: As you can see, the label that we want, `data-center=sydney`, does not exist.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们想要的标签`data-center=sydney`并不存在。
- en: 'Now, let''s apply the desired label to our node using this command:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用这个命令将期望的标签应用到我们的节点上：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You will see the following response indicating that the node was labeled:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到以下响应，表明节点已被标记：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Verify whether the label is applied to the node using the `describe` command:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`describe`命令验证标签是否应用到节点上：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should see the following response:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 17.6: Checking the label on the minikube node'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.6：检查minikube节点上的标签'
- en: '](image/B14870_17_06.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_06.jpg)'
- en: 'Figure 17.6: Checking the label on the minikube node'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.6：检查minikube节点上的标签
- en: As you can see in this image, our label has now been applied.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这张图片中看到的，我们的标签现在已经被应用。
- en: 'Now try to run the Pod again and see if it can be executed:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在再次尝试运行Pod，看看它是否可以被执行：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You should see the following response:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let''s check whether the Pod is successfully running:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们检查一下 Pod 是否成功运行：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You should see the following response:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Thus, our Pod is successfully running.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的 Pod 成功运行。
- en: 'Let''s check out how Pod scheduling is displayed in `events`:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看 `events` 中如何显示 Pod 调度：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You will get the following response:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下响应：
- en: '![Figure 17.7: Checking out scheduling events'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.7：查看调度事件'
- en: '](image/B14870_17_07.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_07.jpg)'
- en: 'Figure 17.7: Checking out scheduling events'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.7：查看调度事件
- en: As you can see in the preceding output, the Pod has been successfully scheduled.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的输出中看到的，Pod 已成功调度。
- en: 'Now, let''s do some housekeeping to avoid conflicts with further exercises
    and activities. Delete the Pod using this command:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们进行一些清理工作，以避免与进一步的练习和活动发生冲突。使用以下命令删除 Pod：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should see the following response:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Remove the label from the node using the following command:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从节点中删除标签：
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note that the syntax for deleting the label from the Pod has an additional
    hyphen (`–`) after the label name. You should see the following response:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从 Pod 中删除标签的语法在标签名称后有一个额外的连字符（`–`）。你应该看到以下响应：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this exercise, we have seen how node affinity works by labeling a node and
    then scheduling a Pod on the labeled node. We have also seen how Kubernetes events
    can be used to see the status of Pod scheduling.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们已经看到了节点亲和力是如何工作的，通过给节点贴标签，然后在贴有标签的节点上调度 Pod。我们还看到了 Kubernetes 事件如何用于查看
    Pod 调度的状态。
- en: The `data-center=sydney` label that we used in this exercise also hints at an
    interesting use case. We can use node affinity and anti-affinity rules to target
    not just a specific Pod, but also specific server racks or data centers. We would
    simply assign specific labels to all nodes in a specific server rack, data center,
    availability zone, and so on. Then, we can simply pick and choose the desired
    targets for our Pods.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中我们使用的 `data-center=sydney` 标签也暗示了一个有趣的用例。我们可以使用节点亲和性和反亲和性规则来定位不仅特定的 Pod，还有特定的服务器机架或数据中心。我们只需为特定服务器机架、数据中心、可用区等的所有节点分配特定的标签。然后，我们可以简单地挑选所需的目标来为我们的
    Pod。
- en: Pod Affinity and Anti-Affinity
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod 亲和性和反亲和性
- en: Pod affinity and Pod anti-affinity allow your Pods to check what other Pods
    are running on a given node before they are scheduled on that node. Note that
    other Pods in this context do not mean a new copy of the same Pod, but Pods related
    to different workloads.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 亲和力和 Pod 反亲和力允许你的 Pod 在被调度到节点之前检查在该节点上运行的其他 Pod。请注意，在这种情况下，其他 Pod 并不意味着相同
    Pod 的新副本，而是与不同工作负载相关的 Pod。
- en: Pod affinity allows you to control on which node your Pod is eligible to be
    scheduled based on the labels of the other Pods that are already running on that
    node. The idea is to cater to the need to place two different types of containers
    relative to each other at the same place or to keep them apart.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 亲和力允许你控制 Pod 有资格被调度到哪个节点，这取决于已经在该节点上运行的其他 Pod 的标签。其想法是满足在同一位置放置两种不同类型的容器的需求，或者将它们分开。
- en: 'Consider that your application has two components: a frontend part (for example,
    a GUI) and a backend (for example, an API). Let''s assume that you want to run
    them on the same host because the communications between frontend and backend
    Pods would be faster if they are hosted on the same node. By default, on a multi-node
    cluster (not Minikube), the Scheduler will schedule such Pods on different nodes.
    Pod affinity provides a way to control the scheduling of Pods relative to each
    other so that we can ensure the optimal performance of our application.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的应用程序有两个组件：前端部分（例如GUI）和后端（例如API）。假设您希望将它们运行在同一主机上，因为如果前端和后端Pod在同一节点上托管，它们之间的通信将更快。在多节点集群（而不是Minikube）上，默认情况下，调度程序将在不同的节点上调度这样的Pod。Pod亲和提供了一种控制Pod相对于彼此的调度的方式，以便我们可以确保应用程序的最佳性能。
- en: There are two components that are required to define Pod affinity. The first
    component defines how the scheduler will relate the target Pod (in our previous
    example, the frontend Pod) to the already running Pods (the backend Pod). This
    is done through labels on the Pod. In the Pod affinity rules, we mention which
    labels of the other Pods should be used to relate to the new Pod. Label selectors
    have similar operators, as described in the Node Affinity and Anti-Affinity section,
    for matching the labels of the Pods.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 定义Pod亲和需要两个组件。第一个组件定义了调度程序如何将目标Pod（在我们之前的示例中，前端Pod）与已经运行的Pod（后端Pod）相关联。这是通过Pod上的标签完成的。在Pod亲和规则中，我们提到了应该用于与新Pod相关联的其他Pod的哪些标签。标签选择器具有与节点亲和和反亲和部分中描述的类似操作符，用于匹配Pod的标签。
- en: The second component describes where you want to run the target Pods. Just as
    we have seen in the previous exercise, we can use Pod affinity rules to schedule
    a Pod on the same node as the other Pod (in our example, we are assuming that
    the backend Pod is the otherPod that is already running), any node on the same
    rack as the other Pod, any node on the same data center as the other Pod, and
    so on. This component defines the set of nodes where the Pods can be allocated.
    To achieve this, we label our group of nodes and define this label as `topologyKey`
    in the Pod specification. For example, if we use the hostname as the value for
    `topologyKey`, the Pods will be placed on the same node.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个组件描述了您希望在哪里运行目标Pod。就像我们在前面的练习中看到的那样，我们可以使用Pod亲和规则将Pod调度到与其他Pod相同的节点（在我们的示例中，我们假设后端Pod是已经在运行的otherPod），与其他Pod相同机架上的任何节点，与其他Pod相同数据中心的任何节点等等。该组件定义了Pod可以分配的节点集。为了实现这一点，我们对节点组进行标记，并在Pod规范中将此标签定义为`topologyKey`。例如，如果我们将主机名作为`topologyKey`的值，Pod将被放置在同一节点上。
- en: If we label our nodes with the rack name on which they are hosted and define
    the rack name as `topologyKey`, then the candidate Pods will be scheduled for
    one of the nodes with the same rack name label.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用机架名称对节点进行标记，并将机架名称定义为`topologyKey`，那么候选Pod将被调度到具有相同机架名称标签的节点之一。
- en: Similar to the node affinity rules defined in the previous section, there are
    hard and soft Pod affinity rules as well. Hard rules are defined with `requiredDuringSchedulingIgnoredDuringExecution`
    while soft rules are defined with `preferredDuringSchedulingIgnoredDuringExecution`.
    It is possible to have multiple combinations of hard and soft rules in the Pod
    affinity configuration.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一节中定义的节点亲和规则类似，硬亲和规则和软亲和规则也存在。硬规则使用`requiredDuringSchedulingIgnoredDuringExecution`进行定义，而软规则使用`preferredDuringSchedulingIgnoredDuringExecution`进行定义。Pod亲和配置中可能存在多种硬和软规则的组合。
- en: 'Exercise 17.02: Running Pods with Pod Affinity'
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习17.02：使用Pod亲和运行Pod
- en: 'In this exercise, we will see how Pod affinity can help the Scheduler to see
    the relationships between different Pods and assign them to suitable nodes. We
    will place Pods using the `preferred` option. In a later part of this exercise,
    we will configure the Pod anti-affinity using the `required` option and see that
    that Pod will not be scheduled until all the criteria are met. We will use the
    same example of frontend and backend Pods that we mentioned earlier:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将看到Pod亲和性如何帮助调度器查看不同Pod之间的关系，并将它们分配到合适的节点上。我们将使用`preferred`选项放置Pod。在这个练习的后面部分，我们将使用`required`选项配置Pod反亲和性，并看到直到满足所有标准为止，该Pod都不会被调度。我们将使用前面提到的前端和后端Pod的相同示例：
- en: 'We need to create and run the backend Pod first. Create a file named `pod-with-pod-affinity-first.yaml`
    with the following contents:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要首先创建并运行后端Pod。创建一个名为`pod-with-pod-affinity-first.yaml`的文件，内容如下：
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This Pod is a simple Pod with just a loop printing a message. Notice that we
    have assigned a label to the Pod so that it can be related to the frontend pod.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Pod是一个简单的Pod，只是循环打印一条消息。请注意，我们为Pod分配了一个标签，以便它与前端Pod相关联。
- en: 'Let''s create the Pod defined in the previous step:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建上一步中定义的Pod：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should see the following response:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s see if the Pod has been successfully created:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看Pod是否已成功创建：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You should see a response like this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到这样的响应：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let''s check the labels on the `minikube` node:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们检查`minikube`节点上的标签：
- en: '[PRE31]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You should see the following response:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 17.8: Describing the minikube node'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.8：描述minikube节点'
- en: '](image/B14870_17_08.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_08.jpg)'
- en: 'Figure 17.8: Describing the minikube node'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.8：描述minikube节点
- en: Since we want to run both the Pods on the same host, we can use the `kubernetes.io/hostname`
    label of the node.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望在同一台主机上运行这两个Pod，我们可以使用节点的`kubernetes.io/hostname`标签。
- en: 'Now, let''s define the second Pod. Create a file named `pod-with-pod-affinity-second.yaml`
    with the following contents:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义第二个Pod。创建一个名为`pod-with-pod-affinity-second.yaml`的文件，内容如下：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Consider this Pod as the frontend application. Notice that we have defined a
    `preferredDuringSchedulingIgnoredDuringExecution` rule in the `podAffinity` section.
    We have also defined the `labels` and the `topologyKey` for the Pods and the nodes.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 将此Pod视为前端应用程序。请注意，我们在`podAffinity`部分定义了`preferredDuringSchedulingIgnoredDuringExecution`规则。我们还为Pod和节点定义了`labels`和`topologyKey`。
- en: 'Let''s create the Pod defined in the previous step:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建上一步中定义的Pod：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You should see the following response:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Verify the status of the Pods using the `get` command:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`get`命令验证Pod的状态：
- en: '[PRE35]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You should see the following response:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As you can see, the `pod-with-pod-affinity-fe` Pod is running. This is not much
    different than the normal Pod placement. This is because we have only one node
    in the Minikube environment and we have defined the Pod affinity using `preferredDuringSchedulingIgnoredDuringExecution`,
    which is the soft variation of the matching criteria.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`pod-with-pod-affinity-fe` Pod正在运行。这与普通的Pod放置没有太大不同。这是因为在Minikube环境中只有一个节点，并且我们使用了`preferredDuringSchedulingIgnoredDuringExecution`来定义Pod亲和性，这是匹配标准的软变体。
- en: The next steps of this exercise will talk about anti-affinity using `requiredDuringSchedulingIgnoredDuringExecution`
    or the hard variation of the matching criteria, and you will see that the Pod
    does not reach the `Running` state.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的下一步将讨论使用`requiredDuringSchedulingIgnoredDuringExecution`或匹配标准的硬变体的反亲和性，并且您将看到该Pod不会达到`Running`状态。
- en: 'First, let''s delete the `pod-with-pod-affinity-fe` Pod:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们删除`pod-with-pod-affinity-fe` Pod：
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You should see the following response:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Confirm that the Pod has been deleted by listing all the Pods:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过列出所有的Pod来确认Pod已被删除：
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You should see the following response:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now create another Pod definition with the following contents and save it as
    `pod-with-pod-anti-affinity-second.yaml`:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在创建另一个Pod定义，内容如下，并将其保存为`pod-with-pod-anti-affinity-second.yaml`：
- en: '[PRE41]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, the configuration is for `podAntiAffinity` and it uses the `requiredDuringSchedulingIgnoredDuringExecution`
    option, which is the hard variation of Pod affinity rules. Here, the Scheduler
    will not schedule any Pod if the condition is not met. We are using the `In` operator
    so that our Pod will not run on the same host as any Pod with the parameters defined
    in the `labelSelector` component of the configuration.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，配置是针对`podAntiAffinity`，它使用`requiredDuringSchedulingIgnoredDuringExecution`选项，这是Pod亲和性规则的硬变体。在这里，调度程序将不会调度任何Pod，如果条件不满足。我们使用`In`运算符，以便我们的Pod不会在与配置的`labelSelector`组件中定义的任何Pod相同的主机上运行。
- en: 'Try creating the Pod with the preceding specification:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用上述规范创建Pod：
- en: '[PRE42]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You should see the following response:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE43]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, check the status of this Pod:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，检查此Pod的状态：
- en: '[PRE44]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You should see the following response:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE45]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: From this output, you can see the Pod is in the `Pending` state.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中，您可以看到Pod处于“Pending”状态。
- en: 'You can verify that the Pod is not being scheduled because of Pod anti-affinity
    by checking events:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过检查事件来验证Pod反亲和性导致Pod无法调度：
- en: '[PRE46]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You should see the following response:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 17.9: Checking out the event for failed scheduling'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.9：检查调度失败的事件'
- en: '](image/B14870_17_09.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_09.jpg)'
- en: 'Figure 17.9: Checking out the event for failed scheduling'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.9：检查调度失败的事件
- en: In this exercise, we have seen how Pod affinity can help place two different
    Pods on the same node. We have also seen how Pod anti-affinity options can help
    us schedule the Pods on different sets of hosts.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们已经看到Pod亲和性如何帮助将两个不同的Pod放置在同一个节点上。我们还看到了Pod反亲和性选项如何帮助我们在不同的主机集上调度Pod。
- en: Pod Priority
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod优先级
- en: Kubernetes allows you to associate a priority with a Pod. If there are resource
    constraints, if a new Pod with high priority is requested to be scheduled, the
    Kubernetes scheduler may evict the Pods with lower priority in order to make room
    for the new high-priority Pod.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes允许您为Pod关联一个优先级。如果存在资源约束，如果请求调度一个具有较高优先级的新Pod，则Kubernetes调度程序可能会驱逐优先级较低的Pod，以便为新的高优先级Pod腾出空间。
- en: Consider an example where you are a cluster administrator and you run both critical
    and non-critical workloads in the cluster. An example is a Kubernetes cluster
    for a bank. In this case, you would have a payment service as well as the bank's
    website. You may decide that processing payments are of higher importance than
    running the website. By configuring Pod priority, you can prevent lower-priority
    workloads from impacting critical workloads in your cluster, especially in cases
    where the cluster starts to reach its resource capacity. This technique of evicting
    lower-priority Pods to schedule more critical Pods could be faster than adding
    additional nodes and would help you better manage traffic spikes on the cluster.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个例子，您是一个集群管理员，您在集群中运行关键和非关键的工作负载。一个例子是银行的Kubernetes集群。在这种情况下，您可能会有一个支付服务以及银行的网站。您可能会决定处理付款比运行网站更重要。通过配置Pod优先级，您可以防止低优先级的工作负载影响集群中的关键工作负载，特别是在集群开始达到其资源容量的情况下。将低优先级的Pod驱逐以安排更关键的Pod的技术可能比添加额外的节点更快，并且可以帮助您更好地管理集群上的流量波动。
- en: The way we associate a priority with a Pod is to define an object known as `PriorityClass`.
    This object holds the priority, which is defined as a number between 1 and 1 billion.
    The higher the number, the higher the priority. Once we have defined our priority
    classes, we assign a priority to a Pod by associating a `PriorityClass` with the
    Pod. By default, if there is no priority class associated with the Pod, the Pod
    either gets assigned the default priority class if it is available, or it gets
    assigned the priority value of 0.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 将Pod与优先级关联的方式是定义一个名为`PriorityClass`的对象。该对象包含优先级，定义为1到10亿之间的数字。数字越高，优先级越高。一旦我们定义了我们的优先级类，我们通过将`PriorityClass`与Pod关联来为Pod分配优先级。默认情况下，如果Pod没有与其关联的优先级类，则Pod将被分配默认的优先级类（如果可用），或者将被分配优先级值为0。
- en: 'You can get the list of priority classes similarly to any other objects:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像获取其他对象一样获取优先级类的列表：
- en: '[PRE47]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'You should see a response like this:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE48]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Note that in Minikube, there are two priority classes predefined in the environment.
    Let''s learn more about the `system-cluster-critical` class. Issue the following
    command to get the details about it:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在Minikube中，环境中预定义了两个优先级类。让我们更多地了解`system-cluster-critical`类。发出以下命令以获取有关它的详细信息：
- en: '[PRE49]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You should see the following response:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 17.10: Describing the system-cluster-critical PriorityClass'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.10：描述system-cluster-critical PriorityClass'
- en: '](image/B14870_17_10.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_10.jpg)'
- en: 'Figure 17.10: Describing the system-cluster-critical PriorityClass'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.10：描述system-cluster-critical PriorityClass
- en: The output here mentions that this class is reserved for the Pods that are absolutely
    critical for the cluster. etcd is one such Pod. Let's see if this priority class
    is associated with it.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的输出提到这个类是为绝对关键的集群Pod保留的。etcd就是这样的一个Pod。让我们看看这个优先级类是否与它关联。
- en: 'Issue the following command to get details about the etcd Pod running in Minikube:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 发出以下命令以获取有关在Minikube中运行的etcd Pod的详细信息：
- en: '[PRE50]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You should see the following response:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 17.11: Getting information about the etcd-minikube Pod'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.11：获取有关etcd-minikube Pod的信息'
- en: '](image/B14870_17_11.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_11.jpg)'
- en: 'Figure 17.11: Getting information about the etcd-minikube Pod'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.11：获取有关etcd-minikube Pod的信息
- en: You can see from this output that the Pod has been associated with the `system-cluster-critical`
    priority.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从此输出中看到Pod已与`system-cluster-critical`优先级关联。
- en: In the following exercise, we will add a default priority class and a higher-priority
    class to better understand the behavior of the Kubernetes scheduler.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将添加一个默认的优先级类和一个更高的优先级类，以更好地理解Kubernetes调度程序的行为。
- en: It is important to understand that Pod priority works in coordination with other
    rules, such as Pod affinity. If the Scheduler determines that a high-priority
    Pod cannot be scheduled even if lower-priority Pods are evicted, it will not evict
    lower-priority Pods.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解Pod优先级与其他规则（如Pod亲和性）协同工作。如果调度程序确定无法安排高优先级的Pod，即使低优先级的Pod被驱逐，它也不会驱逐低优先级的Pod。
- en: Similarly, if high-priority and low-priority Pods are waiting to be scheduled
    and the scheduler determines that high-priority Pods cannot be scheduled due to
    affinity or anti-affinity rules, the scheduler will schedule the suitable low-priority
    Pods.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果高优先级和低优先级的Pod正在等待调度，并且调度程序确定由于亲和性或反亲和性规则而无法安排高优先级的Pod，则调度程序将安排适当的低优先级的Pod。
- en: 'Exercise 17.03: Pod Priority and Preemption'
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习17.03：Pod优先级和抢占
- en: 'In this exercise, we shall define two priority classes: default (low priority)
    and high priority. We will then create 10 Pods with default priority and allocate
    some CPU and memory to each Pod. After this, we will check how much capacity is
    being used from our local cluster. We will then create 10 more Pods with high
    priority and allocate resources to them. We will see that the Pods with the default
    priority will be terminated and the higher-priority Pods will be scheduled on
    the cluster. We will then reduce the number of high-priority Pods from 10 to 5
    and then see that some of the low-priority Pods are being scheduled again. This
    is because reducing the number of high-priority Pods should free up some resources:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将定义两个优先级类：默认（低优先级）和高优先级。然后，我们将创建10个具有默认优先级的Pod，并为每个Pod分配一些CPU和内存。之后，我们将检查从我们的本地集群中使用了多少容量。然后，我们将创建10个具有高优先级的Pod，并为它们分配资源。我们将看到具有默认优先级的Pod将被终止，并且更高优先级的Pod将被调度到集群上。然后，我们将把高优先级Pod的数量从10减少到5，然后看到一些低优先级的Pod再次被调度。这是因为减少高优先级Pod的数量应该释放一些资源：
- en: 'First, let''s create the definition for the default priority class. Create
    a file named `priority-class-default.yaml` with the following contents:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们为默认优先级类创建定义。使用以下内容创建一个名为`priority-class-default.yaml`的文件：
- en: '[PRE51]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Note that we have marked this priority class as default by setting the value
    of `globalDefault` as `true`. Also, the priority number, `1`, is very low.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们通过将`globalDefault`的值设置为`true`来将此优先级类标记为默认。此外，优先级数字`1`非常低。
- en: 'Create this priority class using the following command:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建此优先级类：
- en: '[PRE52]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'You should see the following response:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE53]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note that we have not mentioned the namespace as this object is not a namespace-level
    object. A priority class is a cluster scope object in Kubernetes.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于此对象不是命名空间级对象，因此我们没有提及命名空间。优先级类是Kubernetes中的集群范围对象。
- en: 'Let''s check whether our priority class has been created:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下我们的优先级类是否已经创建：
- en: '[PRE54]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'You should see the following list:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下列表：
- en: '[PRE55]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In this output, you can see the priority class that we just created under the
    name `default-priority`, and it is the global default as you can see in the `GLOBAL-DEFAULT`
    column. Now create another priority class with higher priority.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在此输出中，您可以看到我们刚刚创建的优先级类的名称为`default-priority`，并且正如您在`GLOBAL-DEFAULT`列中所看到的那样，它是全局默认的。现在创建另一个优先级更高的优先级类。
- en: 'Create a file named `priority-class-highest.yaml` with the following contents:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下内容创建一个名为`priority-class-highest.yaml`的文件：
- en: '[PRE56]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Note the very high value of the `value` field in this object.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意此对象中`value`字段的非常高的值。
- en: 'Use the definition from the previous step to create a Pod priority class using
    the following command:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一步的定义使用以下命令创建一个Pod优先级类：
- en: '[PRE57]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'You should see the following response:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE58]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now let''s create a definition for a Deployment with `10` Pods and a default
    priority. Create a file named `pod-with-default-priority.yaml` using the following
    contents to define our Deployment:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们创建一个具有`10`个Pod和默认优先级的部署的定义。使用以下内容创建一个名为`pod-with-default-priority.yaml`的文件来定义我们的部署：
- en: '[PRE59]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let''s create the Deployment that we defined in the previous step:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建我们在上一步中定义的部署：
- en: '[PRE60]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'You should see this response:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到这个响应：
- en: '[PRE61]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now, increase the memory and CPU allocated to each of them to 128 MiB and 1/10
    of the CPU by using the following commands:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过使用以下命令将每个Pod分配的内存和CPU增加到128 MiB和CPU的1/10：
- en: '[PRE62]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You should see the following response:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE63]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You may need to adjust this resource allocation as per the resources available
    on your computer. You can start with 1/10 CPU and verify the resources as mentioned
    in *step 10*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要根据计算机上可用的资源调整此资源分配。您可以从1/10的CPU开始，并按照*步骤10*中提到的方式验证资源。
- en: 'Verify that the Pods are running using the following command:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证Pod是否正在运行：
- en: '[PRE64]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'You should see the following list of Pods:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到以下Pod列表：
- en: '![Figure 17.12: Getting the list of Pods'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.12：获取Pod列表'
- en: '](image/B14870_17_12.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_12.jpg)'
- en: 'Figure 17.12: Getting the list of Pods'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.12：获取Pod列表
- en: 'Check the resource usage in our cluster. Note that we have only one node, and
    thus we can easily see the values by issuing the `describe` command:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查我们集群中的资源使用情况。请注意，我们只有一个节点，因此我们可以通过发出`describe`命令轻松地看到这些值：
- en: '[PRE65]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The following screenshot is truncated for a better presentation. Find the `Allocated
    resources` section in your output:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图已经被截断以便更好地呈现。在您的输出中找到`分配的资源`部分：
- en: '![Figure 17.13: Checking the resource utilization on the minikube node'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.13：检查minikube节点上的资源利用率'
- en: '](image/B14870_17_13.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_13.jpg)'
- en: 'Figure 17.13: Checking the resource utilization on the minikube node'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.13：检查minikube节点上的资源利用率
- en: Note that CPU usage is at 77% and memory at 64% for the `minikube` host. Please
    note that the resource utilization is dependent on the hardware of your computer
    and the resources allocated to Minikube. If your CPU is too powerful or if you
    have a huge amount of memory (or even if you have a slower CPU and less memory),
    you may see resource utilization values vastly different from what we see here.
    Please adjust the CPU and memory resources as mentioned in *step 8* so that we
    get similar resource utilization as we see here. This will enable you to see a
    similar result to the one we have demonstrated in the following steps of this
    exercise.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`minikube`主机的CPU使用率为77%，内存使用率为64%。请注意，资源利用率取决于您计算机的硬件和分配给Minikube的资源。如果您的CPU太强大，或者您有大量的内存（甚至如果您的CPU较慢，内存较少），您可能会看到与我们在这里看到的资源利用率值大不相同。请根据*步骤8*中提到的方式调整CPU和内存资源，以便我们获得与我们在这里看到的类似的资源利用率。这将使您能够看到与我们在本练习的后续步骤中演示的类似结果。
- en: 'Now let''s schedule Pods with high priority. Create 10 Pods using the Kubernetes
    Deployment object. For this, create a file named `pod-with-high-priority.yaml`
    with the following contents:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们安排具有高优先级的Pod。使用Kubernetes部署对象创建10个Pod。为此，请创建一个名为`pod-with-high-priority.yaml`的文件，其中包含以下内容：
- en: '[PRE66]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Note that `priorityClassName` has been set to the `highest-priority` class in
    the preceding specification.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的规范中，`priorityClassName`已设置为`highest-priority`类。
- en: 'Now create the Deployment that we created in the previous step:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在创建我们在上一步中创建的部署：
- en: '[PRE67]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'You should get the following output:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会得到以下输出：
- en: '[PRE68]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Allocate a similar amount of CPU and memory to these Pods as you did for the
    Pods with default priority:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为这些Pod分配与具有默认优先级的Pod相似的CPU和内存量：
- en: '[PRE69]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You should see the following response:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE70]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'After a minute or so, run the following command to see which Pods are running:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大约一分钟后，运行以下命令以查看正在运行的Pod：
- en: '[PRE71]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'You should see a response similar to this:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似于这样的响应：
- en: '![Figure 17.14: Getting the list of Pods'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.14：获取Pod列表'
- en: '](image/B14870_17_14.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_14.jpg)'
- en: 'Figure 17.14: Getting the list of Pods'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.14：获取Pod列表
- en: You can see that most of our high-priority Pods are in the `Running` state and
    the Pods with low-priority Pods are moved to the `Pending` state. This tells us
    the Kubernetes Scheduler has actually terminated the lower-priority Pods, and
    it is now waiting for the resources to be available to schedule them again.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们大多数高优先级的Pod都处于“Running”状态，而低优先级的Pod已经移动到“Pending”状态。这告诉我们Kubernetes调度程序实际上已经终止了低优先级的Pod，并且现在正在等待资源再次安排它们。
- en: 'Try changing the number of high-priority Pods from 10 to 5 and see if additional
    low-priority Pods can be scheduled. Change the number of replicas using this command:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试将高优先级的Pod数量从10个更改为5个，看看是否可以安排额外的低优先级Pod。使用此命令更改副本的数量：
- en: '[PRE72]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'You should see the following response:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE73]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Verify that high-priority Pods are reduced from 10 to 5 using the following
    command:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证高优先级的Pod是否从10个减少到5个：
- en: '[PRE74]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '![Figure 17.15: Getting the list of Pods'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.15：获取Pod列表'
- en: '](image/B14870_17_15.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_15.jpg)'
- en: 'Figure 17.15: Getting the list of Pods'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.15：获取Pod列表
- en: As you can see in this screenshot, some more low-priority Pods changed from
    the `Pending` state to the `Running` state. Thus, we can see that the Scheduler
    is working to make optimal use of the available resources based on the priority
    of workloads.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在此截图中所看到的，一些更低优先级的Pod已经从“Pending”状态变为“Running”状态。因此，我们可以看到调度程序正在根据工作负载的优先级来充分利用可用资源。
- en: In this exercise, we have used the Pod priority rules and seen how the Kubernetes
    Scheduler may choose to terminate the Pods with a lower priority if there are
    requests for a Pod with a higher priority to be fulfilled.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们已经使用了Pod优先级规则，并看到了Kubernetes调度程序可能会选择终止具有较低优先级的Pod，如果有对具有较高优先级的Pod的请求需要满足。
- en: Taints and Tolerations
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 污点和忍受度
- en: Previously, we have seen how Pods can be configured to control which node they
    run on. Now we will see how nodes can control which Pods can run on them using
    taints and tolerations.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们已经看到Pod可以配置以控制它们在哪个节点上运行。现在我们将看到节点如何控制可以在其上运行的Pod，使用污点和忍受度。
- en: A taint prevents the scheduling of a pod unless that Pod has a matching toleration
    for the Pod. Think of taint as an attribute of a node and a toleration is an attribute
    of a Pod. The Pod will get scheduled on the node only if the Pod's toleration
    matches the node's taint. The taints on a node tell the scheduler to check which
    Pods tolerate the taint and run only those Pods that match their toleration with
    the node's taint.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 污点阻止了Pod的调度，除非该Pod具有与之匹配的忍受度。将污点视为节点的属性，而忍受度是Pod的属性。只有当Pod的忍受度与节点的污点匹配时，Pod才会被安排在该节点上。节点上的污点告诉调度程序检查哪些Pod能容忍污点，并且只运行与节点的污点匹配的Pod。
- en: A taint definition contains the key, value, and effect. The key and value will
    match the Pod toleration definition in the Pod specification, while the effect
    instructs the scheduler what should be done once the node's taint matches the
    Pod's toleration.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 污点定义包含键、值和效果。键和值将与Pod规范中的Pod忍受度定义匹配，而效果指示调度程序一旦节点的污点与Pod的忍受度匹配应该执行什么操作。
- en: The following diagram provides an overview of how the process of controlling
    scheduling based on taints and tolerations works. Notice that a Pod with toleration
    can also be scheduled on a node with no taint.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表提供了一个概述，说明了基于污点和忍受度控制调度的过程是如何工作的。请注意，具有忍受度的Pod也可以安排在没有污点的节点上。
- en: '![Figure 17.16: Overview of how taints and tolerations are used to influence
    scheduling'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.16：污点和忍受度如何影响调度的概述'
- en: '](image/B14870_17_16.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_16.jpg)'
- en: 'Figure 17.16: Overview of how taints and tolerations are used to influence
    scheduling'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.16：污点和忍受度如何影响调度的概述
- en: 'When we define a taint, we also need to specify the behavior of the taint.
    This can be specified by the following values:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义一个污点时，我们还需要指定污点的行为。这可以通过以下值来指定：
- en: '`NoSchedule` provides the ability to reject the scheduling of new Pods on the
    node. Existing Pods that were scheduled before the taint was defined will continue
    to run on the node.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NoSchedule`提供了拒绝在节点上调度新Pod的能力。在定义污点之前已经调度的现有Pod将继续在节点上运行。'
- en: '`NoExecute` taint provides the ability to resist new Pods that do not have
    a toleration that matches the taint. It further checks whether all the existing
    Pods running on the node match this taint, and removes the ones that don''t.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NoExecute`污点提供了抵抗没有与污点匹配的容忍的新Pod的能力。它进一步检查所有正在节点上运行的现有Pod是否匹配此污点，并删除不匹配的Pod。'
- en: '`PreferNoSchedule` instructs the scheduler to avoid scheduling Pods that do
    not tolerate the taint on the node. This is a soft rule, where the scheduler will
    try to find the right node but it will still schedule the Pods on the node if
    it cannot find any other node that is appropriate as per the defined taint and
    toleration rules.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PreferNoSchedule`指示调度器避免在不容忍节点上调度Pod。这是一个软规则，调度器会尝试找到正确的节点，但如果找不到其他适合定义的污点和容忍规则的节点，它仍会在节点上调度Pod。'
- en: 'In order to apply a taint to a node, we can use the `kubectl taint` command
    as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对节点应用污点，我们可以使用`kubectl taint`命令，如下所示：
- en: '[PRE75]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: There can be many reasons why you would want certain Pods (applications) not
    to be run on specific nodes. An example use case could be the requirement of specialized
    hardware, such as a GPU for machine learning applications. Another case could
    be when a license restriction for software on the Pod dictates that it needs to
    run on specific nodes. For example, out of 10 worker nodes in your cluster, only
    2 nodes are allowed to run particular software. Using the taints and tolerations
    combination, you can help the scheduler to schedule Pods on the right node.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有很多原因你希望某些Pod（应用程序）不在特定节点上运行。一个例子可能是需要专门的硬件，比如用于机器学习应用的GPU。另一个情况可能是Pod上的软件的许可限制要求它在特定节点上运行。例如，在你的集群中有10个工作节点，只有2个节点被允许运行特定软件。使用污点和容忍的组合，你可以帮助调度器在正确的节点上调度Pod。
- en: 'Exercise 17.04: Taints and Tolerations'
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习17.04：污点和容忍
- en: 'In this exercise, we will see how taints and tolerations can allow us to schedule
    Pods on the nodes we desire. We will define a taint and try to schedule a Pod
    on the node. We then showcase the `NoExecute` functionality in which a Pod can
    be removed from a node if that taint on the node changes:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将看到污点和容忍如何允许我们在所需的节点上调度Pod。我们将定义一个污点，并尝试在节点上调度一个Pod。然后展示`NoExecute`功能，如果节点上的污点发生变化，Pod可以从节点中移除：
- en: 'Get the list of nodes using the following command:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令获取节点列表：
- en: '[PRE76]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'You should see the following list of nodes:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下节点列表：
- en: '[PRE77]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Recall that in our Minikube environment, we have only one node.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在我们的Minikube环境中，我们只有一个节点。
- en: 'Create a taint for the `minikube` node using the following command:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令为`minikube`节点创建一个污点：
- en: '[PRE78]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'You should see the following response:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '[PRE79]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Verify that the node has been tainted correctly. You can use the `describe`
    command to see what taints are applied to the node:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证节点是否已正确被污点。你可以使用`describe`命令查看节点上应用了哪些污点：
- en: '[PRE80]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'You should see the following response:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下响应：
- en: '![Figure 17.17: Checking the taints on the minikube node'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.17：检查minikube节点上的污点'
- en: '](image/B14870_17_17.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_17.jpg)'
- en: 'Figure 17.17: Checking the taints on the minikube node'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.17：检查minikube节点上的污点
- en: 'Now we need to create a Pod with toleration defined as per the taint. Create
    a file named `pod-toleration-noschedule.yaml` with the following contents:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要根据污点定义创建一个具有容忍度的Pod。创建一个名为`pod-toleration-noschedule.yaml`的文件，内容如下：
- en: '[PRE81]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Notice that the toleration value is the same as the taint defined in *step 1*,
    that is, `app=banking`. The `effect` attribute controls the type of toleration
    behavior. Here, we have defined `effect` as `NoSchedule`.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，容忍度值与*步骤1*中定义的污点相同，即`app=banking`。`effect`属性控制容忍度行为的类型。在这里，我们将`effect`定义为`NoSchedule`。
- en: 'Let''s create the Pod as per the preceding specification:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们根据前面的规范创建Pod：
- en: '[PRE82]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'This should give the following response:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该得到以下响应：
- en: '[PRE83]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Verify that the Pod is running using the following command:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证Pod是否正在运行：
- en: '[PRE84]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'You should see the following response:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 17.18: Getting the list of Pods'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.18：获取Pod列表'
- en: '](image/B14870_17_18.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_18.jpg)'
- en: 'Figure 17.18: Getting the list of Pods'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.18：获取Pod列表
- en: 'Now let''s define a different Pod with a toleration that does not match the
    taint on the node. Create a file named `pod-toleration-noschedule2.yaml` with
    the following contents:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们定义一个不匹配节点污点的容忍度的不同Pod。创建一个名为`pod-toleration-noschedule2.yaml`的文件，内容如下：
- en: '[PRE85]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Notice that here we have the toleration set to `app=hr`. We need a Pod with
    the same taint to match this toleration. Since we have tainted our node with `app=banking`,
    this Pod should not be scheduled by the scheduler. Let's try this in the following
    steps.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里我们将容忍度设置为`app=hr`。我们需要一个具有相同污点以匹配此容忍度的Pod。由于我们已经用`app=banking`污点了我们的节点，这个Pod不应该被调度程序调度。让我们在以下步骤中尝试一下。
- en: 'Create the Pod using the definition from the previous step:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一步的定义创建Pod：
- en: '[PRE86]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'This should give the following response:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该得到以下响应：
- en: '[PRE87]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Check the status of the Pod using the following command:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令检查Pod的状态：
- en: '[PRE88]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'You should see this response:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 17.19: Getting the list of Pods'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.19：获取Pod列表'
- en: '](image/B14870_17_19.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_19.jpg)'
- en: 'Figure 17.19: Getting the list of Pods'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.19：获取Pod列表
- en: You can see that Pod is in the `Pending` state and not in the `Running` state.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到Pod处于`Pending`状态，而不是`Running`状态。
- en: 'In the remaining part of this exercise, we shall see how the `NoExecute` effect
    instructs the scheduler to even remove Pods after they have been scheduled to
    the node. Before that, we need to do some cleanup. Delete both Pods using the
    following command:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本练习的剩余部分中，我们将看到`NoExecute`效果如何指示调度程序甚至在将Pod调度到节点后将其删除。在此之前，我们需要进行一些清理。使用以下命令删除两个Pod：
- en: '[PRE89]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'You should see the following response:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE90]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Let''s remove the taint from the node using the following command:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从节点中删除污点：
- en: '[PRE91]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Note the hyphen (`-`) at the end of the command, which tells Kubernetes to
    remove this label. You should see the following response:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意命令末尾的连字符（`-`），它告诉Kubernetes删除此标签。您应该看到以下响应：
- en: '[PRE92]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Our node is in the state where there is no taint defined. Now, we want to run
    a Pod first with the toleration as `app=banking` and allocate the Pod. Once the
    Pod is in the `Running` state, we will remove the taint from the node and see
    whether the Pod has been removed.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的节点处于未定义污点的状态。现在，我们想先以`app=banking`的容忍度运行一个Pod并分配该Pod。一旦Pod处于`Running`状态，我们将从节点中删除污点并查看Pod是否已被删除。
- en: 'Now, taint the node again with the `NoExecute` type as follows:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，再次使用`NoExecute`类型对节点进行污染：
- en: '[PRE93]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'You should see the following response:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE94]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Now, we need to define a Pod with matching toleration. Create a file called
    `pod-toleration-noexecute.yaml` with the following contents:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要定义一个具有匹配容忍度的Pod。创建一个名为`pod-toleration-noexecute.yaml`的文件，内容如下：
- en: '[PRE95]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Note that the `tolerations` section defines the label as `app=banking` and the
    `effect` as `NoExecute`.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，“tolerations”部分将标签定义为“app=banking”，效果定义为“NoExecute”。
- en: 'Create the Pod that we defined in the previous step using the following command:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建我们在上一步中定义的Pod：
- en: '[PRE96]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'You should see the following response:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '[PRE97]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Verify that the Pod is in the `Running` state using the following command:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证Pod是否处于“Running”状态：
- en: '[PRE98]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'You should see the following response:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下响应：
- en: '![Figure 17.20: Getting the list of Pods'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.20：获取Pod列表'
- en: '](image/B14870_17_20.jpg)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_20.jpg)'
- en: 'Figure 17.20: Getting the list of Pods'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.20：获取Pod列表
- en: 'Now remove the taint from the node using this command:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用以下命令从节点中删除污点：
- en: '[PRE99]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Note the hyphen (`-`) at the end of this command, which tells Kubernetes to
    remove the taint. You will see the following response:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意此命令末尾的连字符（`-`），它告诉Kubernetes删除污点。您将看到以下响应：
- en: '[PRE100]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: As mentioned earlier, Pods with tolerations can be attached to nodes with no
    taints. After you remove the taint, the Pod will still be executed. Note that
    we have not deleted the Pod and it is still running.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，具有容忍度的Pod可以附加到没有污点的节点。删除污点后，Pod仍将被执行。请注意，我们尚未删除Pod，它仍在运行。
- en: 'Now, if we add a new taint with `NoExecute` to the node, the Pod should be
    removed from it. To see this in action, add a new taint that is different than
    the Pod toleration:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果我们向节点添加一个带有`NoExecute`的新污点，Pod应该会从中删除。要查看此操作，请添加一个与Pod容忍度不同的新污点：
- en: '[PRE101]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'As you can see, we have added the `app=hr` taint to the Pod. You should see
    the following response:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们已将“app=hr”污点添加到Pod中。您应该看到以下响应：
- en: '[PRE102]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Now, let''s check the status of the Pod:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们检查一下Pod的状态：
- en: '[PRE103]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'You will see the following response:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下响应：
- en: '![Figure 17.21: Checking the status of our Pod'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '![图17.21：检查我们的Pod的状态'
- en: '](image/B14870_17_21.jpg)'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_21.jpg)'
- en: 'Figure 17.21: Checking the status of our Pod'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.21：检查我们的Pod的状态
- en: The Pod will either be removed or go into the `Terminating` (marked for removal)
    state. After a few seconds, Kubernetes will remove the Pod.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: Pod将被删除或进入“Terminating”（标记为删除）状态。几秒钟后，Kubernetes将删除Pod。
- en: In this exercise, you have seen how we can configure taints on nodes so that
    they accept only specific Pods. You have also configured the taint to affect the
    running Pods.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您已经看到我们如何在节点上配置污点，以便它们只接受特定的Pod。您还配置了污点以影响正在运行的Pod。
- en: Using a Custom Kubernetes Scheduler
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义Kubernetes调度程序
- en: Building your own fully featured scheduler is out of the scope of this workshop.
    However, it is important to understand that the Kubernetes platform allows you
    to write your own scheduler if your use case requires it, although it is not recommended
    to use a custom scheduler unless you have a very specialized use case.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 构建自己的功能齐全的调度程序超出了本研讨会的范围。但是，重要的是要理解，Kubernetes平台允许您编写自己的调度程序，如果您的用例需要，尽管不建议使用自定义调度程序，除非您有非常专业的用例。
- en: 'A custom scheduler runs as a normal Pod. You can specify in the definition
    of the Pod running your application to use the custom scheduler. You can add a
    `schedulerName` field in the Pod specification with the name of the custom scheduler
    as shown in this sample definition:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义调度程序作为普通Pod运行。您可以在运行应用程序的Pod的定义中指定使用自定义调度程序。您可以在Pod规范中添加一个`schedulerName`字段，其中包含自定义调度程序的名称，如此示例定义所示：
- en: '[PRE104]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: For this configuration to work, it is assumed that a custom scheduler called
    `custom-scheduler` is available in the cluster.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 为使此配置工作，假定集群中有一个名为`custom-scheduler`的自定义调度程序。
- en: 'Activity 17.01: Configuring a Kubernetes Scheduler to Schedule Pods'
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动17.01：配置Kubernetes调度程序以安排Pod
- en: 'Consider you are the administrator of a Kubernetes cluster and you have the
    following scenario:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您是 Kubernetes 集群的管理员，并且您面临以下情景：
- en: There is an API Pod that provides the current currency conversion rate.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一个 API Pod 提供当前的货币转换率。
- en: There is a GUI Pod that displays the conversion rate on a website.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一个 GUI Pod 在网站上显示转换率。
- en: There is a Pod that provides services for stock exchanges to get the real-time
    currency conversion rate.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一个 Pod 为股票交易所提供实时货币转换率的服务。
- en: You have been tasked to make sure that the API and GUI Pods run on the same
    node. You have also been asked to give higher priority to the real-time currency
    converter Pod if the traffic spikes. In this activity, you will control the behavior
    of the Kubernetes Scheduler to complete the activity.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 您被要求确保 API 和 GUI Pod 在同一节点上运行。您还被要求在流量激增时给予实时货币转换器 Pod 更高的优先级。在此活动中，您将控制 Kubernetes
    调度程序的行为以完成此活动。
- en: Each of the Pods in this activity should have 0.1 CPU and 100 MiB of memory
    allocated to it. Note that we have named the Pods API, GUI, and real-time to make
    things easier. The Pods in this activity are expected to be just printing expressions
    on the console. You can use the `k8s.gcr.io/busybox` image for all of them.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动中的每个 Pod 应分配 0.1 CPU 和 100 MiB 内存。请注意，我们已经将 Pod 命名为 API、GUI 和实时，以便操作更简单。此活动中的
    Pod 预计只会在控制台上打印表达式。您可以为它们全部使用 `k8s.gcr.io/busybox` 镜像。
- en: Note
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before starting this activity, make sure that the nodes are not tainted from
    the previous exercises. To see how to remove a taint, please see *step 15* of
    *Exercise 17.01*, *Running a Pod with Node Affinity* in this chapter.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始此活动之前，请确保节点没有从之前的练习中被污染。要了解如何去除污点，请参阅本章的“练习 17.01”中的“步骤 15”，“在节点亲和性下运行 Pod”。
- en: 'Here are some guidelines for the activity:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此活动的一些指南：
- en: Create a namespace called `scheduleractivity`.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为“scheduleractivity”的命名空间。
- en: Create the Pod priority for the API Pods.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 API Pod 创建 Pod 优先级。
- en: Deploy and make sure that the API and GUI Pods are using Pod affinity to be
    on the same node. The GUI Pod should define the affinity to be on the same node
    as the API pod.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署并确保 API 和 GUI Pod 使用 Pod 亲和性在同一节点上。GUI Pod 应定义与 API Pod 在同一节点上的亲和性。
- en: Scale the replicas of the API and GUI Pod to two each.
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 API 和 GUI Pod 的副本扩展到各自的两个。
- en: Create a Pod priority for the real-time currency converter Pod. Make sure that
    the API Pod priority, defined earlier, is less than the real-time Pod but greater
    than 0.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为实时货币转换器 Pod 创建一个 Pod 优先级。确保之前定义的 API Pod 优先级低于实时 Pod，但大于 0。
- en: Deploy and run the real-time currency converter Pod with one replica.
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署并运行一个实时货币转换器 Pod，副本数为 1。
- en: Make sure that all Pods are in the `Running` state.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保所有 Pod 都处于“运行”状态。
- en: Now, increase the number of replicas for the real-time currency converter Pod
    from 1 to 10.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将实时货币转换器 Pod 的副本数量从 1 增加到 10。
- en: See whether the real-time currency converter Pods are being started and whether
    the GUI Pods are being evicted. If not, keep on increasing the real-time Pods
    by a factor of 5.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看实时货币转换器 Pod 是否正在启动，GUI Pod 是否正在被驱逐。如果没有，请继续以 5 的倍数增加实时 Pod。
- en: Depending on your resources and the number of Pods, the scheduler may start
    evicting API Pods.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据您的资源和 Pod 的数量，调度程序可能会开始驱逐 API Pod。
- en: Reduce the number of replicas of the real-time Pod from 10 to 1 and see that
    the API and GUI Pods are scheduled back on the cluster.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将实时 Pod 的副本数量从 10 减少到 1，并确保 API 和 GUI Pod 被重新调度到集群上。
- en: 'Once you have completed the activity, two Pods each of the API and GUI Pods
    are expected to be in the `Running` state, along with one real-time Pod as shown
    in the following screenshot:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 完成活动后，预计 API 和 GUI Pod 每个将处于“运行”状态，以及一个实时 Pod，如下截图所示：
- en: '![Figure 17.22: Expected output of Activity 17.01'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 17.22：活动 17.01 的预期输出'
- en: '](image/B14870_17_22.jpg)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/B14870_17_22.jpg)'
- en: 'Figure 17.22: Expected output of Activity 17.01'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.22：活动 17.01 的预期输出
- en: Note that your output will vary as per your system resources, and hence, you
    may not see exactly what you see in this screenshot.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您的输出将根据系统资源而变化，因此您可能看不到与此截图完全相同的内容。
- en: Note
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The solution to this activity can be found at the following address: [https://packt.live/304PEoD](https://packt.live/304PEoD).'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在以下地址找到：[https://packt.live/304PEoD](https://packt.live/304PEoD)。
- en: Summary
  id: totrans-448
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The Kubernetes Scheduler is a powerful software that abstracts the work of selecting
    the appropriate node for a Pod on a cluster. The Scheduler watches for unscheduled
    Pods and attempts to find suitable nodes for them. Once it finds a suitable node
    for a Pod, it updates etcd (via the API server) that the Pod has been bound to
    the node.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 调度器是一个强大的软件，它抽象了在集群上为 Pod 选择合适节点的工作。调度器会监视未调度的 Pod，并尝试为它们找到合适的节点。一旦找到一个适合的节点，它会通过
    API 服务器更新 etcd，表示该 Pod 已绑定到该节点。
- en: The scheduler has matured with every release of Kubernetes. The default behavior
    of the scheduler is sufficient for a variety of workloads, although you have also
    seen many ways to customize the way that the Scheduler associates resources with
    Pods. You have seen how node affinity can help you schedule Pods on your desired
    nodes. Pod affinity can help you schedule a Pod relative to another Pod, and it
    is a good tool for applications where multiple modules are targeted to be placed
    next to each other. Taints and tolerations can also help you assign specific workloads
    to specific nodes. You have also seen that Pod priority can help you schedule
    the workloads as per the total resources available in the cluster.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Kubernetes 的每一个发布，调度器都得到了成熟。调度器的默认行为对各种工作负载已经足够，尽管您也看到了许多定制调度器与 Pod 关联资源的方式。您已经看到了节点亲和性如何帮助您在所需的节点上调度
    Pod。Pod 亲和性可以帮助您相对于另一个 Pod 调度一个 Pod，这对于多个模块被放置在一起的应用程序是一个很好的工具。污点和容忍也可以帮助您将特定的工作负载分配给特定的节点。您还看到了
    Pod 优先级如何帮助您根据集群中可用的总资源调度工作负载。
- en: In the next chapter, we will upgrade a Kubernetes cluster with no downtime.
    If you have configured custom scheduling in your cluster using any of the techniques
    shown in this chapter, you may need to plan your upgrade accordingly. Since the
    upgrade will take down one worker node at a time, it may be possible that some
    of your Pods may become non-schedulable because of your configuration, and that
    may not be an acceptable solution.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将升级一个 Kubernetes 集群，实现零停机。如果您在集群中使用本章展示的任何技术配置了自定义调度，您可能需要相应地计划升级。由于升级将逐个关闭一个工作节点，可能会导致一些
    Pod 由于您的配置而变得不可调度，这可能不是一个可接受的解决方案。
