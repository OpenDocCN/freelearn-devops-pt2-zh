- en: Playing with Containers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩转容器
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Scaling your containers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展您的容器
- en: Updating live containers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新实时容器
- en: Forwarding container ports
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转发容器端口
- en: Ensuring flexible usage of your containers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保灵活使用您的容器
- en: Submitting Jobs on Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes上提交作业
- en: Working with configuration files
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配置文件
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: When talking about container management, you need to know some of the differences
    compared to application package management, such as `rpm`/`dpkg`, because you
    can run multiple containers on the same machine. You also need to care about network
    port conflicts. This chapter covers how to update, scale, and launch a container
    application using Kubernetes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈论容器管理时，您需要了解与应用程序包管理（如`rpm`/`dpkg`）相比的一些差异，因为您可以在同一台机器上运行多个容器。您还需要关心网络端口冲突。本章介绍了如何使用Kubernetes更新、扩展和启动容器应用程序。
- en: Scaling your containers
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展您的容器
- en: Scaling up and down the application or service based on predefined criteria
    is a common way to utilize the most compute resources in most efficient way. In
    Kubernetes, you can scale up and down manually or use a **Horizontal Pod Autoscaler**
    (**HPA**) to do autoscaling. In this section, we'll describe how to perform both
    operations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 根据预定义的标准扩展和缩小应用程序或服务的规模是利用最有效的计算资源的常见方式。在Kubernetes中，您可以手动扩展和缩小，也可以使用**水平Pod自动缩放器**（**HPA**）进行自动缩放。在本节中，我们将描述如何执行这两个操作。
- en: Getting ready
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做好准备
- en: 'Prepare the following YAML file, which is a simple Deployment that launches
    two `nginx` containers. Also, a NodePort service with TCP—`30080` exposed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 准备以下YAML文件，这是一个简单的部署，启动了两个`nginx`容器。还有一个暴露了TCP-`30080`的NodePort服务：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`NodePort` will bind to all the Kubernetes nodes (port range: `30000` to `32767`);
    therefore, make sure `NodePort` is not used by other processes.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`NodePort`将绑定到所有Kubernetes节点（端口范围：`30000`到`32767`）；因此，请确保`NodePort`没有被其他进程使用。'
- en: 'Let''s use `kubectl` to create the resources used by the preceding configuration
    file:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`kubectl`来创建前面配置文件使用的资源：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After a few seconds, we should see that the `pods` are scheduled and up and
    running:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，我们应该看到`pods`已经被调度并正在运行：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The service is up, too:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 服务也已经启动：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Assume our services are expected to have a traffic spike at a certain of time.
    As a DevOps, you might want to scale it up manually, and scale it down after the
    peak time. In Kubernetes, we can use the `kubectl scale` command to do so. Alternatively,
    we could leverage a HPA to scale up and down automatically based on compute resource
    conditions or custom metrics.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的服务预计在某个时间会有流量激增。作为DevOps，您可能希望手动扩展它，并在高峰时间之后缩小它。在Kubernetes中，我们可以使用`kubectl
    scale`命令来做到这一点。或者，我们可以利用HPA根据计算资源条件或自定义指标自动扩展和缩小。
- en: Let's see how to do it manually and automatically in Kubernetes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Kubernetes中手动和自动执行。
- en: Scale up and down manually with the kubectl scale command
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用kubectl scale命令手动扩展和缩小规模
- en: 'Assume that today we''d like to scale our `nginx` Pods from two to four:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设今天我们想要将我们的`nginx` Pod从两个扩展到四个：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s check how many `pods` we have now:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下现在有多少`pods`：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We could find two more Pods are scheduled. One is already running and another
    one is creating. Eventually, we will have four Pods up and running if we have
    enough compute resources.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会发现另外两个Pod已经被调度。一个已经在运行，另一个正在创建。如果我们有足够的计算资源，最终我们将有四个Pod正在运行。
- en: Kubectl scale (also kubectl autoscale!) supports **Replication Controller**
    (**RC**) and **Replica Set** (**RS**), too. However, deployment is the recommended
    way to deploy Pods.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kubectl scale（也支持kubectl autoscale！）也支持**复制控制器**（**RC**）和**副本集**（**RS**）。但是，部署是部署Pod的推荐方式。
- en: 'We could also scale down with the same `kubectl` command, just by setting the `replicas`
    parameter lower:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用相同的`kubectl`命令进行缩减，只需将`replicas`参数设置为较低：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we''ll see two Pods are scheduled to be terminated:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到两个Pod被安排终止：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is an option, `--current-replicas`, which specifies the expected current
    replicas. If it doesn''t match, Kubernetes doesn''t perform the scale function
    as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个选项，`--current-replicas`，它指定了预期的当前副本。如果不匹配，Kubernetes不会执行如下的扩展功能：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Horizontal Pod Autoscaler (HPA)
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水平Pod自动缩放（HPA）
- en: An HPA queries the source of metrics periodically and determines whether scaling
    is required by a controller based on the metrics it gets. There are two types
    of metrics that could be fetched; one is from Heapster ([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)),
    another is from RESTful client access. In the following example, we'll show you
    how to use Heapster to monitor Pods and expose the metrics to an HPA.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: HPA定期查询指标的来源，并根据其获取的指标确定是否需要由控制器进行扩展。可以获取两种类型的指标；一种是来自Heapster（[https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)），另一种是来自RESTful客户端访问。在下面的示例中，我们将向您展示如何使用Heapster来监视Pod并将指标暴露给HPA。
- en: 'First, Heapster has to be deployed in the cluster:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Heapster必须部署在集群中：
- en: If you're running minikube, use the `minikube addons enable heapster` command
    to enable heapster in your cluster. Note that `minikube logs | grep heapster command` could
    also be used to check the logs of heapster.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在运行minikube，请使用`minikube addons enable heapster`命令在您的集群中启用heapster。请注意，`minikube
    logs | grep heapster command`也可以用来检查heapster的日志。
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Check if the `heapster` `pods` are up and running:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`heapster` `pods`是否正在运行：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Assuming we continue right after the *Getting Ready* section, we will have
    two `my-nginx` Pods running in our cluster:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在*准备就绪*部分之后继续，我们将在我们的集群中运行两个`my-nginx` Pod：
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we can use the `kubectl autoscale` command to deploy an HPA:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`kubectl autoscale`命令来部署HPA：
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To check if it''s running as expected:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 检查它是否按预期运行：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We find the target shows as unknown and replicas are 0. Why is this? the runs
    as a control loop, at a default interval of 30 seconds. There might be a delay
    before it reflects the real metrics.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现目标显示为未知，副本为0。为什么会这样？它以默认间隔30秒的控制循环运行。在它反映真实指标之前可能会有延迟。
- en: 'The default sync period of an HPA can be altered by changing the following
    parameter in control manager: `--horizontal-pod-autoscaler-sync-period`.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: HPA的默认同步周期可以通过更改控制管理器中的以下参数来改变：`--horizontal-pod-autoscaler-sync-period`。
- en: 'After waiting a couple of seconds, we will find the current metrics are there
    now. The number showed in the target column presents (`current / target`). It
    means the load is currently `0%`, and scale target is `50%`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 等待几秒钟后，我们将发现当前指标现在已经存在。在目标列中显示的数字表示（`current / target`）。这意味着负载当前为`0%`，扩展目标为`50%`：
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To test if HPA can scale the Pod properly, we''ll manually generate some loads
    to `my-nginx` service:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试HPA是否可以正确地扩展Pod，我们将手动为`my-nginx`服务生成一些负载：
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding command, we ran a `busybox` image which allowed us to run a
    simple command on it. We used the `–c` parameter to specify the default command,
    which is an infinite loop, to query `my-nginx` service.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，我们运行了一个`busybox`镜像，这使我们能够在其上运行一个简单的命令。我们使用`-c`参数来指定默认命令，这是一个无限循环，用于查询`my-nginx`服务。
- en: 'After about one minute, you can see that the current value is changing:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 大约一分钟后，您可以看到当前值正在改变：
- en: '[PRE16]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'With the same command, we can run more loads with different Pod names repeatedly.
    Finally, we see that the condition has been met. It''s scaling up to `3` replicas,
    and up to `4` replicas afterwards:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的命令，我们可以重复使用不同的Pod名称运行更多负载。最后，我们看到条件已经满足。它正在扩展到`3`个副本，之后再扩展到`4`个副本：
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can see that HPA just scaled our Pods from `4` to `2`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，HPA刚刚将我们的Pod从`4`扩展到`2`。
- en: How it works...
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '它是如何工作的... '
- en: 'Note that cAdvisor acts as a container resource utilization monitoring service,
    which is running inside kubelet on each node. The CPU utilizations we just monitored
    are collected by cAdvisor and aggregated by Heapster. Heapster is a service running
    in the cluster that monitors and aggregates the metrics. It queries the metrics
    from each cAdvisor. When HPA is deployed, the controller will keep observing the
    metrics which are reported by Heapster, and scale up and down accordingly. An
    illustration of the process is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，cAdvisor充当容器资源利用监控服务，运行在每个节点的kubelet内。我们刚刚监控的CPU利用率是由cAdvisor收集并由Heapster聚合的。Heapster是在集群中运行的监视和聚合指标的服务。它从每个cAdvisor查询指标。当HPA部署时，控制器将继续观察由Heapster报告的指标，并相应地扩展和缩小。该过程的示意图如下：
- en: '![](assets/290e9c1e-2be5-4a53-a740-802ddb0147da.png)Based on the specified
    metrics, HPA determines whether scaling is required'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/290e9c1e-2be5-4a53-a740-802ddb0147da.png)基于指定的指标，HPA确定是否需要扩展'
- en: There is more…
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Alternatively, you could use custom metrics, such as Pod metrics or object metrics,
    to determine if it's time to scale up or down. Kubernetes also supports multiple
    metrics. HPA will consider each metric sequentially. Check out [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale)
    for more examples.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用自定义指标，例如Pod指标或对象指标，来确定是时候扩展还是缩小规模。Kubernetes还支持多个指标。HPA将依次考虑每个指标。请查看[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale)获取更多示例。
- en: See also
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'This recipe described how to change the number of Pods using the scaling option
    of the deployment. It is useful to scale up and scale down your application quickly.
    To know more about how to update your container, refer to the following recipes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方描述了如何使用部署的扩展选项来更改Pod的数量。这对于快速扩展和缩小应用程序非常有用。要了解有关如何更新容器的更多信息，请参考以下配方：
- en: '*Updating live containers* in [Chapter 3](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml),
    *Playing with Containers*'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更新实时容器*在[第3章](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml)，*玩转容器*'
- en: '*Ensuring flexible usage of your containers* in [Chapter 3](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml)*,
    Playing with Containers*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*确保您的容器的灵活使用*在[第3章](51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml)*，玩转容器*'
- en: Updating live containers
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新实时容器
- en: For the benefit of containers, we can easily publish new programs by executing
    the latest image, and reduce the headache of environment setup. But, what about
    publishing the program on running containers? While managing a container natively,
    we have to stop the running containers prior to booting up new ones with the latest
    images and the same configurations. There are some simple and efficient methods
    for updating your program in the Kubernetes system. One is called rolling-update,
    which means Deployment can update its Pods without downtime to clients. The other
    method is called *recreate*, which just terminates all Pods then create a new
    set. We will demonstrate how these solutions are applied in this recipe.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了容器的利益，我们可以通过执行最新的镜像轻松发布新程序，并减少环境设置的麻烦。但是，在运行容器上发布程序怎么办？在本地管理容器时，我们必须在启动具有最新镜像和相同配置的新容器之前停止正在运行的容器。在Kubernetes系统中，有一些简单而有效的方法来更新您的程序。其中一种称为滚动更新，意味着Deployment可以在不影响客户端的情况下更新其Pod。另一种方法称为*重新创建*，它只是终止所有Pod，然后创建一个新集合。我们将演示这些解决方案是如何应用在这个配方中。
- en: 'Rolling-update in Docker swarmTo achieve zero downtime application updating,
    there is a similar managing function in Docker swarm. In Docker swarm, you can
    leverage the command docker service update with the flag `--update-delay`, `--update-parallelism`
    and `--update-failure-action`. Check the official website for more details about
    Docker swarm''s rolling-update: [https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/](https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Docker swarm中的滚动更新为了实现零停机应用程序更新，在Docker swarm中有一个类似的管理功能。在Docker swarm中，您可以利用带有`--update-delay`、`--update-parallelism`和`--update-failure-action`标志的docker
    service update命令。有关Docker swarm滚动更新的更多详细信息，请查看官方网站：[https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/](https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/)。
- en: Getting ready
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For a later demonstration, we are going to update `nginx` Pods . Please make
    sure all Kubernetes nodes and components are working healthily:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以后的演示，我们将更新`nginx` Pods。请确保所有Kubernetes节点和组件都正常工作：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Furthermore, to well understand the relationship between ReplicaSet and Deployment,
    please check *Deployment API *section in *[Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    Walking through Kubernetes Concepts*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了更好地理解ReplicaSet和部署之间的关系，请查看*[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)*中的*部署API*部分，深入了解Kubernetes概念。
- en: 'To illustrate the updating of the containers in Kubernetes system, we will
    create a Deployment, change its configurations of application, and then check
    how the updating mechanism handles it. Let''s get all our resources ready:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明Kubernetes系统中容器的更新，我们将创建一个部署，改变其应用程序的配置，然后检查更新机制如何处理。让我们准备好所有资源：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This Deployment is created with `5` replicas. It is good for us to discover
    the updating procedure with multiple numbers of Pods:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部署是用`5`个副本创建的。对我们来说，使用多个Pod的更新过程是很好的发现：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Attaching a Service on the Deployment will help to simulate the real experience
    of clients.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署上附加一个服务将有助于模拟客户端的真实体验。
- en: How to do it...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'At the beginning, take a look at the Deployment you just created and its ReplicaSet
    by executing the following code block:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过执行以下代码块来查看您刚刚创建的部署及其ReplicaSet：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Based on the preceding output, we know that the default updating strategy of
    deployment is rolling-update. Also, there is a single ReplicaSet named `<Deployment
    Name>-<hex decimal hash>` that is created along with the Deployment.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的输出，我们知道部署的默认更新策略是滚动更新。此外，还创建了一个名为`<Deployment Name>-<hex decimal hash>`的单个ReplicaSet，与部署一起创建。
- en: 'Next, check the content of the current Service endpoint for the sake of verifying
    our update later:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了验证我们的更新，立即检查当前服务端点的内容：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We will get the welcome message in the title of the HTML response with the original
    `nginx` image.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在HTML响应的标题中得到欢迎消息，使用原始的`nginx`镜像。
- en: Deployment update strategy – rolling-update
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署更新策略-滚动更新
- en: 'The following will introduce the subcommands `edit` and `set`, for the purpose
    of updating the containers under Deployment:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下将介绍`edit`和`set`子命令，用于更新部署下的容器：
- en: 'First, let''s update the Pods in Deployment with a new command:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们用一个新命令更新部署中的Pods：
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are not only doing the update; we record this change as well. With the flag
    `--record`, we keep the command line as a tag in revision.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅在做更新；我们也记录这个改变。使用`--record`标志，我们将命令行作为标签保存在修订中。
- en: 'After editing the Deployment, check the status of rolling-update with the subcommand
    `rollout` right away:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑部署后，立即使用子命令`rollout`检查滚动更新的状态：
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It is possible that you get several `Waiting for …` lines, as shown in the preceding
    code. They are the standard output showing the status of the update.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会出现几行`等待...`，如前面的代码所示。它们是标准输出，显示更新的状态。
- en: 'For whole updating procedures, check the details of the Deployment to list
    its events:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于整个更新过程，请检查部署的详细信息以列出其事件：
- en: '[PRE25]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you see, a new `replica set simple-nginx-694f94f77d` is created in the Deployment
    `simple-nginx`. Each time the new ReplicaSet scales one Pod up successfully, the
    old ReplicaSet will scale one Pod down. The scaling process finishes at the moment
    that the new ReplicaSet meets the original desired Pod number (as said, `5` Pods),
    and the old ReplicaSet has zero Pods.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在部署`simple-nginx`中创建了一个新的`replica set simple-nginx-694f94f77d`。每当新的ReplicaSet成功扩展一个Pod时，旧的ReplicaSet将缩减一个Pod。扩展过程在新的ReplicaSet满足原始期望的Pod数量（如所说的`5`个Pod）并且旧的ReplicaSet没有Pod时结束。
- en: 'Go ahead and check the new ReplicaSet and existing Service for this update:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续检查这次更新的新ReplicaSet和现有的Service：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Let's make another update! This time, use the subcommand `set` to modify a specific
    configuration of a Pod.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们进行另一个更新！这次，使用子命令`set`来修改Pod的特定配置。
- en: 'To set a new image to certain containers in a Deployment, the subcommand format
    would look like this: `kubectl set image deployment <Deployment name> <Container
    name>=<image name>`:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将新图像设置为部署中某些容器的子命令格式如下：`kubectl set image deployment <Deployment name> <Container
    name>=<image name>`：
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'What else could the subcommand "set" help to configure?  The subcommand set
    helps to define the configuration of the application. Until version 1.9, CLI with
    set could assign or update the following resources:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 子命令"set"还可以帮助配置什么？子命令set有助于定义应用程序的配置。直到1.9版本，具有set的CLI可以分配或更新以下资源：
- en: '| **Subcommand after set** | **Acting resource** | **Updating item** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: 在设置之后的子命令 | 操作资源 | 更新项目
- en: '| `env` | Pod | Environment variables |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `env` | Pod | 环境变量 |'
- en: '| `image` | Pod | Container image |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `image` | Pod | 容器镜像 |'
- en: '| `resources` | Pod | Computing resource requirement or limitation |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `resources` | Pod | 计算资源需求或限制 |'
- en: '| `selector` | Any resource | Selector |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `selector` | 任何资源 | 选择器 |'
- en: '| `serviceaccount` | Any resource | ServiceAccount |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `serviceaccount` | 任何资源 | ServiceAccount |'
- en: '| `subject` | RoleBinding or ClusterRoleBinding | User, group, or ServiceAccount
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `subject` | RoleBinding或ClusterRoleBinding | 用户、组或ServiceAccount |'
- en: 'Now, check if the update has finished and whether the image is changed:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，检查更新是否已完成，以及图像是否已更改：
- en: '[PRE28]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You can also check out the ReplicaSets. There should be another one taking
    responsibility of the Pods for Deployment:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以查看ReplicaSets。应该有另一个负责部署Pod的ReplicaSet：
- en: '[PRE29]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Rollback the update
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回滚更新
- en: 'Kubernetes system records every update for Deployment:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes系统记录每次部署的更新：
- en: 'We can list all of the revisions with the subcommand `rollout`:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用子命令`rollout`列出所有的修订版本：
- en: '[PRE30]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You will get three revisions, as in the preceding lines, for the Deployment
    `simple-nginx`. For Kubernetes Deployment, each revision has a matched `ReplicaSet`
    and represents a stage of running an update command. The first revision is the
    initial state of `simple-nginx`. Although there is no command tag for indication,
    Kubernetes takes its creation as its first version. However, you could still record
    the command when you create the Deployment.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到三个修订版本，就像前面的行一样，用于部署`simple-nginx`。对于Kubernetes部署，每个修订版本都有一个匹配的`ReplicaSet`，代表运行更新命令的阶段。第一个修订版本是`simple-nginx`的初始状态。尽管没有命令标签来指示，但Kubernetes将其创建视为其第一个版本。但是，您仍然可以在创建部署时记录命令。
- en: Add the flag `--record` after the subcommand `create` or `run`.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在子命令`create`或`run`之后添加标志`--record`。
- en: 'With the revisions, we can easily resume the change, which means rolling back
    the update. Use the following commands to rollback to previous revisions:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了修订版，我们可以轻松恢复更改，这意味着回滚更新。使用以下命令回滚到以前的修订版：
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Without specifying the revision number, the rollback process will simply jump
    back to previous version:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不指定修订号，回滚过程将简单地跳回到以前的版本：
- en: '[PRE32]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Deployment update strategy – recreate
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署更新策略 - 重新创建
- en: 'Next, we are going to introduce the other update strategy, `recreate`, for
    Deployment. Although there is no subcommand or flag to create a recreate-strategy deployment,
    users could fulfill this creation by overriding the default element with the specified
    configuration:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍另一种部署的更新策略，“重新创建”。虽然没有子命令或标志来创建重新创建策略的部署，但用户可以通过覆盖指定配置的默认元素来实现此创建：
- en: '[PRE33]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In our understanding, the `recreate` mode is good for an application under
    development. With `recreate`, Kubernetes just scales the current ReplicaSet down
    to zero Pods, and creates a new ReplicaSet with the full desired number of Pods.
    Therefore, recreate has a shorter total updating time than rolling-update because
    it scales ReplicaSets up or down simply, once for all. Since a developing Deployment
    doesn''t need to take care of any user experience, it is acceptable to have downtime
    while updating and enjoy faster updates:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的理解中，“重新创建”模式适用于正在开发中的应用程序。使用“重新创建”，Kubernetes只是将当前的ReplicaSet缩减为零个Pod，并创建一个新的ReplicaSet，其中包含完整数量的Pod。因此，“重新创建”比滚动更新具有更短的总更新时间，因为它只是简单地扩展或缩减ReplicaSets，一次性完成。由于正在开发的部署不需要关注任何用户体验，因此在更新时出现停机是可以接受的，并且可以享受更快的更新：
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How it works...
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Rolling-update works on the units of the ReplicaSet in a Deployment. The effect
    is to create a new ReplicaSet to replace the old one. Then, the new ReplicaSet
    is scaling up to meet the desired numbers, while the old ReplicaSet is scaling
    down to terminate all the Pods in it. The Pods in the new ReplicaSet are attached
    to the original labels. Therefore, if any service exposes this Deployment, it
    will take over the newly created Pods directly.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新适用于部署中ReplicaSet的单位。其效果是创建一个新的ReplicaSet来替换旧的ReplicaSet。然后，新的ReplicaSet正在扩展以满足所需的数量，而旧的ReplicaSet正在缩减以终止其中的所有Pod。新ReplicaSet中的Pod附加到原始标签。因此，如果任何服务公开此部署，它将直接接管新创建的Pod。
- en: An experienced Kubernetes user may know that the resource ReplicationController
    can be rolling-update as well. So, what are the differences of rolling-update
    between ReplicationController and deployment? The scaling processing uses the
    combination of ReplicationController and a client such as `kubectl`. A new ReplicationController
    will be created to replace the previous one. Clients don't feel any interruption
    since the service is in front of ReplicationController while doing replacement.
    However, it is hard for developers to roll back to previous ReplicationControllers
    (they have been removed), because there is no built-in mechanism that records
    the history of updates.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有经验的Kubernetes用户可能知道，资源ReplicationController也可以进行滚动更新。那么，滚动更新在ReplicationController和部署之间有什么区别？扩展处理使用ReplicationController和客户端（如`kubectl`）的组合。将创建一个新的ReplicationController来替换以前的ReplicationController。客户端在进行替换时不会感到任何中断，因为服务位于ReplicationController之前。然而，开发人员很难回滚到以前的ReplicationController（它们已被删除），因为没有记录更新历史的内置机制。
- en: In addition, rolling-update might fail if the client connection is disconnected
    while rolling-update is working. Most important of all, Deployment with ReplicaSet
    is the most recommended deploying resource than ReplicationController or standalone
    ReplicaSet.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果在滚动更新工作时客户端连接断开，滚动更新可能会失败。最重要的是，部署与ReplicaSet是比ReplicationController或独立ReplicaSet更推荐的部署资源。
- en: 'While paying close attention to the history of update in deployment, be aware
    that it is not always listed in sequence. The algorithm of adding revisions could
    be clarified as the following bullet points show:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在关注部署更新历史时，请注意它并不总是按顺序列出。添加修订的算法可以澄清为以下要点所示：
- en: Take the number of last revision as *N*
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将最后修订的编号视为*N*
- en: When a new rollout update comes, it would be *N+1*
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当新的滚动更新到来时，它将是*N+1*
- en: Roll back to a specific revision number *X*, *X* would be removed and it would
    become *N+1*
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚到特定修订编号*X*，*X*将被移除，它将变为*N+1*
- en: Roll back to the previous version, which means *N-1,* then *N-1* would be removed
    and it would become *N+1*
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚到上一个版本，也就是*N-1*，然后*N-1*将被移除，它将变为*N+1*
- en: With this revision management, no stale and overlapped updates occupy the rollout
    history.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种修订管理，不会有陈旧和重叠的更新占据滚动历史。
- en: There's more...
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Taking Deployment update into consideration is a good step towards building
    a CI/CD (continuous integration and continuous delivery) pipeline. For a more
    common usage, developers don''t exploit command lines to update the Deployment.
    They may prefer to fire some API calls from CI/CD platform, or update from a previous
    configuration file. Here comes an example working with the subcommand `apply`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑部署更新是构建CI/CD（持续集成和持续交付）流水线的一个良好步骤。对于更常见的用法，开发人员不会利用命令行来更新部署。他们可能更喜欢从CI/CD平台发出一些API调用，或者从先前的配置文件进行更新。以下是一个使用子命令`apply`的示例：
- en: '[PRE35]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As a demonstration, modifying the container image from `nginx` to `nginx:stable`
    (you may check the code bundle `my-update-nginx-updated.yaml` for the modification).
    Then, we can use the changed file to update with the subcommand `apply`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 作为演示，将容器镜像从`nginx`修改为`nginx:stable`（您可以检查代码包`my-update-nginx-updated.yaml`进行修改）。然后，我们可以使用更改后的文件来使用子命令`apply`进行更新：
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, you can learn another way to update your Deployment.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以学习另一种更新部署的方法。
- en: 'Digging deeper into rolling-update on Deployment, there are some parameters
    we may leverage when doing updates:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究部署中的滚动更新，我们可能会利用一些参数进行更新：
- en: '`minReadySeconds`: After a Pod is considered to be ready, the system still
    waits for a period of time for going on to the next step. This time slot is the
    minimum ready seconds, which will be helpful when waiting for the application
    to complete post-configuration.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minReadySeconds`：在Pod被视为就绪后，系统仍然会等待一段时间才能进行下一步。这个时间段是最小就绪秒数，在等待应用程序完成后配置时会很有帮助。'
- en: '`maxUnavailable`: The maximum number of Pods that can be unavailable during
    updating. The value could be a percentage (the default is 25%) or an integer.
    If the value of `maxSurge` is `0`, which means no tolerance of the number of Pods over
    the desired number, the value of `maxUnavailable` cannot be `0`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxUnavailable`：在更新期间可以不可用的Pod的最大数量。该值可以是百分比（默认为25%）或整数。如果`maxSurge`的值为`0`，这意味着Pod的数量不能超过所需数量，`maxUnavailable`的值不能为`0`。'
- en: '`maxSurge`: The maximum number of Pods that can be created over the desired
    number of ReplicaSet during updating. The value could be a percentage (the default
    is 25%) or an integer. If the value of `maxUnavailable` is `0`, which means the
    number of serving Pods should always meet the desired number, the value of `maxSurge`
    cannot be `0`.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxSurge`：在更新期间可以创建的Pod的最大数量，超过所需的ReplicaSet数量。该值可以是百分比（默认为25%）或整数。如果`maxUnavailable`的值为`0`，这意味着正在提供的Pod的数量应始终满足所需的数量，`maxSurge`的值不能为`0`。'
- en: Based on the configuration file `my-update-nginx-advanced.yaml` in the code
    bundle, try playing with these parameters by yourself and see if you can feel
    the ideas at work.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据代码包中的配置文件`my-update-nginx-advanced.yaml`，尝试自己玩弄这些参数，并看看您是否能感受到这些想法的运作。
- en: See also
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'You could continue studying the following recipes to learn more ideas about
    deploying Kubernetes resources efficiently:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以继续学习以下配方，以了解更多关于有效部署Kubernetes资源的想法：
- en: Scaling your containers
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展您的容器
- en: Working with configuration files
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配置文件
- en: The *Moving monolithic to microservices*, *Integrating with Jenkins*, *Working
    with the private Docker registry*, and *Setting up the Continuous Delivery Pipeline*
    recipes in [Chapter 5](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml)*, Building
    a Continuous Delivery Pipeline*
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第5章](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml)*，构建持续交付管道*中的*将单片式转换为微服务*、*与Jenkins集成*、*使用私有Docker注册表*和*设置持续交付管道*配方
- en: Forwarding container ports
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转发容器端口
- en: In previous chapters, you have learned how to work with the Kubernetes Services
    to forward the container port internally and externally. Now, it's time to take
    it a step further to see how it works.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，您已经学会了如何使用Kubernetes服务来在内部和外部转发容器端口。现在，是时候更进一步，看看它是如何工作的了。
- en: 'There are four networking models in Kubernetes, and we''ll explore the details
    in the following sections:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中有四种网络模型，我们将在以下部分探讨详细信息：
- en: Container-to-container communications
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器到容器的通信
- en: Pod-to-pod communications
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod到Pod的通信
- en: Pod-to-service communications
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod到服务的通信
- en: External-to-internal communications
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部到内部的通信
- en: Getting ready
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Before we go digging into Kubernetes networking, let''s study the networking
    of Docker to understand the basic concept. Each container will have a network
    namespace with its own routing table and routing policy. By default, the network
    bridge `docker0` connects the physical network interface and virtual network interfaces
    of containers, and the virtual network interface is the bidirectional cable for
    the container network namespace and the host one. As a result, there is a pair
    of virtual network interfaces for a single container: the Ethernet interface (**eth0**)
    on the container and the virtual Ethernet interface (**veth-**) on the host.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究Kubernetes网络之前，让我们学习Docker的网络，以了解基本概念。每个容器都有自己的网络命名空间，具有自己的路由表和路由策略。默认情况下，网络桥`docker0`连接物理网络接口和容器的虚拟网络接口，虚拟网络接口是容器网络命名空间和主机之间的双向电缆。因此，对于单个容器，存在一对虚拟网络接口：容器上的以太网接口（**eth0**）和主机上的虚拟以太网接口（**veth-**）。
- en: 'The network structure can be expressed as in the following image:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 网络结构可以用以下图像表示：
- en: '![](assets/cbce3365-1d24-4212-9777-b99db15a7bf6.png)Container network interfaces
    on hostWhat is a network namespace?A network namespace is the technique provided
    by Linux kernel. With this feature, the operating system can fulfill network virtualization
    by separating the network capability into independent resources. Each network
    namespace has its own iptable setup and network devices.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/cbce3365-1d24-4212-9777-b99db15a7bf6.png)主机上的容器网络接口什么是网络命名空间？网络命名空间是Linux内核提供的技术。有了这个功能，操作系统可以通过将网络功能分离为独立的资源来实现网络虚拟化。每个网络命名空间都有自己的iptables设置和网络设备。'
- en: How to do it...
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: A Pod contains one or more containers, which run on the same host. Each Pod has
    their own IP address on an overlay network; all the containers inside a Pod see
    each other as on the same host. Containers inside a Pod will be created, deployed,
    and deleted almost at the same time. We will illustrate four communication models
    between container, Pod, and Service.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Pod包含一个或多个容器，它们在同一主机上运行。每个Pod在覆盖网络上都有自己的IP地址；Pod内的所有容器都将彼此视为同一主机上的容器。Pod内的容器将几乎同时创建、部署和删除。我们将阐述容器、Pod和服务之间的四种通信模型。
- en: Container-to-container communication
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器到容器的通信
- en: 'In this scenario, we would focus on the communications between containers within
    single Pod:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将关注单个 Pod 内容器之间的通信：
- en: 'Let''s create two containers in one Pod: a nginx web application and a CentOS,
    which checks port `80` on localhost:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在一个 Pod 中创建两个容器：一个是 nginx web 应用程序，另一个是 CentOS，它会检查本地主机上的端口`80`：
- en: '[PRE37]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We see the count in the `READY` column becomes `2/2`, since there are two containers
    inside this Pod.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到`READY`列中的计数变为`2/2`，因为这个 Pod 中有两个容器。
- en: 'Using the `kubectl describe` command, we may see the details of the Pod:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kubectl describe`命令，我们可以查看 Pod 的详细信息：
- en: '[PRE38]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We can see that the Pod is run on node `ubuntu02` and that its IP is `192.168.79.198`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 Pod 运行在节点`ubuntu02`上，它的 IP 是`192.168.79.198`。
- en: 'Also, we may find that the Centos container can access the `nginx` on localhost:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们可能会发现 Centos 容器可以访问本地的`nginx`：
- en: '[PRE39]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s log in to node `ubuntu02` to check the network setting of these two
    containers:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们登录到节点`ubuntu02`，检查这两个容器的网络设置：
- en: '[PRE40]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now, we know that the two containers created are `9e35275934c1` and `e832d294f176`.
    On the other hand, there is another container, `9b3e9caf5149`, that is created
    by Kubernetes with the Docker image `gcr.io/google_containers/pause-amd64`. We
    will introduce it later. Thereafter, we may get a detailed inspection of the containers
    with the command `docker inspect`, and by adding the command `jq `([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))
    as a pipeline, we can parse the output information to show network settings only.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道创建的两个容器分别是`9e35275934c1`和`e832d294f176`。另一方面，还有另一个容器`9b3e9caf5149`，由
    Kubernetes 使用 Docker 镜像`gcr.io/google_containers/pause-amd64`创建。我们稍后会介绍它。之后，我们可以使用`docker
    inspect`命令对容器进行详细检查，并通过添加`jq`命令([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))作为管道，来解析输出信息以仅显示网络设置。
- en: 'Taking a look at both containers covered in the same Pod:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看看在同一个 Pod 中覆盖的两个容器：
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We can see that both containers have identical network settings; the network
    mode is set to mapped container mode, leaving the other configurations cleaned.
    The network bridge container is `container:9b3e9caf5149ffb0ec14c1ffc36f94b2dd55b223d0d20e4d48c4e33228103723`.
    What is this container? It is the one created by Kubernetes, container ID `9b3e9caf5149`,
    with the image `gcr.io/google_containers/pause-amd64`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到两个容器具有相同的网络设置；网络模式设置为映射容器模式，其他配置被清除。网络桥接容器是`container:9b3e9caf5149ffb0ec14c1ffc36f94b2dd55b223d0d20e4d48c4e33228103723`。这个容器是什么？它是由
    Kubernetes 创建的，容器 ID 是`9b3e9caf5149`，使用的镜像是`gcr.io/google_containers/pause-amd64`。
- en: What does the container "pause" do?Just as its name suggests, this container
    does nothing but "pause". However, it preserves the network settings, and the
    Linux network namespace, for the Pod. Anytime the container shutdowns and restarts,
    the network configuration will still be the same and not need to be recreated,
    because the "pause" container holds it. You can check its code and Dockerfile
    at [https://github.com/kubernetes/kubernetes/tree/master/build/pause](https://github.com/kubernetes/kubernetes/tree/master/build/pause)
    for more information.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '"pause"容器是做什么的？正如其名字所暗示的，这个容器除了“暂停”之外什么也不做。然而，它保留了网络设置和 Linux 网络命名空间，供 Pod
    使用。每当容器关闭和重新启动时，网络配置仍然保持不变，无需重新创建，因为“pause”容器保存了它。您可以在 [https://github.com/kubernetes/kubernetes/tree/master/build/pause](https://github.com/kubernetes/kubernetes/tree/master/build/pause)
    查看它的代码和 Dockerfile 以获取更多信息。'
- en: The "pause" container is a network container, which is created when a Pod
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: “pause”容器是一个网络容器，当 Pod 创建时会被创建。
- en: is created and used to handle the route of the Pod network. Then, two containers
    will share the network namespace with pause; that's why they see each other as
    localhost.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 它被创建并用于处理 Pod 网络的路由。然后，两个容器将与 pause 共享网络命名空间；这就是为什么它们将彼此视为本地主机。
- en: 'Create a network container in DockerIn Docker, you can easily make a container
    into a network container, sharing its network namespace with another container.
    Use the command line: `$ docker run --network=container:<CONTAINER_ID or CONTAINER_NAME>
    [other options].` Then, you will be able to start a container which uses the network
    namespace of the assigned container.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在Docker中创建网络容器在Docker中，您可以轻松地将一个容器变成一个网络容器，与另一个容器共享其网络命名空间。使用命令行：`$ docker run
    --network=container:<CONTAINER_ID or CONTAINER_NAME> [other options].`然后，您将能够启动一个使用分配的容器的网络命名空间的容器。
- en: Pod-to-Pod communication
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod与Pod之间的通信
- en: 'As mentioned, containers in a Pod share the same network namespace. And a Pod is
    the basic computing unit in Kubernetes. Kubernetes assigns an IP to a Pod in its
    world. Every Pod can see every other with the virtual IP in Kubernetes network.
    While talking about the communication between Pods , we can separate into two
    scenarios: Pods that communicate within a node, or Pods that communicate across
    nodes. For Pods in single node, since they have separate IPs, their transmissions
    can be held by bridge, same as containers in a Docker node. However, for communication
    between Pods across nodes, how would be the package routing work while Pod doesn''t
    have the host information (the host IP)?'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，Pod中的容器共享相同的网络命名空间。Pod是Kubernetes中的基本计算单元。Kubernetes为Pod在其世界中分配一个IP。每个Pod都可以在Kubernetes网络中使用虚拟IP看到其他Pod。在谈论Pod之间的通信时，我们可以分为两种情况：在节点内部通信的Pod，或者在节点之间通信的Pod。对于单个节点中的Pod，由于它们有单独的IP，它们的传输可以通过桥接来实现，就像Docker节点中的容器一样。然而，对于跨节点之间的Pod通信，当Pod没有主机信息（主机IP）时，数据包的路由工作将如何进行？
- en: 'Kubernetes uses the CNI to handle cluster networking. CNI is a framework for
    managing connective containers, for assigning or deleting the network resource
    on a container. While Kubernetes takes CNI as a plugin, users can choose the implementation
    of CNI on demand. Commonly, there are the following types of CNI:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用CNI来处理集群网络。CNI是一个用于管理连接容器的框架，用于为容器分配或删除网络资源。虽然Kubernetes将CNI作为插件，用户可以根据需要选择CNI的实现。通常有以下类型的CNI：
- en: '**Overlay**: With the technique of packet encapsulation. Every data is wrapped
    with host IP, so it is routable in the internet. An example is flannel ([https://github.com/coreos/flannel](https://github.com/coreos/flannel)).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**覆盖**：使用数据封装技术。每个数据都包装有主机IP，因此可以在互联网中进行路由。一个例子是flannel ([https://github.com/coreos/flannel](https://github.com/coreos/flannel))。'
- en: '**L3 gateway**: Transmission between containers pass to a gateway node first.
    The gateway will maintain the routing table to map the container subnet and host
    IP. An example is Project Calico ([https://www.projectcalico.org/](https://www.projectcalico.org/)).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L3网关**：容器之间的传输首先经过网关节点。网关将维护路由表，将容器子网和主机IP进行映射。一个例子是Project Calico ([https://www.projectcalico.org/](https://www.projectcalico.org/))。'
- en: '**L2 adjacency**: Happening on L2 switching. In Ethernet, two nodes have adjacency
    if the package can be transmitted directly from source to destination, without
    passing by other nodes. An example is Cisco ACI ([https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html](https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html)).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2邻接**：发生在L2交换上。在以太网中，如果数据包可以直接从源传输到目的地，而不经过其他节点，则两个节点具有邻接性。一个例子是Cisco ACI
    ([https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html](https://www.cisco.com/c/en/us/td/docs/switches/datacenter/aci/apic/sw/kb/b_Kubernetes_Integration_with_ACI.html))。'
- en: 'There are pros and cons to every type of CNI. The former type within the bullet
    points has better scalability but bad performance, while the latter one has a
    shorter latency but requires complex and customized setup. Some CNIs cover all
    three types in different modes, for example, Contiv ([https://github.com/contiv/netplugin](https://github.com/contiv/netplugin)).
    You can get more information about CNI while checking its spec at: [https://github.com/containernetworking/cni](https://github.com/containernetworking/cni).
    Additionally, look at the CNI list on official website of Kubernetes to try out
    these CNIs: [https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 每种类型的CNI都有优缺点。前一种类型在可扩展性方面更好，但性能较差，而后一种类型的延迟较短，但需要复杂和定制的设置。一些CNI以不同模式覆盖了所有三种类型，例如Contiv（[https://github.com/contiv/netplugin](https://github.com/contiv/netplugin)）。您可以在[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni)上检查其规范以获取有关CNI的更多信息。此外，查看Kubernetes官方网站上的CNI列表，以尝试这些CNI：[https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this)。
- en: After introducing the basic knowledge of the packet transaction between Pods ,
    we will continue to bring you a Kubernetes API, `NetworkPolicy`, which provides
    advanced management between the communication of Pods .
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了Pod之间的数据包传输的基本知识后，我们将继续为您带来一个Kubernetes API，即`NetworkPolicy`，它提供了Pod之间通信的高级管理。
- en: Working with NetworkPolicy
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NetworkPolicy
- en: As a resource of Kubernetes, NetworkPolicy uses label selectors to configure
    the firewall of Pods from infrastructure level. Without a specified NetworkPolicy,
    any Pod in the same cluster can communicate with each other by default. On the
    other hand, once a NetworkPolicy with rules is attached to a Pod, either it is
    for ingress or egress, or both, and all traffic that doesn't follow the rules
    will be blocked.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Kubernetes的资源，NetworkPolicy使用标签选择器来配置Pod的基础级别防火墙。没有指定的NetworkPolicy，同一集群中的任何Pod默认情况下都可以相互通信。另一方面，一旦将具有规则的NetworkPolicy附加到Pod，无论是入口还是出口，或者两者都是如此，所有不遵循规则的流量都将被阻止。
- en: 'Before demonstrating how to build a NetworkPolicy, we should make sure the
    network plugin in Kubernetes cluster supports it. There are several CNIs that
    support NetworkPolicy: Calico, Contive, Romana ([https://github.com/romana/kube](https://github.com/romana/kube)),
    Weave Net ([https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)),
    Trireme ([https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes)),
    and others.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在演示如何构建NetworkPolicy之前，我们应该确保Kubernetes集群中的网络插件支持它。有几种CNI支持NetworkPolicy：Calico、Contive、Romana（[https://github.com/romana/kube](https://github.com/romana/kube)）、Weave
    Net（[https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)）、Trireme（[https://github.com/aporeto-inc/trireme-kubernetes](https://github.com/aporeto-inc/trireme-kubernetes)）等。
- en: 'Enable CNI with NetworkPolicy support as network plugin in minikubeWhile working
    on minikube, users will not need to attach a CNI specifically, since it is designed
    as a single local Kubernetes node. However, to enable the functionality of NetworkPolicy,
    it is necessary to start a NetworkPolicy-supported CNI. Be careful, as, while
    you configure the minikube with CNI, the configuration options and procedures
    could be quite different to various CNI implementations. The following steps show
    you how to start minikube with CNI, Calico:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在minikube中将CNI与NetworkPolicy支持作为网络插件启用。在minikube上工作时，用户不需要专门附加CNI，因为它被设计为单个本地Kubernetes节点。但是，为了启用NetworkPolicy的功能，有必要启动一个支持NetworkPolicy的CNI。请注意，当您配置minikube与CNI时，配置选项和流程可能与各种CNI实现大不相同。以下步骤向您展示了如何使用CNI
    Calico启动minikube：
- en: We take this issue [https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943](https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943)
    as reference for these building steps.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将这个问题[https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943](https://github.com/projectcalico/calico/issues/1013#issuecomment-325689943)作为这些构建步骤的参考。
- en: The minikube used here is the latest version, 0.24.1.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里使用的minikube是最新版本0.24.1。
- en: 'Reboot your minikube: `minikube start --network-plugin=cni \` `--host-only-cidr
    172.17.17.1/24 \ --extra-config=kubelet.PodCIDR=192.168.0.0/16 \ --extra-config=proxy.ClusterCIDR=192.168.0.0/16
    \` `--extra-config=controller-manager.ClusterCIDR=192.168.0.0/16`.'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新启动您的minikube：`minikube start --network-plugin=cni \` `--host-only-cidr 172.17.17.1/24
    \ --extra-config=kubelet.PodCIDR=192.168.0.0/16 \ --extra-config=proxy.ClusterCIDR=192.168.0.0/16
    \` `--extra-config=controller-manager.ClusterCIDR=192.168.0.0/16`。
- en: Create Calico with the configuration file "minikube-calico.yaml" from the code
    bundle `kubectl create -f minikue-calico.yaml`.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用代码包中的配置文件"minikube-calico.yaml"创建Calico `kubectl create -f minikue-calico.yaml`。
- en: 'To illustrate the functionality of NetworkPolicy, we are going to create a
    Pod and expose it as a service, then attach a NetworkPolicy on the Pod to see
    what happens:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明NetworkPolicy的功能，我们将创建一个Pod并将其公开为服务，然后在Pod上附加一个NetworkPolicy来查看发生了什么：
- en: '[PRE42]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we can go ahead and check the Pod''s connection from a simple Deployment,
    `busybox`, using the command `wget` with `--spider` flag to verify the existence
    of endpoint:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续并使用`wget`命令和`--spider`标志从一个简单的部署`busybox`检查Pod的连接，以验证端点的存在。
- en: '[PRE43]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'As shown in the preceding result, we know that the `nginx` service can be accessed
    without any constraints. Later, let''s run a `NetworkPolicy` that restricts that
    only the Pod tagging `<test: inbound>` can access `nginx` service:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '如前面的结果所示，我们知道`nginx`服务可以在没有任何限制的情况下访问。稍后，让我们运行一个`NetworkPolicy`，限制只有标记为`<test:
    inbound>`的Pod可以访问`nginx`服务：'
- en: '[PRE44]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As you can see, in the spec of NeworkPolicy, it is configured to apply to Pods with
    the label `<run: nginx-pod>`, which is the one we have on the `pod nginx-pod`.
    Also, a rule of ingress is attached in the policy, which indicates that only Pods with
    a specific label can access `nginx-pod`:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '正如您所看到的，在NeworkPolicy的规范中，它被配置为应用于具有标签`<run: nginx-pod>`的Pod，这是我们在`pod nginx-pod`上拥有的标签。此外，策略中附加了一个入口规则，表明只有具有特定标签的Pod才能访问`nginx-pod`：'
- en: '[PRE45]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Great, everything is looking just like what we expected. Next, check the same
    service endpoint on our previous `busybox` Pod:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，一切看起来都和我们预期的一样。接下来，检查我们之前的`busybox` Pod上的相同服务端点：
- en: '[PRE46]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As expected again, now we cannot access the `nginx-pod` service after NetworkPolicy
    is attached. The `nginx-pod` can only be touched by Pod labelled with `<test:
    inbound>`:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '再次如预期，现在在NetworkPolicy附加后我们无法访问`nginx-pod`服务。`nginx-pod`只能被标记为`<test: inbound>`的Pod访问：'
- en: '[PRE47]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Catch up with the concept of label and selector in the recipe *Working with
    labels and selectors* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)*,*
    *Walking through Kubernetes Concepts*.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)的*使用标签和选择器*中，了解标签和选择器的概念。
- en: 'In this case, you have learned how to create a NetworkPolicy with ingress restriction
    by Pod selector. Still, there are other settings you may like to build on your
    Pod:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您已经学会了如何通过Pod选择器创建具有入口限制的NetworkPolicy。但是，您可能还希望在您的Pod上构建其他设置：
- en: '**Egress restriction**: Egress rules can be applied by `.spec.egress`, which
    has similar settings to ingress.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**出口限制**：出口规则可以通过`.spec.egress`应用，其设置与入口类似。'
- en: '**Port restriction**: Each ingress and egress rule can point out what port,
    and with what kind of port protocol, is to be accepted or blocked. Port configuration
    can be applied through `.spec.ingress.ports` or `.spec.egress.ports`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端口限制**：每个入口和出口规则都可以指出要接受或阻止的端口以及端口协议的类型。端口配置可以通过`.spec.ingress.ports`或`.spec.egress.ports`来应用。'
- en: '**Namespace selector**: We can also make limitations on certain Namespaces.
    For example, Pods for the system daemon might only allow access to others in the
    Namespace `kube-system`. Namespace selector can be applied with `.spec.ingress.from.namespaceSelector`
    or `.spec.egress.to.namespaceSelector`.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名空间选择器**：我们还可以对特定的命名空间进行限制。例如，系统守护程序的Pod可能只允许访问`kube-system`命名空间中的其他Pod。命名空间选择器可以通过`.spec.ingress.from.namespaceSelector`或`.spec.egress.to.namespaceSelector`来应用。'
- en: '**IP block**: A more customized configuration is to set rules on certain CIDR
    ranges, which come out as similar ideas to what we work with iptables. We may
    utilize this configuration through `.spec.ingress.from.ipBlock` or `.spec.egress.to.ipBlock`.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IP块**：更定制化的配置是在某些CIDR范围上设置规则，这与我们使用iptables的想法类似。我们可以通过`.spec.ingress.from.ipBlock`或`.spec.egress.to.ipBlock`来利用这种配置。'
- en: 'It is recommended to check more details in the API document: [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking).
    Furthermore, we would like to show you some more interesting setups to fulfill
    general situations:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 建议查看API文档中的更多详细信息：[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#networkpolicyspec-v1-networking)。此外，我们还想向您展示一些更有趣的设置来满足一般情况：
- en: '**Apply to all Pod**: A NetworkPolicy can be easily pushed to every Pod by
    setting `.spec.podSelector` with an empty value.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用于所有Pod**：可以通过将`.spec.podSelector`设置为空值，轻松地将NetworkPolicy推送到每个Pod。'
- en: '**Allow all traffic**: We may allow all incoming traffic by assigning `.spec.ingress`
    with empty value, an empty array; accordingly, outgoing traffic could be set without
    any restriction by assigning `.spec.egress` with empty value.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**允许所有流量**：我们可以通过将`.spec.ingress`分配为空值（空数组）来允许所有传入流量；相应地，可以通过将`.spec.egress`分配为空值来设置无限制的传出流量。'
- en: '**Deny all traffic**: We may deny all incoming or outgoing traffic by simply
    indicating the type of NetworkPolicy without setting any rule. The type of the
    NetworkPolicy can be set at `.spec.policyTypes`. At the same time, do not set
    `.spec.ingress or .spec.egress`.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拒绝所有流量**：我们可以通过简单地指定NetworkPolicy的类型而不设置任何规则来拒绝所有传入或传出的流量。NetworkPolicy的类型可以在`.spec.policyTypes`中设置。同时，不要设置`.spec.ingress`或`.spec.egress`。'
- en: Go check the code bundle for the example files `networkpolicy-allow-all.yaml`
    and `networkpolicy-deny-all.yaml`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 去检查示例文件`networkpolicy-allow-all.yaml`和`networkpolicy-deny-all.yaml`的代码包。
- en: Pod-to-Service communication
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod与Service之间的通信
- en: 'In the ordinary course of events, Pods can be stopped accidentally. Then, the
    IP of the Pod can be changed. When we expose the port for a Pod or a Deployment,
    we create a Kubernetes Service that acts as a proxy or a load balancer. Kubernetes
    would create a virtual IP, which receives the request from clients and proxies
    the traffic to the Pods in a service. Let''s review how to do this:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常情况下，Pod可能会意外停止。然后，Pod的IP可能会更改。当我们为Pod或Deployment公开端口时，我们创建一个充当代理或负载均衡器的Kubernetes服务。Kubernetes会创建一个虚拟IP，该IP接收来自客户端的请求，并将流量代理到服务中的Pod。让我们来看看如何做到这一点：
- en: 'First, we would create a Deployment and expose it to a Service:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个Deployment并将其公开为一个Service：
- en: '[PRE48]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'At this moment, check the details of the Service with the subcommand `describe`:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，使用子命令`describe`来检查服务的详细信息：
- en: '[PRE49]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The virtual IP of the Service is `10.101.160.245`, which exposes the port `8080`.
    The Service would then dispatch the traffic into the two endpoints `192.168.80.5:80`
    and `192.168.80.6:80`. Moreover, because the Service is created in `NodePort`
    type, clients can access this Service on every Kubernetes node at `<NODE_IP>:30615`.
    As with our understanding of the recipe *Working with Services* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*, it is the Kubernetes daemon `kube-proxy` that
    helps to maintain and update routing policy on every node.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的虚拟IP是`10.101.160.245`，暴露端口`8080`。然后，服务将流量分发到两个端点`192.168.80.5:80`和`192.168.80.6:80`。此外，由于服务是以`NodePort`类型创建的，客户端可以在每个Kubernetes节点上通过`<NODE_IP>:30615`访问此服务。根据我们在[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)中*使用服务*的理解，*深入了解Kubernetes概念*，是Kubernetes守护程序`kube-proxy`帮助在每个节点上维护和更新路由策略。
- en: 'Continue on, checking the `iptable` on any Kubernetes node:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续，在任何Kubernetes节点上检查`iptable`。
- en: Attention! If you are in minikube environment, you should jump into the node
    with the command `minikube ssh`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意！如果您在minikube环境中，您应该使用命令`minikube ssh`跳转到节点。
- en: '[PRE50]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'There will be a lot of rules showing out. To focus on policies related to the
    Service `nodeport-svc`, go through the following steps for checking them all.
    The output on your screen may not be listed in the expected order:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 将会显示很多规则。为了专注于与服务`nodeport-svc`相关的策略，请按以下步骤检查它们。您屏幕上的输出可能不会按预期顺序列出：
- en: Find targets under chain `KUBE-NODEPORTS` with the comment mentioned `nodeport-svc`. One
    target will be named with the prefix `KUBE-SVC-`. In the preceding output, it
    is the one named `KUBE-SVC-GFPAJ7EGCNM4QF4H`. Along with the other target `KUBE-MARK-MASQ`,
    they work on passing traffics at port `30615` to the Service.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在带有提到`nodeport-svc`注释的`Chain KUBE-NODEPORTS`下找到目标。一个目标将以前缀`KUBE-SVC-`命名。在前面的输出中，它是名为`KUBE-SVC-GFPAJ7EGCNM4QF4H`的目标。与其他目标`KUBE-MARK-MASQ`一起，它们用于将端口`30615`的流量传递到服务。
- en: Find a specific target named `KUBE-SVC-XXX` under `Chain KUBE-SERVICES`. In
    this case, it is the target named `KUBE-SVC-GFPAJ7EGCNM4QF4H`, ruled as allowing
    traffics from "everywhere" to the endpoint of `nodeport-svc`, `10.160.245:8080`.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Chain KUBE-SERVICES`下找到名为`KUBE-SVC-XXX`的特定目标。在这种情况下，它是名为`KUBE-SVC-GFPAJ7EGCNM4QF4H`的目标，规定允许来自“任何地方”的流量到达`nodeport-svc`的端点`10.160.245:8080`。
- en: Find targets under the specific `Chain KUBE-SVC-XXX`. In this case, it is `Chain
    KUBE-SVC-GFPAJ7EGCNM4QF4H`. Under the Service chain, you will have number of targets
    based on the according Pods with the prefix `KUBE-SEP-`. In the preceding output,
    they are `KUBE-SEP-TC6HXYYMMLGUSFNZ` and `KUBE-SEP-DIS6NYZTQKZ5ALQS`.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特定的`Chain KUBE-SVC-XXX`下找到目标。在这种情况下，它是`Chain KUBE-SVC-GFPAJ7EGCNM4QF4H`。在服务链下，您将根据相应的Pods的前缀`KUBE-SEP-`拥有多个目标。在前面的输出中，它们是`KUBE-SEP-TC6HXYYMMLGUSFNZ`和`KUBE-SEP-DIS6NYZTQKZ5ALQS`。
- en: Find targets under specific `Chain KUBE-SEP-YYY`. In this case, the two chains
    required to take a look are `Chain KUBE-SEP-TC6HXYYMMLGUSFNZ` and `Chain KUBE-SEP-DIS6NYZTQKZ5ALQS`.
    Each of them covers two targets, `KUBE-MARK-MASQ` and `DNAT`, for incoming and
    outgoing traffics between "everywhere" to the endpoint of Pod, `192.168.80.5:80`
    or `192.168.80.6:80`.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特定的`Chain KUBE-SEP-YYY`下找到目标。在这种情况下，需要查看的两个链是`Chain KUBE-SEP-TC6HXYYMMLGUSFNZ`和`Chain
    KUBE-SEP-DIS6NYZTQKZ5ALQS`。它们各自涵盖两个目标，`KUBE-MARK-MASQ`和`DNAT`，用于“任何地方”到Pod端点`192.168.80.5:80`或`192.168.80.6:80`的进出流量。
- en: One key point here is that the Service target `KUBE-SVC-GFPAJ7EGCNM4QF4H` exposing
    its cluster IP to outside world will dispatch the traffic to chain `KUBE-SEP-TC6HXYYMMLGUSFNZ`
    and `KUBE-SEP-DIS6NYZTQKZ5ALQS` with a statistic mode random probability of 0.5\.
    Both chains have DNAT targets that work on changing the destination IP of the
    packets to the private subnet one, the one of a specific Pod.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个关键点是，服务目标`KUBE-SVC-GFPAJ7EGCNM4QF4H`将其集群IP暴露给外部世界，将流量分发到链`KUBE-SEP-TC6HXYYMMLGUSFNZ`和`KUBE-SEP-DIS6NYZTQKZ5ALQS`，统计模式为随机概率0.5。这两个链都有DNAT目标，用于改变数据包的目标IP为私有子网的IP，即特定Pod的IP。
- en: External-to-internal communication
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部到内部的通信
- en: To publish applications in Kubernetes, we can leverage either Kubernetes Service,
    with type `NodePort` or `LoadBalancer`, or Kubernetes Ingress. For NodePort service,
    as introduced in previous section, the port number of the node will be a pair
    with the Service. Like the following diagram, port `30361` on both node 1 and
    node 2 points to Service A, which dispatch the traffics to Pod1 and a Pod with
    static probability.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Kubernetes中发布应用程序，我们可以利用Kubernetes服务，类型为`NodePort`或`LoadBalancer`，或Kubernetes
    Ingress。对于NodePort服务，如前一节介绍的，节点的端口号将与服务配对。就像下图中所示，节点1和节点2上的端口`30361`指向服务A，该服务将流量分发到Pod1和一个具有静态概率的Pod。
- en: 'LoadBalancer Service, as you may have learned from the recipe *Working with
    Services* in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)*, Walking
    through Kubernetes Concepts*, includes the configurations of NodePort. Moreover,
    a LoadBalancer Service can work with an external load balancer, providing users
    with the functionality to integrate load balancing procedures between cloud infrastructure
    and Kubernetes resource, such as the settings `healthCheckNodePort` and `externalTrafficPolicy`.
    **Service B** in the following image is a LoadBalancer Service. Internally, **Service
    B** works the same as **Service A**, relying on **iptables** to redirect packets
    to Pod; Externally, cloud load balancer doesn''t realize Pod or container, it
    only dispatches the traffic by the number of nodes. No matter which node is chosen
    to get the request, it would still be able to pass packets to the right Pod:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器服务，正如您可能从[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)*中的*“使用服务”中学到的那样，包括NodePort的配置。此外，负载均衡器服务可以与外部负载均衡器一起工作，为用户提供在云基础架构和Kubernetes资源之间集成负载平衡程序的功能，例如设置`healthCheckNodePort`和`externalTrafficPolicy`。下图中的**服务B**是一个负载均衡器服务。在内部，**服务B**与**服务A**的工作方式相同，依赖于**iptables**将数据包重定向到Pod；在外部，云负载均衡器并不了解Pod或容器，它只根据节点数量分发流量。无论选择哪个节点来获取请求，它仍然能够将数据包传递给正确的Pod：
- en: '![](assets/161638c9-b9f8-44e4-84e5-48ab67514b83.png)Kubernetes Services with
    type NodePort and type LoadBalancer'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/161638c9-b9f8-44e4-84e5-48ab67514b83.png)带有NodePort和LoadBalancer类型的Kubernetes服务'
- en: Working with Ingress
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ingress
- en: Walking through the journey of Kubernetes networking, users get the idea that
    each Pod and Service has its private IP and corresponding port to listen on request.
    In practice, developers may deliver the endpoint of service, the private IP or
    Kubernetes DNS name, for internal clients; or, developers may expose Services
    externally by type of NodePort or LoadBalancer. Although the endpoint of Service
    is more stable than Pod, the Services are offered separately, and clients should
    record the IPs without much meaning to them. In this section, we will introduce
    `Ingress`, a resource that makes your Services work as a group. More than that,
    we could easily pack our service union as an API server while we set Ingress rules
    to recognize the different URLs, and then ingress controller works for passing
    the request to specific Services based on the rules.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes网络的旅程中，用户会了解到每个Pod和Service都有自己的私有IP和相应的端口来监听请求。在实践中，开发人员可以为内部客户端提供服务的端点，即私有IP或Kubernetes
    DNS名称；或者，开发人员可以通过NodePort或LoadBalancer类型将服务外部暴露。虽然Service的端点比Pod更稳定，但服务是分开提供的，客户端应该记录这些IP而没有太多意义。在本节中，我们将介绍`Ingress`，这是一个使您的服务作为一个组工作的资源。更重要的是，我们可以在设置Ingress规则以识别不同的URL时，轻松地将我们的服务联合打包为一个API服务器，然后Ingress控制器根据规则将请求传递给特定的服务。
- en: 'Before we try on Kubernetes Ingress, we should create an ingress controller
    in cluster. Different from other controllers in `kube-controller-manager`([https://kubernetes.io/docs/reference/generated/kube-controller-manager/](https://kubernetes.io/docs/reference/generated/kube-controller-manager/)),
    ingress controller is run by custom implementation instead of working as a daemon.
    In the latest Kubernetes version, 1.10, nginx ingress controller is the most stable
    one and also generally supports many platforms. Check the official documents for
    the details of deployment: [https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md).
    We will only demonstrate our example on minikube; please see the following information
    box for the setup of the ingress controller.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们尝试Kubernetes Ingress之前，我们应该在集群中创建一个Ingress控制器。与`kube-controller-manager`中的其他控制器不同，Ingress控制器是由自定义实现而不是作为守护程序运行的。在最新的Kubernetes版本1.10中，nginx
    Ingress控制器是最稳定的，通常也支持许多平台。查看部署的官方文档：[https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md)。我们将只在minikube上演示我们的示例；请查看以下信息框以设置Ingress控制器。
- en: 'Enable Ingress functionality in minikubeIngress in minikube is an add-on function.
    Follow these steps to start this feature in your environment:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在minikube中启用Ingress功能Ingress在minikube中是一个附加功能。按照以下步骤在您的环境中启用此功能：
- en: 'Check if the add-on ingress is enabled or not: Fire the command `minikube addons
    list` on your terminal. If it is not enabled, means it shows `ingress: disabled`,
    you should keep follow below steps.'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '检查附加功能ingress是否已启用：在终端上输入命令`minikube addons list`。如果未启用，表示显示为`ingress: disabled`，则应继续以下步骤。'
- en: 'Enable ingress: Enter the command `minikube addons enable ingress`, you will
    see an output like `ingress was successfully enabled`.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用ingress：输入命令`minikube addons enable ingress`，您将看到类似`ingress was successfully
    enabled`的输出。
- en: Check the add-on list again to verify that the last step does work. We expect
    that the field ingress shows as `enabled`.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次检查附加功能列表，以验证上一步是否有效。我们期望字段ingress显示为`enabled`。
- en: 'Here comes an example to demonstrate how to work with Ingress. We would run
    up two Deployments and their Services, and an additional Ingress to expose them
    as a union. In the beginning, we would add a new hostname in the host file of
    Kubernetes master. It is a simple way for our demonstration. If you work on the
    production environment, a general use case is that the hostname should be added
    as a record in the DNS server:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，演示如何使用Ingress。我们将启动两个部署和它们的服务，以及一个额外的Ingress将它们公开为一个联合体。首先，我们将在Kubernetes主节点的主机文件中添加一个新的主机名。这是我们演示的一个简单方法。如果您在生产环境中工作，一个常见的用例是将主机名作为记录添加到DNS服务器中：
- en: '[PRE51]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Our first Kubernetes Deployment and Service would be `echoserver`, a dummy
    Service showing server and request information. For the other pair of Deployment
    and Service, we would reuse the NodePort Service example from the previous section:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个Kubernetes部署和服务将是`echoserver`，一个显示服务器和请求信息的虚拟服务。对于另一对部署和服务，我们将重用上一节中的NodePort服务示例：
- en: '[PRE52]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Go ahead and create both set of resources through configuration files:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 继续通过配置文件创建这两组资源：
- en: '[PRE53]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Our first Ingress makes two Services that listen at the separate URLs `/nginx`
    and `/echoserver`, with the hostname `happy.k8s.io`, the dummy one we added in
    the local host file. We use annotation `rewrite-target` to guarantee that traffic
    redirection starts from root, `/`. Otherwise, the client may get page not found
    because of surfing the wrong path. More annotations we may play with are listed
    at [https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个Ingress使两个服务在不同的URL `/nginx` 和 `/echoserver` 上监听，主机名为`happy.k8s.io`，这是我们在本地主机文件中添加的虚拟主机名。我们使用注释`rewrite-target`来确保流量重定向从根目录`/`开始。否则，客户端可能因为浏览错误的路径而得到页面未找到的错误。我们可以使用的更多注释列在[https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md)上：
- en: '[PRE54]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, just create the Ingress and check its information right away:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，只需创建Ingress并立即检查其信息：
- en: '[PRE55]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'You may find that there is no IP address in the field of description. It will
    be attached after the first DNS lookup:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述字段中可能找不到IP地址。在第一次DNS查找后，它将被附加：
- en: '[PRE56]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Although working with Ingress is not as straightforward as other resources,
    as you have to start an ingress controller implementation by yourself, it still
    makes our application exposed and flexible. There are many network features coming
    that are more stable and user friendly. Keep up with the latest updates and have
    fun!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用Ingress不像其他资源那样直接，因为你必须自己启动Ingress控制器的实现，但它仍然使我们的应用程序暴露和灵活。即将推出的许多网络功能更加稳定和用户友好。保持最新更新并且玩得开心！
- en: There's more...
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In the last part of external-to-internal communication, we learned about Kubernetes
    Ingress, the resource that makes services work as a union and dispatches requests
    to target services. Does any similar idea jump into your mind? It sounds like
    a microservice, the application structure with several loosely coupled services.
    A complicated application would be distributed to multiple lighter services. Each
    service is developed independently while all of them can cover original functions.
    Numerous working units, such as Pods in Kubernetes, run volatile and can be dynamically
    scheduled on Services by the system controller. However, such a multi-layered
    structure increases the complexity of networking and also suffers potential overhead
    costs.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在外部到内部通信的最后部分，我们了解了Kubernetes Ingress，这是使服务作为一个整体工作并将请求分派给目标服务的资源。你脑海中是否跳出了类似的想法？听起来像是微服务，这是一个由几个松散耦合的服务组成的应用结构。一个复杂的应用会分布到多个轻量级服务中。每个服务都是独立开发的，同时它们都可以覆盖原始功能。许多工作单元，比如Kubernetes中的Pod，都是不稳定的，并且可以由系统控制器动态地调度到服务上。然而，这样的多层结构增加了网络的复杂性，也会带来潜在的开销。
- en: 'External load balancers are not aware the existence of Pods; they only balance
    the workload to hosts. A host without any served Pod running would then redirect
    the loading to other hosts. This situation comes out of a user''s expectation
    for fair load balancing. Moreover, a Pod may crash accidentally, in which case
    it is difficult to do failover and complete the request. To make up the shortcomings,
    the idea of a service mesh focus on the networking management of microservice
    was born, dedicated to delivering more reliable and performant communications
    on orchestration like Kubernetes:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 外部负载均衡器不知道Pod的存在；它们只会将工作负载平衡到主机上。没有运行任何Pod的主机将把负载重定向到其他主机。这种情况是用户对公平负载均衡的期望所导致的。此外，Pod可能会意外崩溃，在这种情况下很难进行故障转移和完成请求。为弥补这些缺点，服务网格的理念专注于微服务的网络管理，致力于在像Kubernetes这样的编排中提供更可靠和高性能的通信：
- en: '![](assets/efbc7c62-a23c-41ed-95d0-4c5dd0540871.png)Simpe service mesh structure'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/efbc7c62-a23c-41ed-95d0-4c5dd0540871.png)简单的服务网格结构'
- en: 'The preceding diagram illustrates the main components in a service mesh. They
    work together to achieve features as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 上图说明了服务网格中的主要组件。它们共同工作以实现以下功能：
- en: '**Service mesh ingress**: Using applied Ingress rules to decide which Service
    should handle the incoming requests. It could also be a proxy that is able to
    check the runtime policies.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务网格入口**：使用应用的入口规则来决定哪个服务应该处理传入的请求。它也可以是一个能够检查运行时策略的代理。'
- en: '**Service mesh proxy**: Proxies on every node not only direct the packets,
    but can also be used as an advisory agent reporting the overall status of the
    Services.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务网格代理**：每个节点上的代理不仅指导数据包，还可以作为报告服务整体状态的咨询代理。'
- en: '**Service mesh service discovery pool**: Serving the central management for
    mesh and pushing controls over proxies. Its responsibility includes procedures
    of network capability, authentication, failover, and load balancing.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务网格服务发现池**：为网格提供中央管理，并控制代理。其责任包括网络能力、认证、故障转移和负载均衡的程序。'
- en: Although well-known service mesh implementations such as Linkerd ([https://linkerd.io](https://linkerd.io))
    and Istio ([https://istio.io](https://istio.io)) are not mature enough for production
    usage, the idea of service mesh is not ignorable.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管诸如Linkerd（[https://linkerd.io](https://linkerd.io)）和Istio（[https://istio.io](https://istio.io)）等著名服务网格实现尚不够成熟以用于生产，但服务网格的理念却是不可忽视的。
- en: See also
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Kubernetes forwards ports based on the overlay network. In this chapter, we
    also run Pods and Services with nginx. Reviewing the previous sections will help
    you to understand more about how to manipulate it. Also, look at the following
    recipes:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes基于覆盖网络转发端口。在本章中，我们还将使用nginx运行Pod和服务。回顾之前的部分将帮助您更多地了解如何操作它。另外，看看以下的食谱：
- en: The *Creating an overlay network* and*Running your first container in Kubernetes*
    recipes in [Chap](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)[ter](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)
    [1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml), *Building Your Own Kubernetes Cluster*
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第1章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)中的*创建覆盖网络*和*在Kubernetes中运行您的第一个容器*食谱，*构建您自己的Kubernetes集群*'
- en: The *Working with Pods *and*Working with Services* recipes in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)**,
    Walking through Kubernetes Concepts**
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)**中的*使用Pod*和*使用服务*食谱，深入了解Kubernetes概念**'
- en: The *Moving monolithic to microservices* recipe in [Chapter 5](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml)*,
    Building Continuous Delivery Pipelines*
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第5章](669edaf0-c274-48fa-81d8-61150fa36df5.xhtml)*中的*将单片式转换为微服务*食谱，构建持续交付管道*'
- en: Ensuring flexible usage of your containers
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保容器的灵活使用
- en: Pod, in Kubernetes, means a set of containers, which is also the smallest computing
    unit. You may have know about the basic usage of Pod in the previous recipes.
    Pods are usually managed by deployments and exposed by services; they work as
    applications with this scenario.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Pod，在Kubernetes中意味着一组容器，也是最小的计算单元。您可能已经了解了上一篇文章中Pod的基本用法。Pod通常由部署管理，并由服务公开；它们在这种情况下作为应用程序工作。
- en: 'In this recipe, we will discuss two new features: **DaemonSets** and **StatefulSets**.
    These two features can manage Pods with more specific purpose.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将讨论两个新功能：**DaemonSets**和**StatefulSets**。这两个功能可以管理具有更具体目的的Pod。
- en: Getting ready
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: What are **Daemon-like Pod **and **Stateful Pod**? The regular Pods in Kubernetes
    will determine and dispatch to particular Kubernetes nodes based on current node
    resource usage and your configuration.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是**类似守护程序的Pod**和**有状态的Pod**？Kubernetes中的常规Pod将根据当前节点资源使用情况和您的配置确定并分派到特定的Kubernetes节点。
- en: However, a **Daemon-like Pod **will be created in each node. For example, if
    you have three nodes, three daemon-like Pods will be created and deployed to each
    node. Whenever a new node is added, DaemonSets Pod will be deployed to the new
    node automatically. Therefore, it will be useful to use node level monitoring
    or log correction.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**类似守护程序的Pod**将在每个节点上创建。例如，如果您有三个节点，将创建三个类似守护程序的Pod，并部署到每个节点。每当添加新节点时，守护程序Pod将自动部署到新节点。因此，使用节点级监控或日志更正将非常有用。
- en: On the other hand, a **Stateful Pod **will stick to some resources such as network
    identifier (Pod name and DNS) and **p****ersistent volume** (**PV**). This also
    guarantees an order during deployment of multiple Pods and during rolling update.
    For example, if you deploy a Pod named `my-pod`, and set the scale to **4**, then
    Pod name will be assigned as `my-pod-0`, `my-pod-1`, `my-pod-2`, and `my-pod-3`.
    Not only Pod name but also DNS and persistent volume are preserved. For example,
    when `my-pod-2` is recreated due to resource shortages or application crash, those
    names and volumes are taken over by a new Pod which is also named `my-pod-2`.
    It is useful for some cluster based applications such as HDFS and ElasticSearch.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**Stateful Pod**将固定一些资源，如网络标识符（Pod名称和DNS）和**持久卷（PV）**。这也保证了在部署多个Pod时的顺序和在滚动更新期间的顺序。例如，如果您部署了一个名为`my-pod`的Pod，并将规模设置为**4**，那么Pod名称将被分配为`my-pod-0`、`my-pod-1`、`my-pod-2`和`my-pod-3`。不仅Pod名称，DNS和持久卷也会被保留。例如，当`my-pod-2`由于资源短缺或应用程序崩溃而重新创建时，这些名称和卷将被新的名为`my-pod-2`的Pod接管。这对于一些基于集群的应用程序如HDFS和ElasticSearch非常有用。
- en: In this recipe, we will demonstrate how to use DaemonSets and StatefulSet; however,
    to have a better understanding, it should use multiple Kubernetes Nodes environment.
    To do this, minikube is not ideal, so instead, use either kubeadm/kubespray to
    create a multiple Node environment.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将演示如何使用DaemonSets和StatefulSet；然而，为了更好地理解，应该使用多个Kubernetes节点环境。为了做到这一点，minikube并不理想，所以可以使用kubeadm/kubespray来创建多个节点环境。
- en: Using kubeadm or kubespray to set up Kubernetes cluster was described in [Chapter
    1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)*,* *Build Your Own Kubernetes Cluster.*
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 使用kubeadm或kubespray来设置Kubernetes集群在《构建您自己的Kubernetes集群》一章中有描述。
- en: 'To confirm whether that has 2 or more nodes, type `kubectl get nodes` as follows
    to check how many nodes you have:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 要确认是否有2个或更多节点，输入`kubectl get nodes`来检查你有多少个节点：
- en: '[PRE57]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In addition, if you want to execute the StatefulSet recipe later in this chapter,
    you need a StorageClass to set up a dynamic provisioning environment. It was described
    in *Working with volumes* section in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml), *Walking
    through Kubernetes Concepts*. It is recommended to use public cloud such as AWS
    and GCP with a CloudProvider; this will be described in [Chapter 6](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml),
    *Building Kubernetes on AWS* and [Chapter 7](dfc46490-f109-4f07-ba76-1a381b006d76.xhtml),
    *Building Kubernetes on GCP*, as well.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您想在本章后面执行StatefulSet示例，您需要一个StorageClass来设置一个动态配置环境。这在《走进Kubernetes概念》一章中的《使用卷》部分中有描述。建议使用AWS和GCP等公共云与CloudProvider；这将在《在AWS上构建Kubernetes》和《在GCP上构建Kubernetes》中描述。
- en: 'To check whether `StorageClass` is configured or not, use `kubectl get sc`:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查`StorageClass`是否配置了，使用`kubectl get sc`命令：
- en: '[PRE58]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: How to do it...
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: There is no CLI for us to create DaemonSets or StatefulSets. Therefore, we will
    build these two resource types by writing all the configurations in a YAML file.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有CLI来创建DaemonSets或StatefulSets。因此，我们将通过在YAML文件中编写所有配置来构建这两种资源类型。
- en: Pod as DaemonSets
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod作为DaemonSets
- en: If a Kubernetes DaemonSet is created, the defined Pod will be deployed in every
    single node. It is guaranteed that the running containers occupy equal resources
    in each node. In this scenario, the container usually works as the daemon process.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如果创建了一个Kubernetes DaemonSet，定义的Pod将部署在每个单独的节点上。保证运行的容器在每个节点上占用相等的资源。在这种情况下，容器通常作为守护进程运行。
- en: 'For example, the following template has an Ubuntu image container that keeps
    checking its memory usage half a minute at a time:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下模板有一个Ubuntu镜像容器，每隔半分钟检查一次内存使用情况：
- en: 'To build it as a DaemonSet, execute the following code block:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将其构建为DaemonSet，请执行以下代码块：
- en: '[PRE59]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: As the Job, the selector could be ignored, but it takes the values of the labels.
    We will always configure the restart policy of the DaemonSet as `Always`, which
    makes sure that every node has a Pod running.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 与Job一样，选择器可以被忽略，但它会获取标签的值。我们将始终将DaemonSet的重启策略配置为`Always`，以确保每个节点都有一个正在运行的Pod。
- en: 'The abbreviation of the `daemonset` is `ds` in `kubectl` command, use this
    shorter one in the CLI for convenience:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`daemonset`的缩写在`kubectl`命令中是`ds`，在CLI中使用这个更短的命令更方便：'
- en: '[PRE60]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Here, we have two Pods running in separated nodes. They can still be recognized
    in the channel of the `pod`:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们有两个运行在不同节点的Pods。它们仍然可以在`pod`的通道中被识别：
- en: '[PRE61]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'It is good for you to evaluate the result using the subcommand `kubectl logs`:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用子命令`kubectl logs`来评估结果是很好的：
- en: '[PRE62]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Whenever, you add a Kubernetes node onto your existing cluster, DaemonSets will
    recognize and deploy a Pod automatically.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你向现有集群添加一个Kubernetes节点时，DaemonSets会自动识别并部署一个Pod。
- en: 'Let''s check again current status of DaemonSets, there are two Pods that have
    been deployed due to having two nodes as follows:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次检查DaemonSets的当前状态，由于有两个节点，已经部署了两个Pods：
- en: '[PRE63]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'So, now we are adding one more node onto the cluster through either `kubespray`
    or `kubeadm`, based on your setup:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，现在我们通过`kubespray`或`kubeadm`向集群添加了一个节点，具体取决于你的设置：
- en: '[PRE64]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'A few moments later, without any operation, the DaemonSet''s size become `3`
    automatically, which aligns to the number of nodes:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几分钟后，没有任何操作，DaemonSet的大小会自动变为`3`，与节点的数量保持一致：
- en: '[PRE65]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Running a stateful Pod
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行一个有状态的Pod
- en: Let's see another use case. We used Deployments/ReplicaSets to replicate the
    Pods. It scales well and is easy to maintain and Kubernetes assigns a DNS to the
    Pod using the Pod's IP address, such as `<Pod IP address>.<namespace>.pod.cluster.local`.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看另一个用例。我们使用Deployments/ReplicaSets来复制Pods。它的扩展性很好，易于维护，Kubernetes会为Pod分配一个DNS，使用Pod的IP地址，例如`<Pod
    IP address>.<namespace>.pod.cluster.local`。
- en: 'The following example demonstrates how the Pod DNS will be assigned:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了如何分配Pod的DNS：
- en: '[PRE66]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'However, this DNS entry is not guaranteed to stay in use for this Pod, because
    the Pod might crash due to an application error or node resource shortage. In
    such a case, the IP address will possibly be changed:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个DNS条目不能保证会一直用于这个Pod，因为Pod可能会因为应用程序错误或节点资源短缺而崩溃。在这种情况下，IP地址可能会发生变化：
- en: '[PRE67]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: For some applications, this will cause an issue; for example, if you manage
    a cluster application that needs to be managed by DNS or IP address. As of the
    current Kubernetes implementation, IP addresses can't be preserved for Pods .
    How about we use Kubernetes Service? Service preserves a DNS name. Unfortunately,
    it's not realistic to create the same amount of service with Pod. In the previous
    case, create three Services that bind to three Pods one to one.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些应用程序，这可能会导致问题；例如，如果你管理一个需要通过DNS或IP地址进行管理的集群应用程序。根据当前的Kubernetes实现，IP地址无法保留给Pods。那我们使用Kubernetes
    Service怎么样？Service会保留一个DNS名称。不幸的是，创建与Pod相同数量的Service并不现实。在之前的情况下，创建三个绑定到三个Pods的Service。
- en: Kubernetes has a solution for this kind of use case that uses StatefulSet**.** It
    preserves not only the DNS but also the persistent volume to keep a bind to the
    same Pod. Even if Pod is crashed, StatefulSet guarantees the binding of the same
    DNS and persistent volume to the new Pod. Note that the IP address is not preserved
    due to the current Kubernetes implementation.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes针对这种类型的用例有一个解决方案，即使用**StatefulSet**。它不仅保留了DNS，还保留了持久卷，以保持与同一Pod的绑定。即使Pod崩溃，StatefulSet也保证将相同的DNS和持久卷绑定到新的Pod。请注意，由于当前Kubernetes实现，IP地址不会被保留。
- en: 'To demonstrate, use **Hadoop Distributed File System** (**HDFS**) to launch
    one NameNode and three DataNodes. To perform this, use a Docker image from [https://hub.docker.com/r/uhopper/hadoop/](https://hub.docker.com/r/uhopper/hadoop/)
    that has NameNode and DataNode images. In addition, borrow the YAML configuration
    files `namenode.yaml` and `datanode.yaml` from [https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582](https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582)
    and change a little bit:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，使用**Hadoop分布式文件系统**（**HDFS**）启动一个NameNode和三个DataNodes。为此，使用来自[https://hub.docker.com/r/uhopper/hadoop/](https://hub.docker.com/r/uhopper/hadoop/)的Docker镜像，其中包含NameNode和DataNode镜像。此外，从[https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582](https://gist.github.com/polvi/34ef498a967de563dc4252a7bfb7d582)借用YAML配置文件`namenode.yaml`和`datanode.yaml`并稍作修改：
- en: 'Let''s launch a Service and StatefulSet for `namenode` and `datanode`:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们为`namenode`和`datanode`启动一个Service和StatefulSet：
- en: '[PRE68]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: As you can see, the Pod naming convention is `<StatefulSet-name>-<sequence number>`. For
    example, NameNode Pod's name is `hdfs-namenode-0`. Also DataNode Pod's names are
    `hdfs-datanode-0`, `hdfs-datanode-1` and `hdfs-datanode-2`.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，Pod的命名约定是`<StatefulSet-name>-<sequence number>`。例如，NameNode Pod的名称是`hdfs-namenode-0`。此外，DataNode
    Pod的名称分别是`hdfs-datanode-0`、`hdfs-datanode-1`和`hdfs-datanode-2`。
- en: 'In addition, both NameNode and DataNode have a service that is configured as
    Headless mode (by `spec.clusterIP: None`). Therefore, you can access these Pods using
    DNS as `<pod-name>.<service-name>.<namespace>.svc.cluster.local`. In this case,
    this NameNode DNS entry could be `hdfs-namenode-0.hdfs-namenode-svc.default.svc.cluster.local`*.*'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，NameNode和DataNode都有一个配置为无头模式的服务（通过`spec.clusterIP: None`）。因此，您可以使用DNS来访问这些Pod，格式为`<pod-name>.<service-name>.<namespace>.svc.cluster.local`。在这种情况下，NameNode的DNS条目可能是`hdfs-namenode-0.hdfs-namenode-svc.default.svc.cluster.local`。'
- en: 'Let''s check what NameNode Pod''s IP address is, you can get this using `kubectl
    get pods -o wide` as follows:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一下NameNode Pod的IP地址，您可以使用`kubectl get pods -o wide`来获取：
- en: '[PRE69]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, log in (run `/bin/bash`) to one of the DataNodes using `kubectl exec`
    to resolve this DNS name and check whether the IP address is `10.52.2.8` or not:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，登录（运行`/bin/bash`）到其中一个DataNodes，使用`kubectl exec`来解析此DNS名称，并检查IP地址是否为`10.52.2.8`：
- en: '[PRE70]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Looks all good! For demonstration purposes, let's access the HDFS web console
    to see DataNode's status.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来一切正常！为了演示目的，让我们访问HDFS Web控制台以查看DataNode的状态。
- en: 'To do that, use `kubectl port-forward` to access to the NameNode web port (tcp/`50070`):'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了做到这一点，使用`kubectl port-forward`来访问NameNode的web端口（tcp/`50070`）：
- en: '[PRE71]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The preceding result indicates that your local machine TCP port `60107` (you
    result will vary) has been forwarded to NameNode Pod TCP port `50070`. Therefore,
    use a web browser to access `http://127.0.0.1:60107/` as follows:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述结果表明，您的本地机器TCP端口`60107`（您的结果会有所不同）已被转发到NameNode Pod的TCP端口`50070`。因此，请使用Web浏览器访问`http://127.0.0.1:60107/`如下：
- en: '![](assets/5bd29558-4248-4abf-8913-3959eb180f8c.png)HDFS Web console shows
    three DataNodes'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/5bd29558-4248-4abf-8913-3959eb180f8c.png)HDFS Web控制台显示三个DataNodes'
- en: As you may see, three DataNodes have been registered to NameNode successfully.
    The DataNodes are also using the Headless Service so that same name convention
    assigns DNS names for DataNode as well.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，三个DataNodes已成功注册到NameNode。DataNodes也在使用无头服务，因此相同的命名约定也为DataNode分配了DNS名称。
- en: How it works...
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: DaemonSets and StatefulSets; both concepts are similar but behave differently,
    especially when Pod is crashed. Let's take a look at how it works.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSets和StatefulSets；这两个概念都很相似，但行为不同，特别是在Pod崩溃时。让我们看看它是如何工作的。
- en: Pod recovery by DaemonSets
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 由DaemonSets进行的Pod恢复
- en: DaemonSets keep monitoring every Kubernetes node, so when one of the Pods crashes,
    DaemonSets recreates it on the same Kubernetes node.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSets会监视每个Kubernetes节点，因此当其中一个Pod崩溃时，DaemonSets会在同一Kubernetes节点上重新创建它。
- en: 'To simulate this, go back to the DaemonSets example and use `kubectl delete
    pods` to delete an existing Pod from `node1` manually, as follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟这一点，回到DaemonSets的示例，并使用`kubectl delete pods`手动从`node1`中删除一个现有的Pod，如下所示：
- en: '[PRE72]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: As you can see, a new Pod has been created automatically to recover the Pod in
    `node1`. Note that the Pod name has been changed from `ram-check-6ldng` to `ram-check-dh5hq`—it
    has beenassigned a random suffix name. In this use case, Pod name doesn't matter,
    because we don't use hostname or DNS to manage this application.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，一个新的Pod已经自动创建以恢复`node1`中的Pod。请注意，Pod的名称已从`ram-check-6ldng`更改为`ram-check-dh5hq`—它已被分配了一个随机后缀名称。在这种用例中，Pod名称并不重要，因为我们不使用主机名或DNS来管理此应用程序。
- en: Pod recovery by StatefulSet
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 由StatefulSet进行的Pod恢复
- en: StatefulSet behaves differently to DaemonSet during Pod recreation. In StatefulSet
    managed Pods, the Pod name is always consisted to assign an ordered number such
    as `hdfs-datanode-0`, `hdfs-datanode-1` and`hdfs-datanode-2`*,* and if you delete
    one of them, a new Pod will take over the same Pod name.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pod重新创建期间，StatefulSet的行为与DaemonSet不同。在StatefulSet管理的Pod中，Pod的名称始终保持一致，分配一个有序的编号，例如`hdfs-datanode-0`，`hdfs-datanode-1`和`hdfs-datanode-2`，如果您删除其中一个，新的Pod将接管相同的Pod名称。
- en: 'To simulate this, let''s delete one DataNode (`hdfs-datanode-1`) to see how
    StatefulSet recreates a Pod:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟这一点，让我们删除一个DataNode（`hdfs-datanode-1`）来看看StatefulSet如何重新创建一个Pod：
- en: '[PRE73]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'As you see, the same Pod name (`hdfs-datanode-1`) has been assigned. Approximately
    after 10 minutes (due to HDFS''s heart beat interval), HDFS web console shows
    that the old Pod has been marked as dead and the new Pod has the in service state,
    shown as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，已分配了相同的Pod名称（`hdfs-datanode-1`）。大约10分钟后（由于HDFS的心跳间隔），HDFS Web控制台显示旧的Pod已被标记为死亡，新的Pod处于服务状态，如下所示：
- en: '![](assets/8bb7096f-30a6-4cff-9aac-c1d831eb27e4.png)Status when one DataNode
    is dead'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/8bb7096f-30a6-4cff-9aac-c1d831eb27e4.png)一个DataNode死亡时的状态'
- en: Note that this is not a perfect ideal case for HDFS, because DataNode-1 lost
    data and expects to re-sync from other DataNodes. If the data size is bigger,
    it may take a long time to complete re-sync.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这对于HDFS来说并不是一个完美的理想情况，因为DataNode-1丢失了数据，并希望从其他DataNodes重新同步。如果数据量更大，可能需要很长时间才能完成重新同步。
- en: Fortunately, StatefulSets has an capability that preserve a persistent volume
    while replacing a Pod. Let's see how HDFS DataNode can preserve data during Pod recreation.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，StatefulSets具有保留持久卷的能力，同时替换Pod。让我们看看HDFS DataNode在Pod重新创建期间如何保留数据。
- en: There's more...
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: StatefulSet with persistent volume; it requires a `StorageClass` that provisions
    a volume dynamically. Because each Pod is created by StatefulSets, it will create
    a **persistent volume claim** (**PVC**) with a different identifier. If your StatefulSets
    specify a static name of PVC, there will be trouble if multiple Pods try to attach
    the same PVC.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 具有持久卷的StatefulSet；它需要一个`StorageClass`来动态分配卷。因为每个Pod都是由StatefulSets创建的，它将创建一个具有不同标识符的**持久卷索赔**（**PVC**）。如果您的StatefulSets指定了PVC的静态名称，如果多个Pod尝试附加相同的PVC，将会出现问题。
- en: 'If you have `StorageClass` on your cluster, update `datanode.yaml` to add `spec.volumeClaimTemplates`
    as follows:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的集群上有`StorageClass`，请更新`datanode.yaml`以添加`spec.volumeClaimTemplates`如下：
- en: '[PRE74]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'This tells Kubernetes to create a PVC and PV when a new Pod is created by StatefulSet.
    So, that Pod template (`spec.template.spec.containers.volumeMounts`) should specify
    `hdfs-data`, as follows:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉Kubernetes在StatefulSet创建新的Pod时创建PVC和PV。因此，该Pod模板（`spec.template.spec.containers.volumeMounts`）应指定`hdfs-data`，如下所示：
- en: '[PRE75]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Let''s recreate HDFS cluster again:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次重新创建HDFS集群：
- en: '[PRE76]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'To demonstrate, use `kubectl exec` to access the NameNode, then copy some dummy
    files to HDFS:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，使用`kubectl exec`访问NameNode，然后将一些虚拟文件复制到HDFS：
- en: '[PRE77]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'At this moment, `DataNode-1` is restarting, as shown in the following image.
    However, the data directory of `DataNode-1` is kept by PVC as `hdfs-data-hdfs-datanode-1`.
    The new Pod `hdfs-datanode-1` will take over this PVC again:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`DataNode-1`正在重新启动，如下图所示。但是，`DataNode-1`的数据目录由PVC保留为`hdfs-data-hdfs-datanode-1`。新的Pod
    `hdfs-datanode-1`将再次接管此PVC：
- en: '![](assets/d97460f9-8986-43df-bf2b-3311fc0f4aca.png)StatefulSet keeps PVC/PV
    while restarting'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d97460f9-8986-43df-bf2b-3311fc0f4aca.png)StatefulSet在重新启动时保留PVC/PV'
- en: 'Therefore, when you access HDFS after `hdfs-datanode-1` has recovered, you
    don''t see any data loss or re-sync processes:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在`hdfs-datanode-1`恢复后访问HDFS时，您不会看到任何数据丢失或重新同步过程：
- en: '[PRE78]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'As you see, the Pod and PV pair is fully managed by StatefulSets. It is convenient
    if you want to scale more HDFS DataNode using just the `kubectl scale` command
    to make it double or hundreds—whatever you need:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Pod和PV对由StatefulSets完全管理。如果您想要通过`kubectl scale`命令轻松扩展更多的HDFS DataNode，只需将其加倍或增加到数百个，这将非常方便：
- en: '[PRE79]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: You can also use PV to NameNode to persist metadata. However, `kubectl` scale
    does not work well due to HDFS architecture. In order to have high availability
    or scale out HDFS NameNode, please visit the HDFS Federation document at : [https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用PV将元数据持久化到NameNode。但是，由于HDFS架构，`kubectl` scale的效果不佳。为了实现HDFS NameNode的高可用性或扩展，请访问HDFS联邦文档：[https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/Federation.html)。
- en: See also
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'In this recipe, we went deeply into Kubernetes Pod management through DaemonSets
    and StatefulSet. It manages Pod in a particular way, such as Pod per node and
    consistent Pod names. It is useful when the Deployments/ReplicaSets stateless
    Pod management style can''t cover your application use cases. For further information,
    consider the following:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们深入介绍了通过DaemonSets和StatefulSet管理Kubernetes Pod。它以一种特定的方式管理Pod，比如每个节点一个Pod和一致的Pod名称。当Deployments/ReplicaSets无法满足您的应用程序用例时，它非常有用。有关更多信息，请参考以下内容：
- en: The *Working with Pods *recipe in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)的*Working with Pods*示例中，*深入了解Kubernetes概念*
- en: Working with configuration files
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配置文件
- en: Submitting Jobs on Kubernetes
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上提交作业
- en: Your container application is designed not only for daemon processes such as
    nginx, but also for some batch Jobs which eventually exit when the task is complete.
    Kubernetes supports this scenario; you can submit a container as a Job and Kubernetes
    will dispatch to an appropriate node and execute your Job.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 您的容器应用程序不仅适用于诸如nginx之类的守护进程，还适用于一些批处理作业，这些作业在任务完成时最终退出。Kubernetes支持这种情况；您可以将一个容器提交为一个作业，Kubernetes将分派到一个适当的节点并执行您的作业。
- en: 'In this recipe, we will discuss two new features: **Jobs** and **CronJob**.
    These two features can make another usage of Pods to utilize your resources.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将讨论两个新功能：**作业**和**CronJob**。这两个功能可以利用Pod来利用你的资源。
- en: Getting ready
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Since Kubernetes version 1.2, Kubernetes Jobs has been introduced as a stable
    feature (`apiVersion: batch/v1`). In addition, CronJob is a beta feature (`apiVersion:
    batch/v1beta1`) as of Kubernetes version 1.10.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '自Kubernetes版本1.2以来，Kubernetes Jobs已经作为一个稳定的功能引入（`apiVersion: batch/v1`）。此外，CronJob是一个beta功能（`apiVersion:
    batch/v1beta1`），截至Kubernetes版本1.10。'
- en: Both work well on **minikube,** which was introduced at [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml),
    *Building Your Own Kubernetes Cluster. T*herefore, this recipe will use minikube
    version 0.24.1.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在**minikube**上都能很好地工作，这是在[第1章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)中介绍的，*构建您自己的Kubernetes集群。*因此，这个示例将使用minikube版本0.24.1。
- en: How to do it...
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'When submitting a Job to Kubernetes, you have three types of Job that you can
    define:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在向Kubernetes提交作业时，您可以定义三种类型的作业：
- en: Single Job
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个作业
- en: Repeat Job
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复作业
- en: Parallel Job
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行作业
- en: Pod as a single Job
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod作为单个作业
- en: 'A Job-like Pod is suitable for testing your containers, which can be used for
    unit test or integration test; alternatively, it can be used for batch programs:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 类似作业的Pod适合测试您的容器，可以用于单元测试或集成测试；或者，它可以用于批处理程序：
- en: 'In the following example, we will write a Job template to check the packages
    installed in image Ubuntu:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将编写一个Job模板来检查Ubuntu镜像中安装的软件包：
- en: '[PRE80]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Note that restart policy for Pods created in a Job should be set to `Never`
    or `OnFailure`, since a Job goes to termination once it is completed successfully.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，作业中创建的Pod的重启策略应设置为`Never`或`OnFailure`，因为一旦成功完成作业，作业就会终止。
- en: 'Now, you are ready to create a `job` using your template:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以使用您的模板创建一个`job`：
- en: '[PRE81]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'After creating a `job` object, it is possible to verify the status of both
    the Pod and Job:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`job`对象后，可以验证Pod和Job的状态：
- en: '[PRE82]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'This result indicates that Job is already done, executed (by `SUCCESSFUL =
    1`) in `26` seconds. In this case, Pod has already disappeared:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个结果表明作业已经完成，在`26`秒内执行（通过`SUCCESSFUL = 1`）。在这种情况下，Pod已经消失：
- en: '[PRE83]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'As you can see, the `kubectl` command hints to us that we can use `--show-all` or
    `-a` option to find the completed Pod, as follows:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如您所看到的，`kubectl`命令提示我们可以使用`--show-all`或`-a`选项来查找已完成的Pod，如下所示：
- en: '[PRE84]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Here you go. So why does the `Completed` Pod object remain? Because you may
    want to see the result after your program has ended. You will find that a Pod is
    booting up for handling this task. This Pod is going to be stopped very soon at
    the end of the process.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。那么为什么`Completed` Pod对象会保留呢？因为您可能希望在程序结束后查看结果。您会发现一个Pod正在启动来处理这个任务。这个Pod将在进程结束时很快停止。
- en: 'Use the subcommand `kubectl logs` to get the result:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用子命令`kubectl logs`获取结果：
- en: '[PRE85]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Please go ahead and check the `job package-check` using the subcommand `kubectl describe`;
    the confirmation for Pod completion and other messages are shown as system information:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请继续使用子命令`kubectl describe`检查`job package-check`；Pod完成和其他消息的确认显示为系统信息：
- en: '[PRE86]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Later, to remove the `job` you just created, delete it with the name. This
    also removes the completed Pod as well:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稍后，要删除您刚刚创建的`job`，请使用名称将其删除。这也会删除已完成的Pod：
- en: '[PRE87]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Create a repeatable Job
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个可重复的作业
- en: 'Users can also decide the number of tasks that should be finished in a single Job.
    It is helpful to solve some random and sampling problems. Let''s try it on the
    same template in the previous example:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 用户还可以决定在单个作业中应完成的任务数量。这有助于解决一些随机和抽样问题。让我们在前面示例中的相同模板上尝试一下：
- en: 'Add the `spec.completions` item to indicate the Pod number:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加`spec.completions`项目以指示Pod的数量：
- en: '[PRE88]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'After creating this Job, check how the Pod looks with the subcommand `kubectl describe`:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建此作业后，使用子命令`kubectl describe`检查Pod的外观：
- en: '[PRE89]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: As you can see, three Pods are created to complete this Job. This is useful
    if you need to run your program repeatedly at particular times. However, as you
    may have noticed from the `Age` column in preceding result, these Pods ran sequentially,
    one by one. This means that the 2nd Job was started after the 1st Job was completed,
    and the 3rd Job was started after the 2nd Job was completed.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，创建了三个Pod来完成此作业。如果您需要在特定时间重复运行程序，这将非常有用。但是，正如您从前面结果的`Age`列中注意到的那样，这些Pod是顺序运行的，一个接一个。这意味着第二个作业在第一个作业完成后开始，第三个作业在第二个作业完成后开始。
- en: Create a parallel Job
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个并行作业
- en: 'If your batch Job doesn''t have a state or dependency between Jobs, you may
    consider submitting Jobs in parallel. Similar to the `spec.completions` parameter,
    the Job template has a `spec.parallelism` parameter to specify how many Jobs you
    want to run in parallel:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的批处理作业没有状态或作业之间的依赖关系，您可以考虑并行提交作业。与`spec.completions`参数类似，作业模板具有一个`spec.parallelism`参数，用于指定要并行运行多少个作业：
- en: '1\. Re-use a repeatable Job but change it to specify `spec.parallelism: 3`
    as follows:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '1. 重复使用一个可重复的作业，但更改它以指定`spec.parallelism: 3`如下：'
- en: '[PRE90]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The result is similar to `spec.completions=3`*,* which made `3` Pods to run
    your application:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果类似于`spec.completions=3`，这使得`3`个Pod来运行您的应用程序：
- en: '[PRE91]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'However, if you see an `Age` column through the `kubectl describe` command,
    it indicates that `3` Pods ran at the same time:'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但是，如果您通过`kubectl describe`命令看到一个`Age`列，表示`3`个Pod同时运行：
- en: '[PRE92]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: In this setting, Kubernetes can dispatch to an available node to run your application
    and that easily scale your Jobs. It is useful if you want to run something like
    a worker application to distribute a bunch of Pods to different nodes.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置中，Kubernetes可以分派到可用节点来运行您的应用程序，并且可以轻松扩展您的作业。如果您想要运行类似工作程序的东西，将一堆Pod分发到不同的节点上，这将非常有用。
- en: Schedule to run Job using CronJob
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CronJob调度运行作业
- en: If you are familiar with **UNIX CronJob **or **Java Quartz** ([http://www.quartz-scheduler.org](http://www.quartz-scheduler.org)),
    Kubernetes CronJob is a very straightforward tool that you can define a particular
    timing to run your Kubernetes Job repeatedly.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉**UNIX CronJob**或**Java Quartz**（[http://www.quartz-scheduler.org](http://www.quartz-scheduler.org)），Kubernetes
    CronJob是一个非常直接的工具，您可以定义特定的时间来重复运行您的Kubernetes作业。
- en: 'The scheduling format is very simple; it specifies the following five items:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 调度格式非常简单；它指定了以下五个项目：
- en: Minutes (0 – 59)
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分钟（0 - 59）
- en: Hours (0 – 23)
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小时（0 - 23）
- en: Day of Month (1 – 31)
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份（1 - 12）
- en: Month (1 – 12)
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月份（1 - 12）
- en: 'Day of week (0: Sunday – 6: Saturday)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 星期几（0：星期日 - 6：星期六）
- en: For example, if you want to run your Job only at 9:00am on November 12th, every
    year, to send a birthday greeting to me :-), the schedule format could be `0 9
    12 11 *`.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想要在每年的11月12日上午9:00运行您的作业，以向我发送生日祝福:-)，调度格式可以是`0 9 12 11 *`。
- en: You may also use slash (`/`) to specify a step value; a `run every 5 minutes`
    interval for the previous Job example would have the following schedule format: `*/5
    * * * *`.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用斜杠（`/`）来指定步长值；对于前面作业示例的`每5分钟运行一次`间隔，将具有以下调度格式：`*/5 * * * *`。
- en: 'In addition, there is an optional parameter, `spec.concurrencyPolicy`, that
    you can specify a behavior if the previous Job is not finished but the next Job schedule
    is approaching, to determine how the next Job runs. You can set either:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个可选参数`spec.concurrencyPolicy`，您可以指定一个行为，如果前一个作业没有完成，但下一个作业的计划接近，以确定下一个作业的运行方式。您可以设置为：
- en: '**Allow**: Allow execution of the next Job'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Allow**：允许执行下一个作业'
- en: '**Forbid**: Skip execution of the next Job'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Forbid**：跳过执行下一个作业'
- en: '**Replace**: Delete the current Job, then execute the next Job'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Replace**：删除当前作业，然后执行下一个作业'
- en: 'If you set as `Allow`, there might be a potential risk of accumulating some
    unfinished Jobs in the Kubernetes cluster. Therefore, during the testing phase,
    you should set either `Forbid` or `Replace` to monitor Job execution and completion:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设置为`Allow`，可能会在Kubernetes集群中积累一些未完成的作业潜在风险。因此，在测试阶段，您应该设置为`Forbid`或`Replace`以监视作业的执行和完成：
- en: '[PRE93]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'After a few moments, the Job  will be triggered by your desired timing—in this
    case, every 5 minutes. You may then see the Job entry through the `kubectl get
    jobs` and `kubectl get pods -a` commands, as follows:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，作业将按照您期望的时间触发——在本例中，每5分钟。然后，您可以通过`kubectl get jobs`和`kubectl get pods -a`命令查看作业条目，如下所示：
- en: '[PRE94]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: CronJob will keep remaining until you delete; this means that, every 5 minutes,
    CronJob will create a new Job entry and related Pods will also keep getting created.
    This will impact the consumption of Kubernetes resources. Therefore, by default,
    CronJob will keep up to `3` successful Jobs (by `spec.successfulJobsHistoryLimit`)
    and one failed Job (by `spec.failedJobsHistoryLimit`). You can change these parameters
    based on your requirements.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: CronJob将保持剩余状态，直到您删除；这意味着，每5分钟，CronJob将创建一个新的作业条目，并且相关的Pod也将继续被创建。这将影响Kubernetes资源的消耗。因此，默认情况下，CronJob将保留最多`3`个成功的作业（通过`spec.successfulJobsHistoryLimit`）和一个失败的作业（通过`spec.failedJobsHistoryLimit`）。您可以根据自己的需求更改这些参数。
- en: Overall, CronJob supplement allows Jobs to automatically to run in your application
    with the desired timing. You can utilize CronJob to run some report generation
    Jobs, daily or weekly batch Jobs, and so on.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，CronJob补充允许作业根据所需的时间自动运行在您的应用程序中。您可以利用CronJob来运行一些报告生成作业，每日或每周批处理作业等。
- en: How it works...
  id: totrans-447
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Although Jobs and CronJob are the special utilities of Pods, the Kubernetes
    system has different management systems between them and Pods.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管作业和CronJob是Pod的特殊实用程序，但Kubernetes系统在它们和Pod之间有不同的管理系统。
- en: 'For Job, its selector cannot point to an existing pod. It is a bad idea to
    take a Pod controlled by the deployment/ReplicaSets as a Job. The deployment/ReplicaSets
    have a desired number of Pods running, which is against Job''s ideal situation:
    Pods should be deleted once they finish their tasks. The Pod in the Deployments/ReplicaSets
    won''t reach the state of end.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 对于作业，其选择器不能指向现有的Pod。将由部署/副本集控制的Pod作为作业是一个不好的主意。部署/副本集有一个期望数量的正在运行的Pod，这与作业的理想情况相违背：Pod应该在完成任务后被删除。部署/副本集中的Pod将无法达到结束状态。
- en: See also
  id: totrans-450
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'In this recipe, we executed Jobs and CronJob, demonstrating another usage of
    Kubernetes Pod that has a completion state. Even once a Pod is completed, Kubernetes
    can preserve the logs and Pod object so that you can retrieve the result easily.
    For further information, consider:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们执行了作业和CronJob，展示了Kubernetes Pod具有完成状态的另一种用法。即使Pod完成了，Kubernetes也可以保留日志和Pod对象，以便您可以轻松检索结果。有关更多信息，请参考：
- en: The *Working with Pods *recipe in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml),
    *Walking through Kubernetes Concepts*
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)中的*使用Pod*示例，*深入了解Kubernetes概念*'
- en: '*Working with configuration files *'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用配置文件*'
- en: Working with configuration files
  id: totrans-454
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用配置文件
- en: Kubernetes supports two different file formats, *YAML* and *JSON*. Each format
    can describe the same function of Kubernetes.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持两种不同的文件格式，*YAML*和*JSON*。每种格式都可以描述Kubernetes的相同功能。
- en: Getting ready
  id: totrans-456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Before we study how to write a Kubernetes configuration file, learning how to
    write a correct template format is important. We can learn the standard format
    of both YAML and JSON from their official websites.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习如何编写Kubernetes配置文件之前，学习如何编写正确的模板格式是很重要的。我们可以从它们的官方网站学习YAML和JSON的标准格式。
- en: YAML
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YAML
- en: 'The YAML format is very simple, with few syntax rules; therefore, it is easy
    to read and write, even for users. To know more about YAML, you can refer to the
    following website link: [http://www.yaml.org/spec/1.2/spec.html](http://www.yaml.org/spec/1.2/spec.html).
    The following example uses the YAML format to set up the `nginx` Pod:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: YAML格式非常简单，语法规则很少；因此，即使对用户来说，它也很容易阅读和编写。要了解更多关于YAML的信息，可以参考以下网站链接：[http://www.yaml.org/spec/1.2/spec.html](http://www.yaml.org/spec/1.2/spec.html)。以下示例使用YAML格式设置`nginx`
    Pod：
- en: '[PRE95]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: JSON
  id: totrans-461
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JSON
- en: 'The JSON format is also simple and easy to read for users, but more program-friendly.
    Because it has data types (number, string, Boolean, and object), it is popular
    to exchange the data between systems. Technically, YAML is a superset of JSON,
    so JSON is a valid YAML, but not the other way around. To know more about JSON,
    you can refer to the following website link: [http://json.org/](http://json.org/).'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: JSON格式对用户来说也很简单易读，但更适合程序使用。因为它具有数据类型（数字、字符串、布尔和对象），所以在系统之间交换数据很受欢迎。从技术上讲，YAML是JSON的超集，因此JSON是有效的YAML，但反之则不然。要了解更多关于JSON的信息，可以参考以下网站链接：[http://json.org/](http://json.org/)。
- en: 'The following example of the Pod is the same as the preceding YAML format,
    but using the JSON format:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Pod的示例与前面的YAML格式相同，但使用JSON格式：
- en: '[PRE96]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: How to do it...
  id: totrans-465
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作步骤
- en: 'Kubernetes has a schema that is defined using a verify configuration format;
    schema can be generated after the first instance of running the subcommand `create`
    with a configuration file. The cached schema will be stored under the `.kube/cache/discovery/<SERVICE_IP>_<PORT>`,
    based on the version of API server you run:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes有一个使用验证配置格式定义的模式；在使用配置文件运行子命令`create`的第一个实例后，可以生成模式。缓存的模式将根据您运行的API服务器版本存储在`.kube/cache/discovery/<SERVICE_IP>_<PORT>`下：
- en: '[PRE97]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Each directory listed represents an API category. You will see a file named
    `serverresources.json` under the last layer of each directory, which clearly defines
    every resource covered by this API category. However, there are some alternative
    and easier ways to check the schema. From the website of Kubernetes, we can get
    any details of how to write a configuration file of specific resources. Go ahead
    and check the official API documentation of the latest version: [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/).
    In the webpage, there are three panels: from left to right, they are the resource
    list, description, and the input and output of HTTP requests or the command kubectl.
    Taking Deployment as an example, you may click Deployment v1 app at the resource
    list, the leftmost panel, and the following screenshot will show up:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 每个列出的目录代表一个API类别。您将在每个目录的最后一层下看到一个名为`serverresources.json`的文件，其中清楚地定义了此API类别涵盖的每个资源。但是，还有一些替代和更简单的方法来检查模式。从Kubernetes的网站上，我们可以获取如何编写特定资源的配置文件的任何细节。继续并检查最新版本的官方API文档：[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/)。在网页上，有三个面板：从左到右，它们是资源列表，描述以及HTTP请求或kubectl命令的输入和输出。以Deployment为例，您可以在资源列表，最左边的面板中点击Deployment
    v1 app，然后会显示以下截图：
- en: '![](assets/cc7417bc-6efa-409f-bd4f-24b297969fd7.png)Documentation of Kubernetes
    Deployment API'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '**类型**：用户应始终为项目设置相应的类型。![](assets/cc7417bc-6efa-409f-bd4f-24b297969fd7.png)Kubernetes
    Deployment API文档的说明'
- en: 'But, how do we know the details of setting the container part at the marked
    place on the preceding image? In the field part of object description, there are
    two values. The first one, like apiVersion, means the name, and the second one,
    like string, is the type. Type could be integer, string, array, or the other resource
    object. Therefore, for searching the containers configuration of deployment, we
    need to know the structure of layers of objects. First, according to the example
    configuration file on web page, the layer of objects to containers is `spec.template.spec.containers.`
    So, start by clicking the hyperlink spec DeploymentSpec under Deployment''s fields,
    which is the type of resource object, and go searching hierarchically. Finally,
    you can find the details listed on this page: [https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core).'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 点击spec DeploymentSpec
- en: 'Solution for tracing the configuration of containers of DeploymentHere comes
    the solution for the preceding example:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪Deployment容器配置的解决方案
- en: Click spec DeploymentSpec
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在你明白了！
- en: Click template PodTemplateSpec
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击containers Container array
- en: Click spec PodSpec
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击template PodTemplateSpec
- en: Click containers Container array
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是前面示例的解决方案：
- en: Now you got it!
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 点击spec PodSpec
- en: 'Taking a careful look at the definition of container configuration. The following
    are some common descriptions you should pay attention to:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们如何知道在前面图像的标记位置设置容器部分的详细信息？在对象描述的字段部分，有两个值。第一个，如apiVersion，表示名称，第二个，如string，是类型。类型可以是整数、字符串、数组或其他资源对象。因此，要搜索部署的容器配置，我们需要了解对象的层次结构。首先，根据网页上的示例配置文件，到容器的对象层是`spec.template.spec.containers.`。因此，首先点击Deployment的字段下的超链接spec
    DeploymentSpec，这是资源对象的类型，并进行分层搜索。最后，您可以在此页面上找到列出的详细信息：[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/#container-v1-core)。
- en: '**Type**: The user should always set the corresponding type for an item.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细查看容器配置的定义。以下是一些您应该注意的常见描述：
- en: '**Optional or not**: Some items are indicated as optional, which means not
    necessary, and can be applied as a default value, or not set if you don''t specify
    it.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可选或不可选**：一些项目被标记为可选，这意味着不是必需的，并且可以作为默认值应用，或者如果你不指定它，就不设置它。 '
- en: '**Cannot be updated**: If the item is indicated as failed to be updated, it
    is fixed when the resource is created. You need to recreate a new one instead
    of updating it.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无法更新**：如果项目被指示为无法更新，则在创建资源时会被固定。你需要重新创建一个新的，而不是更新它。'
- en: '**Read-only**: Some of the items are indicated as `read-only`, such as UID.
    Kubernetes generates these items. If you specify this in the configuration file,
    it will be ignored.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**只读**：一些项目被标记为`只读`，比如UID。Kubernetes生成这些项目。如果你在配置文件中指定了这些项目，它们将被忽略。'
- en: 'Another method for checking the schema is through swagger UI. Kubernetes uses
    swagger ([https://](https://swagger.io)[swagger.io/](https://swagger.io)) and
    OpenAPI ([https://www.openapis.org](https://www.openapis.org)) to generate the
    REST API. Nevertheless, the web console for swagger is by default disabled in
    the API server. To enable the swagger UI of your own Kubernetes API server, just
    add the flag `--enable-swagger-ui=ture` when you start the API server. Then, by
    accessing the endpoint `https://<KUBERNETES_MASTER>:<API_SERVER_PORT>/swagger-ui`,
    you can successfully browse the API document through the web console:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 通过swagger UI来检查模式的另一种方法。Kubernetes使用swagger ([https://](https://swagger.io)[swagger.io/](https://swagger.io))
    和 OpenAPI ([https://www.openapis.org](https://www.openapis.org)) 来生成REST API。然而，默认情况下，swagger的web控制台在API服务器中是被禁用的。要启用你自己的Kubernetes
    API服务器的swagger UI，只需在启动API服务器时添加标志`--enable-swagger-ui=ture`。然后，通过访问端点`https://<KUBERNETES_MASTER>:<API_SERVER_PORT>/swagger-ui`，你可以成功地通过web控制台浏览API文档：
- en: '![](assets/6a34175a-0a30-4e6a-a20b-70392e605d69.png)The swagger web console
    of Kubernetes API'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/6a34175a-0a30-4e6a-a20b-70392e605d69.png)Kubernetes API的swagger
    web控制台'
- en: How it works...
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Let's introduce some necessary items in configuration files for creating Pod,
    Deployment, and Service.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们介绍一些在创建Pod、Deployment和Service的配置文件中必要的项目。
- en: Pod
  id: totrans-486
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod
- en: '| **Item** | **Type** | **Example** |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| **项目** | **类型** | **示例** |'
- en: '| `apiVersion` | String | `v1` |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| `apiVersion` | 字符串 | `v1` |'
- en: '| `kind` | String | `Pod` |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| `kind` | 字符串 | `Pod` |'
- en: '| `metadata.name` | String | `my-nginx-pod` |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| `metadata.name` | 字符串 | `my-nginx-pod` |'
- en: '| `spec` | `v1.PodSpec` |  |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| `spec` | `v1.PodSpec` |  |'
- en: '| `v1.PodSpec.containers` | Array[`v1.Container`] |  |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| `v1.PodSpec.containers` | 数组[`v1.Container`] |  |'
- en: '| `v1.Container.name` | String | `my-nginx` |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| `v1.Container.name` | 字符串 | `my-nginx` |'
- en: '| `v1.Container.image` | String | `nginx` |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '`v1.Container.image` | String | `nginx` |'
- en: Deployment
  id: totrans-495
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: '| **Item** | **Type** | **Example** |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| **项目** | **类型** | **示例** |'
- en: '| `apiVersion` | String | `apps`/`v1beta1` |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| `apiVersion` | 字符串 | `apps`/`v1beta1` |'
- en: '| `kind` | String | `Deployment` |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| `kind` | 字符串 | `Deployment` |'
- en: '| `metadata.name` | String | `my-nginx-deploy` |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| `metadata.name` | 字符串 | `my-nginx-deploy` |'
- en: '| `spec` | `v1.DeploymentSpec` |  |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| `spec` | `v1.DeploymentSpec` |  |'
- en: '| `v1.DeploymentSpec.template` | `v1.PodTemplateSpec` |  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| `v1.DeploymentSpec.template` | `v1.PodTemplateSpec` |  |'
- en: '| `v1.PodTemplateSpec.metadata.labels` | Map of string | `env: test` |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| `v1.PodTemplateSpec.metadata.labels` | 字符串映射 | `env: test` |'
- en: '| `v1.PodTemplateSpec.spec` | `v1.PodSpec` | `my-nginx` |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| `v1.PodTemplateSpec.spec` | `v1.PodSpec` | `my-nginx` |'
- en: '| `v1.PodSpec.containers` | Array[`v1.Container`] | As same as Pod |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| `v1.PodSpec.containers` | 数组[`v1.Container`] | 与Pod相同 |'
- en: Service
  id: totrans-505
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务
- en: '| **Item** | **Type** | **Example** |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| **项目** | **类型** | **示例** |'
- en: '| `apiVersion` | String | `v1` |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| `apiVersion` | 字符串 | `v1` |'
- en: '| `kind` | String | `Service` |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| `kind` | 字符串 | `Service` |'
- en: '| `metadata.name` | String | `my-nginx-svc` |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| `metadata.name` | 字符串 | `my-nginx-svc` |'
- en: '| `spec` | `v1.ServiceSpec` |  |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| `spec` | `v1.ServiceSpec` |  |'
- en: '| `v1.ServiceSpec.selector` | Map of string | `env: test` |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| `v1.ServiceSpec.selector` | 字符串映射 | `env: test` |'
- en: '| `v1.ServiceSpec.ports` | Array[`v1.ServicePort`] |  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| `v1.ServiceSpec.ports` | 数组[`v1.ServicePort`] |  |'
- en: '| `v1.ServicePort.protocol` | String | `TCP` |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| `v1.ServicePort.protocol` | 字符串 | `TCP` |'
- en: '| `v1.ServicePort.port` | Integer | `80` |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| `v1.ServicePort.port` | 整数 | `80` |'
- en: Please check the code bundle file `minimal-conf-resource.yaml` to find these
    three resources with minimal configuration.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 请检查代码包文件`minimal-conf-resource.yaml`，找到这三个资源的最小配置。
- en: See also
  id: totrans-516
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'This recipe described how to find and understand a configuration syntax. Kubernetes
    has some detailed options to define containers and components. For more details,
    the following recipes will describe how to define Pods, Deployments, and Services:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱描述了如何找到和理解配置语法。Kubernetes有一些详细的选项来定义容器和组件。更多细节，请参考以下食谱，将描述如何定义Pods、Deployments和Services：
- en: The *Working with Pods*, *Deployment API*, and *Working with Services* recipes
    in [Chapter 2](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml), *Walking through Kubernetes
    Concepts*
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第2章](e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml)的*使用Pods*、*部署API*和*使用服务*食谱，*Kubernetes概念概述*中。
