# 第四章：构建高可用性集群

在本章中，我们将涵盖以下配方：

+   集群化 etcd

+   构建多个主节点

# 介绍

避免单点故障是一个我们需要时刻牢记的概念。在本章中，您将学习如何在 Kubernetes 中构建具有高可用性的组件。我们还将介绍构建一个三节点 etcd 集群和多节点主节点的步骤。

# 集群化 etcd

etcd 在 Kubernetes 中存储网络信息和状态。任何数据丢失都可能是至关重要的。在生产环境中强烈建议对 etcd 进行集群化。etcd 支持集群化；N 个成员的集群可以容忍最多(N-1)/2 个故障。通常有三种创建 etcd 集群的机制。它们如下：

+   静态

+   etcd 发现

+   DNS 发现

如果我们在启动之前已经为所有 etcd 成员进行了配置，静态是一种简单的引导 etcd 集群的方法。然而，如果我们使用现有的 etcd 集群来引导新成员，那么发现方法就会发挥作用。发现服务使用现有集群来引导自身。它允许 etcd 集群中的新成员找到其他现有成员。在这个配方中，我们将讨论如何通过静态和 etcd 发现手动引导 etcd 集群。

我们在第一章中学习了如何使用 kubeadm 和 kubespray，*构建您自己的 Kubernetes 集群*。在撰写本文时，kubeadm 中的 HA 工作仍在进行中。官方文档建议定期备份您的 etcd 节点。另一个我们介绍的工具 kubespray 则原生支持多节点 etcd。在本章中，我们还将描述如何在 kubespray 中配置 etcd。

# 准备就绪

在我们学习更灵活的设置 etcd 集群之前，我们应该知道 etcd 目前有两个主要版本，即 v2 和 v3。etcd3 是一个旨在更稳定、高效和可靠的更新版本。以下是一个简单的比较，介绍它们实现中的主要区别：

|  | **etcd2** | **etcd3** |
| --- | --- | --- |
| **协议** | http | gRPC |
| **密钥过期** | TTL 机制 | 租约 |
| **观察者** | 通过 HTTP 进行长轮询 | 通过双向 gRPC 流 |

etcd3 旨在成为 etcd2 的下一代。etcd3 默认支持 gRPC 协议。gRPC 使用 HTTP2，允许在 TCP 连接上进行多个 RPC 流。然而，在 etcd2 中，每个 HTTP 请求必须在其进行的每个请求中建立连接。对于处理密钥到期，在 etcd2 中，TTL 附加到密钥；客户端应定期刷新密钥以查看是否有任何密钥已过期。这将建立大量连接。

在 etcd3 中，引入了租约的概念。租约可以附加多个键；当租约到期时，它将删除所有附加的键。对于观察者，etcd2 客户端通过 HTTP 创建长轮询-这意味着每次观察都会打开一个 TCP 连接。然而，etcd3 使用双向 gRPC 流实现，允许多个流共享同一个连接。

尽管 etcd3 更受青睐。但是，一些部署仍在使用 etcd2。我们仍然会介绍如何使用这些工具来实现集群，因为 etcd 中的数据迁移有很好的文档记录并且顺利。有关更多信息，请参考[`coreos.com/blog/migrating-applications-etcd-v3.html`](https://coreos.com/blog/migrating-applications-etcd-v3.html)上的升级迁移步骤。

在我们开始构建 etcd 集群之前，我们必须决定需要多少成员。etcd 集群的规模取决于您想要创建的环境。在生产环境中，建议至少有三个成员。然后，集群可以容忍至少一个永久性故障。在本教程中，我们将使用三个成员作为开发环境的示例：

| **名称/主机名** | **IP 地址** |
| --- | --- |
| `ip-172-31-3-80` | `172.31.3.80` |
| `ip-172-31-14-133` | `172.31.14.133` |
| `ip-172-31-13-239` | `172.31.13.239` |

其次，etcd 服务需要`端口 2379`（`4001`用于旧版本）用于 etcd 客户端通信，`端口 2380`用于对等通信。这些端口必须在您的环境中暴露。

# 如何做...

有很多方法可以提供 etcd 集群。通常，您会使用 kubespray、kops（在 AWS 中）或其他提供工具。

在这里，我们将简单地向您展示如何执行手动安装。这也很容易：

```
// etcd installation script
$ cat install-etcd.sh
ETCD_VER=v3.3.0

# ${DOWNLOAD_URL} could be ${GOOGLE_URL} or ${GITHUB_URL}
GOOGLE_URL=https://storage.googleapis.com/etcd
GITHUB_URL=https://github.com/coreos/etcd/releases/download
DOWNLOAD_URL=${GOOGLE_URL}

# delete tmp files
rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz
rm -rf /tmp/etcd && rm -rf /etc/etcd && mkdir -p /etc/etcd

curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz
tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /etc/etcd --strip-components=1
rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz

# check etcd version
/etc/etcd/etcd --version
```

此脚本将在`/etc/etcd`文件夹下放置`etcd`二进制文件。您可以自由地将它们放在不同的位置。在这种情况下，我们需要`sudo`来将它们放在`/etc`下：

```
// install etcd on linux
# sudo sh install-etcd.sh
…
etcd Version: 3.3.0
Git SHA: c23606781
Go Version: go1.9.3
Go OS/Arch: linux/amd64
```

我们现在使用的版本是 3.3.0。在检查`etcd`二进制文件在您的机器上工作后，我们可以将其附加到默认的`$PATH`上。然后我们就不需要每次执行`etcd`命令时都包含`/etc/etcd`路径了：

```
$ export PATH=/etc/etcd:$PATH
$ export ETCDCTL_API=3
```

您还可以将其放入您的`.bashrc`或`.bash_profile`中，以便默认设置它。

在至少有三个 etcd 服务器供应后，是时候让它们配对了。

# 静态机制

静态机制是设置集群的最简单方式。但是，每个成员的 IP 地址都应该事先知道。这意味着如果在云提供商环境中引导 etcd 集群，则静态机制可能不太实用。因此，etcd 还提供了一种发现机制，可以从现有集群中引导自己。

为了使 etcd 通信安全，etcd 支持 TLS 通道来加密对等方之间以及客户端和服务器之间的通信。每个成员都需要有一个唯一的密钥对。在本节中，我们将向您展示如何使用自动生成的证书来构建一个集群。

在 CoreOs GitHub 中，有一个方便的工具，我们可以用来生成自签名证书（[`github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup`](https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup)）。克隆存储库后，我们必须修改`config/req-csr.json`下的配置文件。这是一个例子：

```
// sample config, put under $repo/config/req-csr.json
$ cat config/req-csr.json
{
  "CN": "etcd",
  "hosts": [
    "172.31.3.80",
    "172.31.14.133",
    "172.31.13.239"
  ],
  "key": {
    "algo": "ecdsa",
    "size": 384
  },
  "names": [
    {
      "O": "autogenerated",
      "OU": "etcd cluster",
      "L": "the internet"
    }
  ]
}
```

在下一步中，我们需要安装并设置 Go（[`golang.org/`](https://golang.org/)）和设置`$GOPATH`：

```
$ export GOPATH=$HOME/go
$ make
```

然后证书将在`./certs/`下生成。

首先，我们必须设置一个引导配置来声明集群中将有哪些成员：

```
// set as environment variables, or alternatively, passing by –-initial-cluster and –-initial-cluster-state parameters inside launch command.
# ETCD_INITIAL_CLUSTER="etcd0=http://172.31.3.80:2380,etcd1=http://172.31.14.133:2380,etcd2=http://172.31.13.239:2380"
ETCD_INITIAL_CLUSTER_STATE=new
```

在所有三个节点中，我们都需要单独启动 etcd 服务器：

```
// first node: 172.31.3.80
# etcd --name etcd0 --initial-advertise-peer-urls https://172.31.3.80:2380 \
  --listen-peer-urls https://172.31.3.80:2380 \
  --listen-client-urls https://172.31.3.80:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.31.3.80:2379 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster etcd0=https://172.31.3.80:2380,etcd1=https://172.31.14.133:2380,etcd2=https://172.31.13.239:2380 \
  --initial-cluster-state new \
  --auto-tls \
  --peer-auto-tls

```

然后，您将看到以下输出：

```
2018-02-06 22:15:20.508687 I | etcdmain: etcd Version: 3.3.0
2018-02-06 22:15:20.508726 I | etcdmain: Git SHA: c23606781
2018-02-06 22:15:20.508794 I | etcdmain: Go Version: go1.9.3
2018-02-06 22:15:20.508824 I | etcdmain: Go OS/Arch: linux/amd64
…
2018-02-06 22:15:21.439067 N | etcdserver/membership: set the initial cluster version to 3.0
2018-02-06 22:15:21.439134 I | etcdserver/api: enabled capabilities for version 3.0

```

让我们唤醒第二个`etcd`服务：

```
// second node: 172.31.14.133
$ etcd --name etcd1 --initial-advertise-peer-urls https://172.31.14.133:2380 \
  --listen-peer-urls https://172.31.14.133:2380 \
  --listen-client-urls https://172.31.14.133:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.31.14.133:2379 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster etcd0=https://172.31.3.80:2380,etcd1=https://172.31.14.133:2380,etcd2=https://172.31.13.239:2380 \
  --initial-cluster-state new \
  --auto-tls \
  --peer-auto-tls
```

您将在控制台中看到类似的日志：

```
2018-02-06 22:15:20.646320 I | etcdserver: starting member ce7c9e3024722f01 in cluster a7e82f7083dba2c1
2018-02-06 22:15:20.646384 I | raft: ce7c9e3024722f01 became follower at term 0
2018-02-06 22:15:20.646397 I | raft: newRaft ce7c9e3024722f01 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]
2018-02-06 22:15:20.646403 I | raft: ce7c9e3024722f01 became follower at term 1
…
2018-02-06 22:15:20.675928 I | rafthttp: starting peer 25654e0e7ea045f8...
2018-02-06 22:15:20.676024 I | rafthttp: started HTTP pipelining with peer 25654e0e7ea045f8
2018-02-06 22:15:20.678515 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (writer)
2018-02-06 22:15:20.678717 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (writer)
```

它开始与我们之前的节点（`25654e0e7ea045f8`）配对。让我们在第三个节点中触发以下命令：

```
// third node: 172.31.13.239
$ etcd --name etcd2 --initial-advertise-peer-urls https://172.31.13.239:2380 \
  --listen-peer-urls https://172.31.13.239:2380 \
  --listen-client-urls https://172.31.13.239:2379,https://127.0.0.1:2379 \
  --advertise-client-urls https://172.31.13.239:2379 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-cluster etcd0=https://172.31.3.80:2380,etcd1=https://172.31.14.133:2380,etcd2=https://172.31.13.239:2380 \
  --initial-cluster-state new \
  --auto-tls \
  --peer-auto-tls

// in node2 console, it listens and receives new member (4834416c2c1e751e) added.
2018-02-06 22:15:20.679548 I | rafthttp: starting peer 4834416c2c1e751e...
2018-02-06 22:15:20.679642 I | rafthttp: started HTTP pipelining with peer 4834416c2c1e751e
2018-02-06 22:15:20.679923 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (stream Message reader)
2018-02-06 22:15:20.680190 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (stream MsgApp v2 reader)
2018-02-06 22:15:20.680364 I | rafthttp: started streaming with peer 4834416c2c1e751e (writer)
2018-02-06 22:15:20.681880 I | rafthttp: started peer 4834416c2c1e751e
2018-02-06 22:15:20.681909 I | rafthttp: added peer 4834416c2c1e751e
After all nodes are in, it'll start to elect the leader inside the cluster, we could find it in the logs:
2018-02-06 22:15:21.334985 I | raft: raft.node: ce7c9e3024722f01 elected leader 4834416c2c1e751e at term 27
...
2018-02-06 22:17:21.510271 N | etcdserver/membership: updated the cluster version from 3.0 to 3.3
2018-02-06 22:17:21.510343 I | etcdserver/api: enabled capabilities for version 3.3
```

集群已经设置好了。我们应该检查一下它是否正常工作：

```
$ etcdctl cluster-health
member 25654e0e7ea045f8is healthy: got healthy result from http://172.31.3.80:2379
member ce7c9e3024722f01 is healthy: got healthy result from http://172.31.14.133:2379
member 4834416c2c1e751e is healthy: got healthy result from http://172.31.13.239:2379
```

# 发现机制

发现提供了一种更灵活的方式来创建集群。它不需要预先知道其他对等 IP。它使用现有的 etcd 集群来引导一个新的集群。在本节中，我们将演示如何利用它来启动一个三节点的 etcd 集群：

1.  首先，我们需要有一个具有三节点配置的现有集群。幸运的是，`etcd`官方网站提供了一个发现服务（`https://discovery.etcd.io/new?size=n`）；n 将是您的`etcd`集群中节点的数量，它已经准备好使用：

```
// get a request URL
# curl -w "n" 'https://discovery.etcd.io/new?size=3'
https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8
```

1.  然后我们可以使用 URL 轻松地引导一个集群。命令行基本上与静态机制中的相同。我们需要做的是将`-initial-cluster`改为`-discovery`，用于指定发现服务的 URL：

```
// in node1, 127.0.0.1 is used for internal client listeneretcd -name ip-172-31-3-80 -initial-advertise-peer-urls http://172.31.3.80:2380  -listen-peer-urls http://172.31.3.80:2380  -listen-client-urls http://172.31.3.80:2379,http://127.0.0.1:2379  -advertise-client-urls http://172.31.3.80:2379  -discovery https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8

// in node2, 127.0.0.1 is used for internal client listener
etcd -name ip-172-31-14-133 -initial-advertise-peer-urls http://172.31.14.133:2380  -listen-peer-urls http://172.31.14.133:2380  -listen-client-urls http://172.31.14.133:2379,http://127.0.0.1:2379  -advertise-client-urls http://172.31.14.133:2379  -discovery https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8

// in node3, 127.0.0.1 is used for internal client listener
etcd -name ip-172-31-13-239 -initial-advertise-peer-urls http://172.31.13.239:2380  -listen-peer-urls http://172.31.13.239:2380  -listen-client-urls http://172.31.13.239:2379,http://127.0.0.1:2379  -advertise-client-urls http://172.31.13.239:2379  -discovery https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8
```

1.  让我们仔细看一下 node1 的日志：

```
2018-02-10 04:58:03.819963 I | etcdmain: etcd Version: 3.3.0
...
2018-02-10 04:58:03.820400 I | embed: listening for peers on http://172.31.3.80:2380
2018-02-10 04:58:03.820427 I | embed: listening for client requests on
127.0.0.1:2379
2018-02-10 04:58:03.820444 I | embed: listening for client requests on 172.31.3.80:2379
2018-02-10 04:58:03.947753 N | discovery: found self f60c98e749d41d1b in the cluster
2018-02-10 04:58:03.947771 N | discovery: found 1 peer(s), waiting for 2 more
2018-02-10 04:58:22.289571 N | discovery: found peer 6645fe871c820573 in the cluster
2018-02-10 04:58:22.289628 N | discovery: found 2 peer(s), waiting for 1 more
2018-02-10 04:58:36.907165 N | discovery: found peer 1ce61c15bdbb20b2 in the cluster
2018-02-10 04:58:36.907192 N | discovery: found 3 needed peer(s)
...
2018-02-10 04:58:36.931319 I | etcdserver/membership: added member 1ce61c15bdbb20b2 [http://172.31.13.239:2380] to cluster 29c0e2579c2f9563
2018-02-10 04:58:36.931422 I | etcdserver/membership: added member 6645fe871c820573 [http://172.31.14.133:2380] to cluster 29c0e2579c2f9563
2018-02-10 04:58:36.931494 I | etcdserver/membership: added member f60c98e749d41d1b [http://172.31.3.80:2380] to cluster 29c0e2579c2f9563
2018-02-10 04:58:37.116189 I | raft: f60c98e749d41d1b became leader at term 2
```

我们可以看到第一个节点等待其他两个成员加入，并将成员添加到集群中，在第 2 个任期的选举中成为了领导者：

1.  如果您检查其他服务器的日志，您可能会发现一些成员投票给了当前的领导者：

```
// in node 2
2018-02-10 04:58:37.118601 I | raft: raft.node: 6645fe871c820573 elected leader f60c98e749d41d1b at term 2
```

1.  我们还可以使用成员列表来检查当前的领导者：

```
# etcdctl member list
1ce61c15bdbb20b2: name=ip-172-31-13-239 peerURLs=http://172.31.13.239:2380 clientURLs=http://172.31.13.239:2379 isLeader=false
6645fe871c820573: name=ip-172-31-14-133 peerURLs=http://172.31.14.133:2380 clientURLs=http://172.31.14.133:2379 isLeader=false
f60c98e749d41d1b: name=ip-172-31-3-80 peerURLs=http://172.31.3.80:2380 clientURLs=http://172.31.3.80:2379 isLeader=true
```

1.  然后我们可以确认当前的领导者是`172.31.3.80`。我们还可以使用`etcdctl`来检查集群的健康状况：

```
# etcdctl cluster-health
member 1ce61c15bdbb20b2 is healthy: got healthy result from http://172.31.13.239:2379
member 6645fe871c820573 is healthy: got healthy result from http://172.31.14.133:2379
member f60c98e749d41d1b is healthy: got healthy result from http://172.31.3.80:2379
cluster is healthy
```

1.  如果我们通过`etcdctl`命令删除当前的领导者：

```
# etcdctl member remove f60c98e749d41d1b
```

1.  我们可能会发现当前的领导者已经改变了：

```
# etcdctl member list
1ce61c15bdbb20b2: name=ip-172-31-13-239 peerURLs=http://172.31.13.239:2380 clientURLs=http://172.31.13.239:2379 isLeader=false
6645fe871c820573: name=ip-172-31-14-133 peerURLs=http://172.31.14.133:2380 clientURLs=http://172.31.14.133:2379 isLeader=true
```

通过使用`etcd`发现，我们可以轻松地设置一个集群，`etcd`还为我们提供了许多 API 供我们使用。我们可以利用它来检查集群的统计信息：

1.  例如，使用`/stats/leader`来检查当前的集群视图：

```
# curl http://127.0.0.1:2379/v2/stats/leader
{"leader":"6645fe871c820573","followers":{"1ce61c15bdbb20b2":{"latency":{"current":0.002463,"average":0.0038775,"standardDeviation":0.0014144999999999997,"minimum":0.002463,"maximum":0.005292},"counts":{"fail":0,"success":2}}}}
```

有关 API 的更多信息，请查看官方 API 文档：[`coreos.com/etcd/docs/latest/v2/api.html`](https://coreos.com/etcd/docs/latest/v2/api.html)。

在 EC2 中构建集群

CoreOS 在 AWS 中构建了 CloudFormation 来帮助您动态地引导集群。我们所需要做的就是启动一个 CloudFormation 模板并设置参数，然后就可以开始了。模板中的资源包括自动扩展设置和网络入口（安全组）。请注意，这些 etcd 正在 CoreOS 上运行。要登录到服务器，首先您需要在 KeyPair 参数中设置您的密钥对名称，然后使用命令`ssh –i $your_keypair core@$ip`登录到服务器。

# kubeadm

如果您正在使用 kubeadm（[`github.com/kubernetes/kubeadm`](https://github.com/kubernetes/kubeadm)）来引导您的 Kubernetes 集群，不幸的是，在撰写本书时，HA 支持仍在进行中（v.1.10）。集群将作为单个主节点和单个配置的 etcd 创建。您需要定期备份 etcd 以保护您的数据。请参考官方 Kubernetes 网站上的 kubeadm 限制获取更多信息（[`kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations`](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations)）。

# kubespray

另一方面，如果您正在使用 kubespray 来配置服务器，kubespray 原生支持多节点 etcd。您需要在配置文件（`inventory.cfg`）的 etcd 部分中添加多个节点：

```
# cat inventory/inventory.cfg
my-master-1 ansible_ssh_host=<master_ip>
my-node-1 ansible_ssh_host=<node_ip>
my-etcd-1 ansible_ssh_host=<etcd1_ip>
my-etcd-2 ansible_ssh_host=<etcd2_ip>
my-etcd-3 ansible_ssh_host=<etcd3_ip>

[kube-master]
my-master-1

[etcd]
my-etcd-1
my-etcd-2
my-etcd-3

[kube-node]
my-master-1
my-node-1
```

然后，您可以使用三节点 etcd 来配置一个集群：

```
// provision a cluster 
$ ansible-playbook -b -i inventory/inventory.cfg cluster.yml
```

在启动 ansible playbook 后，它将配置角色，创建用户，检查第一个主节点是否已生成所有证书，并生成和分发证书。在部署结束时，ansible 将检查每个组件是否处于健康状态。

# Kops

Kops 是在 AWS 中创建 Kubernetes 集群的最有效方式。通过 kops 配置文件，您可以轻松在云上启动自定义集群。要构建一个 etcd 多节点集群，您可以在 kops 配置文件中使用以下部分：

```
etcdClusters:
  - etcdMembers:
    - instanceGroup: my-master-us-east-1a
      name: my-etcd-1
    - instanceGroup: my-master-us-east-1b
      name: my-etcd-2
    - instanceGroup: my-master-us-east-1c
      name: my-etcd-3
```

通常，instanceGroup 意味着一个自动扩展组。您还需要在配置文件中声明一个相关的 `intanceGroup my-master-us-east-1x`。我们将在第六章中了解更多信息，*在 AWS 上构建 Kubernetes*。默认情况下，kops 在撰写本书时仍然使用 etcd2；您可以在每个 `instanceGroup` 下的 kops 配置文件中添加一个版本键，例如 **version: 3.3.0**。

# 另请参阅

+   *通过使用 kubespray 在 Linux 上设置 Kubernetes 集群*，*构建您自己的 Kubernetes 集群*中的第一章。

+   *本章的构建多个主节点*部分

+   第六章，*在 AWS 上构建 Kubernetes*

+   *在第九章中处理 etcd 日志*，*日志和监控*

# 构建多个主节点

主节点在 Kubernetes 系统中充当内核组件。其职责包括以下内容：

1.  从 etcd 服务器推送和拉取信息

1.  作为请求的门户

1.  将任务分配给节点

1.  监控正在运行的任务

三个主要的守护程序使主节点能够完成前面的任务；以下图表显示了上述要点的活动：

![](img/a41c9c3b-78c3-4537-8cfd-93ba1c02d7aa.png)Kubernetes 主节点与其他组件之间的交互

正如你所看到的，主节点是工作节点和客户端之间的通信者。因此，如果主节点崩溃，这将是一个问题。多主 Kubernetes 系统不仅具有容错能力，而且负载均衡。如果其中一个崩溃，也不会有问题，因为其他主节点仍然会处理工作。我们将这种基础设施设计称为*高可用性*，缩写为 HA。为了支持 HA 结构，将不再只有一个 API 服务器用于访问数据存储和处理请求。在分离的主节点中有几个 API 服务器将有助于同时解决任务并缩短响应时间。

# 准备就绪

关于构建多主系统，有一些简要的想法你应该了解：

+   在主节点前添加一个负载均衡器服务器。负载均衡器将成为节点和客户端访问的新端点。

+   每个主节点都运行自己的 API 服务器。

+   系统中只有一个调度程序和一个控制器管理器有资格工作，这可以避免不同守护程序之间的冲突方向，同时管理容器。为了实现这一设置，我们在调度程序和控制器管理器中启用了`--leader-elect`标志。只有获得租约的人才能担任职务。

在这个配方中，我们将通过*kubeadm*构建一个双主系统，它具有类似的方法，同时可以扩展更多的主节点。用户也可以使用其他工具来构建高可用的 Kubernetes 集群。我们的目标是阐明一般概念。

在开始之前，除了主节点，您还应该在系统中准备其他必要的组件：

+   两台 Linux 主机，稍后将设置为主节点。这些机器应配置为 kubeadm 主节点。请参考第一章中的*kubeadm 配方在 Linux 上设置 Kubernetes 集群*，*构建您自己的 Kubernetes 集群*。您应该在两台主机上完成*软件包安装和系统配置先决条件*部分。

+   主服务器的负载均衡器。如果你在公共云上工作，比如 AWS 的 ELB 和 GCE 的负载均衡，那将会更容易。

+   一个 etcd 集群。请在本章中检查*集群化*etcd 的配方。

# 如何做…

我们将使用一个配置文件来运行定制的守护程序执行 kubeadm。请按照下一节的步骤将多个主节点作为一个组。

# 设置第一个主服务器

首先，我们将设置一个主服务器，为 HA 环境做好准备。与使用 kubeadm 运行集群的初始步骤一样，重要的是在开始时在主服务器上启用并启动 kubelet。然后它可以在`kube-system`命名空间中作为 pod 运行的守护程序：

```
// you are now in the terminal of host for first master
$ sudo systemctl enable kubelet && sudo systemctl start kubelet
```

接下来，让我们使用自定义的 kubeadm 配置文件启动主服务：

```
$ cat custom-init-1st.conf
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: "<FIRST_MASTER_IP>"
etcd:
  endpoints:
  - "<ETCD_CLUSTER_ENDPOINT>"
apiServerCertSANs:
- "<FIRST_MASTER_IP>"
- "<SECOND_MASTER_IP>"
- "<LOAD_BALANCER_IP>"
- "127.0.0.1"
token: "<CUSTOM_TOKEN: [a-z0-9]{6}.[a-z0-9]{16}>"
tokenTTL: "0"
apiServerExtraArgs:
  endpoint-reconciler-type: "lease"
```

这个配置文件有多个值需要与您的环境设置匹配。IP 的设置很直接。请注意，您现在正在设置第一个主服务器；`<FIRST_MASTER_IP>`变量将是您当前位置的物理 IP。`<ETCD_CLUSTER_ENDPOINT>`将以`"http://<IP>:<PORT>"`的格式，这将是 etcd 集群的负载均衡器。`<CUSTOM_TOKEN>`应该以指定的格式有效（例如，`123456.aaaabbbbccccdddd`）。在您分配所有变量以适应您的系统后，现在可以运行它了：

```
$ sudo kubeadm init --config=custom-init-1st.conf
```

你可能会收到“不支持交换”的错误消息。在`kubeadm init`命令中添加额外的`--ignore-preflight-errors=Swap`标志以避免这种中断。

确保在主服务器的两个文件中更新。

我们需要通过以下命令完成客户端功能：

```
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

就像在使用 kubeadm 运行单个主服务器集群时一样，如果没有容器网络接口，附加的`kube-dns`将始终处于挂起状态。我们将在演示中使用 CNI Calico。也可以应用适合 kubeadm 的其他 CNI：

```
$ kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml
```

现在您可以添加更多的主节点了。

# 使用现有证书设置其他主服务器

与上一节类似，让我们首先启动并启用`kubelet`：

```
// now you're in the second master
$ sudo systemctl enable kubelet && sudo systemctl start kubelet
```

在我们设置好第一个主服务器之后，我们应该与整个系统共享新生成的证书和密钥。这可以确保主服务器以相同的方式进行安全设置：

```
$ sudo scp -r root@$FIRST_MASTER_IP:/etc/kubernetes/pki/* /etc/kubernetes/pki/
```

您会发现一些文件，如证书或密钥，被复制到`/etc/kubernetes/pki/`目录中，只有 root 用户才能访问。但是，我们将删除`apiserver.crt`和`apiserver.key`文件。这是因为这些文件应该根据第二个主节点的主机名和 IP 生成，但共享的客户端证书`ca.crt`也参与了生成过程：

```
$ sudo rm /etc/kubernetes/pki/apiserver.*
```

接下来，在执行主节点初始化命令之前，请更改第二个主节点的配置文件中的 API 广告地址。它应该是第二个主节点的 IP，即您当前的主机。第二个主节点的配置文件与第一个主节点的配置文件非常相似。

不同之处在于我们应该指示`etcd`服务器的信息，并避免创建新的`etcd`集：

```
// Please modify the change by your case
$ cat custom-init-2nd.conf
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: "<SECOND_MASTER_IP>"
...
```

继续执行`kubeadm init`命令，记录`init`命令的最后一行显示的`kubeadm join`命令，以便稍后添加节点，并启用客户端 API 权限：

```
$ sudo kubeadm init --config custom-init-2nd.conf
// copy the "kubeadm join" command showing in the output
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

然后，检查当前节点；您会发现有两个主节点：

```
$ kubectl get nodes
NAME       STATUS    ROLES     AGE       VERSION
master01   Ready     master    8m        v1.10.2
master02   Ready     master    1m        v1.10.2
```

# 在 HA 集群中添加节点

一旦主节点准备就绪，您可以将节点添加到系统中。此节点应完成先决条件配置，作为 kubeadm 集群中的工作节点。并且，在开始时，您应该像主节点一样启动 kubelet：

```
// now you're in the second master
$ sudo systemctl enable kubelet && sudo systemctl start kubelet
```

之后，您可以继续并推送您复制的加入命令。但是，请将主节点 IP 更改为负载均衡器的 IP：

```
// your join command should look similar to following one
$ sudo kubeadm join --token <CUSTOM_TOKEN> <LOAD_BALANCER_IP>:6443 --discovery-token-ca-cert-hash sha256:<HEX_STRING>
```

然后，您可以跳转到第一个主节点或第二个主节点，以检查节点的状态：

```
// you can see the node is added
$ kubectl get nodes
NAME       STATUS    ROLES     AGE       VERSION
master01   Ready     master    4h        v1.10.2
master02   Ready     master    3h        v1.10.2
node01     Ready     <none>    22s       v1.10.2
```

# 它是如何工作的...

验证我们的 HA 集群，请查看`kube-system`命名空间中的 pod：

```
$ kubectl get pod -n kube-system
NAME                                      READY     STATUS    RESTARTS   AGE
calico-etcd-6bnrk                         1/1       Running   0          1d
calico-etcd-p7lpv                         1/1       Running   0          1d
calico-kube-controllers-d554689d5-qjht2   1/1       Running   0          1d
calico-node-2r2zs                         2/2       Running   0          1d
calico-node-97fjk                         2/2       Running   0          1d
calico-node-t55l8                         2/2       Running   0          1d
kube-apiserver-master01                   1/1       Running   0          1d
kube-apiserver-master02                   1/1       Running   0          1d
kube-controller-manager-master01          1/1       Running   0          1d
kube-controller-manager-master02          1/1       Running   0          1d
kube-dns-6f4fd4bdf-xbfvp                  3/3       Running   0          1d
kube-proxy-8jk69                          1/1       Running   0          1d
kube-proxy-qbt7q                          1/1       Running   0          1d
kube-proxy-rkxwp                          1/1       Running   0          1d
kube-scheduler-master01                   1/1       Running   0          1d
kube-scheduler-master02                   1/1       Running   0          1d
```

这些 pod 作为系统守护程序运行：Kubernetes 系统服务，如 API 服务器，Kubernetes 附加组件，如 DNS 服务器，以及 CNI 组件；在这里我们使用了 Calico。但等等！当您仔细查看 pod 时，您可能会好奇为什么控制器管理器和调度器在两个主节点上都在运行。在 HA 集群中不是只有一个吗？

正如我们在前一节中了解的那样，我们应该避免在 Kubernetes 系统中运行多个控制器管理器和多个调度器。这是因为它们可能同时尝试接管请求，这不仅会创建冲突，而且还会浪费计算资源。实际上，在使用 kubeadm 启动整个系统时，默认情况下会启动具有`leader-elect`标志的控制器管理器和调度器：

```
// check flag leader-elect on master node
$ sudo cat /etc/kubernetes/manifests/kube-controller-manager.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
...
    - --leader-elect=true...
```

您可能会发现调度程序也已设置为`leader-elect`。然而，为什么还会有多个 pod 呢？事实上，具有相同角色的其中一个 pod 是空闲的。我们可以通过查看系统端点获取详细信息：

```
// ep is the abbreviation of resource type "endpoints"
$ kubectl get ep -n kube-system
NAME                      ENDPOINTS                                   AGE
calico-etcd               192.168.122.201:6666,192.168.122.202:6666   1d
kube-controller-manager   <none>                                      1d
kube-dns                  192.168.241.67:53,192.168.241.67:53         1d
kube-scheduler            <none>                                      1d

// check endpoint of controller-manager with YAML output format
$ kubectl get ep kube-controller-manager -n kube-system -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"master01_bf4e22f7-4f56-11e8-aee3-52540048ed9b","leaseDurationSeconds":15,"acquireTime":"2018-05-04T04:51:11Z","renewTime":"2018-05-04T05:28:34Z","leaderTransitions":0}'
  creationTimestamp: 2018-05-04T04:51:11Z
  name: kube-controller-manager
  namespace: kube-system
  resourceVersion: "3717"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager
  uid: 5e2717b0-0609-11e8-b36f-52540048ed9b
```

以`kube-controller-manager`的端点为例：它没有任何 pod 或服务的虚拟 IP（与`kube-scheduler`相同）。如果我们深入研究这个端点，我们会发现`kube-controller-manager`的端点依赖于`annotations`来记录租约信息；它还依赖于`resourceVersion`来进行 pod 映射和传递流量。根据`kube-controller-manager`端点的注释，我们的第一个主节点控制了情况。让我们检查两个主节点上的控制器管理器：

```
// your pod should be named as kube-controller-manager-<HOSTNAME OF MASTER>
$ kubectl logs kube-controller-manager-master01 -n kube-system | grep "leader"
I0504 04:51:03.015151 1 leaderelection.go:175] attempting to acquire leader lease kube-system/kube-controller-manager...
...
I0504 04:51:11.627737 1 event.go:218] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"kube-controller-manager", UID:"5e2717b0-0609-11e8-b36f-52540048ed9b", APIVersion:"v1", ResourceVersion:"187", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' master01_bf4e22f7-4f56-11e8-aee3-52540048ed9b became leader
```

正如您所看到的，只有一个主节点作为领导者处理请求，而另一个节点持续存在，获取租约，但不执行任何操作。

为了进行进一步的测试，我们尝试删除当前的领导者 pod，看看会发生什么。通过`kubectl`请求删除系统 pod 的部署时，kubeadm Kubernetes 会创建一个新的，因为它保证会启动`/etc/kubernetes/manifests`目录下的任何应用程序。因此，为了避免 kubeadm 的自动恢复，我们将配置文件从清单目录中移除。这会使停机时间足够长，以放弃领导权：

```
// jump into the master node of leader
// temporary move the configuration file out of kubeadm's control
$ sudo mv /etc/kubernetes/manifests/kube-controller-manager.yaml ./
// check the endpoint
$ kubectl get ep kube-controller-manager -n kube-system -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"master02_4faf95c7-4f5b-11e8-bda3-525400b06612","leaseDurationSeconds":15,"acquireTime":"2018-05-04T05:37:03Z","renewTime":"2018-05-04T05:37:47Z","leaderTransitions":1}'
  creationTimestamp: 2018-05-04T04:51:11Z
  name: kube-controller-manager
  namespace: kube-system
  resourceVersion: "4485"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager
  uid: 5e2717b0-0609-11e8-b36f-52540048ed9b
subsets: null
```

`/etc/kubernetes/manifests`目录在 kubelet 中由`--pod-manifest-path`标志定义。检查`/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`，这是 kubelet 的系统守护程序配置文件，以及 kubelet 的帮助消息，以获取更多详细信息。

现在，轮到另一个节点唤醒其控制器管理器并让其工作了。一旦放回控制器管理器的配置文件，您会发现旧的领导者现在正在等待租约：

```
$ kubectl logs kube-controller-manager-master01 -n kube-system
I0504 05:40:10.218946 1 controllermanager.go:116] Version: v1.10.2
W0504 05:40:10.219688 1 authentication.go:55] Authentication is disabled
I0504 05:40:10.219702 1 insecure_serving.go:44] Serving insecurely on 127.0.0.1:10252
I0504 05:40:10.219965 1 leaderelection.go:175] attempting to acquire leader lease kube-system/kube-controller-manager...
```

# 另请参阅

在阅读本文之前，您应该已经掌握了通过 kubeadm 进行单主安装的基本概念。请参考这里提到的相关食谱，以了解如何自动构建多主系统的想法：

+   在 Linux 上通过 kubeadm 设置 Kubernetes 集群*在第一章中，*构建您自己的 Kubernetes 集群*

+   etcd 集群
