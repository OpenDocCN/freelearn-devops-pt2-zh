- en: Building High-Availability Clusters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建高可用性集群
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下配方：
- en: Clustering etcd
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群化etcd
- en: Building multiple masters
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建多个主节点
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Avoiding a single point of failure is a concept we need to always keep in mind.
    In this chapter, you will learn how to build components in Kubernetes with high
    availability. We will also go through the steps to build a three-node etcd cluster
    and masters with multinodes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 避免单点故障是一个我们需要时刻牢记的概念。在本章中，您将学习如何在Kubernetes中构建具有高可用性的组件。我们还将介绍构建一个三节点etcd集群和多节点主节点的步骤。
- en: Clustering etcd
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群化etcd
- en: 'etcd stores network information and states in Kubernetes. Any data loss could
    be crucial. Clustering etcd is strongly recommended in a production environment.
    etcd comes with support for clustering; a cluster of N members can tolerate up
    to (N-1)/2 failures. Typically, there are three mechanisms for creating an etcd
    cluster. They are as follows:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: etcd在Kubernetes中存储网络信息和状态。任何数据丢失都可能是至关重要的。在生产环境中强烈建议对etcd进行集群化。etcd支持集群化；N个成员的集群可以容忍最多(N-1)/2个故障。通常有三种创建etcd集群的机制。它们如下：
- en: Static
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态
- en: etcd discovery
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd发现
- en: DNS discovery
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS发现
- en: Static is a simple way to bootstrap an etcd cluster if we have all etcd members
    provisioned before starting. However, it's more common if we use an existing etcd
    cluster to bootstrap a new member. Then, the discovery method comes into play.
    The discovery service uses an existing cluster to bootstrap itself. It allows
    a new member in an etcd cluster to find other existing members. In this recipe,
    we will discuss how to bootstrap an etcd cluster via static and etcd discovery
    manually.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在启动之前已经为所有etcd成员进行了配置，静态是一种简单的引导etcd集群的方法。然而，如果我们使用现有的etcd集群来引导新成员，那么发现方法就会发挥作用。发现服务使用现有集群来引导自身。它允许etcd集群中的新成员找到其他现有成员。在这个配方中，我们将讨论如何通过静态和etcd发现手动引导etcd集群。
- en: We learned how to use kubeadm and kubespray in [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml),
    *Building Your Own Kubernetes Cluster*. At the time of writing, HA work in kubeadm
    is still in progress. Regularly backing up your etcd node is recommended in the
    official documentation. The other tool we introduced, kubespray, on the other
    hand, supports multi-nodes etcd natively. In this chapter, we'll also describe
    how to configure etcd in kubespray.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)中学习了如何使用kubeadm和kubespray，*构建您自己的Kubernetes集群*。在撰写本文时，kubeadm中的HA工作仍在进行中。官方文档建议定期备份您的etcd节点。另一个我们介绍的工具kubespray则原生支持多节点etcd。在本章中，我们还将描述如何在kubespray中配置etcd。
- en: Getting ready
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Before we learn a more flexible way to set up an etcd cluster, we should know
    etcd comes with two major versions so far, which are v2 and v3\. etcd3 is a newer
    version that aims to be more stable, efficient, and reliable. Here is a simple
    comparison to introduce the major differences in their implementation:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习更灵活的设置etcd集群之前，我们应该知道etcd目前有两个主要版本，即v2和v3。etcd3是一个旨在更稳定、高效和可靠的更新版本。以下是一个简单的比较，介绍它们实现中的主要区别：
- en: '|  | **etcd2** | **etcd3** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  | **etcd2** | **etcd3** |'
- en: '| **Protocol** | http | gRPC |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **协议** | http | gRPC |'
- en: '| **Key expiration** | TTL mechanism | Leases |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **密钥过期** | TTL机制 | 租约 |'
- en: '| **Watchers** | Long polling over HTTP | Via a bidirectional gRPC stream |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **观察者** | 通过HTTP进行长轮询 | 通过双向gRPC流 |'
- en: etcd3 aims to be the next generation of etcd2 . etcd3 supports the gRPC protocol
    by default. gRPC uses HTTP2, which allows multiple RPC streams over a TCP connection.
    In etcd2, however, a HTTP request must establish a connection in every request
    it makes. For dealing with key expiration, in etcd2, a TTL attaches to a key;
    the client should periodically refresh the keys to see if any keys have expired.
    This will establish lots of connections.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: etcd3旨在成为etcd2的下一代。etcd3默认支持gRPC协议。gRPC使用HTTP2，允许在TCP连接上进行多个RPC流。然而，在etcd2中，每个HTTP请求必须在其进行的每个请求中建立连接。对于处理密钥到期，在etcd2中，TTL附加到密钥；客户端应定期刷新密钥以查看是否有任何密钥已过期。这将建立大量连接。
- en: In etcd3, the lease concept was introduced. A lease can attach multiple keys;
    when a lease expires, it'll delete all attached keys. For the watcher, the etcd2
    client creates long polling over HTTP—this means a TCP connection is opened per
    watch. However, etcd3 uses bidirectional gRPC stream implementation, which allows
    multiple steams to share the same connection.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在etcd3中，引入了租约的概念。租约可以附加多个键；当租约到期时，它将删除所有附加的键。对于观察者，etcd2客户端通过HTTP创建长轮询-这意味着每次观察都会打开一个TCP连接。然而，etcd3使用双向gRPC流实现，允许多个流共享同一个连接。
- en: Although etcd3 is preferred. However, some deployments still use etcd2\. We'll
    still introduce how to use those tools to achieve clustering, since data migration
    in etcd is well-documented and smooth. For more information, please refer to the
    upgrade migration steps at [https://coreos.com/blog/migrating-applications-etcd-v3.html](https://coreos.com/blog/migrating-applications-etcd-v3.html).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管etcd3更受青睐。但是，一些部署仍在使用etcd2。我们仍然会介绍如何使用这些工具来实现集群，因为etcd中的数据迁移有很好的文档记录并且顺利。有关更多信息，请参考[https://coreos.com/blog/migrating-applications-etcd-v3.html](https://coreos.com/blog/migrating-applications-etcd-v3.html)上的升级迁移步骤。
- en: 'Before we start building an etcd cluster, we have to decide how many members
    we need. How big the etcd cluster should be really depends on the environment
    you want to create. In the production environment, at least three members are
    recommended. Then, the cluster can tolerate at least one permanent failure. In
    this recipe, we will use three members as an example of a development environment:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建etcd集群之前，我们必须决定需要多少成员。etcd集群的规模取决于您想要创建的环境。在生产环境中，建议至少有三个成员。然后，集群可以容忍至少一个永久性故障。在本教程中，我们将使用三个成员作为开发环境的示例：
- en: '| **Name/hostname** | **IP address** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **名称/主机名** | **IP地址** |'
- en: '| `ip-172-31-3-80` | `172.31.3.80` |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `ip-172-31-3-80` | `172.31.3.80` |'
- en: '| `ip-172-31-14-133` | `172.31.14.133` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `ip-172-31-14-133` | `172.31.14.133` |'
- en: '| `ip-172-31-13-239` | `172.31.13.239` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `ip-172-31-13-239` | `172.31.13.239` |'
- en: Secondly, the etcd service requires `port 2379` (`4001` for legacy uses) for
    etcd client communication and `port 2380` for peer communication. These ports
    have to be exposed in your environment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，etcd服务需要`端口2379`（`4001`用于旧版本）用于etcd客户端通信，`端口2380`用于对等通信。这些端口必须在您的环境中暴露。
- en: How to do it...
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: There are plenty of ways to provision an etcd cluster. Normally, you'll use
    kubespray, kops (in AWS), or other provisioning tools.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以提供etcd集群。通常，您会使用kubespray、kops（在AWS中）或其他提供工具。
- en: 'Here, we''ll simply show you how to perform a manual install. It''s fairly
    easy as well:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将简单地向您展示如何执行手动安装。这也很容易：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This script will put `etcd` binary under `/etc/etcd` folder. You''re free to
    put them in different place. We''ll need `sudo` in order to put them under `/etc`
    in this case:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本将在`/etc/etcd`文件夹下放置`etcd`二进制文件。您可以自由地将它们放在不同的位置。在这种情况下，我们需要`sudo`来将它们放在`/etc`下：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The version we''re using now is 3.3.0\. After we check the `etcd` binary work
    on your machine, we can attach it to the default `$PATH` as follows. Then we don''t
    need to include the`/etc/etcd` path every time we execute the `etcd` command:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用的版本是3.3.0。在检查`etcd`二进制文件在您的机器上工作后，我们可以将其附加到默认的`$PATH`上。然后我们就不需要每次执行`etcd`命令时都包含`/etc/etcd`路径了：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You also can put it into your `.bashrc` or `.bash_profile` to let it set by
    default.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将其放入您的`.bashrc`或`.bash_profile`中，以便默认设置它。
- en: After we have at least three etcd servers provisioned, it's time to make them
    pair together.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在至少有三个etcd服务器供应后，是时候让它们配对了。
- en: Static mechanism
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 静态机制
- en: A static mechanism is the easiest way to set up a cluster. However, the IP address
    of every member should be known beforehand. This means that if you bootstrap an
    etcd cluster in a cloud provider environment, the static mechanism might not be
    so practical. Therefore, etcd also provides a discovery mechanism to bootstrap
    itself from the existing cluster.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 静态机制是设置集群的最简单方式。但是，每个成员的IP地址都应该事先知道。这意味着如果在云提供商环境中引导etcd集群，则静态机制可能不太实用。因此，etcd还提供了一种发现机制，可以从现有集群中引导自己。
- en: To make etcd communications secure, etcd supports TLS channels to encrypt the
    communication between peers, and also clients and servers. Each member needs to
    have a unique key pair. In this section, we'll show you how to use automatically
    generated certificates to build a cluster.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使etcd通信安全，etcd支持TLS通道来加密对等方之间以及客户端和服务器之间的通信。每个成员都需要有一个唯一的密钥对。在本节中，我们将向您展示如何使用自动生成的证书来构建一个集群。
- en: 'In CoreOs GitHub, there is a handy tool we can use to generate self-signed
    certificates ([https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup](https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup))
    . After cloning the repo, we have to modify a configuration file under `config/req-csr.json`.
    Here is an example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在CoreOs GitHub中，有一个方便的工具，我们可以用来生成自签名证书（[https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup](https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup)）。克隆存储库后，我们必须修改`config/req-csr.json`下的配置文件。这是一个例子：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the next step we''ll need to have Go ([https://golang.org/](https://golang.org/))
    installed and set up `$GOPATH`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们需要安装并设置Go（[https://golang.org/](https://golang.org/)）和设置`$GOPATH`：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then the certs will be generated under `./certs/`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后证书将在`./certs/`下生成。
- en: 'First, we''ll have to set a bootstrap configuration to declare what members
    will be inside the cluster:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须设置一个引导配置来声明集群中将有哪些成员：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In all three nodes, we''ll have to launch the etcd server separately:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有三个节点中，我们都需要单独启动etcd服务器：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, you''ll see the following output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将看到以下输出：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s wake up the second `etcd` service:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们唤醒第二个`etcd`服务：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You''ll see similar logs in the console:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在控制台中看到类似的日志：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It starts pairing with our previous node (`25654e0e7ea045f8`). Let''s trigger
    the following command in the third node:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 它开始与我们之前的节点（`25654e0e7ea045f8`）配对。让我们在第三个节点中触发以下命令：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the cluster is set. We should check to see if it works properly:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 集群已经设置好了。我们应该检查一下它是否正常工作：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Discovery  mechanism
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现机制
- en: 'Discovery provides a more flexible way to create a cluster. It doesn''t need
    to know other peer IPs beforehand. It uses an existing etcd cluster to bootstrap
    one. In this section, we''ll demonstrate how to leverage that to launch a three-node
    etcd cluster:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 发现提供了一种更灵活的方式来创建集群。它不需要预先知道其他对等IP。它使用现有的etcd集群来引导一个新的集群。在本节中，我们将演示如何利用它来启动一个三节点的etcd集群：
- en: 'Firstly, we''ll need to have an existing cluster with three-node configuration.
    Luckily, the `etcd` official website provides a discovery service (`https://discovery.etcd.io/new?size=n`);
    n will be the number of nodes in your `etcd` cluster, which is ready to use:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要有一个具有三节点配置的现有集群。幸运的是，`etcd`官方网站提供了一个发现服务（`https://discovery.etcd.io/new?size=n`）；n将是您的`etcd`集群中节点的数量，它已经准备好使用：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we are able to use the URL to bootstrap a cluster easily. The command
    line is pretty much the same as in the static mechanism. What we need to do is
    `change –initial-cluster` to `–discovery`, which is used to specify the discovery
    service URL:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以使用URL轻松地引导一个集群。命令行基本上与静态机制中的相同。我们需要做的是将`-initial-cluster`改为`-discovery`，用于指定发现服务的URL：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s take a closer look at node1''s log:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们仔细看一下node1的日志：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can see that the first node waited for the other two members to join, and
    added member to cluster, became the leader in the election at term 2:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到第一个节点等待其他两个成员加入，并将成员添加到集群中，在第2个任期的选举中成为了领导者：
- en: 'If you check the other server''s log, you might find a clue to the effect that
    some members voted for the current leader:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您检查其他服务器的日志，您可能会发现一些成员投票给了当前的领导者：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also use member lists to check the current leader:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用成员列表来检查当前的领导者：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then we can confirm the current leader is `172.31.3.80`. We can also use `etcdctl`
    to check cluster health:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以确认当前的领导者是`172.31.3.80`。我们还可以使用`etcdctl`来检查集群的健康状况：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we remove the current leader by `etcdctl` command:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们通过`etcdctl`命令删除当前的领导者：
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We may find that the current leader has been changed:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可能会发现当前的领导者已经改变了：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'By using `etcd` discovery, we can set up a cluster painlessly `etcd` also provides
    lots of APIs for us to use. We can leverage it to check cluster statistics:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`etcd`发现，我们可以轻松地设置一个集群，`etcd`还为我们提供了许多API供我们使用。我们可以利用它来检查集群的统计信息：
- en: 'For example, use `/stats/leader` to check the current cluster view:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，使用`/stats/leader`来检查当前的集群视图：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For more information about APIs, check out the official API document: [https://coreos.com/etcd/docs/latest/v2/api.html](https://coreos.com/etcd/docs/latest/v2/api.html).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有关API的更多信息，请查看官方API文档：[https://coreos.com/etcd/docs/latest/v2/api.html](https://coreos.com/etcd/docs/latest/v2/api.html)。
- en: Building a cluster in EC2
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在EC2中构建集群
- en: CoreOS builds CloudFormation in AWS to help you bootstrap your cluster in AWS
    dynamically. What we have to do is just launch a CloudFormation template and set
    the parameters, and we're good to go. The resources in the template contain AutoScaling
    settings and network ingress (security group). Note that these etcds are running
    on CoreOS. To log in to the server, firstly you'll have to set your keypair name
    in the KeyPair parameter, then use the command `ssh –i $your_keypair core@$ip `to
    log in to the server.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS在AWS中构建了CloudFormation来帮助您动态地引导集群。我们所需要做的就是启动一个CloudFormation模板并设置参数，然后就可以开始了。模板中的资源包括自动扩展设置和网络入口（安全组）。请注意，这些etcd正在CoreOS上运行。要登录到服务器，首先您需要在KeyPair参数中设置您的密钥对名称，然后使用命令`ssh
    –i $your_keypair core@$ip`登录到服务器。
- en: kubeadm
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kubeadm
- en: If you're using kubeadm ([https://github.com/kubernetes/kubeadm](https://github.com/kubernetes/kubeadm))
    to bootstrap your Kubernetes cluster, unfortunately, at the time of writing, HA
    support is still in progress (v.1.10). The cluster is created as a single master
    with a single etcd configured. You'll have to back up etcd regularly to secure
    your data. Refer to the kubeadm limitations at the official Kubernetes website
    for more information ([https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations))).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用 kubeadm（[https://github.com/kubernetes/kubeadm](https://github.com/kubernetes/kubeadm)）来引导您的
    Kubernetes 集群，不幸的是，在撰写本书时，HA 支持仍在进行中（v.1.10）。集群将作为单个主节点和单个配置的 etcd 创建。您需要定期备份
    etcd 以保护您的数据。请参考官方 Kubernetes 网站上的 kubeadm 限制获取更多信息（[https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations)）。
- en: kubespray
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kubespray
- en: 'On the other hand, if you''re using kubespray to provision your servers, kubespray
    supports multi-node etcd natively. What you need to do is add multiple nodes in
    the etcd section in the configuration file (`inventory.cfg`):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您正在使用 kubespray 来配置服务器，kubespray 原生支持多节点 etcd。您需要在配置文件（`inventory.cfg`）的
    etcd 部分中添加多个节点：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then you are good to provision a cluster with three-node etcd:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用三节点 etcd 来配置一个集群：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: After the ansible playbook is launched, it will configure the role, create the
    user, check if all certs have already been generated in the first master, and
    generate and distribute the certs. At the end of the deployment, ansible will
    check if every component is in a healthy state.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动 ansible playbook 后，它将配置角色，创建用户，检查第一个主节点是否已生成所有证书，并生成和分发证书。在部署结束时，ansible
    将检查每个组件是否处于健康状态。
- en: Kops
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kops
- en: 'Kops is the most efficient way to create Kubernetes clusters in AWS. Via the
    kops configuration file, you can easily launch a custom cluster on the cloud.
    To build an etcd multi-node cluster, you could use the following section inside
    the kops configuration file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Kops 是在AWS中创建Kubernetes集群的最有效方式。通过kops配置文件，您可以轻松在云上启动自定义集群。要构建一个etcd多节点集群，您可以在kops配置文件中使用以下部分：
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Normally, an instanceGroup means an auto-scaling group. You''ll have to declare
    a related `intanceGroup my-master-us-east-1x` in the configuration file as well.
    We''ll learn more about it in [Chapter 6](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml),
    *Building Kubernetes on AWS*. By default, kops still uses etcd2 at the time this
    book is being written; you could add a version key inside the kops configuration
    file, such as **version: 3.3.0**, under each `instanceGroup`.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，instanceGroup 意味着一个自动扩展组。您还需要在配置文件中声明一个相关的 `intanceGroup my-master-us-east-1x`。我们将在[第6章](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml)中了解更多信息，*在AWS上构建Kubernetes*。默认情况下，kops
    在撰写本书时仍然使用 etcd2；您可以在每个 `instanceGroup` 下的 kops 配置文件中添加一个版本键，例如 **version: 3.3.0**。'
- en: See also
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*Setting up Kubernetes clusters on Linux by using kubespray* in [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml), *Building
    Your Own Kubernetes Cluster*'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过使用kubespray在Linux上设置Kubernetes集群*，*构建您自己的Kubernetes集群*中的[第1章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)。'
- en: '*The Building multiple masters* section of  this chapter'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*本章的构建多个主节点*部分'
- en: '[Chapter 6](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml), *Building Kubernetes
    on AWS*'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第6章](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml)，*在AWS上构建Kubernetes*'
- en: '*Working with etcd logs* in [Chapter 9](54bceded-1d48-4d1a-bdb3-e3d659940411.xhtml),
    *Logging and Monitoring*'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在[第9章](54bceded-1d48-4d1a-bdb3-e3d659940411.xhtml)中处理etcd日志*，*日志和监控*'
- en: Building multiple masters
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建多个主节点
- en: 'The master node serves as a kernel component in the Kubernetes system. Its
    duties include the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点在Kubernetes系统中充当内核组件。其职责包括以下内容：
- en: Pushing and pulling information from etcd servers
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从etcd服务器推送和拉取信息
- en: Acting as the portal for requests
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为请求的门户
- en: Assigning tasks to nodes
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将任务分配给节点
- en: Monitoring the running tasks
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控正在运行的任务
- en: 'Three major daemons enable the master to fulfill the preceding duties; the
    following diagram indicates the activities of the aforementioned bullet points:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 三个主要的守护程序使主节点能够完成前面的任务；以下图表显示了上述要点的活动：
- en: '![](assets/a41c9c3b-78c3-4537-8cfd-93ba1c02d7aa.png)The interaction between
    the Kubernetes master and other components'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/a41c9c3b-78c3-4537-8cfd-93ba1c02d7aa.png)Kubernetes主节点与其他组件之间的交互'
- en: As you can see, the master is the communicator between workers and clients.
    Therefore, it will be a problem if the master crashes. A multiple-master Kubernetes
    system is not only fault tolerant, but also workload-balanced. It would not be
    an issue if one of them crashed, since other masters would still handle the jobs.
    We call this infrastructure design *high availability*, abbreviated to HA. In
    order to support HA structures, there will no longer be only one API server for
    accessing datastores and handling requests. Several API servers in separated master
    nodes would help to solve tasks simultaneously and shorten the response time.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，主节点是工作节点和客户端之间的通信者。因此，如果主节点崩溃，这将是一个问题。多主Kubernetes系统不仅具有容错能力，而且负载均衡。如果其中一个崩溃，也不会有问题，因为其他主节点仍然会处理工作。我们将这种基础设施设计称为*高可用性*，缩写为HA。为了支持HA结构，将不再只有一个API服务器用于访问数据存储和处理请求。在分离的主节点中有几个API服务器将有助于同时解决任务并缩短响应时间。
- en: Getting ready
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'There are some brief ideas you should understand about building a multiple-master
    system:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 关于构建多主系统，有一些简要的想法你应该了解：
- en: Add a load balancer server in front of the masters. The load balancer will become
    the new endpoint accessed by nodes and clients.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主节点前添加一个负载均衡器服务器。负载均衡器将成为节点和客户端访问的新端点。
- en: Every master runs its own API server.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个主节点都运行自己的API服务器。
- en: Only one scheduler and one controller manager are eligible to work in the system,
    which can avoid conflicting directions from different daemons while managing containers.
    To achieve this setup, we enable the `--leader-elect` flag in the scheduler and
    controller manager. Only the one getting the lease can take duties.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统中只有一个调度程序和一个控制器管理器有资格工作，这可以避免不同守护程序之间的冲突方向，同时管理容器。为了实现这一设置，我们在调度程序和控制器管理器中启用了`--leader-elect`标志。只有获得租约的人才能担任职务。
- en: In this recipe, we are going to build a two-master system via *kubeadm*, which
    has similar methods while scaling more masters. Users may also use other tools
    to build up HA Kubernetes clusters. Our target is to illustrate the general concepts.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将通过*kubeadm*构建一个双主系统，它具有类似的方法，同时可以扩展更多的主节点。用户也可以使用其他工具来构建高可用的Kubernetes集群。我们的目标是阐明一般概念。
- en: 'Before starting, in addition to master nodes, you should prepare other necessary
    components in the systems:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，除了主节点，您还应该在系统中准备其他必要的组件：
- en: Two Linux hosts, which will be set up as master nodes later. These machines
    should be configured as kubeadm masters. Please refer to the *Setting up Kubernetes
    clusters on Linux by kubeadm recipe* in [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml), *Building
    Your Own Kubernetes Cluster*. You should finish the *Package installation and
    System configuring prerequisites* parts on both hosts.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两台Linux主机，稍后将设置为主节点。这些机器应配置为kubeadm主节点。请参考[第1章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)中的*kubeadm配方在Linux上设置Kubernetes集群*，*构建您自己的Kubernetes集群*。您应该在两台主机上完成*软件包安装和系统配置先决条件*部分。
- en: A LoadBalancer for masters. It would be much easier if you worked on the public
    cloud, that's said EL*B* of AWS and Load balancing of GCE.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主服务器的负载均衡器。如果你在公共云上工作，比如AWS的ELB和GCE的负载均衡，那将会更容易。
- en: An etcd cluster. Please check the *Clustering* *etcd *recipe in this chapter.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个etcd集群。请在本章中检查*集群化*etcd的配方。
- en: How to do it...
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: We will use a configuration file to run kubeadm for customized daemon execution.
    Please follow the next sections to make multiple master nodes as a group.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个配置文件来运行定制的守护程序执行kubeadm。请按照下一节的步骤将多个主节点作为一个组。
- en: Setting up the first master
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置第一个主服务器
- en: 'First, we are going to set up a master, ready for the HA environment. Like
    the initial step, running a cluster by using kubeadm, it is important to enable
    and start kubelet on the master at the beginning. It can then take daemons running
    as pods in the `kube-system` namespace:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将设置一个主服务器，为HA环境做好准备。与使用kubeadm运行集群的初始步骤一样，重要的是在开始时在主服务器上启用并启动kubelet。然后它可以在`kube-system`命名空间中作为pod运行的守护程序：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, let''s start the master services with the custom kubeadm configuration
    file:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用自定义的kubeadm配置文件启动主服务：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This configuration file has multiple values required to match your environment
    settings. The IP ones are straightforward. Be aware that you are now setting the
    first master; the `<FIRST_MASTER_IP>` variable will be the physical IP of your
    current location. `<ETCD_CLUSTER_ENDPOINT>` will be in a format like `"http://<IP>:<PORT>"`,
    which will be the load balancer of the etcd cluster. `<CUSTOM_TOKEN>` should be
    valid in the specified format (for example, `123456.aaaabbbbccccdddd`). After
    you allocate all variables aligning to your system, you can run it now:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置文件有多个值需要与您的环境设置匹配。IP的设置很直接。请注意，您现在正在设置第一个主服务器；`<FIRST_MASTER_IP>`变量将是您当前位置的物理IP。`<ETCD_CLUSTER_ENDPOINT>`将以`"http://<IP>:<PORT>"`的格式，这将是etcd集群的负载均衡器。`<CUSTOM_TOKEN>`应该以指定的格式有效（例如，`123456.aaaabbbbccccdddd`）。在您分配所有变量以适应您的系统后，现在可以运行它了：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You may get the Swap is not supported error message. Add an additional `--ignore-preflight-errors=Swap` flag
    with `kubeadm init` to avoid this interruption.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会收到“不支持交换”的错误消息。在`kubeadm init`命令中添加额外的`--ignore-preflight-errors=Swap`标志以避免这种中断。
- en: Make sure to update in both files of the masters.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在主服务器的两个文件中更新。
- en: 'We need to complete client functionality via the following commands:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通过以下命令完成客户端功能：
- en: '[PRE27]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Like when running a single master cluster via kubeadm, without a container
    network interface the add-on `kube-dns` will always have a pending status. We
    will use CNI Calico for our demonstration. It is fine to apply the other CNI which
    is suitable to kubeadm:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在使用kubeadm运行单个主服务器集群时一样，如果没有容器网络接口，附加的`kube-dns`将始终处于挂起状态。我们将在演示中使用CNI Calico。也可以应用适合kubeadm的其他CNI：
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now it is OK for you to add more master nodes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以添加更多的主节点了。
- en: Setting up the other master with existing certifications
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用现有证书设置其他主服务器
- en: 'Similar to the last session, let''s start and enable `kubelet` first:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一节类似，让我们首先启动并启用`kubelet`：
- en: '[PRE29]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After we have set up the first master, we should share newly generated certificates
    and keys with the whole system. It makes sure that the masters are secured in
    the same manner:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设置好第一个主服务器之后，我们应该与整个系统共享新生成的证书和密钥。这可以确保主服务器以相同的方式进行安全设置：
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You will have found that several files such as certificates or keys are copied
    to the `/etc/kubernetes/pki/` directly, where they can only be accessed by the
    root. However, we are going to remove the files  `apiserver.crt` and `apiserver.key`.
    It is because these files should be generated in line with the hostname and IP
    of the second master, but the shared client certificate `ca.crt` is also involved
    in the generating process:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现一些文件，如证书或密钥，被复制到`/etc/kubernetes/pki/`目录中，只有root用户才能访问。但是，我们将删除`apiserver.crt`和`apiserver.key`文件。这是因为这些文件应该根据第二个主节点的主机名和IP生成，但共享的客户端证书`ca.crt`也参与了生成过程：
- en: '[PRE31]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Next, before we fire the master initialization command, please change the API
    advertise address in the configuration file for the second master. It should be
    the IP of the second master, your current host. The configuration file of the
    second master is quite similar to the first master's.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在执行主节点初始化命令之前，请更改第二个主节点的配置文件中的API广告地址。它应该是第二个主节点的IP，即您当前的主机。第二个主节点的配置文件与第一个主节点的配置文件非常相似。
- en: 'The difference is that we should indicate the information of `etcd` server
    and avoid creating a new set of them:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于我们应该指示`etcd`服务器的信息，并避免创建新的`etcd`集：
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Go ahead and fire the `kubeadm init` command, record the `kubeadm join` command
    shown in the last line of the `init` command to add the node later, and enable
    the client API permission:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 继续执行`kubeadm init`命令，记录`init`命令的最后一行显示的`kubeadm join`命令，以便稍后添加节点，并启用客户端API权限：
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, check the current nodes; you will find there are two master :'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，检查当前节点；您会发现有两个主节点：
- en: '[PRE34]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Adding nodes in a HA cluster
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在HA集群中添加节点
- en: 'Once the masters are ready, you can add nodes into the system. This node should
    be finished with the prerequisite configuration as a worker node in the kubeadm
    cluster. And, in the beginning, you should start kubelet as the master ones:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦主节点准备就绪，您可以将节点添加到系统中。此节点应完成先决条件配置，作为kubeadm集群中的工作节点。并且，在开始时，您应该像主节点一样启动kubelet：
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After that, you can go ahead and push the join command you copied. However,
    please change the master IP to the load balancer one:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，您可以继续并推送您复制的加入命令。但是，请将主节点IP更改为负载均衡器的IP：
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can then jump to the first master or second master to check the nodes''
    status:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以跳转到第一个主节点或第二个主节点，以检查节点的状态：
- en: '[PRE37]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: How it works...
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'To verify our HA cluster, take a look at the pods in the namespace `kube-system`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 验证我们的HA集群，请查看`kube-system`命名空间中的pod：
- en: '[PRE38]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'These pods are working as system daemons: Kubernetes system services such as
    the API server, Kubernetes add-ons such as the DNS server, and CNI ones; here
    we used Calico. But wait! As you take a closer look at the pods, you may be curious
    about why the controller manager and scheduler runs on both masters. Isn''t there
    just single one in the HA cluster?'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这些pod作为系统守护程序运行：Kubernetes系统服务，如API服务器，Kubernetes附加组件，如DNS服务器，以及CNI组件；在这里我们使用了Calico。但等等！当您仔细查看pod时，您可能会好奇为什么控制器管理器和调度器在两个主节点上都在运行。在HA集群中不是只有一个吗？
- en: 'As we understood in the previous section, we should avoid running multiple
    controller managers and multiple schedulers in the Kubernetes system. This is
    because they may try to take over requests at the same time, which not only creates
    conflict but is also a waste of computing power. Actually, while booting up the
    whole system by using kubeadm, the controller manager and scheduler are started
    with the `leader-elect` flag enabled by default:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中了解的那样，我们应该避免在Kubernetes系统中运行多个控制器管理器和多个调度器。这是因为它们可能同时尝试接管请求，这不仅会创建冲突，而且还会浪费计算资源。实际上，在使用kubeadm启动整个系统时，默认情况下会启动具有`leader-elect`标志的控制器管理器和调度器：
- en: '[PRE39]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You may find that the scheduler has also been set with `leader-elect.` Nevertheless,
    why is there still more than one pod? The truth is, one of the pods with the same
    role is idle. We can get detailed information by looking at system endpoints:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会发现调度程序也已设置为`leader-elect`。然而，为什么还会有多个pod呢？事实上，具有相同角色的其中一个pod是空闲的。我们可以通过查看系统端点获取详细信息：
- en: '[PRE40]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Take the endpoint for `kube-controller-manager`, for example: there is no virtual
    IP of a pod or service attached to it (the same as `kube-scheduler`). If we dig
    deeper into this endpoint, we find that the endpoint for `kube-controller-manager`
    relies on `annotations` to record lease information; it also relies on `resourceVersion`
    for pod mapping and to pass traffic. According to the annotation of the `kube-controller-manager`
    endpoint, it is our first master that took control. Let''s check the controller
    manager on both masters:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以`kube-controller-manager`的端点为例：它没有任何pod或服务的虚拟IP（与`kube-scheduler`相同）。如果我们深入研究这个端点，我们会发现`kube-controller-manager`的端点依赖于`annotations`来记录租约信息；它还依赖于`resourceVersion`来进行pod映射和传递流量。根据`kube-controller-manager`端点的注释，我们的第一个主节点控制了情况。让我们检查两个主节点上的控制器管理器：
- en: '[PRE41]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, only one master works as a leader and handles the requests,
    while the other one persists, acquires the lease, and does nothing.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，只有一个主节点作为领导者处理请求，而另一个节点持续存在，获取租约，但不执行任何操作。
- en: 'For a further test, we are trying to remove our current leader pod, to see
    what happens. While deleting the deployment of system pods by a `kubectl` request,
    a kubeadm Kubernetes would create a new one since it''s guaranteed to boot up
    any application under the`/etc/kubernetes/manifests` directory.  Therefore, avoid
    the automatic recovery by kubeadm, we remove the configuration file out of the
    manifest directory instead. It makes the downtime long enough to give away the
    leadership:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行进一步的测试，我们尝试删除当前的领导者pod，看看会发生什么。通过`kubectl`请求删除系统pod的部署时，kubeadm Kubernetes会创建一个新的，因为它保证会启动`/etc/kubernetes/manifests`目录下的任何应用程序。因此，为了避免kubeadm的自动恢复，我们将配置文件从清单目录中移除。这会使停机时间足够长，以放弃领导权：
- en: '[PRE42]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `/etc/kubernetes/manifests` directory is defined in kubelet by `--pod-manifest-path
    flag`. Check `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`*,* which
    is the system daemon configuration file for kubelet, and the help messages of
    kubelet for more details.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`/etc/kubernetes/manifests`目录在kubelet中由`--pod-manifest-path`标志定义。检查`/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`，这是kubelet的系统守护程序配置文件，以及kubelet的帮助消息，以获取更多详细信息。'
- en: 'Now, it is the other node''s turn to wake up its controller manager and put
    it to work. Once you put back the configuration file for the controller manager,
    you find the old leader is now waiting for the lease:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，轮到另一个节点唤醒其控制器管理器并让其工作了。一旦放回控制器管理器的配置文件，您会发现旧的领导者现在正在等待租约：
- en: '[PRE43]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: See also
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Before you read this recipe, you should have mastered the basic concept of
    single master installation by kubeadm. Refer to the related recipes mentioned
    here to get an idea for how to build a multiple-master system automatically:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本文之前，您应该已经掌握了通过kubeadm进行单主安装的基本概念。请参考这里提到的相关食谱，以了解如何自动构建多主系统的想法：
- en: '*Setting up a Kubernetes cluster on Linux by kubeadm* in [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml),
    *Building Your Own Kubernetes Cluster*'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Linux上通过kubeadm设置Kubernetes集群*在[第1章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)中，*构建您自己的Kubernetes集群*
- en: Clustering etcd
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd集群
