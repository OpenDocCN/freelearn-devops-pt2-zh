- en: '6: Kubernetes Services'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6：Kubernetes Services
- en: In the previous chapters we’ve launched Pods and used Deployments to add self-healing,
    scalability, and rolling updates. However, despite all of this, **we still cannot
    rely on Pod IPs!** In this chapter, we’ll see how Kubernetes *Services* give us
    networking that we **can** rely on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经启动了Pods并使用Deployments来添加自愈、可伸缩性和滚动更新。然而，尽管所有这些，**我们仍然不能依赖Pod IPs！**在本章中，我们将看到Kubernetes
    *Services*如何给我们提供可靠的网络。
- en: 'We’ll split the chapter up like this:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把本章分成以下几部分：
- en: Setting the scene
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置场景
- en: Theory
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论
- en: Hands-on
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践操作
- en: Real world example
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实世界的例子
- en: Setting the scene
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置场景
- en: Before diving in, we need to remind ourselves that **Pod IPs are unreliable.**
    When Pods fail, they get replaced with new Pods with new IPs. Scaling-up a Deployment
    introduces new Pods with new IP addresses. Scaling-down a Deployment removes Pods.
    All of this creates a large amount of IP churn, and creates a situation where
    Pod IPs cannot be relied on.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入之前，我们需要提醒自己**Pod IPs是不可靠的**。当Pods失败时，它们会被具有新IP的新Pods替换。扩展Deployment会引入具有新IP地址的新Pods。缩减Deployment会移除Pods。所有这些都会产生大量的IP变动，并且会导致无法依赖Pod
    IPs的情况。
- en: We also need to know 3 fundamental things about Kubernetes Services.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要了解Kubernetes Services的3个基本要点。
- en: First, we need to clear up some terminology. When talking about a *Service*
    in this chapter we’re talking about the Service REST object in the Kubernetes
    API. Just like a *Pod* , *ReplicaSet* , or *Deployment* , a Kubernetes ***Service***
    is an object in the API that we define in a manifest and POST to the API server.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要澄清一些术语。在本章中，当谈到*Service*时，我们指的是Kubernetes API中的Service REST对象。就像*Pod*、*ReplicaSet*或*Deployment*一样，Kubernetes
    ***Service***是API中的一个对象，我们在清单中定义并POST到API服务器中。
- en: Second, we need to know that every Service gets its own **stable IP address**
    , its own **stable DNS name** , and its own **stable port** .
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们需要知道每个Service都有自己的**稳定IP地址**，自己的**稳定DNS名称**和自己的**稳定端口**。
- en: Third, we need to know that Services use labels to dynamically select the Pods
    in the cluster they will send traffic to.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们需要知道Services使用标签来动态选择集群中要发送流量的Pods。
- en: The last two points are what allow Services to provide stable networking to
    a dynamic set of Pods.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两点是允许Services为动态的一组Pods提供稳定网络的关键。
- en: Theory
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理论
- en: Figure 6.1 shows a simple Pod-based application deployed via a Kubernetes Deployment.
    It shows a client (which could be another component of the app) that does not
    have a reliable network endpoint for accessing the Pods - remember that Pod IPs
    are unreliable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1显示了通过Kubernetes Deployment部署的一个简单的基于Pod的应用程序。它显示了一个客户端（可能是应用程序的另一个组件），它没有可靠的网络端点来访问Pods
    - 请记住Pod IPs是不可靠的。
- en: '![Figure 6.1](Image00036.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1](Image00036.jpg)'
- en: Figure 6.1
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1
- en: Figure 6.2 shows the same application with a Service added into the mix. The
    Service is associated with the Pods and provides them with a stable IP, DNS and
    port. It also load-balances requests across the Pods.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2显示了相同的应用程序，其中添加了一个Service。该Service与Pods关联，并为它们提供稳定的IP、DNS和端口。它还在Pods之间负载均衡请求。
- en: '![Figure 6.2](Image00037.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2](Image00037.jpg)'
- en: Figure 6.2
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2
- en: With a Service in front of a set of Pods, the Pods can scale up and down, they
    can fail, and they can be updated… As these things happen, the Service in front
    of them notices the changes and updates its knowledge of the Pods. But it never
    changes the stable IP, DNS and port that it exposes!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有了一个Service在一组Pods的前面，Pods可以扩展和缩减，它们可以失败，它们可以被更新...当这些事情发生时，Service会注意到这些变化并更新对Pods的了解。但它永远不会改变它暴露的稳定IP、DNS和端口！
- en: Labels and loose coupling
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标签和松耦合
- en: Pods and Services are loosely coupled via *labels* and *label selectors* . This
    is the same technology that links Deployments to Pods. Figure 6.3 shows an example
    where 3 Pods are labelled as `zone=prod` and `version=1` , and the Service has
    a *label selector* that matches.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*标签*和*标签选择器*，Pod和服务松散耦合。这与将部署与Pod链接的技术相同。图6.3显示了一个示例，其中3个Pod被标记为`zone=prod`和`version=1`，而Service具有一个*标签选择器*与之匹配。
- en: '![Figure 6.3](Image00038.gif)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3](Image00038.gif)'
- en: Figure 6.3
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3
- en: In Figure 6.3, the Service is providing stable networking to all three Pods
    - you can send requests to the Service and it will proxy them on to the Pods.
    It also provides simple load-balancing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.3中，服务为所有三个Pod提供稳定的网络 - 您可以向服务发送请求，它将代理它们到Pod。它还提供简单的负载平衡。
- en: For a Service to match a set of Pods, and therefore provide stable networking
    and load-balancing, it only needs to match *some* of the labels on a Pod. However,
    for a Pod to match a Service, it has to have all of the labels the Service is
    looking for. If that sounds confusing, the examples in Figure’s 6.4 and 6.5 should
    help.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使服务匹配一组Pod，从而提供稳定的网络和负载平衡，它只需要匹配Pod上的*一些*标签。然而，为了使Pod匹配服务，它必须具有服务正在寻找的所有标签。如果这听起来令人困惑，图6.4和6.5中的示例应该有所帮助。
- en: Figure 6.4 shows an example where the Service does not match any of the Pods.
    This is because the Service is looking for Pods that have two labels, but the
    Pods only possess one of them. The logic behind this is a Boolean `AND` operation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4显示了一个示例，其中Service与任何Pod都不匹配。这是因为Service正在寻找具有两个标签的Pod，但Pod只拥有其中一个。这背后的逻辑是布尔`AND`操作。
- en: '![Figure 6.4](Image00039.gif)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4](Image00039.gif)'
- en: Figure 6.4
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4
- en: Figure 6.5 shows an example that does work. It works because the Service is
    selecting on two labels, and the Pods in the diagram possess both. It doesn’t
    matter that the Pods have additional labels that the Service isn’t looking for.
    The Service is looking for Pods with two labels, it finds them, and ignores the
    fact that the Pods have additional labels - all that is important is that the
    Pods possess the labels the Service is looking for.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5显示了一个确实有效的示例。这是因为服务正在选择两个标签，并且图中的Pod都具有这两个标签。并不重要Pod具有服务不在寻找的额外标签。服务正在寻找具有两个标签的Pod，它找到了它们，并忽略了Pod具有额外标签的事实
    - 重要的是Pod具有服务正在寻找的标签。
- en: '![Figure 6.5](Image00040.gif)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5](Image00040.gif)'
- en: Figure 6.5
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5
- en: The following excerpts, from a Service YAML and Deployment YAML, show how *selectors*
    and *labels* are implemented. We’ve added comments to the lines we’re interested
    in.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下摘录来自Service YAML和Deployment YAML，展示了*选择器*和*标签*的实现方式。我们已经在我们感兴趣的行上添加了注释。
- en: '**svc.yml**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**svc.yml**'
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**deploy.yml**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**deploy.yml**'
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the example files, the Service has a label selector (`.spec.selector` ) with
    a single value `app=hello-world` . This is the label that the Service is looking
    for when it queries the cluster for matching Pods. The Deployment specifies a
    Pod template with the same `app=hello-world` label (`.spec.template.metadata.labels`
    ). This means that any Pods it deploys will have the `app=hello-world` label.
    It is these two attributes that loosely tie the Service to the Deployments Pods.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例文件中，服务具有一个标签选择器（`.spec.selector`），其具有单个值`app=hello-world`。这是服务在查询匹配的Pod时寻找的标签。部署指定了具有相同`app=hello-world`标签的Pod模板（`.spec.template.metadata.labels`）。这意味着它部署的任何Pod都将具有`app=hello-world`标签。这两个属性松散地将服务与部署的Pod绑定在一起。
- en: When the Deployment and the Service are deployed, the Service will select all
    10 Pod replicas and provide them with a stable networking endpoint and perform
    load-balancing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当部署和服务部署时，服务将选择所有10个Pod副本，并为它们提供稳定的网络端点并执行负载平衡。
- en: Services and Endpoint objects
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 服务和端点对象
- en: As Pods come-and-go (scaling up and down, failures, rolling updates etc.), the
    Service dynamically updates its list of healthy matching Pods. It does this through
    a combination of the label selector and a construct called an *Endpoint* object.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Pod的出现和消失（扩展和缩减，故障，滚动更新等），Service动态更新其健康匹配Pod的列表。它通过标签选择器和一个称为*Endpoint*对象的构造来实现这一点。
- en: Each Service that is created, automatically gets an associated *Endpoint* object.
    All this Endpoint object is, is a dynamic list of all of the healthy Pods on the
    cluster that match the Service’s label selector.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个创建的Service都会自动获得一个关联的*Endpoint*对象。这个Endpoint对象只是一个动态列表，列出了集群上与Service的标签选择器匹配的所有健康Pod。
- en: It works like this…
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作原理是这样的…
- en: Kubernetes is constantly evaluating the Service’s label selector against the
    current list of healthy Pods in the cluster. Any new Pods that match the selector
    get added to the Endpoint object, and any Pods that disappear get removed. This
    means the Endpoint is always up-to-date. When a Service is sending traffic to
    Pods, it queries its Endpoint for the latest list of healthy matching Pods - this
    ensures the Service is kept up-to-date as Pods come and go.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes不断评估Service的标签选择器与集群中当前健康Pod列表的匹配情况。任何新的匹配选择器的Pod都会被添加到Endpoint对象中，任何消失的Pod都会被移除。这意味着Endpoint始终是最新的。当Service向Pod发送流量时，它会查询其Endpoint以获取最新的健康匹配Pod列表-这确保了Service随着Pod的出现和消失而保持最新。
- en: The Endpoint object has its own API endpoint that Kubernetes-native apps can
    query for the latest list of matching Pods (like the Service object does). Non-native
    Kubernetes apps can just use the Service’s stable IP (VIP).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Endpoint对象有自己的API端点，Kubernetes本地应用程序可以查询匹配Pod的最新列表（就像Service对象一样）。非本地的Kubernetes应用程序可以直接使用Service的稳定IP（VIP）。
- en: Now that we know the fundamentals of how Services work, let’s look at some use-cases.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了Service工作原理的基础，让我们来看一些用例。
- en: Accessing Services from inside the cluster
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从集群内部访问服务
- en: Kubernetes supports several *types* of Service. The default type is **ClusterIP**
    .
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持几种*类型*的Service。默认类型是**ClusterIP**。
- en: A ClusterIP Service does the following. It gets the Service a stable IP address
    (ClusterIP) and port that is only accessible from inside the cluster. It then
    registers them against the name of the Service on the cluster’s DNS service.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterIP Service执行以下操作。它为Service获取一个稳定的IP地址（ClusterIP）和端口，只能从集群内部访问。然后将它们注册到集群的DNS服务的名称下。
- en: All Pods in the cluster know about the cluster’s DNS service, meaning all Pods
    are able to resolve Service names.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的所有Pod都知道集群的DNS服务，这意味着所有Pod都能解析Service的名称。
- en: Let’s look at a simple example.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子。
- en: Creating a new Service called “hellcat-svc” will trigger the following. Kubernetes
    will register the name “hellcat-svc”, along with an IP address and port, with
    the cluster’s DNS service. The name, IP and port are guaranteed to be long-lived
    and stable, and all Pods in the cluster will be able to resolve “hellcat-svc”
    to the Service’s ClusterIP.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为“hellcat-svc”的新Service将触发以下操作。Kubernetes将使用集群的DNS服务注册名称“hellcat-svc”，以及一个IP地址和端口。名称、IP和端口保证是长期稳定的，集群中的所有Pod都能将“hellcat-svc”解析为Service的ClusterIP。
- en: Net net… as long as a Pod (application microservice) knows the name of a Service,
    it can resolve that to its ClusterIP address and connect to it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 网络网络…只要一个Pod（应用微服务）知道一个Service的名称，它就可以将其解析为其ClusterIP地址并连接到它。
- en: This only works for Pods and other objects on the cluster, as it requires access
    to the cluster’s DNS service. It does not work outside of the cluster.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅适用于集群中的Pod和其他对象，因为它需要访问集群的DNS服务。它在集群外部无法工作。
- en: Accessing from outside the cluster
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从集群外部访问
- en: Kubernetes has another type of Service called a **NodePort Service** . This
    builds on top of ClusterIP and enables access from outside of the cluster.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还有另一种称为**NodePort服务**的服务类型。这建立在ClusterIP之上，并允许从集群外部访问。
- en: We already know that the default Service type is ClusterIP, and that this registers
    a DNS name, Virtual IP, and port with the clusters DNS. NodePort Services add
    to this by adding another port that can be used to reach the Service from outside
    the cluster. This additional port is called the NodePort.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道默认的服务类型是ClusterIP，这会在集群的DNS中注册一个DNS名称、虚拟IP和端口。NodePort服务通过添加另一个端口来扩展这一功能，该端口可用于从集群外部访问服务。这个额外的端口称为NodePort。
- en: 'The following example represents a NodePort Service:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例代表一个NodePort服务：
- en: '**Name:** hellcat-svc'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名称：**hellcat-svc'
- en: '**ClusterIP:** 172.12.5.17'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ClusterIP：**172.12.5.17'
- en: '**port:** 8080'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端口：**8080'
- en: '**NodePort:** 30050'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodePort：**30050'
- en: This Service can be accessed directly from inside the cluster via any of the
    first three values (Name, ClusterIP, port). It can also be accessed from outside
    of the cluster by hitting any cluster node on port `30050` .
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个服务可以通过前三个值（名称、ClusterIP、端口）之一直接从集群内部访问。也可以通过在端口`30050`上访问任何集群节点来从集群外部访问。
- en: At the lowest level, we have *nodes* in the cluster hosting Pods. Then we create
    a Service and use labels to associate it with Pods. The Service object has a reliable
    NodePort mapped to every node in the cluster – the NodePort is the same on every
    node. This means that traffic from outside of the cluster can hit any node in
    the cluster on the NodePort and get through to the application (Pods).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在最低级别，我们有集群中托管Pod的*节点*。然后我们创建一个服务，并使用标签将其与Pod关联起来。服务对象在集群中的每个节点上都有一个可靠的NodePort映射
    - 每个节点上的NodePort都是相同的。这意味着来自集群外部的流量可以在集群中的任何节点上通过NodePort到达应用程序（Pod）。
- en: Figure 6.6 shows an NodePort Service where 3 Pods are exposed externally on
    port `30050` on every node in the cluster. In step 1, an external client hits
    **Node2** on port `30050` . In step 2 it is redirected to the Service object (this
    happens even though **Node2** isn’t running a Pod from the Service). Step 3 shows
    that the Service has an associated Endpoint object with an always-up-to-date list
    of Pods matching the label selector. Step 4 shows the client being directed to
    **pod1** on **Node1** .
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6显示了一个NodePort服务，其中3个Pod在集群中的每个节点上都在端口`30050`上对外开放。在第1步，外部客户端在端口`30050`上访问Node2。在第2步，它被重定向到服务对象（即使Node2没有运行来自服务的Pod，也会发生这种情况）。第3步显示服务有一个关联的Endpoint对象，其中包含一个始终保持最新的与标签选择器匹配的Pod列表。第4步显示客户端被引导到Node1上的pod1。
- en: '![Figure 6.6](Image00041.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6](Image00041.jpg)'
- en: Figure 6.6
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6
- en: The Service could just as easily have directed the client to pod2 or pod3\.
    In fact, future requests may go to other Pods as the Service load-balances traffic
    between them.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 服务也可以轻松地将客户端引导到pod2或pod3。事实上，未来的请求可能会在Pod之间进行负载均衡。
- en: There are other types of Services, such as LoadBalancer Services. These integrate
    with load-balancers from your cloud provider such as AWS, Azure, and GCP. They
    build on top of NodePort Services (which in turn build on top of ClusterIP Services)
    and allow clients on the internet to reach your Pods via one of your cloud’s load-balancers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他类型的服务，比如负载均衡器服务。这些服务与云提供商（如AWS、Azure和GCP）的负载均衡器集成。它们建立在NodePort服务的基础上（NodePort服务又建立在ClusterIP服务的基础上），允许互联网上的客户端通过云的负载均衡器之一访问您的Pod。
- en: LoadBalancer Services are extremely easy to setup. However, they only work if
    you’re running your Kubernetes cluster on a supported cloud platform. E.g. you
    cannot leverage an ELB load-balancer on AWS if your Kubernetes cluster is an on-premises
    cluster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡服务非常容易设置。但是，它们只在您在受支持的云平台上运行Kubernetes集群时才起作用。例如，如果您的Kubernetes集群是本地集群，则无法利用AWS上的ELB负载均衡器。
- en: Service discovery
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 服务发现
- en: 'Kubernetes implements Service discovery in a couple of ways:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes以几种方式实现服务发现：
- en: DNS (preferred)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS（推荐）
- en: Environment variables (definitely not preferred)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境变量（绝对不推荐）
- en: DNS-based Service discovery requires the DNS *cluster-add-on* - this is just
    a fancy name for the native Kubernetes DNS service. If you followed the installation
    methods from the “Installing Kubernetes” chapter, you’ll already have this. It
    implements a Pod-based DNS service in the cluster and configures all kubelets
    (nodes) to use it for DNS.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DNS的服务发现需要DNS *cluster-add-on* - 这只是本地Kubernetes DNS服务的一个花哨的名称。如果您按照“安装Kubernetes”章节中的安装方法进行安装，您已经拥有了这个服务。它在集群中实现了基于Pod的DNS服务，并配置所有kubelet（节点）以用于DNS。
- en: The DNS add-on constantly watches the API server for new Services and automatically
    registers them in DNS. This means every Service gets a DNS name that is resolvable
    across the entire cluster.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: DNS插件不断监视API服务器以获取新的服务，并自动在DNS中注册它们。这意味着每个服务都有一个可以在整个集群中解析的DNS名称。
- en: The alternative form of service discovery is through environment variables.
    Every Pod gets a set of environment variables that resolve every Service currently
    on the cluster. However, this is a fall-back in case you’re not using DNS in your
    cluster.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种服务发现的形式是通过环境变量。每个Pod都会得到一组环境变量，解析出集群中当前的每个服务。但是，这是一种备用方法，以防您的集群中没有使用DNS。
- en: The problem with environment variables, is that they’re only inserted into Pods
    at creation time. This means that Pods have no way of learning about new Services
    that are added to the cluster after the Pod itself is created - far from ideal!
    This is why DNS is the preferred method.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 环境变量的问题在于它们只在Pod创建时插入。这意味着Pod在创建后无法了解添加到集群中的新服务的方式 - 这远非理想！这就是为什么DNS是首选方法。
- en: Summary of Service theory
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 服务理论总结
- en: Services are all about providing stable networking for Pods. They also provide
    load-balancing and ways to be accessed from outside of the cluster. They’re dynamically
    associated with Pods using labels and label selectors.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 服务主要提供Pod的稳定网络。它们还提供负载均衡和从集群外部访问的方式。它们使用标签和标签选择器动态与Pod相关联。
- en: Hands-on with Services
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务实践
- en: We’re about to get hands-on and put the theory to the test.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要动手实践并将理论付诸实践。
- en: 'We’ll augment a simple single-Pod app with a Kubernetes Service. And we’ll
    show how to do it in two ways:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Kubernetes服务来增强一个简单的单Pod应用程序。我们将展示两种方法：
- en: The imperative way (only use in emergencies)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令式方式（仅在紧急情况下使用）
- en: The declarative way
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声明式方式
- en: The imperative way
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 命令式方式
- en: '**Warning!** The imperative way is **not** the Kubernetes way! It introduces
    the risk that changes made imperatively never make it into declarative manifests,
    rendering the manifests stale. This introduces the risk that stale manifests are
    used to update the cluster at a later date, unintentionally overwriting important
    changes that were made imperatively.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告！** 命令式方式**不是**Kubernetes的方式！它引入了一种风险，即通过命令方式进行的更改永远不会出现在声明式清单中，使得清单变得陈旧。这会导致陈旧的清单被用来在以后的某个日期更新集群，无意中覆盖了通过命令方式进行的重要更改。'
- en: Use `kubectl` to declaratively deploy the following Deployment (later steps
    will be done imperatively). The command assumes the Deployment is defined in a
    file called `deploy.yml` and has the following content.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl`以声明方式部署以下部署（后续步骤将以命令方式完成）。该命令假定部署在名为`deploy.yml`的文件中，并具有以下内容。
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that the Deployment is running, it’s time to imperatively deploy a Service
    for it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在部署正在运行，是时候为其部署一个服务了。
- en: Use the following `kubectl` command to create a new Service that will provide
    networking and load-balancing for the Pods deployed in the previous step.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下`kubectl`命令创建一个新的服务，为先前步骤部署的Pod提供网络和负载平衡。
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s explain what the command is doing. `kubectl expose` is the imperative
    way to create a new *Service* object. `deployment web-deploy` is telling Kubernetes
    to expose the Deployment called `web-deploy` that was created in a previous step.
    `--name=hello-svc` tells Kubernetes that we want to call this Service “hello-svc”,
    and `--target-port=8080` tells it which port the app is listening on (this is
    **not** the cluster-wide NodePort that we’ll access the Service on). Finally,
    `--type=NodePort` tells Kubernetes we want a cluster-wide port for the Service.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一下这个命令在做什么。`kubectl expose`是创建一个新的*Service*对象的命令方式。`deployment web-deploy`告诉Kubernetes暴露先前创建的名为`web-deploy`的部署。`--name=hello-svc`告诉Kubernetes我们要将此服务称为“hello-svc”，`--target-port=8080`告诉它应用程序正在侦听的端口（这**不是**我们将访问服务的整个集群NodePort）。最后，`--type=NodePort`告诉Kubernetes我们需要一个服务的整个集群端口。
- en: Once the Service is created, you can inspect it with the `kubectl describe svc
    hello-svc` command.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务后，您可以使用`kubectl describe svc hello-svc`命令检查它。
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Some interesting values in the output include:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中一些有趣的值包括：
- en: '`Selector` is the list of labels that Pods must have in order for the Service
    to send traffic to them'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Selector` 是Pod必须具有的标签列表，以便服务将流量发送到它们'
- en: '`IP` is the permanent ClusterIP (VIP) of the Service'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IP` 是服务的永久ClusterIP（VIP）'
- en: '`Port` is the port that the app and Service listens on'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Port`是应用程序和服务侦听的端口'
- en: '`NodePort` is the cluster-wide port that can be used to access it externally'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NodePort` 是整个集群可以用来外部访问的端口'
- en: '`Endpoints` is the dynamic list of healthy Pods that currently match the Service’s
    label selector.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Endpoints` 是当前与服务标签选择器匹配的健康Pod的动态列表。'
- en: Now that we know the cluster-wide port that the Service is accessible on, we
    can open a web browser and access the app. In order to do this, you will need
    to know the IP address of at least one of the nodes in your cluster, and you will
    need to be able to reach it from your browser - e.g. a publicly routable IP if
    you’re accessing via the internet.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了服务可访问的整个集群端口，我们可以打开一个网页浏览器并访问该应用程序。为了做到这一点，您需要知道集群中至少一个节点的IP地址，并且需要能够从浏览器访问它
    - 例如，如果您通过互联网访问，则需要一个公共可路由的IP。
- en: Figure 6.7 shows a web browser accessing a cluster node with an IP address of
    `54.246.255.52` on the cluster-wide NodePort `30175` .
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7显示了一个网页浏览器访问具有IP地址`54.246.255.52`的集群节点，该节点使用整个集群NodePort`30175`。
- en: '![Figure 6.7](Image00042.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7](Image00042.jpg)'
- en: Figure 6.7
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7
- en: The app we’ve deployed is a simple web app. It’s built to listen on port 8080,
    and we’ve configured a Kubernetes *Service* to map port `30175` on every cluster
    node back to port 8080 on the app. By default, cluster-wide ports (NodePort values)
    are between 30,000 - 32,767.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们部署的应用程序是一个简单的Web应用程序。它构建为侦听端口8080，并且我们已经配置了一个Kubernetes *Service*，将每个集群节点上的端口`30175`映射回应用程序上的端口8080。默认情况下，整个集群端口（NodePort值）在30,000
    - 32,767之间。
- en: Coming up next we’re going to see how to do the same thing the proper way -
    the declarative way! To do that, we need to clean up by deleting the Service we
    just created. We can do this with the `kubectl delete svc` command
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何以正确的方式进行相同的操作 - 声明式的方式！为了做到这一点，我们需要通过删除刚刚创建的服务来进行清理。我们可以使用`kubectl
    delete svc`命令来做到这一点
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The declarative way
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 声明式方式
- en: Time to do things the proper way… the Kubernetes way!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候按照正确的方式来做事了... Kubernetes的方式！
- en: A Service manifest file
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 服务清单文件
- en: We’ll use the following Service manifest file to deploy the same *Service* that
    we deployed in the previous section. Only this time we’ll specify a value for
    the cluster-wide port.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下服务清单文件来部署与上一节中部署的相同的*服务*。只是这一次我们将为整个集群端口指定一个值。
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let’s step through some of the lines.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行解释一些内容。
- en: Services are mature objects and are fully defined in the `v1` API (`.apiVersion`
    ).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 服务是成熟的对象，并且在`v1` API（`.apiVersion`）中完全定义。
- en: The `.kind` field tells Kubernetes to pass the manifest to the Service controller
    for deployment as a Service.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`.kind`字段告诉Kubernetes将清单传递给服务控制器以部署为服务。'
- en: The `.metadata` section defines a name and a label for the Service. The label
    here is a label for the Service itself. It is not the label that the Service uses
    to select Pods.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`.metadata` 部分定义了服务的名称和标签。这里的标签是服务本身的标签。它不是服务用来选择Pod的标签。'
- en: The `.spec` section is where we actually define the Service. In this example,
    we’re telling Kubernetes to deploy a `NodePort` Service (other types such as `ClusterIP`
    and `LoadBalancer` exist) and to map port `8080` to port `30001` on each node
    in the cluster. Then we’re explicitly telling it to use TCP (default).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`.spec` 部分是我们实际定义服务的地方。在这个例子中，我们告诉Kubernetes部署一个`NodePort`服务（其他类型如`ClusterIP`和`LoadBalancer`也存在），并将端口`8080`映射到集群中每个节点的端口`30001`。然后我们明确告诉它使用TCP（默认）。'
- en: Finally, `.spec.selector` tells the Service to send traffic to all Pods in the
    cluster that have the `app=hello-world` label. This means it will provide stable
    networking and load-balancing across all Pods with that label.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`.spec.selector` 告诉服务将流量发送到集群中具有`app=hello-world`标签的所有Pod。这意味着它将为具有该标签的所有Pod提供稳定的网络和负载均衡。
- en: Common Service types
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 常见的服务类型
- en: 'The three common *ServiceTypes* are:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 三种常见的*ServiceTypes*是：
- en: '`ClusterIP.` This is the default option, and gives the *Service* a stable IP
    address internally within the cluster. It will not make the Service available
    outside of the cluster.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClusterIP.` 这是默认选项，并在集群内部为*服务*提供稳定的IP地址。它不会使服务在集群外部可用。'
- en: '`NodePort.` This builds on top of `ClusterIP` and adds a cluster-wide TCP or
    UDP port. It makes the Service available outside of the cluster on this port.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NodePort.` 这是建立在`ClusterIP`之上，并添加了一个集群范围的TCP或UDP端口。它使服务在该端口在集群外部可用。'
- en: '`LoadBalancer.` This builds on top of `NodePort` and integrates with cloud-based
    load-balancers.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LoadBalancer.` 这是建立在`NodePort`之上，并与基于云的负载均衡器集成。'
- en: The manifest needs POSTing to the API server. The simplest way to do this is
    with `kubectl apply` .
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 清单需要通过POST发送到API服务器。最简单的方法是使用`kubectl apply`来做到这一点。
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This command tells Kubernetes to deploy a new object from a file called `svc.yml`
    . The `-f` flag lets you tell Kubernetes which manifest file to use. Kubernetes
    knows to deploy a Service object based on the value of the `.kind` field in the
    manifest.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令告诉Kubernetes从名为`svc.yml`的文件部署一个新对象。`-f`标志让您告诉Kubernetes使用哪个清单文件。Kubernetes知道根据清单中`.kind`字段的值来部署服务对象。
- en: Introspecting Services
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 服务内省
- en: Now that the Service is deployed, you can inspect it with the usual `kubectl
    get` and `kubectl describe` commands.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务已经部署，您可以使用通常的`kubectl get`和`kubectl describe`命令来检查它。
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the previous example, we’ve exposed the Service as a `NodePort` on port `30001`
    across the entire cluster. This means we can point a web browser to that port
    on any node and get to the Service. You will need to use a node IP that you can
    reach, and you will need to make sure that any firewall and security rules allow
    the traffic.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们已将Service暴露为整个集群上端口为`30001`的`NodePort`。这意味着我们可以将Web浏览器指向任何节点上的该端口并访问该服务。您需要使用可以访问的节点IP，并确保任何防火墙和安全规则允许流量通过。
- en: Figure 6.8 shows a web browser accessing the app via a cluster node with an
    IP address of `54.246.255.52` on the cluster-wide port `30001` .
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8显示了一个Web浏览器通过集群节点的IP地址为`54.246.255.52`的集群范围端口`30001`访问应用程序。
- en: '![Figure 6.8](Image00043.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8](Image00043.jpg)'
- en: Figure 6.8
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8
- en: Endpoint objects
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 端点对象
- en: Earlier in the chapter, we said that every Service gets its own Endpoint object
    with the same name as the Service. This holds a list of all the Pods the Service
    matches and is dynamically updated as Pods come and go. We can see Endpoints with
    the normal `kubectl` commands.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，我们说每个服务都有一个与服务同名的Endpoint对象。这个对象包含服务匹配的所有Pod的列表，并且随着Pod的出现和消失而动态更新。我们可以使用普通的`kubectl`命令查看Endpoints。
- en: In the following command, we abbreviate Endpoint to `ep` .
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下命令中，我们将Endpoint缩写为`ep`。
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Summary of deploying Services
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部署服务的总结
- en: As with all Kubernetes objects, the preferred way of deploying and managing
    Services is the declarative way. Leveraging labels allows them to be dynamic.
    This means you can deploy new Services that will work with Pods and Deployments
    that are already running on the cluster and in use. Each Service gets its own
    Endpoint object that maintains an up-to-date list of matching Pods.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有Kubernetes对象一样，部署和管理服务的首选方式是声明式的方式。利用标签使它们具有动态性。这意味着您可以部署新服务，这些服务将与集群中已经运行并正在使用的Pod和Deployment一起工作。每个服务都有自己的Endpoint对象，该对象维护与匹配的Pod的最新列表。
- en: Real world example
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现实世界的例子
- en: 'Although everything we’ve learned so far is cool and interesting, the important
    questions are: *How does it bring value?* and *How does it keep businesses running
    and make them more agile and resilient?*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们到目前为止学到的一切都很酷和有趣，但重要的问题是：*它如何带来价值？*以及*它如何保持业务运行并使其更具敏捷性和弹性？*
- en: Let’s take a minute to run through a common real-world example - making updates
    to applications.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一分钟来运行一个常见的现实世界的例子-更新应用程序。
- en: We all know that updating applications is a fact of life - bug fixes, new features
    etc.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道更新应用程序是生活中的一个事实-错误修复，新功能等。
- en: Figure 6.9 shows a simple application deployed on a Kubernetes cluster as a
    bunch of Pods managed by a Deployment. As part of it, we’ve got a Service selecting
    on Pods with labels that match `app=biz1` and `zone=prod` (notice how the Pods
    have both of the labels listed in the label selector). The application is up and
    running.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9显示了一个简单的应用程序部署在一个由Deployment管理的一堆Pod组成的Kubernetes集群上。作为其中的一部分，我们有一个Service选择了具有与`app=biz1`和`zone=prod`匹配的标签的Pod（注意Pod具有标签选择器中列出的两个标签）。应用程序正在运行。
- en: '![Figure 6.9](Image00044.gif)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9](Image00044.gif)'
- en: Figure 6.9
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9
- en: Let’s assume we need to push a new version. But we need to do it without incurring
    downtime.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要推送一个新版本。但我们需要在不产生停机时间的情况下进行。
- en: To do this, we can add Pods running the new version of the app as shown in Figure
    6.10.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以像图6.10所示添加运行新版本应用程序的Pod。
- en: '![Figure 6.10](Image00045.gif)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10](Image00045.gif)'
- en: Figure 6.10
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10
- en: Behind the scenes, the updated *Pods* are deployed through their own ReplicaSet
    and are labelled so that they match the existing label selector. The Service is
    now load-balancing requests across **both versions of the app** (`version=4.1`
    and `version=4.2` ).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，更新后的*Pods*通过它们自己的ReplicaSet部署，并被标记，以便它们与现有的标签选择器匹配。服务现在正在跨**应用程序的两个版本**（`version=4.1`和`version=4.2`）进行负载均衡请求。
- en: This happens because the Service’s label selector is being constantly evaluated,
    and its Endpoint object and ClusterIP are constantly being updated with new matching
    Pods.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为服务的标签选择器正在不断地进行评估，并且它的端点对象和ClusterIP正在不断地更新以匹配新的Pods。
- en: Once you’re happy with the updated version, forcing all traffic to use it is
    as simple as updating the Service’s label selector to include the label `version=4.2`
    . Suddenly the older version no longer matches, and the Service is only forwarding
    traffic to the new version (Figure 6.11).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您满意更新后的版本，强制所有流量使用它就像更新服务的标签选择器以包含标签`version=4.2`一样简单。突然之间，旧版本不再匹配，服务只转发流量到新版本（图6.11）。
- en: '![Figure 6.11](Image00046.gif)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11](Image00046.gif)'
- en: Figure 6.11
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11
- en: However, the old version still exists - we’re just not sending traffic to it
    any more. This means that if we experience an issue with the new version, we can
    switch back to the previous version by simply changing the label selector on the
    Service to include `version=4.1` instead of `version=4.2` . See Figure 6.12.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，旧版本仍然存在 - 我们只是不再向其发送流量。这意味着如果我们遇到新版本的问题，我们可以通过简单地将服务的标签选择器更改为包含`version=4.1`而不是`version=4.2`来切换回先前的版本。参见图6.12。
- en: '![Figure 6.12](Image00047.gif)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12](Image00047.gif)'
- en: Figure 6.12
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12
- en: Now everybody’s getting the old version.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每个人都在使用旧版本。
- en: This functionality can be used for all kinds of things - blue-greens, canaries,
    you name it. So simple, yet so powerful!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这种功能可以用于各种事情 - 蓝绿部署，金丝雀发布，你想到的都可以。如此简单，却如此强大！
- en: Clean-up the lab with the following commands. These will delete the Deployment
    and Service used in the examples.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令清理实验室。这些命令将删除示例中使用的部署和服务。
- en: '[PRE11]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Chapter Summary
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 章节总结
- en: In this chapter, we learned that *Services* bring stable and reliable networking
    to apps deployed on Kubernetes. They also perform load-balancing and allow us
    to expose elements of our application to the outside world (outside of the Kubernetes
    cluster).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解到*服务*为部署在Kubernetes上的应用程序带来了稳定可靠的网络连接。它们还执行负载均衡，并允许我们将应用程序的元素暴露给外部世界（超出Kubernetes集群之外）。
- en: Services are first-class objects in the Kubernetes API and can be defined in
    the standard YAML manifest files. They use label selectors to dynamically match
    Pods, and the best way to work with them is declaratively.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 服务是Kubernetes API中的一流对象，并可以在标准YAML清单文件中定义。它们使用标签选择器动态匹配Pods，并且最好的工作方式是声明性地与它们一起工作。
