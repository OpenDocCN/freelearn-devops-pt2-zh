["```\n# __main__ () {\n. scripts/cluster_common.bash\n. scripts/libtest.bash\n# Create the 'minikube' or 'dind' cluster\ncreate_k8s_cluster ${TEST_CONTEXT}\n# Deploy our stack\nbats tests/deploy-stack.bats\n```", "```\n# Bit of sanity\n@test \"Verify needed kubernetes tools installed\" {\n verify_k8s_tools\n}\n@test \"Deploy stack\" {\n# Deploy the stack we want to test\n./scripts/deploy.sh delete >& /dev/null || true\n./scripts/deploy.sh create\n   k8s_wait_for_pod_running --namespace=kube-system -lname=traefik-ingress-lb\n   k8s_wait_for_pod_running -lapp=my-nginx\n}\n```", "```\n# Set env vars for our test suite\n# INGRESS_IP: depend on the deployed cluster (dind or minikube)\nINGRESS_IP=$(get_ingress_ip ${TEST_CONTEXT})\n# URL_PATH: Dynamically find it from 1st ingress rule\nURL_PATH=$(kubectl get ing -ojsonpath='{.items[0].spec.rules[0].http.paths[0].path}')\n# Verify no empty vars:\n: ${INGRESS_IP:?} ${URL_PATH:?}\n```", "```\n# With the stack ready, now run the tests thru bats:\nexport SVC_URL=\"http://my-nginx.default.svc${URL_PATH:?}\"\nexport ING_URL=\"${INGRESS_IP:?}${URL_PATH:?}\"\nbats tests/integration-tests.bats\nexit_code=$?\n\n[[ ${exit_code} == 0 ]] && echo \"TESTS: PASS\" || echo \"TESTS: FAIL\"\nexit ${exit_code}\n# }\n```", "```\ngit clone https://github.com/kubernetes-for-developers/kfd-flask/ -b 0.7.0\n```", "```\npytest\npytest-dependency\nkubernetes\nrequests\n```", "```\nvirtualenv .venv\nsource .venv/bin/activate\npip3 install -r test-requirements.txt\npytest -v\n```", "```\n======= test session starts =======\nplatform darwin -- Python 3.6.4, pytest-3.4.2, py-1.5.2, pluggy-0.6.0 -- /Users/heckj/src/kfd-flask/e2e_tests/.venv/bin/python3.6\ncachedir: .pytest_cache\nrootdir: /Users/heckj/src/kfd-flask/e2e_tests, inifile:\nplugins: dependency-0.3.2\ncollected 7 items\n\ntests/test_smoke.py::test_kubernetes_components_healthy PASSED [ 14%]\ntests/test_smoke.py::test_deployment PASSED [ 28%]\ntests/test_smoke.py::test_list_pods PASSED [ 42%]\ntests/test_smoke.py::test_deployment_ready PASSED [ 57%]\ntests/test_smoke.py::test_pods_running PASSED [ 71%]\ntests/test_smoke.py::test_service_response PASSED [ 85%]\ntests/test_smoke.py::test_python_client_service_response PASSED [100%]\n\n======= 7 passed in 1.27 seconds =======\n```", "```\npytest --junitxml=results.xml\n```", "```\n@pytest.mark.dependency()\ndef test_kubernetes_components_healthy(kube_v1_client):\n    # iterates through the core kuberneters components to verify the cluster is reporting healthy\n    ret = kube_v1_client.list_component_status()\n    for item in ret.items:\n        assert item.conditions[0].type == \"Healthy\"\n        print(\"%s: %s\" % (item.metadata.name, item.conditions[0].type))\n```", "```\n@pytest.mark.dependency(depends=[\"test_kubernetes_components_healthy\"])\n```", "```\n@pytest.fixture\ndef kube_v1_client():\n    kubernetes.config.load_kube_config()\n    v1 = kubernetes.client.CoreV1Api()\n    return v1\n```", "```\n@pytest.mark.dependency(depends=[\"test_kubernetes_components_healthy\"])\ndef test_deployment():\n    # https://docs.python.org/3/library/subprocess.html#subprocess.run\n    # using check=True will throw an exception if a non-zero exit code is returned, saving us the need to assert\n    # using timeout=10 will throw an exception if the process doesn't return within 10 seconds\n    # Enables the deployment\n    process_result = subprocess.run('kubectl apply -f ../deploy/', check=True, shell=True, timeout=10)\n```", "```\n@pytest.mark.dependency(depends=[\"test_deployment_ready\"])\ndef test_pods_running(kube_v1_client):\n    TOTAL_TIMEOUT_SECONDS = 300\n    DELAY_BETWEEN_REQUESTS_SECONDS = 5\n    now = time.time()\n    while (time.time() < now+TOTAL_TIMEOUT_SECONDS):\n        pod_list = kube_v1_client.list_namespaced_pod(\"default\")\n        print(\"name\\tphase\\tcondition\\tstatus\")\n        for pod in pod_list.items:\n            for condition in pod.status.conditions:\n                print(\"%s\\t%s\\t%s\\t%s\" % (pod.metadata.name, pod.status.phase, condition.type, condition.status))\n                if condition.type == 'Ready' and condition.status == 'True':\n                    return\n        time.sleep(DELAY_BETWEEN_REQUESTS_SECONDS)\n    assert False\n```", "```\n@pytest.mark.dependency(depends=[\"test_deployment_ready\"])\ndef test_service_response(kubectl_proxy):\n    NAMESPACE=\"default\"\n    SERVICE_NAME=\"flask-service\"\n    URI = \"http://localhost:8001/api/v1/namespaces/%s/services/%s/proxy/\" % (NAMESPACE, SERVICE_NAME)\n    print(\"requesting %s\" % (URI))\n    r = requests.get(URI)\n    assert r.status_code == 200\n```", "```\n@pytest.fixture(scope=\"module\")\ndef kubectl_proxy():\n    # establish proxy for kubectl communications\n    # https://docs.python.org/3/library/subprocess.html#subprocess-replacements\n    proxy = subprocess.Popen(\"kubectl proxy &\", stdout=subprocess.PIPE, shell=True)\n    yield\n    # terminate the proxy\n    proxy.kill()\n```", "```\n@pytest.mark.dependency(depends=[\"test_deployment_ready\"]) def test_python_client_service_response(kube_v1_client):\n    from pprint import pprint\n    from kubernetes.client.rest import ApiException\n    NAMESPACE=\"default\"\n    SERVICE_NAME=\"flask-service\"\n    try:\n        api_response = kube_v1_client.proxy_get_namespaced_service(SERVICE_NAME, NAMESPACE)\n        pprint(api_response)\n        api_response = kube_v1_client.proxy_get_namespaced_service_with_path(SERVICE_NAME, NAMESPACE, \"/metrics\")\n        pprint(api_response)\n    except ApiException as e:\n        print(\"Exception when calling CoreV1Api->proxy_get_namespaced_service: %s\\n\" % e)\n```", "```\ngit clone https://github.com/kubernetes-for-developers/kfd-nodejs/ -b 0.7.0\n```", "```\nnpm test\n```", "```\n> kfd-nodejs@0.0.0 test /Users/heckj/src/kfd-nodejs\n> mocha --exit\n\nexpress app\nGET / 200 283.466 ms - 170\n \u2713 should respond at the root (302ms)\nGET /probes/alive 200 0.930 ms - 3\n \u2713 should respond at the liveness probe point\n\n 2 passing (323ms)\n```", "```\nnpm run integration\n```", "```\n> kfd-nodejs@0.0.0 integration /Users/heckj/src/kfd-nodejs\n> mocha e2e_tests --exit\n\nkubernetes\n cluster\n \u2713 should have a healthy cluster\n \u2713 should deploy the manifests (273ms)\n should repeat until the pods are ready\n - delay 5 seconds...\n \u2713 check to see that all pods are reporting ready (5016ms)\n should interact with the deployed services\n \u2713 should access by pod...\n\n 4 passing (5s)\n```", "```\nconst k8s = require('@kubernetes/client-node');\nvar chai = require('chai')\n , expect = chai.expect\n , should = chai.should();\n\nvar k8sApi = k8s.Config.defaultClient();\n\ndescribe('kubernetes', function() {\n  describe('cluster', function() {\n    it('should have a healthy cluster', function() {\n       return k8sApi.listComponentStatus()\n       .then((res) => {\n         // console.log(util.inspect(res.body));\n         res.body.items.forEach(function(component) {\n         // console.log(util.inspect(value));\n         expect(component.conditions[0].type).to.equal(\"Healthy\");\n         expect(component.conditions[0].status).to.equal(\"True\");\n       })\n     }, (err) => {\n        expect(err).to.be.null;\n     });\n   }) // it\n```", "```\nconst util = require('util');\nconsole.log(util.inspect(res.body));\n```", "```\nit('should deploy the manifests', function() {\n  var manifest_directory = path.normalize(path.join(path.dirname(__filename), '..', '/deploy'))\n  const exec = util.promisify(require('child_process').exec);\n  return exec('kubectl apply -f '+manifest_directory)\n  .then((res) => {\n    // console.log(util.inspect(res));\n    expect(res.stdout).to.not.be.null;\n    expect(res.stderr).to.be.empty;\n  }, (err) => {\n    expect(err).to.be.null;\n  })\n})\n```", "```\ndescribe('should repeat until the pods are ready', function() {\n  // Mocha supports a retry mechanism limited by number of retries...\n  this.retries(30);\n  // an a default timeout of 20,000ms that we can increase\n  this.timeout(300000);\n\nit('check to see that all pods are reporting ready', function() {\n   return new Promise(function(resolve, reject) {\n       console.log(' - delay 5 seconds...')\n       setTimeout(() => resolve(1), 5000);\n   }).then(function(result) {\n       return k8sApi.listNamespacedPod('default')\n      .then((res) => {\n         res.body.items.forEach(function(pod) {\n           var readyCondition = _.filter(pod.status.conditions, { 'type': 'Ready' })\n          //console.log(\"checking: \"+pod.metadata.name+\" ready: \"+readyCondition[0].status);\n          expect(readyCondition[0].status).to.equal('True')\n        }) // pod forEach\n    })\n  })\n}) // it\n\n}) // describe pods available\n```", "```\ndescribe('should interact with the deployed services', function() {\n  // path to access the port through the kubectl proxy:\n  // http://localhost:8001/api/v1/namespaces/default/services/nodejs-service:web/proxy/\n it('should access by pod...', function() {\n   return k8sApi.proxyGETNamespacedServiceWithPath(\"nodejs-service:web\", \"default\", \"/\")\n   .then(function(res) {\n      // console.log(util.inspect(res,{depth:1}));\n      expect(res.body).to.not.be.null;\n    });\n  })\n}) // interact with the deployed services\n```", "```\nlanguage: node_js\nnode_js:\n - lts/*\ncache:\n directories:\n\n - \"node_modules\"\nsudo: required\nservices:\n - docker\nenv:\n- CHANGE_MINIKUBE_NONE_USER=true\n\nbefore_script:\n- curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.9.0/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/\n- curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/\n- sudo minikube start --vm-driver=none --kubernetes-version=v1.9.0\n- minikube update-context\n- JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}'; until kubectl get nodes -o jsonpath=\"$JSONPATH\" 2>&1 | grep -q \"Ready=True\"; do sleep 1; done\n\nscript:\n- npm run integration\n```", "```\nhelm init\n```", "```\n$HELM_HOME has been configured at /Users/heckj/.helm.\n\nTiller (the Helm server-side component) has been installed into your Kubernetes Cluster.\n\nPlease note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.\nFor more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\nHappy Helming!\n```", "```\nkubectl get pods -n kube-system\n```", "```\nNAME READY STATUS RESTARTS AGE\ncoredns-599474b9f4-gh99f 1/1 Running 0 3m\nkube-addon-manager-minikube 1/1 Running 0 3m\nkubernetes-dashboard-77d8b98585-f4qh9 1/1 Running 0 3m\nstorage-provisioner 1/1 Running 0 3m\ntiller-deploy-865dd6c794-5b9g5 1/1 Running 0 3m\n```", "```\nhelm repo update\n```", "```\nHang tight while we grab the latest from your chart repositories...\n...Skip local chart repository\n...Successfully got an update from the \"stable\" chart repository\nUpdate Complete. \u2388 Happy Helming!\u2388\n```", "```\nNAME CHART VERSION APP VERSION DESCRIPTION\nstable/jenkins 0.14.1 2.73 Open source continuous integration server. It s...\n```", "```\n helm inspect stable/jenkins\n```", "```\nappVersion: \"2.73\"\ndescription: Open source continuous integration server. It supports multiple SCM tools\n including CVS, Subversion and Git. It can execute Apache Ant and Apache Maven-based\n projects as well as arbitrary scripts.\nhome: https://jenkins.io/\nicon: https://wiki.jenkins-ci.org/download/attachments/2916393/logo.png\nmaintainers:\n- email: lachlan.evenson@microsoft.com\n name: lachie83\n- email: viglesias@google.com\n name: viglesiasce\nname: jenkins\nsources:\n- https://github.com/jenkinsci/jenkins\n- https://github.com/jenkinsci/docker-jnlp-slave\nversion: 0.14.1\n\n---\n# Default values for jenkins.\n# This is a YAML-formatted file.\n# Declare name/value pairs to be passed into your templates.\n# name: value\n\n## Overrides for generated resource names\n# See templates/_helpers.tpl\n# nameOverride:\n# fullnameOverride:\n\nMaster:\n Name: jenkins-master\n Image: \"jenkins/jenkins\"\n ImageTag: \"lts\"\n ImagePullPolicy: \"Always\"\n# ImagePullSecret: jenkins\n Component: \"jenkins-master\"\n UseSecurity: true\n```", "```\nMaster:\n  ServiceType: NodePort\n```", "```\nhelm install stable/jenkins --name j \\\n-f jenkins.yaml --dry-run --debug\n```", "```\nhelm install stable/jenkins --name j -f jenkins.yaml\n```", "```\nNAME: j\nLAST DEPLOYED: Sun Mar 11 20:33:34 2018\nNAMESPACE: default\nSTATUS: DEPLOYED\n\nRESOURCES:\n==> v1/Pod(related)\nNAME READY STATUS RESTARTS AGE\nj-jenkins-6ff797cc8d-qlhbk 0/1 Init:0/1 0 0s\n==> v1/Secret\nNAME TYPE DATA AGE\nj-jenkins Opaque 2 0s\n==> v1/ConfigMap\nNAME DATA AGE\nj-jenkins 3 0s\nj-jenkins-tests 1 0s\n==> v1/PersistentVolumeClaim\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE\nj-jenkins Bound pvc-24a90c2c-25a6-11e8-9548-0800272e7159 8Gi RWO standard 0s\n==> v1/Service\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nj-jenkins-agent ClusterIP 10.107.112.29 <none> 50000/TCP 0s\nj-jenkins NodePort 10.106.245.61 <none> 8080:30061/TCP 0s\n==> v1beta1/Deployment\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE\nj-jenkins 1 1 1 0 0s\n\nNOTES:\n1\\. Get your 'admin' user password by running:\n printf $(kubectl get secret --namespace default j-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n2\\. Get the Jenkins URL to visit by running these commands in the same shell:\n export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].nodePort}\" services j-jenkins)\n export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\"{.items[0].status.addresses[0].address}\")\n echo http://$NODE_IP:$NODE_PORT/login\n\n3\\. Login with the password from step 1 and the username: admin\n\nFor more information on running Jenkins on Kubernetes, visit:\nhttps://cloud.google.com/solutions/jenkins-on-container-engine\n```", "```\nhelm status j\n```", "```\n2018-03-11 20:08:23 -0700 PDT 2018-03-11 20:08:23 -0700 PDT 1 minikube.151b0d76e3a375e1 Node Normal NodeReady kubelet, minikube Node minikube status is now: NodeReady\n\n2018-03-11 20:38:28 -0700 PDT 2018-03-11 20:38:28 -0700 PDT 1 j-jenkins-6ff797cc8d-qlhbk.151b0f1b339a1485 Pod spec.containers{j-jenkins} Normal Pulling kubelet, minikube pulling image \"jenkins/jenkins:lts\"\n\n2018-03-11 20:38:29 -0700 PDT 2018-03-11 20:38:29 -0700 PDT 1 j-jenkins-6ff797cc8d-qlhbk.151b0f1b7a153b09 Pod spec.containers{j-jenkins} Normal Pulled kubelet, minikube Successfully pulled image \"jenkins/jenkins:lts\"\n\n2018-03-11 20:38:29 -0700 PDT 2018-03-11 20:38:29 -0700 PDT 1 j-jenkins-6ff797cc8d-qlhbk.151b0f1b7d270e5e Pod spec.containers{j-jenkins} Normal Created kubelet, minikube Created container\n\n2018-03-11 20:38:30 -0700 PDT 2018-03-11 20:38:30 -0700 PDT 1 j-jenkins-6ff797cc8d-qlhbk.151b0f1b8359a5e4 Pod spec.containers{j-jenkins} Normal Started kubelet, minikube Started container\n```", "```\nprintf $(kubectl get secret --namespace default j-jenkins -o jsonpath=\"{.data.jenkins-admin-password}\" | base64 --decode);echo\n```", "```\nminikube service j-jenkins\n```"]