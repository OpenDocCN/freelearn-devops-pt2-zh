- en: Deploying Microservices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署微服务
- en: 'In this chapter, we will deal with two related, yet separate themes: production
    deployments and development deployments. The concerns, processes, and tools that
    are used for these two areas are very different. In both cases, the goal is to
    deploy new software to the cluster, but everything else is different. With production
    deployments, it''s desirable to keep the system stable, be able to have a predictable
    build and deployment experience, and most importantly, to identify and be able
    to roll back bad deployments. With development deployments, it''s desirable to
    have isolated deployments for each developer, a fast turnaround, and the ability
    to avoid cluttering source control or the **continuous integration **/ **continuous
    deployment** (**CI**/**CD**) system (including image registries) with temporary
    development versions. Due to this, divergent emphasis is beneficial to isolate
    production deployments from development deployments.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理两个相关但分开的主题：生产部署和开发部署。这两个领域使用的关注点、流程和工具都非常不同。在两种情况下，目标都是将新软件部署到集群中，但其他一切都不同。对于生产部署，保持系统稳定、能够获得可预测的构建和部署体验，最重要的是识别并能够回滚错误的部署是可取的。对于开发部署，希望为每个开发人员拥有隔离的部署，快速周转，并且能够避免在源代码控制或**持续集成**
    / **持续部署**（**CI**/**CD**）系统（包括镜像注册表）中堆积临时开发版本。因此，分歧的重点有利于将生产部署与开发部署隔离开来。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Kubernetes deployments
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes部署
- en: Deploying to multiple environments
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署到多个环境
- en: Understanding deployment strategies (rolling updates, blue-green deployment,
    canary deployment)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解部署策略（滚动更新、蓝绿部署、金丝雀部署）
- en: Rolling back deployments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚部署
- en: Managing versions and upgrades
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理版本和升级
- en: Local development deployments
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地开发部署
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will install many tools, including the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将安装许多工具，包括以下内容：
- en: KO
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KO
- en: Ksync
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ksync
- en: Draft
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Draft
- en: Skaffold
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skaffold
- en: Tilt
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tilt
- en: There is no need to install them ahead of time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 无需提前安装它们。
- en: The code
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: 'The code is split between two Git repositories:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 代码分为两个Git存储库：
- en: You can find the code samples here: [https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter11](https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter11)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在这里找到代码示例：[https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter11](https://github.com/PacktPublishing/Hands-On-Microservices-with-Kubernetes/tree/master/Chapter11)
- en: 'You can find the updated Delinkcious application here: [https://github.com/the-gigi/delinkcious/releases/tag/v0.9](https://github.com/the-gigi/delinkcious/releases/tag/v0.9)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在这里找到更新的Delinkcious应用程序：[https://github.com/the-gigi/delinkcious/releases/tag/v0.9](https://github.com/the-gigi/delinkcious/releases/tag/v0.9)
- en: Kubernetes deployments
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes部署
- en: We talked about deployments briefly in [Chapter 1](b66b66c3-db8c-474a-84b6-b87091f137f3.xhtml), *Introduction
    to Kubernetes for Developers*, and we've used Kubernetes deployments in almost
    every chapter. However, before diving into more sophisticated patterns and strategies,
    it will be useful to review the basic building blocks and the relationships between
    Kubernetes deployments, Kubernetes services, and scaling or autoscaling.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在《第1章》中简要讨论了部署，*开发人员的Kubernetes简介*，并且我们在几乎每一章中都使用了Kubernetes部署。然而，在深入研究更复杂的模式和策略之前，回顾基本构建块以及Kubernetes部署、Kubernetes服务和扩展或自动扩展之间的关系将是有用的。
- en: 'Deployments are Kubernetes resources that manage pods via a ReplicaSet. A Kubernetes
    ReplicaSet is a group of pods that are identified by a common set of labels with
    a certain number of replicas. The connection between the ReplicaSet to its pods
    is the `ownerReferences` field in the pod''s metadata. The ReplicaSet controller
    ensures that the correct number of replicas are always running. If a pod dies
    for whatever reason, the ReplicaSet controller will schedule a new pod in its
    place. The following diagram illustrates this relationship:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 部署是通过 ReplicaSet 管理 pod 的 Kubernetes 资源。Kubernetes ReplicaSet 是一组由一组共同的标签标识并具有一定数量副本的
    pod。ReplicaSet 与其 pod 之间的连接是 pod 元数据中的 `ownerReferences` 字段。ReplicaSet 控制器确保始终运行正确数量的副本。如果某个
    pod 因任何原因死亡，ReplicaSet 控制器将安排一个新的 pod 来替代它。以下图示了这种关系：
- en: '![](assets/61cb77c8-f8de-4945-8808-8b9b7651c0db.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/61cb77c8-f8de-4945-8808-8b9b7651c0db.png)'
- en: Deployment and ReplicaSet
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 部署和 ReplicaSet
- en: 'We can also observe the ownership chain in the metadata with kubectl. First,
    let''s get the name of the social graph manager pod and find the name of its ReplicaSet
    owner from the `ownerReferences` metadata:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 kubectl 在元数据中观察所有权链。首先，让我们获取社交图管理器 pod 的名称，并从 `ownerReferences` 元数据中找到其
    ReplicaSet 所有者的名称：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we''ll get the name of the deployment that owns the ReplicaSet:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将获取拥有 ReplicaSet 的部署的名称：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So, if the ReplicaSet controller takes care of managing the number of pods,
    what does the `Deployment` object add? The `Deployment` object encapsulates the
    concept of a deployment, including a deployment strategy and rollout history.
    It also provides deployment-oriented operations such as updating a deployment
    and rolling back a deployment, which we will look at later.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果 ReplicaSet 控制器负责管理 pod 的数量，那么 `Deployment` 对象添加了什么呢？`Deployment` 对象封装了部署的概念，包括部署策略和部署历史。它还提供了面向部署的操作，如更新部署和回滚部署，我们稍后会看到。
- en: Deploying to multiple environments
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署到多个环境
- en: In this section, we will create a staging environment for Delinkcious in a new
    staging namespace. The `staging` namespace will be a full-fledged copy of the
    default namespace that will serve as our production environment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在新的 staging 命名空间中为 Delinkcious 创建一个 staging 环境。`staging` 命名空间将是默认命名空间的一个完整副本，将作为我们的生产环境。
- en: 'First, let''s create the namespace:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建命名空间：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, in Argo CD, we can create a new project called `staging`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在 Argo CD 中，我们可以创建一个名为 `staging` 的新项目：
- en: '![](assets/d9bd625b-c230-4165-9dad-f8fda2201a97.png)Argo CD staging project'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/d9bd625b-c230-4165-9dad-f8fda2201a97.png)Argo CD staging 项目'
- en: 'Now, we need to configure all our services so that Argo CD can sync them to
    the staging environment. This can be a little tedious to do in the UI now that
    we have a substantial amount of services. Instead, we will use the Argo CD CLI
    and a Python 3 program called `bootstrap_staging.py` to automate the process.
    The program expects the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要配置所有服务，以便 Argo CD 可以将它们同步到 staging 环境。现在在 UI 中执行这项工作可能有点繁琐，因为我们有大量的服务。相反，我们将使用
    Argo CD CLI 和一个名为 `bootstrap_staging.py` 的 Python 3 程序来自动化这个过程。该程序需要以下内容：
- en: The staging namespace has been created.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已创建 staging 命名空间。
- en: The Argo CD CLI is installed and in the path.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argo CD CLI 已安装并在路径中。
- en: The Argo CD service is available through the localhost on port `8080`.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argo CD 服务可以通过本地主机的端口 `8080` 访问。
- en: The Argo CD admin password is configured as the environment variable.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argo CD 管理员密码配置为环境变量。
- en: 'To expose Argo CD on the localhost at port `80`, we can run the following command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要在本地主机上的端口 `80` 上暴露 Argo CD，我们可以运行以下命令：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s break down the program and understand how it works. This is a good foundation
    where you can develop your own custom CI/CD solutions by automating CLI tools.
    The only dependencies are Python''s standard library modules: `subprocess` (allows
    you to run command-line tools) and `os` (for accessing environment variables).
    Here, we only need to run the Argo CD CLI.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分解程序并了解它是如何工作的。这是一个很好的基础，您可以通过自动化 CLI 工具来开发自己的定制 CI/CD 解决方案。唯一的依赖是 Python
    的标准库模块：`subprocess`（允许您运行命令行工具）和 `os`（用于访问环境变量）。在这里，我们只需要运行 Argo CD CLI。
- en: 'The `run()` function hides all the implementation details and provides a convenient
    interface where you just need to pass the arguments as a string. The `run()` function
    will prepare a proper command list that can be passed to the `subprocess` module''s
    `check_output()` function, capture the output, and decode it from bytes to a string:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`run()` 函数隐藏了所有实现细节，并提供了一个方便的接口，您只需要将参数作为字符串传递。`run()` 函数将准备一个适当的命令列表，可以传递给
    `subprocess` 模块的 `check_output()` 函数，捕获输出，并将其从字节解码为字符串：'
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `login()` function utilizes `run()`, gets the admin password from the environment,
    and constructs the proper command string with all the necessary flags so that
    you can log in as the admin user to Argo CD:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`login()` 函数利用 `run()`，从环境中获取管理员密码，并构造适当的命令字符串，带有所有必要的标志，以便您可以作为管理员用户登录到 Argo
    CD：'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `get_apps()` function takes a namespace and returns the relevant fields
    of the Argo CD apps in it. This function will be used both on the `default` namespace
    and the `staging` namespace. The function invokes the `app list` command, parses
    the output, and populates a Python dictionary with the relevant information:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_apps()` 函数接受一个命名空间，并返回其中 Argo CD 应用程序的相关字段。这个函数将在 `default` 命名空间和 `staging`
    命名空间上都被使用。该函数调用 `app list` 命令，解析输出，并用相关信息填充一个 Python 字典：'
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `create_project()` function takes all the necessary information to create
    a new Argo CD project. Note that multiple Argo CD projects can coexist in the
    same Kubernetes namespace. It also allows access to all cluster resources, which
    is necessary to create applications. Since we have already created the project
    in the Argo CD UI, there is no need to use it in this program, but it''s good
    to have it around in case we need to create more projects in the future:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_project()` 函数接受创建新的 Argo CD 项目所需的所有信息。请注意，多个 Argo CD 项目可以共存于同一个 Kubernetes
    命名空间中。它还允许访问所有集群资源，这对于创建应用程序是必要的。由于我们已经在 Argo CD UI 中创建了项目，所以在这个程序中不需要使用它，但是如果将来需要创建更多项目，保留它是很好的：'
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The last generic function is called `create_app()`, and takes all the necessary
    information for creating an Argo CD application. It assumes that Argo CD is running
    inside the destination cluster, so `--dest-server` is always `https://kubernetes.default.svc`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个通用函数被称为 `create_app()`，它接受创建 Argo CD 应用程序所需的所有信息。它假设 Argo CD 正在目标集群内运行，因此
    `--dest-server` 始终是 `https://kubernetes.default.svc`：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `copy_apps_from_default_to_staging()` function uses some of the functions
    we declared earlier. It gets all the apps from the default namespace, iterates
    over them, and creates the same app in the staging project and namespace:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`copy_apps_from_default_to_staging()` 函数使用了我们之前声明的一些函数。它获取默认命名空间中的所有应用程序，对它们进行迭代，并在暂存项目和命名空间中创建相同的应用程序：'
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, here''s the `main` function:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是 `main` 函数：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have two environments, let's consider some workflows and promotion
    strategies. Whenever a change is pushed, GitHub CircleCI will detect it. If all
    the tests pass, it will bake a new image for each service and push it to Docker
    Hub. The question is, what should happen on the deployment side? Argo CD has sync
    policies, and we can configure them to automatically sync/deploy whenever a new
    image is available on Docker Hub. For example, a common practice is to automatically
    deploy to staging, deploying to production only after various tests (for example,
    the `smoke` test) have passed on staging. The promotion from staging to production
    may be automated or manual.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个环境，让我们考虑一些工作流程和推广策略。每当有变更被推送时，GitHub CircleCI 将会检测到它。如果所有的测试都通过了，它将为每个服务烘烤一个新的镜像并将其推送到
    Docker Hub。问题是，在部署方面应该发生什么？Argo CD 有同步策略，我们可以配置它们在 Docker Hub 上有新镜像时自动同步/部署。例如，一个常见的做法是自动部署到
    staging，只有在 staging 上通过了各种测试（例如 `smoke` 测试）后才部署到 production。从 staging 到 production
    的推广可能是自动的或手动的。
- en: There is no one-size-fits-all answer. Even within the same organization, different
    deployment policies and strategies are often employed for projects or services
    with different sets of requirements.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一种适合所有情况的答案。即使在同一个组织中，不同的部署策略和策略通常也会用于具有不同需求的项目或服务。
- en: Let's look at some of the more common deployment strategies and what use cases
    they enable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些更常见的部署策略以及它们能够实现的用例。
- en: Understanding deployment strategies
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解部署策略
- en: 'A deployment of a new version of a service in Kubernetes means replacing the
    *N* backing pods of the service, which run version *X* with *N* backing pods running
    version *X+1*. There are multiple ways to get from N pods running version *X*,
    to zero pods running version *X* and *N* pods running version *X+1*. Kubernetes
    deployments support two strategies out of the box: `Recreate` and `RollingUpdate`
    (the default strategy). Blue-green deployments and canary deployments are two
    other popular strategies. Before diving into the various deployment strategies,
    as well as their pros and cons, it''s important to understand the process of updating
    a deployment in Kubernetes.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署一个服务的新版本意味着用运行版本 *X* 的 *N* 个后台 pod 替换为运行版本 *X+1* 的 *N* 个后台 pod。从运行版本
    *X* 的 N 个 pod 到运行版本 *X+1* 的 N 个 pod 有多种方法。Kubernetes 部署支持两种默认策略：`Recreate` 和 `RollingUpdate`（默认策略）。蓝绿部署和金丝雀部署是另外两种流行的策略。在深入研究各种部署策略以及它们的优缺点之前，了解在
    Kubernetes 中更新部署的过程是很重要的。
- en: A rollout of a new set of pods for a deployment happens if and only if the deployment
    spec's pod template has changed. This typically happens when you change the image
    version of the pod template or the set of labels for a container. Note that scaling
    a deployment (increasing or decreasing its number of replicas) is *not* an update,
    so the deployment strategy is not used. The same version of the image as the current
    running pods will be used in any new pods that are added.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当部署规范的 pod 模板发生变化时，才会出现部署的新一组 pod。这通常发生在您更改 pod 模板的镜像版本或容器的标签集时。请注意，扩展部署（增加或减少其副本数量）*不*是更新，因此不会使用部署策略。任何新添加的
    pod 中将使用与当前运行的 pod 相同版本的镜像。
- en: Recreating deployment
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新创建部署
- en: 'A trivial yet naive way to do this is to terminate all the pods running version
    *X*, and then create a new deployment where the image version in the pod template
    spec is set to *X+1*. There are a couple of problems with this approach:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个微不足道但天真的做法是终止所有运行版本 *X* 的 pod，然后创建一个新的部署，其中 pod 模板规范中的镜像版本设置为 *X+1*。这种方法有一些问题：
- en: The service will be unavailable until the new pods come online.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在新的 pod 上线之前，服务将不可用。
- en: If the new version has issues, the service will be unavailable until the process
    is reversed (ignoring errors and data corruption).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果新版本有问题，服务将在过程被逆转之前不可用（忽略错误和数据损坏）。
- en: The `Recreate` deployment strategy is appropriate for development, or when you
    prefer to have a short outage, but ensure that there is no mix of versions that
    are live at the same time. The short outage may be acceptable, for example, if
    the service pulls its work from a queue and there are no adverse consequences
    if the service is briefly down while upgrading to a new version. Another situation
    is if you want to change the public API of the service or one of its dependencies
    in a non-backward compatible way. In this case, the current pods must be terminated
    in one fell swoop, and the new pods must be deployed. There are solutions for
    multi-phase deployments of incompatible changes, but in some cases, it is easier
    and acceptable to just cut the cord and pay the cost of a short outage.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`Recreate`部署策略适用于开发，或者当您希望有短暂的中断，但确保没有同时存在的版本混合。短暂的中断可能是可以接受的，例如，如果服务从队列中获取其工作，并且在升级到新版本时服务短暂下线没有不良后果。另一种情况是，如果您希望以不向后兼容的方式更改服务或其依赖项的公共API。在这种情况下，当前的pod必须一次性终止，并且必须部署新的pod。对于不兼容更改的多阶段部署有解决方案，但在某些情况下，更容易和可接受的方法是直接切断联系，并支付短暂中断的成本。'
- en: 'To enable this strategy, edit the deployment''s manifest, change the strategy
    type to be `Recreate`, and remove the `rollingUpdate` section (this is only allowed
    when the type is `RollingUpdate`):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用这种策略，编辑部署的清单，将策略类型更改为`Recreate`，并删除`rollingUpdate`部分（只有在类型为`RollingUpdate`时才允许这样做）：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For most services, it is desirable to have service continuity and zero downtime
    when upgrading, as well as an instant rollback in case a problem is detected.
    The `RollingUpdate` strategy addresses these situations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数服务，希望在升级时保持服务连续性和零停机时间，并在检测到问题时立即回滚。`RollingUpdate`策略解决了这些情况。
- en: Rolling updates
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滚动更新
- en: 'The default deployment strategy is `RollingUpdate`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的部署策略是`RollingUpdate`：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Rolling updates work as follows: the total number of pods (old and new) is
    going to be the current replica count, plus the max surge. The deployment controller
    will start replacing old pods with new pods, making sure not to exceed the limit.
    The max surge can be an absolute number, such as 4, or a percentage, such as 25%.
    For example, if the number of replicas for the deployment is 4 and the max surge
    is 25%, then an additional new pod can be added, and one of the old pods can be
    terminated. `maxUnavailable` is the number of pods that are below the replica
    count during a deployment.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新的工作方式如下：pod的总数（旧的和新的）将是当前副本数加上最大浪涌。部署控制器将开始用新的pod替换旧的pod，确保不超过限制。最大浪涌可以是绝对数，比如4，也可以是百分比，比如25%。例如，如果部署的副本数是4，最大浪涌是25%，那么可以添加一个额外的新pod，并终止一个旧的pod。`maxUnavailable`是在部署期间低于副本数的pod的数量。
- en: 'The following diagram illustrates how rolling updates work:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了滚动更新的工作方式：
- en: '![](assets/56de03e9-954a-4146-bb61-287e67118e16.png)Rolling update'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![](assets/56de03e9-954a-4146-bb61-287e67118e16.png)滚动更新'
- en: Rolling updates make sense when the new version is compatible with the current
    version. The number of active pods that are ready to handle requests remains within
    a reasonable range of the replica count that you specify using `maxSurge` and
    `maxUnavailable`, and gradually, all the current pods are replaced with new pods.
    The overall service is not disrupted.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新在新版本与当前版本兼容时是有意义的。准备处理请求的活动pod的数量保持在您使用`maxSurge`和`maxUnavailable`指定的副本数的合理范围内，并且逐渐地，所有当前的pod都被新的pod替换。整体服务不会中断。
- en: Sometimes, however, you must replace all the pods at once, and for critical
    services that must remain available, the `Recreate` strategy doesn't work. This
    is where blue-green deployments come in.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时您必须立即替换所有的pod，对于必须保持可用性的关键服务，`Recreate`策略是行不通的。这就是蓝绿部署的用武之地。
- en: Blue-green deployment
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: Blue-green deployment is a well-known pattern. The idea is that you don't update
    the existing deployment; instead, you create a brand new deployment with the new
    version. Initially, your new version doesn't service traffic. Then, when you verify
    that the new deployment is up and running (you can even run some `smoke` tests
    against it), you switch all the traffic in one fell swoop from the current version
    to the new version. If you encounter any problems after you switch to the new
    version, you can instantly switch all the traffic back to the previous deployment,
    which is still up and running. When you are confident that the new deployment
    is doing well, you can destroy the previous deployment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署是一个众所周知的模式。其思想是不更新现有部署；相反，您创建一个带有新版本的全新部署。最初，您的新版本不提供服务流量。然后，当您验证新部署已经运行良好（甚至可以对其运行一些`smoke`测试），您一次性将所有流量从当前版本切换到新版本。如果在切换到新版本后遇到任何问题，您可以立即将所有流量切换回先前的部署，该部署仍在运行。当您确信新部署运行良好时，您可以销毁先前的部署。
- en: One of the greatest advantages of blue-green deployments is that they don't
    have to operate at the level of a single Kubernetes deployment. This can be critical
    in a microservice architecture where you must update multiple interacting services
    at the same time. If you tried to do it just by updating multiple Kubernetes deployments
    at the same time, there could be some services that have already been replaced
    and some that weren't (even if you accept the cost of the `Recreate` strategy).
    If a single service experiences problems during deployment, you now have to roll
    back all the other services. With blue-green deployments, you are safe from these
    issues and are in total control of when you want to switch to the new version
    across all services at once.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署的最大优势之一是它们不必在单个Kubernetes部署的级别上运行。在微服务架构中，您必须同时更新多个交互服务时，这一点至关重要。如果您尝试同时更新多个Kubernetes部署，可能会出现一些服务已经被替换，而另一些没有（即使您接受`Recreate`策略的成本）。如果在部署过程中出现单个服务的问题，现在您必须回滚所有其他服务。通过蓝绿部署，您可以避免这些问题，并完全控制何时要一次性切换到所有服务的新版本。
- en: How do you switch from blue (current) to green (new)? The traditional approach
    that works with Kubernetes is to do it at the load balancer level. Most systems
    that require such a sophisticated deployment strategy will have a load balancer.
    When you use the load balancer to switch traffic, your green deployment includes
    both a green Kubernetes deployment and a green Kubernetes service, as well as
    any other resources if anything needs to change, such as secrets and config maps.
    If you need to update multiple services, then you'll have a collection of green
    resources that all refer to each other.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如何从蓝色（当前）切换到绿色（新）？与Kubernetes兼容的传统方法是在负载均衡器级别进行操作。大多数需要如此复杂部署策略的系统都会有负载均衡器。当您使用负载均衡器切换流量时，您的绿色部署包括绿色Kubernetes部署和绿色Kubernetes服务，以及任何其他资源（如果需要更改），如密码和配置映射。如果需要更新多个服务，那么您将拥有一组相互引用的绿色资源。
- en: If you have an Ingress controller such as contour, then it can often be used
    to switch traffic from blue to green and back, if necessary.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有像contour这样的Ingress控制器，那么它通常可以用于在需要时将流量从蓝色切换到绿色，然后再切换回来。
- en: 'The following diagram illustrates how blue-green deployments work:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了蓝绿部署的工作原理：
- en: '![](assets/2943332f-b426-4dd6-9127-7b0943e69ba9.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/2943332f-b426-4dd6-9127-7b0943e69ba9.png)'
- en: Blue-green deployment
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: 'Let''s do a single-service blue-green deployment for the link manager service.
    We''ll call our starting point *blue*, and we want to deploy the *green* version
    of link manager without disruption. Here''s the plan:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为link manager服务进行单一服务的蓝绿部署。我们将起点称为*blue*，并且我们希望在没有中断的情况下部署*green*版本的link
    manager。这是计划：
- en: 'Add the `deployment: blue` label to the current `link-manager` deployment.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '向当前的`link-manager`部署添加`deployment: blue`标签。'
- en: 'Update the `link-manager` service selector to match the `deployment: blue`
    label.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '更新`link-manager`服务选择器以匹配`deployment: blue`标签。'
- en: Implement the new version of `LinkManager` that prefixes the description of
    each link with the `[green]` string.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`LinkManager`的新版本，该版本使用`[green]`字符串作为每个链接描述的前缀。
- en: 'Add the `deployment: green` label to the deployment''s pod template spec.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '向部署的pod模板规范添加`deployment: green`标签。'
- en: Bump the version number.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提升版本号。
- en: Let CircleCI create a new version.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让CircleCI创建一个新版本。
- en: Deploy the new version as a separate deployment called `green-link-manager`.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新版本部署为名为`green-link-manager`的单独部署。
- en: 'Update the `link-manager` service selector to match the `deployment: green`
    label.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '更新`link-manager`服务选择器以匹配`deployment: green`标签。'
- en: Verify the description of the returned links from the service and include the
    `[green]` prefix.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证服务返回的链接描述，并包括`[green]`前缀。
- en: This may sound complicated, but just like many CI/CD processes, once you establish
    a pattern, you can automate and build tooling around it. This lets you execute
    complex workflows without human involvement, or inject human review and approval
    at important junctures (for example, before really deploying to production). Let's
    go over the steps in detail.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来很复杂，但就像许多CI/CD流程一样，一旦你建立了一个模式，你就可以自动化并构建围绕它的工具。这样你就可以在没有人类参与的情况下执行复杂的工作流程，或者在重要的关键点（例如，在真正部署到生产环境之前）注入人工审查和批准。让我们详细介绍一下步骤。
- en: Adding deployment – the blue label
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加部署-蓝色标签
- en: 'We can just edit the deployment and manually add `deployment: blue`, in addition
    to the existing `svc: link` and `app: manager` labels:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以编辑部署，并手动添加`deployment: blue`，除了现有的`svc: link`和`app: manager`标签：'
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will trigger a redeployment of the pods because we changed the labels.
    Let''s verify that the new pods have the `deployment: blue` label. Here is a pretty
    fancy `kubectl` command that uses custom columns to display the name, the deployment
    label, and the IP addresses of all the pods that match `svc=link` and `app=manager`.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '这将触发pod的重新部署，因为我们改变了标签。让我们验证新的pod是否有`deployment: blue`标签。这里有一个相当花哨的`kubectl`命令，使用自定义列来显示所有匹配`svc=link`和`app=manager`的pod的名称、部署标签和IP地址。'
- en: 'As you can see, all three pods have the `deployment:blue` label, as expected:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，所有三个pod都有`deployment:blue`标签，正如预期的那样：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can even verify that the IP addresses match the endpoints of the `link-manager`
    service:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以验证IP地址是否与`link-manager`服务的端点匹配：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that the pods are labeled with the `blue` label, we need to update the service.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在pod都带有`blue`标签，我们需要更新服务。
- en: Updating the link-manager service to match blue pods only
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新link-manager服务以仅匹配蓝色pod
- en: 'The service, as you may recall, matches any pods with the `svc: link` and `app:
    manager` labels:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '服务，你可能还记得，匹配任何带有`svc: link`和`app: manager`标签的pod：'
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'By adding the `deployment: blue` label, we didn''t interfere with the matching.
    However, in preparation for our green deployment, we should make sure that the
    service only matches the pods of the current blue deployment.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '通过添加`deployment: blue`标签，我们没有干扰匹配。然而，为了准备我们的绿色部署，我们应该确保服务只匹配当前蓝色部署的pod。'
- en: 'Let''s add the `deployment: blue` label to the service''s `selector`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们将`deployment: blue`标签添加到服务的`selector`中：'
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can verify that it worked by using the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用以下命令来验证它是否起作用：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Before we switch to the green version, let's make a change in the code to make
    it clear that's it's a different version.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在切换到绿色版本之前，让我们在代码中做出更改，以清楚表明这是一个不同的版本。
- en: Prefixing the description of each link with [green]
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在每个链接的描述前加上[绿色]
- en: Let's do this in the transport layer of the link service.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在链接服务的传输层中做这个。
- en: The target file is [https://github.com/the-gigi/delinkcious/blob/master/svc/link_service/service/transport.go#L26](https://github.com/the-gigi/delinkcious/blob/master/svc/link_service/service/transport.go#L26).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 目标文件是[https://github.com/the-gigi/delinkcious/blob/master/svc/link_service/service/transport.go#L26](https://github.com/the-gigi/delinkcious/blob/master/svc/link_service/service/transport.go#L26)。
- en: 'The change is very minimal. In the `newLink()` function, we will prefix the
    description with the `[green]` string:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 更改非常小。在`newLink()`函数中，我们将描述前缀为`[green]`字符串：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In order to deploy our new green version, we need to create a new image. This
    requires bumping the Delinkcious version number.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了部署我们的新绿色版本，我们需要创建一个新的镜像。这需要提升Delinkcious版本号。
- en: Bumping the version number
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升版本号
- en: The Delinkcious version is maintained in the `[build.sh]` file at ([https://github.com/the-gigi/delinkcious/blob/master/build.sh#L6](https://github.com/the-gigi/delinkcious/blob/master/build.sh#L6))
    that CircleCI is invoked from, that is, the `[.circleci/config.yml]` file at ([https://github.com/the-gigi/delinkcious/blob/master/.circleci/config.yml#L28](https://github.com/the-gigi/delinkcious/blob/master/.circleci/config.yml#L28))
    file.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Delinkcious版本在`[build.sh]`文件中维护（[https://github.com/the-gigi/delinkcious/blob/master/build.sh#L6](https://github.com/the-gigi/delinkcious/blob/master/build.sh#L6)），CircleCI从中调用，即`[.circleci/config.yml]`文件（[https://github.com/the-gigi/delinkcious/blob/master/.circleci/config.yml#L28](https://github.com/the-gigi/delinkcious/blob/master/.circleci/config.yml#L28)）。
- en: 'The `STABLE_TAG` variable controls the version numbers. The current version
    is `0.3`. Let''s bump it up to `0.4`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`STABLE_TAG`变量控制版本号。当前版本是`0.3`。让我们将其提升到`0.4`：'
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: OK. We bumped the version and we're ready to have CircleCI build a new image.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我们提升了版本号，现在可以让CircleCI构建一个新的镜像。
- en: Letting CircleCI build the new image
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让CircleCI构建新镜像
- en: 'Thanks to GitOps and our CircleCI automation, this step just involves pushing
    our changes to GitHub. CircleCI detects the change, builds the new code, creates
    a new Docker image, and pushes it to the Docker Hub registry. Here it is:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GitOps和我们的CircleCI自动化，这一步只涉及将我们的更改推送到GitHub。CircleCI检测到更改，构建新代码，创建新的Docker镜像，并将其推送到Docker
    Hub注册表。就是这样：
- en: '![](assets/5111b458-c9d5-4b26-97f0-09cfef35f829.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/5111b458-c9d5-4b26-97f0-09cfef35f829.png)'
- en: Docker Hub link service 0.4
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Hub链接服务0.4
- en: Now that the new image has been built and pushed to the Docker Hub registry,
    we can deploy it to the cluster as the green deployment.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在新镜像已经构建并推送到Docker Hub注册表，我们可以将其部署到集群作为绿色部署。
- en: Deploying the new (green) version
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署新的（绿色）版本
- en: 'OK – we''ve got our new `delinkcious-link:0.4` image on Docker Hub. Let''s
    deploy it to the cluster. Remember that we want to deploy it side by side with
    our current (blue) deployment, which is called `link-manager`. Let''s create a
    new deployment called `green-link-manager`. The differences it has to our blue
    deployment are as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 好的 - 我们在Docker Hub上有了我们的新的`delinkcious-link:0.4`镜像。让我们将其部署到集群中。请记住，我们希望将其与我们当前的（蓝色）部署一起部署，即`link-manager`。让我们创建一个名为`green-link-manager`的新部署。它与我们的蓝色部署的区别如下：
- en: The name is `green-link-manager`.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名称是`green-link-manager`。
- en: 'The pod template spec has the `deployment: green` label.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pod模板规范具有`deployment: green`标签。'
- en: The image version is `0.4`.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 镜像版本是`0.4`。
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, it''s time to deploy:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候部署了：
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Before we update the service to use the green deployment, let''s review the
    cluster. As you can see, we have the blue and green deployments running side by
    side:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们更新服务以使用绿色部署之前，让我们先审查一下集群。正如您所看到的，我们有蓝色和绿色部署并行运行：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Updating the link-manager service to use the green deployment
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新链接管理器服务以使用绿色部署
- en: 'First, let''s make sure that the service is still using the blue deployment.
    When we get a link description, there shouldn''t be any `[green]` prefix:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们确保服务仍在使用蓝色部署。当我们得到一个链接描述时，不应该有任何`[green]`前缀：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The description is `nothing to see here..*.*`. This time, instead of interactively
    editing the service using `kubectl edit`, we will use the `kubectl patch` command
    to apply a patch that will switch the deployment label from `blue` to `green`.
    Here is the patch file – `green-patch.yaml`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 描述是`没有什么可看的..*.*`。这一次，我们将使用`kubectl patch`命令应用补丁，而不是交互式地使用`kubectl edit`编辑服务，以应用一个将部署标签从`blue`切换到`green`的补丁。这是补丁文件-`green-patch.yaml`：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s apply the patch:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用补丁：
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The last step is to verify that the service now uses the green deployment.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是验证服务现在是否使用绿色部署。
- en: Verifying that the service now uses the green pods to serve requests
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证服务现在是否使用绿色pod来处理请求。
- en: 'Let''s do this methodically, starting with the selector in the service:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们有条不紊地进行，从服务中的选择器开始：
- en: '[PRE27]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'OK – the selector is green. Let''s get the links again and see if the `[green]`
    prefix shows up:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 好的-选择器是绿色的。让我们再次获取链接，看看`[green]`前缀是否出现：
- en: '[PRE28]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Yes! The description is now `[green] nothing to see here...`
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！描述现在是`[green] 这里没有什么可看的...`
- en: 'Now, we can get rid of the blue deployment and our service will continue running
    against the green deployment:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以摆脱蓝色部署，我们的服务将继续针对绿色部署运行：
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We have successfully performed a blue-green deployment on Delinkcious. Let's
    discuss the last pattern, that is, canary deployments.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功在Delinkcious上执行了蓝绿部署。让我们讨论最后一个模式，即金丝雀部署。
- en: Canary deployments
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金丝雀部署
- en: Canary deployments are another sophisticated deployment pattern. Consider the
    situation of a massive distributed system with lots of users. You want to introduce
    a new version of the service. You have tested this change to the best of your
    ability, but the production system is too complex to mimic completely in a staging
    environment. As a result, you can't be sure that your new version will not cause
    some problems. What do you do? You use canary deployments. The idea is that some
    changes must be tested in production before you can be reasonably sure they work
    as expected. The canary deployment patterns allow you to limit the damage the
    new version might cause if something goes wrong.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署是另一种复杂的部署模式。考虑一个拥有大量用户的大规模分布式系统的情况。您想要引入服务的新版本。您已经尽力测试了这个变化，但是生产系统太复杂，无法在测试环境中完全模拟。因此，您无法确定新版本不会引起一些问题。你该怎么办？您可以使用金丝雀部署。其想法是，某些变化必须在生产环境中进行测试，然后您才能相对确定它们能够按预期工作。金丝雀部署模式允许您限制新版本可能引起的损害，如果出现问题。
- en: Basic canary deployments on Kubernetes work by running the current version on
    most of your pods, and just a small number of pods with the new version. Most
    of the requests will be processed by the current version, and only a small proportion
    will be processed by the new version.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes上的基本金丝雀部署通过在大多数pod上运行当前版本，只在少数pod上运行新版本来工作。大多数请求将由当前版本处理，只有一小部分请求将由新版本处理。
- en: This assumes a round-robin load balancing algorithm (the default), or any other
    algorithm that distributes requests more or less uniformly across all pods.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这假设了一个轮询负载均衡算法（默认），或者任何其他分配请求更或多或少均匀地跨所有pod的算法。
- en: 'The following diagram illustrates what canary deployments look like:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了金丝雀部署的外观：
- en: '![](assets/a183a687-f439-473e-95f6-4b69491d0f41.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: ！[](assets/a183a687-f439-473e-95f6-4b69491d0f41.png)
- en: Canary deployment
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署
- en: Note that canary deployments require that your current version and your new
    version can coexist. For example, if your change involved a schema change, then
    your current and new versions are incompatible, and naive canary deployment will
    not work.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，金丝雀部署要求您的当前版本和新版本可以共存。例如，如果您的更改涉及模式更改，则您的当前版本和新版本是不兼容的，天真的金丝雀部署将无法工作。
- en: 'The nice thing about the basic canary deployment is that it works using existing
    Kubernetes objects and can be configured by an operator from the outside. There''s
    no need for custom code or installing additional components into your cluster.
    However, the basic canary deployment has several limitations:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 基本金丝雀部署的好处在于它使用现有的Kubernetes对象，并且可以由外部的操作员进行配置。无需自定义代码或将其他组件安装到您的集群中。但是，基本金丝雀部署有一些限制：
- en: The granularity is K/N (the worst case is singletons where N = 1).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 粒度为K/N（最坏情况是N = 1的单例）。
- en: Can't control different percentages for different requests to the same service
    (for example, canary deployments of read requests only).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法控制对同一服务的不同请求的不同百分比（例如，仅读请求的金丝雀部署）。
- en: Can't control all requests for a user who goes to the same version.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法控制同一用户对同一版本的所有请求。
- en: In some cases, these limitations are too severe and another solution is needed.
    Sophisticated canary deployments typically utilize application-level knowledge.
    This can be done through Ingress objects, a service mesh, or a dedicated application-level
    traffic shaper. We will look at an example of this in [Chapter 13](b39834c8-859c-42a5-846a-e48b76dfd6cc.xhtml),
    *Service Mesh – Working with Istio*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，这些限制太严重，需要另一种解决方案。复杂的金丝雀部署通常利用应用程序级别的知识。这可以通过Ingress对象、服务网格或专用的应用程序级别流量整形器来实现。我们将在[第13章](b39834c8-859c-42a5-846a-e48b76dfd6cc.xhtml)中看一个例子，*服务网格
    - 与Istio一起使用*。
- en: It's time for a hands-on canary deployment of the link service.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候进行link服务的实际金丝雀部署了。
- en: Employing a basic canary deployment for Delinkcious
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为Delinkcious使用基本金丝雀部署
- en: 'Creating a canary deployment is very similar to blue-green deployment. Our
    `link-manager` service currently runs the green deployment. This means that it
    has a selector with `deployment: green`. Canaries are yellow, so we will create
    a new version of the code that prefixes the link description with `[yellow]`.
    Let''s aim for 10% of requests going to the new version. In order to achieve this,
    we will scale the current versions to nine replicas and add a deployment with
    one pod with the new version. Here is the canary trick – we will drop the deployment
    label from the service selector. This means that it will select both pods; that
    is, `deployment: green` and `deployment: yellow`. We could also drop the labels
    from the deployments (because nobody is selecting based on this label), but it''s
    good to keep them around as metadata, and also in case we want to do another blue-green
    deployment.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '创建金丝雀部署与蓝绿部署非常相似。我们的`link-manager`服务当前正在运行绿色部署。这意味着它具有`deployment: green`的选择器。金丝雀是黄色的，所以我们将创建一个新版本的代码，该代码在链接描述前加上`[yellow]`。让我们的目标是将10%的请求发送到新版本。为了实现这一点，我们将将当前版本扩展到九个副本，并添加一个具有新版本的部署。这就是金丝雀的技巧
    - 我们将从服务选择器中删除部署标签。这意味着它将选择两个pod；即`deployment: green`和`deployment: yellow`。我们也可以从部署中删除标签（因为没有人是基于此标签进行选择），但最好将它们保留为元数据，以及以防我们想要进行另一个蓝绿部署。'
- en: 'Here is the plan:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计划：
- en: Build a new version of the code.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建代码的新版本。
- en: 'Create a deployment with a replica count of one for the new version, which
    is labeled as `deployment: yellow`.'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '创建一个新版本的副本计数为1的部署，标记为`deployment: yellow`。'
- en: Scale the current green deployment to nine replicas.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将当前的绿色部署扩展到九个副本。
- en: 'Update the service to select for `svc: link` and `app: manager` (ignore `deployment:
    <color>`).'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '更新服务以选择`svc: link`和`app: manager`（忽略`deployment: <color>`）。'
- en: Run multiple queries against the service and verify that the ratio of requests
    that are being served by the canary deployment is 10%.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对服务运行多个查询，并验证由金丝雀部署提供服务的请求比例为10%。
- en: 'The code change is `trivial: [green]  -> [yellow]`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '代码更改是`trivial: [green] -> [yellow]`：'
- en: '[PRE30]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, we need to bump the version in `build.sh` from `0.4` to `0.5`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要将`build.sh`中的版本从`0.4`升级到`0.5`：
- en: '[PRE31]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once we push these changes to GitHub, CircleCI will build and push a new image
    to `DockerHub: g1g1/delinkcious-link:0.5`.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦我们将这些更改推送到GitHub，CircleCI将构建并推送一个新的镜像到`DockerHub: g1g1/delinkcious-link:0.5`。'
- en: 'At this point, we can create a deployment with the new `0.5` version, a single
    replica, and updated labels. Let''s call it `yellow_link_manager.yaml`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以创建一个新的`0.5`版本的部署，一个单独的副本，并更新标签。让我们称之为`yellow_link_manager.yaml`：
- en: '[PRE32]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The next step is deploying our canary:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是部署我们的金丝雀：
- en: '[PRE33]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Before changing the service to include the canary deployment, let''s scale
    the green deployment to 9 replicas so that it can receive 90% of the traffic once
    we activate our canary:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在更改服务以包含金丝雀部署之前，让我们将绿色部署扩展到9个副本，以便在激活金丝雀后它可以接收90%的流量：
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Alright, we have nine green pods and one yellow (canary) pod running. Let''s
    update the service to select just based on the `svc: link` and `app: manager` labels,
    which will include all ten pods. We need to remove the `deployment: green` label.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '好了，我们有九个绿色的pod和一个黄色（金丝雀）的pod在运行。让我们更新服务，只基于`svc: link`和`app: manager`标签进行选择，这将包括所有十个pod。我们需要删除`deployment:
    green`标签。'
- en: The YAML patch file method we've used before doesn't work here, because it can
    only add or update a label. We'll use a JSON patch this time with the *remove*
    operation and specify the path to the *deployment* key in the selector.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用的YAML补丁文件方法在这里不起作用，因为它只能添加或更新标签。这次我们将使用JSON补丁，使用*remove*操作，并指定选择器中*deployment*键的路径。
- en: 'Note that before the patch, the selector had `deployment: green`, and that
    after the patch, only `svc: link` and `app: manager` remain:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，在打补丁之前，选择器中有`deployment: green`，而在打补丁之后，只剩下`svc: link`和`app: manager`：'
- en: '[PRE35]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It''s showtime. We''ll send 30 GET requests to Delinkcious and check the description:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始表演。我们将向Delinkcious发送30个GET请求并检查描述：
- en: '[PRE36]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Interesting – we've got 24 green responses and 6 yellow responses. This is much
    higher than expected (three yellow responses on average). I ran it a couple more
    times and got six yellow responses again for the second run, and just one yellow
    response for the third run. This is all running on Minikube, so load balancing
    may be a little special. Let's declare victory.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是-我们得到了24个绿色的响应和6个黄色的响应。这比预期的要高得多（平均三个黄色的响应）。我又运行了几次，第二次运行时又得到了六个黄色的响应，第三次运行时只得到了一个黄色的响应。这都是在Minikube上运行的，所以负载均衡可能有点特殊。让我们宣布胜利。
- en: Using canary deployments for A/B testing
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用金丝雀部署进行A/B测试
- en: Canary deployments can also be used to support A/B testing. We can deploy as
    many versions as we want, as long as we have enough pods to juggle the load. Each
    version could include special code to log the relevant data, and then you can
    gain insights and correlate user behavior with specific versions. This is possible,
    but you'll probably have to build a lot of tooling and conventions to make it
    usable. If A/B testing is an important part of your design workflow, I recommend
    going with one of the established A/B testing solutions. The A/B testing wheel
    is not worth reinventing, in my opinion.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署也可以用于支持A/B测试。只要我们有足够的pod来处理负载，我们可以部署任意多个版本。每个版本都可以包含特殊代码来记录相关数据，然后您可以获得见解并将用户行为与特定版本相关联。这是可能的，但您可能需要构建大量的工具和约定使其可用。如果A/B测试是设计工作流程的重要部分，我建议选择一个已建立的A/B测试解决方案。在我看来，重复发明A/B测试轮子是不值得的。
- en: Let's consider what to do when things go wrong and we need to get back to a
    working state as soon as possible.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑当出现问题时该怎么办，以便尽快恢复到正常状态。
- en: Rolling back deployments
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回滚部署
- en: When things go wrong in production after a deployment, the best-practice response
    is to roll back the changes and get back to the last previous version known to
    work. The way you go about this depends on the deployment pattern you've employed.
    Let's consider them one by one.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署后出现问题时，最佳做法是回滚更改，恢复到已知可用的上一个版本。您进行此操作的方式取决于您采用的部署模式。让我们逐一考虑它们。
- en: Rolling back standard Kubernetes deployments
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回滚标准的Kubernetes部署
- en: 'Kubernetes deployments keep a history. For example, if we edit the user manager
    deployment and set the image version to `0.5`, then we can see that there are
    two revisions now:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes部署保留历史记录。例如，如果我们编辑用户管理器部署并将图像版本设置为`0.5`，那么现在我们可以看到有两个修订版：
- en: '[PRE37]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `CHANGE-CAUSE` column is not recorded by default. Let''s make another change
    to version 0.4, but using the `--record=true` flag:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`CHANGE-CAUSE`列默认情况下不记录。让我们再次进行更改，将版本更改为0.4，但使用`--record=true`标志：'
- en: '[PRE38]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'OK. Let''s roll back to the original 0.3 version. That would be revision 1\.
    We can look at this by using the `rollout history` command at specific revisions,
    too:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。让我们回滚到原始的0.3版本。那将是修订版1。我们也可以使用`rollout history`命令查看特定修订版的情况：
- en: '[PRE39]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'As you can see, revision 1 has version 0.3\. The command to roll back is as
    follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，修订版1具有版本0.3。回滚的命令如下：
- en: '[PRE40]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Rolling back will use the same mechanics of a rolling update, gradually replacing
    pods until all the running pods have the correct version.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚将使用滚动更新的相同机制，逐渐替换pod，直到所有正在运行的pod都具有正确的版本。
- en: Rolling back blue-green deployments
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回滚蓝绿部署
- en: 'Blue-green deployments are not supported directly by Kubernetes. Switching
    back from green to blue (assuming that the blue deployment''s pods are still running)
    is very simple. You just change the `Service` selector and select `deployment:
    blue` instead of `deployment: green`. The instant switch from blue to green and
    vice versa is the main motivation for the blue-green deployment pattern, so it''s
    no wonder that it''s that easy. Once you''ve switched back to blue, you can delete
    the green deployment and figure out what went wrong.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kubernetes不直接支持蓝绿部署。从绿色切换回蓝色（假设蓝色部署的pod仍在运行）非常简单。您只需更改`Service`选择器，选择`deployment:
    blue`而不是`deployment: green`。从蓝色立即切换到绿色，反之亦然，是蓝绿部署模式的主要动机，因此这么简单也就不足为奇了。一旦切换回蓝色，您可以删除绿色部署并找出问题所在。'
- en: Rolling back canary deployments
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回滚金丝雀部署
- en: Canary deployments are arguably even simpler to roll back. The majority of your
    pods run the tried and true version. The canary deployment's pods serve just a
    small amount of requests. If you detect that something is wrong with the canary
    deployment, simply delete the deployment. Your main deployment will keep serving
    incoming requests. If necessary (for example, your canary deployment served a
    small but significant amount of traffic), you can scale up your main deployment
    to make up for the canary pods that are no longer there.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署可能更容易回滚。大多数pod运行经过验证的旧版本。金丝雀部署的pod仅提供少量请求。如果检测到金丝雀部署出现问题，只需删除部署。您的主要部署将继续提供传入请求。如果必要（例如，您的金丝雀部署提供了少量但重要的流量），您可以扩展主要部署以弥补不再存在的金丝雀pod。
- en: Dealing with a rollback after a schema, API, or payload change
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在模式、API或有效负载更改后处理回滚
- en: The deployment strategy you choose often depends on the nature of the change
    the new version introduces. For example, if your change involved a breaking database
    schema change, such as splitting table A into two tables, B and C, then you can't
    simply deploy the new version that reads to/writes from B and C. The database
    needs to be migrated first. However, if you run into problems and want to roll
    back to the previous version, then you'll have the same problem in the reverse
    direction. Your old version will try and read from/write to table A, which doesn't
    exist anymore. The same issue can happen if you change the format of a configuration
    file or payload on some network protocol. API changes can break clients if you
    don't coordinate them.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您选择的部署策略通常取决于新版本引入的更改的性质。例如，如果您的更改涉及破坏性的数据库模式更改，比如将A表拆分为B和C表，那么您不能简单地部署新版本来读取/写入B和C。数据库需要先进行迁移。然而，如果遇到问题并希望回滚到先前的版本，那么您将在相反的方向上遇到相同的问题。您的旧版本将尝试从A表读取/写入，而A表已经不存在了。如果更改配置文件或某些网络协议的有效负载格式，也可能出现相同的问题。如果您不协调API更改，可能会破坏客户端。
- en: The way to address those compatibility issues is to perform those changes across
    multiple deployments, where each deployment is fully compatible with the previous
    deployment. This takes some planning and work. Let's consider the case of splitting
    table A into tables B and C. Suppose we're on version 1.0 and eventually want
    to end up with version 2.0.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些兼容性问题的方法是在多个部署中执行这些更改，其中每个部署与上一个部署完全兼容。这需要一些规划和工作。让我们考虑将A表拆分为B表和C表的情况。假设我们处于版本1.0，并最终希望最终使用版本2.0。
- en: 'Our first change will be marked as version 1.1\. It will perform the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个更改将标记为版本1.1。它将执行以下操作：
- en: Create tables B and C (but leave table A in place).
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建B和C表（但保留A表）。
- en: Change the code to write to B and C.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改代码以写入B和C。
- en: Change the code to read from both A, B, and C and merge the results (old data
    comes from A, while new data comes from B and C).
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改代码以从A、B和C读取并合并结果（旧数据来自A，而新数据来自B和C）。
- en: If data needs to be deleted, it is just marked as deleted instead.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要删除数据，只需标记为已删除。
- en: We deploy version 1.1 and if we see that something is wrong, we will roll back
    to version 1.0\. All our old data is still in table A, which version 1.0 is fully
    compatible with. We might have lost or corrupted a small amount of data in tables
    B and C, but that's the price we have to pay for not testing properly earlier.
    Version 1.1 could have been a canary deployment, so only a small amount of requests
    have been lost.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们部署版本1.1，如果发现有问题，我们将回滚到版本1.0。我们所有的旧数据仍然在A表中，而版本1.0与之完全兼容。我们可能已经丢失或损坏了B表和C表中的少量数据，但这是我们之前没有充分测试的代价。版本1.1可能是一个金丝雀部署，因此只丢失了少量请求。
- en: Then, we discover the issues, fix them, and deploy version 1.2, which is just
    like how version 1.1 writes to B and C, but reads from A, B, and C and doesn't
    delete data from A.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们发现问题，修复它们，并部署版本1.2，这与版本1.1写入B和C的方式相同，但是从A、B和C读取，并且不删除A中的数据。
- en: We observe for a while until we're confident that version 1.2 works as expected.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察一段时间，直到我们确信版本1.2按预期工作。
- en: The next step is to migrate the data. We write the data in table A into tables
    B and C. The active deployment, version 1.2, keeps reading from B and C and only
    merges missing data from A. We still keep all the old data in A until we finish
    all code changes.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是迁移数据。我们将A表中的数据写入B表和C表。活动部署版本1.2继续从B和C读取，并且仅合并A中缺失的数据。在完成所有代码更改之前，我们仍然保留A中的所有旧数据。
- en: At this point, all the data is in tables B and C. We deploy version 1.3, which
    ignores table A and works fully against tables B and C.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，所有数据都在B表和C表中。我们部署版本1.3，它忽略A表，并完全针对B表和C表工作。
- en: We observe again, and can go back to version 1.2 if we encounter any problems
    with 1.3 and release version 1.4, 1.5, and so on. However, at some point, our
    code will work as expected and then we can rename/retag the final version as 2.0,
    or just cut a new version that is identical except for the version number.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次观察，如果遇到1.3的任何问题，可以回到版本1.2，并发布版本1.4、1.5等。但是，在某个时候，我们的代码将按预期工作，然后我们可以将最终版本重命名/重新标记为2.0，或者只是剪切一个除版本号外完全相同的新版本。
- en: The last step is to delete table A.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是删除A表。
- en: This can be a slow process that requires running a lot of tests whenever deploying
    a new version, but it is necessary when you're making dangerous changes that have
    the potential to corrupt your data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个缓慢的过程，每当部署新版本时都需要运行大量测试，但在进行可能损坏数据的危险更改时是必要的。
- en: Of course, you'll back up your data before starting, but for high-throughput
    systems, even short outages during bad upgrades can be very costly.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您将在开始之前备份数据，但是对于高吞吐量系统，即使在糟糕的升级期间短暂的中断也可能非常昂贵。
- en: The bottom line is that updates that include schema changes are complicated.
    The way to manage it is to perform a multi-phase upgrade, where each phase is
    compatible with the previous phase. You move forward only when you have proved
    that the current phase works correctly. The benefit of the principle of a single
    microservice owning each data store is that at least DB schema changes are constrained
    to a single service, and don't require coordination across multiple services.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 底线是包含模式更改的更新是复杂的。管理的方法是执行多阶段升级，其中每个阶段与上一个阶段兼容。只有在证明当前阶段正常工作时才能前进。单个微服务拥有每个数据存储的原则的好处是，至少DB模式更改受限于单个服务，并且不需要跨多个服务协调。
- en: Managing versions and dependencies
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 版本和依赖管理
- en: Managing versions is a tricky topic. In microservice-based architecture, your
    microservices may have many dependencies, as well as many clients, both internal
    and external. There are several categories of versioned resources, and they all
    require different management strategies and versioning schemes.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 管理版本是一个棘手的话题。在基于微服务的架构中，您的微服务可能具有许多依赖项，以及许多内部和外部客户端。有几类版本化资源，它们都需要不同的管理策略和版本化方案。
- en: Managing public APIs
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理公共API
- en: Public APIs are network APIs that are used outside the cluster, often by a large
    number of users and/or developers who may or may not have a formal relationship
    with your organization. Public APIs may require authentication, but sometimes
    may be anonymous. The versioning scheme for public APIs typically involves just
    a major version, such as V1, V2, and so on. The Kubernetes API is a good example
    of such a versioning scheme, although it also has the concept of API groups and
    uses alpha and beta qualifiers because it caters to developers.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 公共API是在集群外部使用的网络API，通常由大量用户和/或开发人员使用，他们可能与您的组织有或没有正式关系。公共API可能需要身份验证，但有时可能是匿名的。公共API的版本控制方案通常只涉及主要版本，例如V1、V2等。Kubernetes
    API就是这种版本控制方案的一个很好的例子，尽管它还有API组的概念，并使用alpha和beta修饰符，因为它面向开发人员。
- en: 'Delinkcious has a single public API that used the `<major>.<minor>` versioning
    scheme up to this point:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Delinkcious到目前为止使用了`<major>.<minor>`版本控制方案的单一公共API：
- en: '[PRE41]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This is overkill, and a major version only is enough. Let''s change it (and
    all the impacted tests, of course):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点过度了，只有一个主要版本就足够了。让我们来改变它（当然还有所有受影响的测试）：
- en: '[PRE42]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note that we keep the same version, even when we make breaking changes during
    this book. This is fine because there are no external users so far, so we have
    the liberty to change our public API. However, once we officially release our
    application, we are obligated to consider the burden on our users if we make breaking
    changes without changing the API version. This is is a pretty bad anti-pattern.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使在编写本书的过程中我们进行了重大更改，我们仍然保持相同的版本。这没问题，因为目前还没有外部用户，所以我们有权更改我们的公共API。然而，一旦我们正式发布我们的应用程序，如果我们进行重大更改而不更改API版本，我们就有义务考虑用户的负担。这是一个相当糟糕的反模式。
- en: Managing cross-service dependencies
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理跨服务依赖
- en: Cross-service dependencies are often well defined and documented as internal
    APIs. However, subtle changes to the implementation and/or to the contract can
    significantly impact other services. For example, if we change the structs in
    `object_model/types.go`, a lot of code might have to be modified. In a well-tested
    mono-repo, this is less of a problem because the developer who makes the changes
    can ensure that all the relevant consumers and tests were updated. Many systems
    are built out of multiple repositories, and it might be challenging to identify
    all the consumers. In these cases, breaking changes can remain and be discovered
    after deployment.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 跨服务依赖通常被定义和记录为内部API。然而，对实现和/或合同的微小更改可能会显着影响其他服务。例如，如果我们更改`object_model/types.go`中的结构，可能需要修改大量代码。在经过充分测试的单体库中，这不是一个问题，因为进行更改的开发人员可以确保所有相关的消费者和测试都已更新。许多系统由多个存储库构建，可能很难识别所有的消费者。在这些情况下，重大更改可能会保留，并在部署后被发现。
- en: 'Delinkcious is a mono-repo, and it is actually not using any versioning scheme
    at all in the URLs of its endpoints. Here is the social graph manager''s API:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Delinkcious是一个单体库，实际上在其端点的URL中根本没有使用任何版本控制方案。这是社交图管理器的API：
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This approach is acceptable if you never intend to run multiple versions of
    the same service. In large systems, this is not a scalable approach. There will
    always be some consumers that don't want to upgrade to the latest and greatest
    immediately.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您永远不打算运行同一服务的多个版本，这种方法是可以接受的。在大型系统中，这不是一种可扩展的方法。总会有一些消费者不愿意立即升级到最新和最好的版本。
- en: Managing third-party dependencies
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理第三方依赖
- en: 'There are three flavors of third-party dependencies:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第三方依赖有三种不同的版本：
- en: Libraries and packages you build your software against (as discussed in [Chapter
    2](d4214218-a4e9-4df8-813c-e00df71da935.xhtml), *Getting Started with Microservices*)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您构建软件的库和软件包（如[第2章](d4214218-a4e9-4df8-813c-e00df71da935.xhtml)中讨论的*使用微服务入门*）
- en: Third-party services that are accessed through an API by your code
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过API由您的代码访问的第三方服务
- en: Services you use to operate and run your system
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您用于操作和运行系统的服务
- en: For example, if you run your system in the cloud, then your cloud provider is
    a huge dependency (Kubernetes can help mitigate risk). Another great example is
    using a third-party service as your CI/CD solution.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您在云中运行系统，那么您的云提供商就是一个巨大的依赖（Kubernetes可以帮助减轻风险）。另一个很好的例子是将第三方服务用作CI/CD解决方案。
- en: 'When choosing a third-party dependency, you relinquish some (or a lot) control.
    You should always consider what happens if the third-party dependency suddenly
    becomes unavailable or unacceptable. There are many reasons why this can happen:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 选择第三方依赖时，您会放弃一些（或很多）控制权。您应该始终考虑如果第三方依赖突然变得不可用或不可接受会发生什么。这可能有很多原因：
- en: Open source project abandoned or loses steam
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源项目被放弃或失去动力
- en: Third-party provider shuts down
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三方提供商关闭
- en: Library has too many security vulnerabilities
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库存在太多安全漏洞
- en: Service has too many outages
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务存在太多故障
- en: 'Assuming that you''ve picked your dependencies wisely, let''s consider two
    cases:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您明智地选择了依赖，让我们考虑两种情况：
- en: Upgrading to a new version of a library
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级到库的新版本
- en: Upgrading to a new API version of a third-party service
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级到第三方服务的新API版本
- en: Every such upgrade requires the corresponding upgrade of any component (a library
    or service) in your system that uses these dependencies. Typically, these upgrades
    shouldn't modify the API of any of your services, nor the public interfaces of
    your libraries. They may change the operational profile of your services (hopefully
    for the better, as in less memory, more performance).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 每次升级都需要相应升级系统中使用这些依赖的任何组件（库或服务）。通常，这些升级不应该修改任何服务的API，也不应该修改库的公共接口。它们可能会改变您的服务的运行配置文件（希望是更好的，比如更少的内存，更高的性能）。
- en: Upgrading your services is a simple matter. You just deploy the new version
    of your service that depends on the new third-party dependency and you're good
    to go. Changes to third-party libraries can be a little more involved. You need
    to identify all of your libraries that depend on this third-party library. Upgrade
    your libraries and then identify every service that uses any of your (now-upgraded)
    libraries and upgrade those services too.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 升级您的服务很简单。您只需部署依赖新第三方依赖的新版本服务，然后就可以继续进行。对第三方库的更改可能会更加复杂。您需要识别所有依赖于这个第三方库的库。升级您的库，然后识别使用任何（现在升级过的）库的每个服务，并升级这些服务。
- en: It is highly recommended to use semantic versioning for your libraries and packages.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议为您的库和软件包使用语义化版本控制。
- en: Managing your infrastructure and toolchain
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理基础设施和工具链
- en: Your infrastructure and toolchain must be managed carefully too, and even versioned.
    In a large system, your CI/CD pipeline will typically invoke various scripts that
    automate important tasks, such as migrating databases, preprocessing data, and
    provisioning cloud resources. These internal tools can change dramatically. Another
    important category in container-based systems are the versions of your base images.
    The infrastructure of code approach, combined with GitOps, advocates versioning
    and storing those aspects of your system in your source control system (Git) as
    well.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 您的基础架构和工具链也必须小心管理，甚至进行版本控制。在大型系统中，您的CI/CD流水线通常会调用各种脚本来自动化重要任务，比如迁移数据库、预处理数据和配置云资源。这些内部工具可能会发生重大变化。容器化系统中另一个重要的类别是基础镜像的版本。代码基础设施方法结合GitOps，主张将系统的这些方面进行版本控制并存储在源代码控制系统（Git）中。
- en: So far, we've covered a lot of dark corners and difficult use cases regarding
    real-world deployments and how to evolve and upgrade large systems safely and
    reliably. Let's get back to the individual developer. There is a very different
    set of requirements and concerns for developers that need a quick edit-test-debug
    cycle in the cluster.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了很多关于真实部署的黑暗角落和困难用例，以及如何安全可靠地演进和升级大型系统。让我们回到个别开发人员。对于需要在集群中进行快速编辑-测试-调试循环的开发人员，有一套非常不同的要求和关注点。
- en: Local development deployments
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地开发部署
- en: Developers want fast iterations. When I make a code change to some code, I want
    to run the tests as soon as possible, and if something is wrong, to fix it as
    soon as possible. We've seen how well this works with unit tests. However, when
    the system uses a microservice architecture packaged as containers and deployed
    to a Kubernetes cluster, this is not enough. To truly evaluate the impact of a
    change, we often have to build an image (which may include updating Kubernetes
    manifests like Deployments, Secrets, and ConfigMaps) and deploy it to the cluster.
    Developing locally against Minikube is awesome, but even deploying to a local
    Minikube cluster takes time and effort. In [Chapter 10](08f2d84f-2a24-473c-9b0d-20e7fc87fd70.xhtml),
    *Testing Microservices*, we used Telepresence to great effect for interactive
    debugging. However, Telepresence has its own quirks and downsides, and it's not
    always the best tool for the job. In the following subsections, we'll cover several
    other alternatives that may be a better choice in certain circumstances.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员希望快速迭代。当我对某些代码进行更改时，我希望尽快运行测试，如果有问题，尽快修复。我们已经看到这在单元测试中运行得很好。然而，当系统使用微服务架构打包为容器并部署到Kubernetes集群时，这是不够的。为了真正评估变更的影响，我们经常需要构建一个镜像（其中可能包括更新Kubernetes清单，如部署、秘钥和配置映射）并将其部署到集群中。在Minikube上本地开发非常棒，但即使部署到本地Minikube集群也需要时间和精力。在[第10章](08f2d84f-2a24-473c-9b0d-20e7fc87fd70.xhtml)中，*测试微服务*，我们使用Telepresence进行了交互式调试，效果很好。然而，Telepresence
    有其自己的怪癖和缺点，并不总是最适合的工具。在接下来的小节中，我们将介绍几种其他替代方案，在某些情况下可能是更好的选择。
- en: Ko
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ko
- en: Ko ([https://github.com/google/ko](https://github.com/google/ko)) is a very
    interesting Go-specific tool. Its goal is to streamline and hide the process of
    building images. The idea is that, in your Kubernetes deployment, you replace
    the image path from the registry with a Go import path. Ko will read this import
    path, build a Docker image for you, publish it to a registry (or locally if using
    Minikube), and deploy it to your cluster. Ko provides ways to specify a base image
    and include static data in the generated image.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Ko（[https://github.com/google/ko](https://github.com/google/ko)）是一个非常有趣的Go特定工具。它的目标是简化和隐藏构建图像的过程。其想法是，在Kubernetes部署中，您将图像路径从注册表替换为Go导入路径。Ko将读取此导入路径，为您构建Docker图像，将其发布到注册表（如果使用Minikube，则为本地），并将其部署到您的集群中。Ko提供了指定基本图像和在生成的图像中包含静态数据的方法。
- en: Let's give it a try and discuss the experience later.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试一试，稍后讨论体验。
- en: 'You can install Ko through the standard `go get` command:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过标准的`go get`命令安装Ko：
- en: '[PRE44]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Ko requires that you work in `GOPATH`. I don''t typically work inside `GOPATH`
    for various reasons (Delinkcious use Go modules that don''t require `GOPATH`).
    To accommodate Ko, I used the following code:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Ko要求您在`GOPATH`中工作。出于各种原因，我通常不在`GOPATH`中工作（Delinkcious使用不需要`GOPATH`的Go模块）。为了适应Ko，我使用了以下代码：
- en: '[PRE45]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Here, I have replicated the directory structure Go expects under `GOPATH`, including
    replicating the path on GitHub to Delinkcious. Then, I got all the dependencies
    of Delinkcious recursively using `go get -d ./...`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我复制了Go在`GOPATH`下期望的目录结构，包括在GitHub上复制到Delinkcious的路径。然后，我使用`go get -d ./...`递归地获取了Delinkcious的所有依赖项。
- en: 'The last preparatory step is to set Ko for local development. When Ko builds
    an image, we shouldn''t push it to Docker Hub or any remote registry. We want
    a fast local loop. Ko allows you to do this in various ways. One of the simplest
    ways is as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的准备步骤是为本地开发设置Ko。当Ko构建图像时，我们不应将其推送到Docker Hub或任何远程注册表。我们希望快速本地循环。Ko允许您以各种方式执行此操作。其中最简单的方法之一如下：
- en: '[PRE46]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Other ways include a configuration file or passing the `-L` flag when running
    Ko.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法包括配置文件或在运行Ko时传递`-L`标志。
- en: Now, we can go ahead and use Ko. Here is the `ko-link-manager.yaml` file where
    the image is replaced with the Go import path to the link manager service (`github.com/the-gigi/delinkcious/svc/link_service`).
    Note that I changed `imagePullPolicy` from `Always` to `IfNotPresent`.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续使用Ko。这是`ko-link-manager.yaml`文件，其中将图像替换为链接管理器服务的Go导入路径（`github.com/the-gigi/delinkcious/svc/link_service`）。请注意，我将`imagePullPolicy`从`Always`更改为`IfNotPresent`。
- en: 'The `Always` policy is the secure and production-ready policy, but when working
    locally, it will ignore the local Ko images and will instead pull from Docker
    Hub:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`Always`策略是安全且已准备就绪的策略，但在本地工作时，它将忽略本地的Ko镜像，而是从Docker Hub拉取：'
- en: '[PRE47]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The next step is running Ko on this modified deployment manifest:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是在修改后的部署清单上运行Ko：
- en: '[PRE48]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'To test the deployment, let''s run our `smoke` test:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试部署，让我们运行我们的`smoke`测试：
- en: '[PRE49]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Everything looks good. The link description contains the `[yellow]` prefix
    from our canary deployment work. Let''s change it to `[ko]` and see how fast Ko
    can redeploy:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 一切看起来都很好。链接描述包含我们金丝雀部署工作的`[yellow]`前缀。让我们将其更改为`[ko]`，看看Ko可以重新部署有多快：
- en: '[PRE50]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Running Ko again on the modified code takes just 19 seconds, all the way to
    deployment in the cluster. That''s impressive:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在修改后的代码上再次运行Ko只需19秒，一直到在集群中部署。这令人印象深刻：
- en: '[PRE51]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `smoke` test works and the description now contains the `[ko]` prefix instead
    of `[yellow]`, which proves that Ko works as advertised and indeed built a Docker
    container very quickly and deployed it to the cluster:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '`smoke`测试有效，并且描述现在包含`[ko]`前缀而不是`[yellow]`，这证明Ko按照广告中的方式工作，并且确实快速构建了一个Docker容器并将其部署到了集群中：'
- en: '[PRE52]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let''s take a look at the image that Ko built. In order to do that, we will
    `ssh` into the Minikube node and check the Docker images:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看Ko构建的镜像。为了做到这一点，我们将`ssh`进入Minikube节点并检查Docker镜像：
- en: '[PRE53]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The image appears to have a creation date of the beginning of the Unix epoch
    (1970) for some reason. Other than that, everything looks good. Note that the
    image is larger than our normal link manager because Ko uses [gcr.io/distroless/base:latest](http://gcr.io/distroless/base:latest) as
    a base image by default, while Delinkcious uses the SCRATCH image. You can override
    the base image if you want, using a `.ko.yaml` configuration file.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像似乎由于某种原因具有Unix纪元（1970年）开始的创建日期。除此之外，一切看起来都很好。请注意，该镜像比我们正常的链接管理器要大，因为Ko默认使用[gcr.io/distroless/base:latest](http://gcr.io/distroless/base:latest)作为基础镜像，而Delinkcious使用SCRATCH镜像。如果你愿意，你可以使用`.ko.yaml`配置文件覆盖基础镜像。
- en: 'In short, Ko is easy to install, configure, and it works very well. Still,
    I find it too limited:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Ko易于安装、配置，并且运行非常好。不过，我觉得它太过受限：
- en: It is a Go-only tool.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个仅适用于Go的工具。
- en: You must have your code in `GOPATH` and use the standard Go directory structure
    (obsolete with Go 1.11+ modules).
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你必须将你的代码放在`GOPATH`中，并使用标准的Go目录结构（在Go 1.11+模块中已过时）。
- en: You have to modify your manifests (or create a copy with the Go import path).
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你必须修改你的清单（或者使用Go导入路径创建一个副本）。
- en: It may be a good option to test new Go services before integrating them into
    your CI/CD system.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在将新的Go服务集成到CI/CD系统之前，测试它可能是一个不错的选择。
- en: Ksync
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ksync
- en: 'Ksync is a very interesting tool. It doesn''t build images at all. It syncs
    files directly between a local directory, and a remote directory inside a running
    container in your cluster. It doesn''t get more streamlined than that, especially
    if you sync to a local Minikube cluster. This awesomeness comes with a price,
    though. Ksync works especially well for services that are implemented using dynamic
    languages, such as Python and Node, that can hot-reload the application when changes
    are synced. If your application doesn''t do hot reloading, Ksync can restart the
    container after each change. Let''s get to work:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Ksync是一个非常有趣的工具。它根本不构建镜像。它直接在集群中的运行容器内部同步本地目录和远程目录中的文件。这是非常简化的操作，特别是如果你同步到本地的Minikube集群。然而，这种便利是有代价的。Ksync特别适用于使用动态语言（如Python和Node）实现的服务，可以在同步更改时进行热重载应用程序。如果你的应用程序不支持热重载，Ksync可以在每次更改后重新启动容器。让我们开始工作吧：
- en: Installing Ksync is very simple, but remember to check what you are installing
    before just piping it to `bash`!
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Ksync非常简单，但记得在将其直接传输到`bash`之前检查你要安装的内容！
- en: '[PRE54]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'If you prefer, you can install it with a `go` command:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，你可以使用`go`命令来安装它：
- en: '[PRE55]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We also need to start the cluster-side component of Ksync, which will create
    a DaemonSet on every node to listen for changes and reflect them into running
    containers:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要启动Ksync的集群端组件，它将在每个节点上创建一个DaemonSet来监听更改并将其反映到运行的容器中：
- en: '[PRE56]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, we can tell Ksync to watch for changes. This is a blocking operation and
    Ksync will watch forever. We can run it in a separate Terminal or tab:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以告诉Ksync监听更改。这是一个阻塞操作，Ksync将永远监听。我们可以在一个单独的终端或选项卡中运行它：
- en: '[PRE57]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The last part of the setup is to establish a mapping between a local directory
    and a remote directory on a target pod or pods. As usual, we identify the pods
    via a label selector. The only Delinkcious service that uses a dynamic language
    is the API gateway, so we''ll use this here:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置的最后一部分是在目标pod或多个pod上的本地目录和远程目录之间建立映射。通常情况下，我们通过标签选择器来识别pod。唯一使用动态语言的Delinkcious服务是API网关，所以我们将在这里使用它：
- en: '[PRE58]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can test that Ksync works by modifying our API gateway. Let''s add a Ksync
    message to our `get()` method:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过修改API网关来测试Ksync是否有效。让我们在`get()`方法中添加一个Ksync消息：
- en: '[PRE59]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'A few seconds later, we will see the `Yeah, it works!` message from Ksync.
    This is a great success:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几秒钟后，我们将看到来自Ksync的`是的，它有效了！`消息。这是一个巨大的成功：
- en: '[PRE60]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: To recap, Ksync is extremely streamlined and fast. I really like the fact that
    it doesn't bake images, push them to a registry, and then deploy to the cluster.
    If all your workloads use a dynamic language, then using Ksync is a no-brainer.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，Ksync非常简洁和快速。我真的很喜欢它不会烘烤图像，将它们推送到注册表，然后部署到集群的事实。如果您的所有工作负载都使用动态语言，那么使用Ksync是一个明智的选择。
- en: Draft
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Draft
- en: 'Draft is another tool from Microsoft (originally from Deis) that lets you quickly
    build images without a Dockerfile. It uses standard buildpacks for various languages.
    It doesn''t seem like you can provide your own base image. This is a problem for
    two reasons:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Draft是微软的另一个工具（最初来自Deis），它可以快速构建图像而无需Dockerfile。它使用各种语言的标准构建包。看起来似乎您无法提供自己的基础图像。这有两个问题：
- en: Your service may be more than just code, and may depend on things that you set
    up in the Dockerfile.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的服务可能不仅仅是代码，可能还依赖于您在Dockerfile中设置的东西。
- en: The base images that Draft uses are pretty big.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Draft使用的基础图像相当大。
- en: Draft depends on Helm, so you must have Helm installed on your cluster. The
    installation is very versatile and supports many methods.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: Draft依赖于Helm，因此您必须在集群上安装Helm。安装非常灵活，支持许多方法。
- en: 'You can be sure that Draft works well on Windows, unlike many other tools in
    the cloud-native area where Windows is a second-class citizen. This mindset is
    starting to change since Microsoft, Azure, and AKS are prominent contributors
    to the Kubernetes ecosystem. OK, let''s take Draft for a test drive:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以确信Draft在Windows上运行良好，不像云原生领域中许多其他工具，其中Windows是一个二等公民。这种心态开始改变，因为微软、Azure和AKS是Kubernetes生态系统的重要贡献者。好的，让我们来试用一下Draft：
- en: 'Installing `draft` on macOS (assuming you''ve installed Helm already) is as
    simple as doing the following:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在macOS上安装`draft`（假设您已经安装了Helm）就像做以下操作一样简单：
- en: '[PRE61]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Let''s configure Draft to push its images directly to Minikube (the same as
    Ko):'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们配置Draft，将其图像直接推送到Minikube（与Ko相同）：
- en: '[PRE62]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'As usual, let''s add a prefix, `[draft]`, to the description:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，让我们在描述中添加一个前缀`[draft]`：
- en: '[PRE63]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Next, we let draft prepare by calling the `draft create` command and also choosing
    the Helm release name using `--app`:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们让draft通过调用`draft create`命令准备，并使用`--app`选择Helm发布名称：
- en: '[PRE64]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, we can deploy to the cluster:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以部署到集群：
- en: '[PRE65]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Unfortunately, draft hung in the `Pushing Docker Image` stage. It worked for
    me in the past, so perhaps it's a new issue with the latest versions.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，draft在`推送Docker镜像`阶段挂起了。过去它对我有用，所以也许是最新版本的一个新问题。
- en: Overall, draft is pretty simple, but too limited. The big images it creates
    and the inability to provide your own base images are deal breakers. The documentation
    is very sparse, too. I recommend using it only if you're on Windows and the other
    tools don't work well enough.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，draft相当简单，但太有限了。它创建的大图像和无法提供自己的基础图像是致命缺陷。文档也非常稀少。我建议只在您使用Windows且其他工具不够好时使用它。
- en: Skaffold
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Skaffold
- en: 'Skaffold ([https://skaffold.dev/](https://skaffold.dev/)) is a very complete
    solution. It is very flexible, supports both local development and integration
    with CI/CD, and has excellent documentation. Here are some of the features of
    Skaffold:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: Skaffold（[https://skaffold.dev/](https://skaffold.dev/)）是一个非常完整的解决方案。它非常灵活，支持本地开发和与CI/CD集成，并且有出色的文档。以下是Skaffold的一些特点：
- en: Detect code changes, build image, push, and deploy.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测代码更改，构建图像，推送和部署。
- en: Can sync source files to pods directly (just like Ksync).
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以直接将源文件同步到pod中（就像Ksync一样）。
- en: It has a sophisticated conceptual model with builders, testers, deployers, tag
    polices, and push strategies.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有一个复杂的概念模型，包括构建器、测试器、部署器、标签策略和推送策略。
- en: You can customize every aspect.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以自定义每个方面。
- en: Integrate with your CI/CD pipeline by running Skaffold from end to end, or use
    specific stages as building blocks.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过从头到尾运行Skaffold来集成您的CI/CD流水线，或者使用特定阶段作为构建模块。
- en: Per-environment configuration via profiles, user-level config, environment variables,
    or command-line flags.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过配置文件、用户级别配置、环境变量或命令行标志进行每个环境的配置。
- en: It is a client-side tool – there is no need to install anything in your cluster.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个客户端工具-无需在集群中安装任何东西。
- en: Automatically forward container ports to the local machine.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动将容器端口转发到本地机器。
- en: Aggregate logs from the deployed pods.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合部署的pod的日志。
- en: 'Here is a diagram that illustrates the workflow of Skaffold:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个说明Skaffold工作流程的图表：
- en: '![](assets/bf28c932-48d4-457f-bf40-9049f773ad0e.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/bf28c932-48d4-457f-bf40-9049f773ad0e.png)'
- en: Skaffold
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: Skaffold
- en: 'Let''s install Skaffold and take it for a ride:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装Skaffold并试用一下：
- en: '[PRE66]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, let''s create a configuration file in the `link_service` directory. Skaffold
    will ask us some questions about which Dockerfile to use for different elements,
    such as the database and the service itself:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在`link_service`目录中创建一个配置文件。Skaffold将询问我们关于不同元素使用哪个Dockerfile的一些问题，例如数据库和服务本身：
- en: '[PRE67]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Let''s try to build an image with Skaffold:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用Skaffold构建一个图像：
- en: '[PRE68]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Oh, no – it fails. I did some searching and there is an open issue:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，不好-它失败了。我进行了一些搜索，发现了一个未解决的问题：
- en: '[PRE69]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Skaffold is a big solution. It does much more than just local development. It
    has a non-trivial learning curve, too (for example, syncing files requires manually
    setting up each directory and file type). If you like its model and you use it
    in your CI/CD solution, then it makes sense to use it for local development as
    well. Definitely check it out and make up your own mind. The fact that it can
    build images as well as directly sync files is a big plus if you have a hybrid
    system similar to Delinkcious.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: Skaffold是一个大型解决方案。它不仅仅是本地开发。它也有一个相当复杂的学习曲线（例如，同步文件需要手动设置每个目录和文件类型）。如果您喜欢它的模型，并且在CI/CD解决方案中使用它，那么在本地开发中使用它也是有意义的。一定要试试看，并自行决定。如果您有类似Delinkcious的混合系统，它可以构建图像以及直接同步文件，这是一个很大的优势。
- en: Tilt
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tilt
- en: Last, but absolutely not least, is Tilt. Tilt is my favorite development tool
    by far. Tilt is also very comprehensive and flexible. It is centered around a
    Tiltfile written in a language called Starlark ([https://github.com/bazelbuild/starlark/](https://github.com/bazelbuild/starlark/)),
    which is a subset of Python. I was hooked right way. What's special about Tilt
    is that it goes beyond just building an image automatically and deploying it to
    the cluster or syncing files. It actually gives you a complete live development
    environment that presents a lot of information, highlights events and errors,
    and lets you drill down and understand what's happening in your cluster. Let's
    get started.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，但绝对不是最不重要的，是Tilt。Tilt是我迄今为止最喜欢的开发工具。Tilt也非常全面和灵活。它以一种称为Starlark的语言编写的Tiltfile为中心（[https://github.com/bazelbuild/starlark/](https://github.com/bazelbuild/starlark/)），这是Python的一个子集。我立刻就着迷了。Tilt的特别之处在于，它不仅仅是自动构建图像并将其部署到集群或同步文件。它实际上为您提供了一个完整的实时开发环境，提供了大量信息，突出显示事件和错误，并让您深入了解集群中正在发生的事情。让我们开始吧。
- en: 'Let''s install Tilt and then get to business:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装Tilt然后开始做生意：
- en: '[PRE70]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: I wrote a Tiltfile for the link service that's very generic.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我为链接服务编写了一个非常通用的Tiltfile。
- en: '[PRE71]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Let''s break this down and analyze it. First, we need all the YAML files under
    the k8s subdirectory. We could just hard code them, but where''s the fun in that?
    Also, there will be a different list of YAML files for different services. Skylark
    is Python-like, but you can''t use Python libraries. For example, the glob library
    is great for enumerating files with wildcards. Here is the Python code to list
    all files with the `.yaml` suffix in the `k8s` subdirectory:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解并分析一下。首先，我们需要k8s子目录下的所有YAML文件。我们可以直接编码它们，但这样做有什么乐趣呢？此外，不同的服务将有不同的YAML文件列表。Skylark类似于Python，但不能使用Python库。例如，glob库非常适合使用通配符枚举文件。以下是列出`k8s`子目录中所有带有`.yaml`后缀的文件的Python代码：
- en: '[PRE72]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We can''t do that directly in Starlark, but we can use the `local()` function,
    which allows us to run any command and capture the output. Therefore, we can execute
    the previous Python code by running the Python interpreter with a little script
    through Tilt''s `local()` function:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法直接在Starlark中执行这个操作，但我们可以使用`local()`函数，它允许我们运行任何命令并捕获输出。因此，我们可以通过Tilt的`local()`函数执行先前的Python代码，通过运行Python解释器并通过Tilt的`local()`函数执行一个小脚本来实现：
- en: '[PRE73]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: There are a few extra details here. First, we convert the list of files returned
    from glob into a comma-separated string. However, the `local()` function returns
    a Tilt object called Blob. We just want a plain string, so we convert the blob
    into a string by wrapping the `local()` call with the `str()` function. Finally,
    we remove the last character (the final `[:-1]`), which is a newline (because
    we used Python's `print()` function).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些额外的细节。首先，我们将从glob返回的文件列表转换为逗号分隔的字符串。但是，`local()`函数返回一个名为Blob的Tilt对象。我们只需要一个普通字符串，所以我们通过用`str()`函数包装`local()`调用来将blob转换为字符串。最后，我们移除最后一个字符（最后的`[:-1]`），这是一个换行符（因为我们使用了Python的`print()`函数）。
- en: The end result is that, in the `yaml_files` variable, we have a string that
    is a comma-separated list of all the YAML manifests.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是，在`yaml_files`变量中，我们有一个字符串，它是所有YAML清单的逗号分隔列表。
- en: 'Next, we split this comma-separated string back into a Python/Starlark list
    of file names:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将这个逗号分隔的字符串拆分回Python/Starlark文件名列表：
- en: '[PRE74]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'For each of these files, we call Tilt''s `k8s_yaml()` function. This function
    tells Tilt to monitor these files for changes:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些文件中的每一个，我们调用Tilt的`k8s_yaml()`函数。这个函数告诉Tilt监视这些文件的更改：
- en: '[PRE75]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Next, we repeat the same trick as before and execute a Python one-liner that
    extracts the service name from the current directory name. All the Delinkcious
    service directories follow the same naming convention, that is, `<service name>_service`.
    This one-liner splits the current directory, disposes of the last component (which
    is always `service`), and joins the components back via `-` as a separator.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们重复之前的技巧，并执行一个Python一行代码，从当前目录名称中提取服务名称。所有Delinkcious服务目录都遵循相同的命名约定，即`<service
    name>_service`。这个一行代码将当前目录拆分，丢弃最后一个组件（始终为`service`），并通过`-`作为分隔符将组件连接起来。
- en: 'Now, we need to get the service name:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要获取服务名称：
- en: '[PRE76]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now that we have the service name, the final step is to build the image by
    calling Tilt''s `docker_build()` function. Remember that the naming convention
    for Docker images that Delinkcious uses is `g1g1/delinkcious-<service name>`.
    I am also using a special `Dockerfile.dev` here, which is different than the production
    Dockerfile, and is more convenient for debugging and troubleshooting. If you don''t
    specify a Docker file, then the default is `Dockerfile`:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了服务名称，最后一步是通过调用Tilt的`docker_build()`函数来构建镜像。请记住，Delinkcious使用的Docker镜像命名约定是`g1g1/delinkcious-<service
    name>`。我这里还使用了一个特殊的`Dockerfile.dev`，它与生产环境的Dockerfile不同，更方便调试和故障排除。如果您没有指定Docker文件，那么默认为`Dockerfile`：
- en: '[PRE77]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: This may seem very complicated and convoluted, but the benefit is that I can
    drop this file in any service directory and it will work as is.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来非常复杂和费解，但好处是我可以将此文件放在任何服务目录中，它将按原样工作。
- en: 'For the link service, the equivalent hardcoded file would be as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 对于链接服务，等效的硬编码文件如下：
- en: '[PRE78]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: That's not too bad, but every time you add a new manifest, you have to remember
    to update your Tiltfile, and you'll need to keep a separate Tiltfile for each
    service.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不算太糟糕，但是每次添加新的清单时，您都必须记得更新Tiltfile，并且您需要为每个服务保留一个单独的Tiltfile。
- en: 'Let''s see Tilt in action. When we type `tilt up`, we will see the following
    text UI:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Tilt的实际效果。当我们输入`tilt up`时，我们将看到以下文本UI：
- en: '![](assets/61e22ed1-9d4e-4a31-b388-80021dc862a6.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/61e22ed1-9d4e-4a31-b388-80021dc862a6.png)'
- en: Tilt
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: Tilt
- en: There are many things you can do in the Tilt console, including checking logs
    and exploring errors. Tilt constantly displays updates and the status of the system,
    and always attempts to surface the most useful information.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在Tilt控制台中，您可以做很多事情，包括检查日志和探索错误。Tilt不断显示更新和系统状态，并始终尝试呈现最有用的信息。
- en: 'It''s interesting to see that Tilt build images with its own tag:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 看到Tilt使用自己的标签构建图像很有趣：
- en: '[PRE79]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Let''s make our standard change and see how Tilt reacts:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行标准更改，看看Tilt的反应：
- en: '[PRE80]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Tilt detected the change and built a new image, then promptly deployed it to
    the cluster:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: Tilt检测到更改并构建了一个新的图像，然后迅速部署到集群中：
- en: '[PRE81]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Let''s try our hand at some file syncing. We must run Flask in debug mode for
    hot reloading to work. This is as simple as adding `FLASK_DEBUG=1` to `ENTRYPOINT`
    in the Dockerfile:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一些文件同步。我们必须在调试模式下运行Flask才能使热重新加载起作用。只需在Dockerfile的`ENTRYPOINT`中添加`FLASK_DEBUG=1`即可：
- en: '[PRE82]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'It''s up to you to decide if you want a separate `Dockerfile.dev` file to use
    with Tilt, as we used for the link service. Here is a Tiltfile for the API gateway
    service that uses the live update facilities of Tilt:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以决定是否要使用单独的`Dockerfile.dev`文件与Tilt一起使用，就像我们用于链接服务一样。这是一个使用Tilt的实时更新功能的API网关服务的Tiltfile：
- en: '[PRE83]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'At this point, we can run `tilt up` and hit the `/links` endpoint:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以运行`tilt up`并访问`/links`端点：
- en: '[PRE85]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Tilt will show us the request and the successful `200` response:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: Tilt将向我们显示请求和成功的`200`响应：
- en: '![](assets/835fcab4-2df6-498d-ac9f-812d89085516.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/835fcab4-2df6-498d-ac9f-812d89085516.png)'
- en: Tilt API gateway
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: Tilt API网关
- en: 'Let''s make a little change and see if tilt picks it up and syncs the code
    in the container. In the `resources.py` file, let''s add to the result of the GET
    links the key-value pair - `tilt: Yeah, sync works!!`.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们做一点小改动，看看tilt是否能够检测到并同步容器中的代码。在`resources.py`文件中，让我们在`GET links`的结果中添加键值对-
    `tilt: Yeah, sync works!!`。'
- en: '[PRE86]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'As you can see in the following screenshot, Tilt detected the code change in
    `resources.py` and copied the new file into the container:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在以下截图中所看到的，Tilt检测到了`resources.py`中的代码更改，并将新文件复制到容器中：
- en: '![](assets/72b50246-0e8c-417b-8910-4c470a15f49d.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/72b50246-0e8c-417b-8910-4c470a15f49d.png)'
- en: Tilt API gateway 2
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: Tilt API网关2
- en: 'Let''s invoke the endpoint again and observe the results. It works as intended.
    We got the expected key-value after the links in the result:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次调用端点并观察结果。它按预期工作。在结果中，我们得到了链接后的预期键值：
- en: '[PRE87]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Overall, Tilt is extremely well done. It's based on a solid conceptual model,
    is very well executed, and it addresses the problems of local development better
    than any of the other tools. Tiltfile and Starlark are powerful and concise. It
    supports both full-fledged Docker builds and file syncing for dynamic languages.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Tilt做得非常好。它基于一个坚实的概念模型，执行得非常好，解决了本地开发的问题，比其他任何工具都要好。Tiltfile和Starlark功能强大而简洁。它支持完整的Docker构建和动态语言的文件同步。
- en: Summary
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered a broad swatch of topics related to deployments
    to Kubernetes. We started with a deep dive into the Kubernetes deployment object
    and considered and implemented deployments to multiple environments (for example,
    staging and production). We delved into advanced deployment strategies like rolling
    updates, blue-green deployments, and canary deployments, and experimented with
    all of them on Delinkcious. Then, we looked at rolling back failed deployments
    and the crucial topic of managing dependencies and versions. Later on, we switched
    gears into local development, and surveyed multiple tools for fast iterations
    where you make changes to your code, and they are automatically deployed to your
    cluster. We covered Ko, Ksync, Draft, Skaffold, and my personal favorite, Tilt.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了与部署到 Kubernetes 相关的广泛主题。我们从深入研究 Kubernetes 部署对象开始，考虑并实施了对多个环境（例如，暂存和生产）的部署。我们深入研究了滚动更新、蓝绿部署和金丝雀部署等高级部署策略，并在
    Delinkcious 上对它们进行了实验。然后，我们看了一下如何回滚失败的部署以及管理依赖和版本的关键主题。之后，我们转向本地开发，并调查了多个用于快速迭代的工具，您可以对代码进行更改，它们会自动部署到您的集群。我们涵盖了
    Ko、Ksync、Draft、Skaffold 和我个人最喜欢的 Tilt。
- en: At this point, you should have a deep understanding of the various deployment
    strategies, when to employ them on your system, and good hands-on experience with
    local development tools for Kubernetes that you can integrate into your workflow.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您应该对各种部署策略有深入的了解，知道何时在您的系统上使用它们，并且对 Kubernetes 的本地开发工具有丰富的实践经验，可以将其整合到您的工作流程中。
- en: In the next chapter, we will take it to the next level and get serious about
    monitoring our system. We will look into failure modes, how to design self-healing
    systems, autoscaling, provisioning, and performance. Then, we will consider logging,
    collecting metrics, and distributed tracing.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把它提升到下一个级别，并严肃地监控我们的系统。我们将研究故障模式，如何设计自愈系统，自动扩展，配置和性能。然后，我们将考虑日志记录，收集指标和分布式跟踪。
- en: Further reading
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Refer to the following links if you want to find out more about what was covered
    in this chapter:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解本章涵盖的更多内容，请参考以下链接：
- en: '**KO**: [https://github.com/google/ko](https://github.com/google/ko)'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KO**: [https://github.com/google/ko](https://github.com/google/ko)'
- en: '**Ksync**: [https://vapor-ware.github.io/ksync/](https://vapor-ware.github.io/ksync/)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ksync**: [https://vapor-ware.github.io/ksync/](https://vapor-ware.github.io/ksync/)'
- en: '**Draft**: [https://draft.sh/](https://draft.sh/)'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Draft**: [https://draft.sh/](https://draft.sh/)'
- en: '**Skaffold**: [https://skaffold.dev/](https://skaffold.dev/)'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Skaffold**: [https://skaffold.dev/](https://skaffold.dev/)'
- en: '**Tilt**: [https://docs.tilt.dev](https://docs.tilt.dev)'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tilt**: [https://docs.tilt.dev](https://docs.tilt.dev)'
