["```\n$ kubectl -n istio-system get po\n\nNAME READY STATUS RESTARTS AGE\nistio-citadel-6995f7bd9-69qhw 1/1 Running 0 11h\nistio-cleanup-secrets-6xkjx 0/1 Completed 0 11h\nistio-egressgateway-57b96d87bd-8lld5 1/1 Running 0 11h\nistio-galley-6d7dd498f6-pm8zz 1/1 Running 0 11h\nistio-ingressgateway-ddd557db7-b4mqq 1/1 Running 0 11h\nistio-pilot-5765d76b8c-l9n5n 2/2 Running 0 11h\nistio-policy-5b47b88467-tfq4b 2/2 Running 0 11h\nistio-sidecar-injector-6b9fbbfcf6-vv2pt 1/1 Running 0 11h\nistio-telemetry-65dcd9ff85-dxrhf 2/2 Running 0 11h\npromsd-7b49dcb96c-cn49l 2/2 Running 1 11h\n```", "```\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n name: link-manager\nspec:\n  hosts:\n  - link-manager # same as link-manager.default.svc.cluster.local\n  http:\n  - route:\n    - destination:\n        host: link-manager\n```", "```\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: link-manager\nspec:\n  host: link-manager\n  trafficPolicy:\n    loadBalancer:\n      simple: ROUND_ROBIN\n```", "```\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: link-manager\nspec:\n  host: link-manager\n  trafficPolicy:\n    portLevelSettings:\n    - port:\n        number: 80\n      loadBalancer:\n        simple: LEAST_CONN\n    - port:\n        number: 8080\n      loadBalancer:\n        simple: ROUND_ROBIN\n```", "```\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n   name: link-manager\nspec:\n  host: link-manager\n  trafficPolicy:\n     connectionPool:\n       tcp:\n         maxConnections: 200\n         connectTimeout: 45ms\n         tcpKeepalive:\n           time: 3600s\n           interval: 75s\n```", "```\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: link-manager\nspec:\n  host: link-manager\n  trafficPolicy:\n     outlierDetection:\n       consecutiveErrors: 10\n       interval: 2m\n       baseEjectionTime: 5m\n```", "```\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n   name: link-manager\n spec:\n   hosts:\n   - link-manager\n   http:\n   - fault:\n       delay:\n         percent: 10\n         fixedDelay: 5s\n```", "```\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: link-service\nspec:\n  hosts:\n    - reviews\n  http:\n  - route:\n    - destination:\n        host: link-service\n        subset: v1\n       weight: 95\n     - destination:\n         host: reviews\n         subset: v2\n       weight: 5\n```", "```\napiVersion: networking.istio.io/v1alpha3\n kind: DestinationRule\n metadata:\n   name: link-manager\n spec:\n   host: link-manager\n   subsets:\n   - name: v1\n     labels:\n       version: v1\n   - name: v2\n     labels:\n       version: v2\n```", "```\napiVersion: \"authentication.istio.io/v1alpha1\"\n kind: \"MeshPolicy\"\n metadata:\n   name: \"default\"\n spec:\n   peers:\n   - mtls: {}\n```", "```\napiVersion: \"authentication.istio.io/v1alpha1\"\n kind: \"Policy\"\n metadata:\n   name: \"default\"\n   namespace: \"some-ns\"\n spec:\n   targets:\n    - name: api-gateway\n    - name: link-manager\n      ports:\n      - number: 8080\n```", "```\npeers:\n   - mtls: {}\n```", "```\norigins:\n - jwt:\n     issuer: \"https://accounts.google.com\"\n     jwksUri: \"https://www.googleapis.com/oauth2/v3/certs\"\n     trigger_rules:\n     - excluded_paths:\n       - exact: /healthcheck\n```", "```\nprincipalBinding: USE_ORIGIN\n```", "```\napiVersion: \"rbac.istio.io/v1alpha1\"\n kind: ClusterRbacConfig\n metadata:\n   name: default\n spec:\n   mode: 'ON_WITH_EXCLUSION'\n   exclusion:\n     namespaces: [\"kube-system\", \"development\"]\n```", "```\napiVersion: \"rbac.istio.io/v1alpha1\"\n kind: ServiceRole\n metadata:\n   name: full-access-reader\n   namespace: default\n spec:\n   rules:\n   - services: [\"*-manager\", \"api-gateway\"]\n     paths:\n     methods: [\"GET\", \"HEAD\"]\n```", "```\napiVersion: \"rbac.istio.io/v1alpha1\"\n kind: ServiceRoleBinding\n metadata:\n   name: test-binding-products\n   namespace: default\n spec:\n   subjects:\n   - user: \"service-account-delinkcious\"\n   - user: \"istio-ingress-service-account\"\n     properties:\n       request.auth.claims[email]: \"the.gigi@gmail.com\"\n   roleRef:\n     kind: ServiceRole\n     name: \"full-access-reader\"\n```", "```\n--set global.disablePolicyChecks=false.\n```", "```\n$ kubectl -n istio-system get cm istio -o jsonpath=\"{@.data.mesh}\" | grep disablePolicyChecks\ndisablePolicyChecks: false\n```", "```\napiVersion: config.istio.io/v1alpha2\n kind: instance\n metadata:\n   name: request-count\n   namespace: istio-system\n spec:\n   compiledTemplate: metric\n   params:\n     value: \"1\" # count each request\n     dimensions:\n       reporter: conditional((context.reporter.kind | \"inbound\") == \"outbound\", \"client\", \"server\")\n       source: source.workload.name | \"unknown\"\n       destination: destination.workload.name | \"unknown\"\n       message: '\"counting requests...\"'\n     monitored_resource_type: '\"UNSPECIFIED\"'```", "```\n\nNow, we can configure a Prometheus handler to receive the metrics. Prometheus is a compiled adapter (which is part of Mixer), so we can just use it in the spec. The `spec | params | metrics` section has a kind of `COUNTER`, a Prometheus metric name (`request_count`), and, most importantly, the instance name that we just defined, which is the source of the metrics:\n\n```", "```\n\nFinally, we tie it all together with a rule, as follows:\n\n```", "```\n\nOkay, so Istio is amazingly powerful. But are there any situations where you shouldn't use Istio?\n\n# When should you avoid Istio?\n\nIstio provides a lot of value. However, this value is not without a cost. The intrusive nature of Istio and its complexity have some significant downsides. You should consider these downsides before you adopt Istio:\n\n*   Additional concepts and management systems on top of the already complex Kubernetes make the learning curve very steep.\n*   Troubleshooting configuration issues is challenging.\n*   Integration with other projects might be missing or partial (for example, NATS and Telepresence).\n*   The proxies add latency and consume CPU and memory resources.\n\nIf you're just starting with Kubernetes, I recommend waiting until you get the hang of it before you even consider using Istio.\n\nNow that we understand what Istio is all about, let's explore how Delinkcious can benefit from Istio.\n\n# Delinkcious on Istio\n\nWith Istio, Delinkcious can potentially shed a lot of extra baggage. So, why is it a good idea to move this functionality from Delinkcious services or Go kit middleware to Istio?\n\nWell, the reason is that this functionality is often unrelated to the application domain. We invested a lot of work to carefully separate concerns and isolate the Delinkcious domain from the way they are deployed and managed. However, as long as all of those concerns are addressed by the microservices themselves, we will need to make changes to the code and rebuild them every time we want to make an operational change. Even if a lot of this is data-driven, it can make it difficult to troubleshoot and debug issues because, when a failure happens, it's not always easy to determine whether it was due to a bug in the domain code or the operational code.\n\nLet's take a look at some specific examples where Istio can simplify Delinkcious.\n\n# Removing mutual authentication between services\n\nAs you may recall, in [Chapter 6](f7718dfe-8c96-495b-9089-36b9bbced4c8.xhtml), *Securing Microservices on Kubernetes*, we created a mutual secret between the `link-manager`\u00a0service and the `social-graph-manager`\u00a0service:\n\n```", "```\n\nIt required a lot of coordination and explicit work to encode the secrets, and then mount the secrets into the containers:\n\n```", "```\n\nThen, the link manager had to get the secret through the `auth_util` package we had to implement, and inject it as a header to the request:\n\n```", "```\n\nFinally, the social graph manager has to be aware of this scheme and explicitly check whether the caller is allowed:\n\n```", "```\n\nThat's a lot of work that has nothing to do with the service itself. Imagine managing access to hundreds of interacting microservices with thousands of methods. This approach is cumbersome, error-prone, and requires code changes being made to two services whenever you add or remove an interaction.\n\nWith Istio, we can externalize this completely as a role and a role binding. Here is a role that allows you to call the GET method of the `/following` endpoint:\n\n```", "```\n\nIn order to allow only the link service to call the method, we can bind the role to the `link-manager` service account as the subject user:\n\n```", "```\n\nIf, later, we need to allow other services to call the `/following` endpoint, we can add more subjects to this role binding. The social service itself doesn't need to know what service is allowed to call its methods. The calling services don't need to provide any credentials explicitly. The service mesh takes care of all that.\n\nAnother area where Istio can really help Delinkcious is with canary deployments.\n\n# Utilizing better canary deployments\n\nIn [Chapter 11](ba776b0b-35e6-4fbd-9450-78b155daa743.xhtml), *Deploying Microservices*, we used Kubernetes deployments and services to do canary deployments. In order to divert 10% of the traffic to a canary version, we scaled the current version to nine replicas and created a canary deployment, with one replica for the new version. We used the same labels (`svc: link` and `app: manager`) for both deployments.\n\nThe `link-manager` service in front of both deployments distributed the load evenly between all the pods, creating the 90/10 split we were aiming for:\n\n```", "```\n\nThis works, but it couples canary deployments with scaling deployments. This can be expensive, especially if you need to run the canary deployment for a while until you are confident that it is okay. Ideally, you shouldn't need to create more pods just to divert a certain percentage of your traffic to a new version.\n\nThe traffic shaping capabilities with the subset concepts of Istio address this use case perfectly. The following virtual service splits the traffic into a ratio of 90/10 between a subset called `v0.5` and another subset called `canary`:\n\n```", "```\n\nDoing canary deployments with Istio's virtual services and subsets is great for Delinkcious. Istio can help with logging and error reporting, too.\n\n# Automatic logging and error reporting\n\nWhen running Delinkcious on GKE with the Istio add-on, you get automatic integration with Stackdriver, which is a one-stop shop for monitoring, including metrics, centralized logging, error reporting, and distributed tracing. Here is the Stackdriver log viewer for when you are searching for the `link-manager` logs:\n\n![](assets/6099c1f0-231e-4ae7-ac23-1e811b7183a1.png)\n\nAlternatively, you can filter by service name through the drop-down list. Here is what it looks like when specifying the api-gateway:\n\n![](assets/1e2866bd-d11d-4642-a65f-be8ee5b65c24.png)\n\nSometimes, the error reporting view is what you need:\n\n![](assets/0f5d208d-1511-4be7-ae49-cf0d143da468.png)\n\nThen, you can drill down into any error and get a lot of additional information that will help you understand what went wrong and how to fix it:\n\n![](assets/87161def-792a-4732-88dc-266613cd7ba3.png)\n\nWhile Istio provides a lot of value and, in the case of Stackdriver, you benefit from automatic setup too, it is not always smooth riding \u2013 it has some limitations and rough edges.\n\n# Accommodating NATS\n\nOne of the limitations I discovered when deploying Istio into the Delinkcious cluster is that NATS doesn't work with Istio because it requires direct connections and it breaks when the Envoy proxy hijacks the communication. The solution is to prevent Istio from injecting the sidecar container and accepting that NATS will not be managed. Adding the`NatsCluster`\u00a0CRD\u00a0to\u00a0the following annotation to the pod spec does the work for us: `sidecar.istio.io/inject: \"false\"`:\n\n```", "```\n\nThe preceding code is the complete `NatsCluster` resource definition with the annotation in place.\n\n# Examining the Istio footprint\n\nIstio deploys a lot of stuff into the cluster, so let's review some of it. Mercifully, the Istio control plane is isolated in its own `istio-system` namespace, but CRDs are always cluster-wide and Istio doesn't skimp on those:\n\n```", "```\n\nIn addition to all of those CRDs, Istio installs all its components into the Istio namespace:\n\n```", "```\n\nFinally, Istio, of course, installs its sidecar proxies into each pod (except Nats, where we disabled it). As you can see, each pod in the default namespace has two containers (2/2 under the `READY` column). One container does the work and the other is the Istio proxy sidecar container:\n\n```"]