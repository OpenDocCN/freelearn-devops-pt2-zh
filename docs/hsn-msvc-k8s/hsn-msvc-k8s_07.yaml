- en: Talking to the World - APIs and Load Balancers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与世界交流- API和负载均衡器
- en: In this chapter, we're finally going to open Delinkcious to the world and let
    users interact with it from outside the cluster.  This is important because Delinkcious
    users can't access the internal services running inside the cluster. We're going
    to significantly expand the capabilities of Delinkcious by adding a Python-based
    API gateway service and expose it to the world (including social login). We'll add
    a gRPC-based news service that users can hit to get news about other users they
    follow. Finally, we will add a message queue that lets services communicate in
    a loosely coupled manner.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们最终将向外部打开Delinkcious，让用户可以从集群外部与其进行交互。这很重要，因为Delinkcious用户无法访问集群内部运行的内部服务。我们将通过添加基于Python的API网关服务并将其暴露给世界（包括社交登录）来显著扩展Delinkcious的功能。我们将添加一个基于gRPC的新闻服务，用户可以使用它来获取关注的其他用户的新闻。最后，我们将添加一个消息队列，让服务以松散耦合的方式进行通信。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Getting familiar with Kubernetes services
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉Kubernetes服务
- en: East-west versus north-south communication
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 东西向与南北向通信
- en: Understanding ingress and load balancing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解入口和负载均衡
- en: Providing and consuming a public REST API
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供和使用公共REST API
- en: Providing and consuming an internal gRPC API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供和使用内部gRPC API
- en: Sending and receiving events via a message queue
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过消息队列发送和接收事件
- en: Preparing for service meshes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为服务网格做准备
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will add a Python service to Delinkcious. There is no need
    to install anything new. We will build a Docker image for the Python service later.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向Delinkcious添加一个Python服务。无需安装任何新内容。我们稍后将为Python服务构建一个Docker镜像。
- en: The code
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: 'You can find the updated Delinkcious application here: [https://github.com/the-gigi/delinkcious/releases/tag/v0.5](https://github.com/the-gigi/delinkcious/releases/tag/v0.5)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到更新的Delinkcious应用程序：[https://github.com/the-gigi/delinkcious/releases/tag/v0.5](https://github.com/the-gigi/delinkcious/releases/tag/v0.5)
- en: Getting familiar with Kubernetes services
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熟悉Kubernetes服务
- en: 'Pods (one or more containers bundled together) are the units of work in Kubernetes.
    Deployments make sure that there are enough pods running. However, individual
    pods are ephemeral. Kubernetes services are where the action is and how you can
    expose your pods as a coherent service to other services in the cluster or even
    externally to the world. A Kubernetes service provides a stable identity and typically
    maps 1:1 to an application service (which may be a microservice or a traditional
    fat service). Let''s look at all the services:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Pod（一个或多个容器捆绑在一起）是Kubernetes中的工作单位。部署确保有足够的Pod在运行。但是，单个Pod是短暂的。Kubernetes服务是行动所在的地方，以及您如何将您的Pod公开为一个连贯的服务，供集群中的其他服务甚至外部世界使用。Kubernetes服务提供稳定的标识，并且通常将应用程序服务（可以是微服务或传统的大型服务）进行1：1映射。让我们看看所有的服务：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You've seen how the Delinkcious microservices are deployed using Kubernetes
    services and how they can discover and call each other through the environment
    variables Kubernetes provides. Kubernetes also provides DNS-based discovery.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经看到了Delinkcious微服务是如何使用Kubernetes服务部署的，以及它们如何通过Kubernetes提供的环境变量进行发现和调用。Kubernetes还提供基于DNS的发现。
- en: 'Each service can be accessed inside the cluster via the DNS name:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务都可以通过DNS名称在集群内部访问：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I prefer to use environment variables because it allows me to run the services
    outside of Kubernetes for testing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我更喜欢使用环境变量，因为这样可以让我在Kubernetes之外运行服务进行测试。
- en: 'Here is how to find the IP address of the `social-graph-manager` service using
    both environment variables and DNS:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用环境变量和DNS查找`social-graph-manager`服务的IP地址：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Kubernetes associates a service with its backing pods by specifying a label
    selector. For example, as shown in the following code, `news-service` is backed
    by pods that have the `svc: link` and `app: manager` labels:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kubernetes通过指定标签选择器将服务与其支持的pod关联起来。例如，如下所示的代码，`news-service`由具有`svc: link`和`app:
    manager`标签的pod支持：'
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, Kubernetes manages the IP addresses of all the pods that match the label
    selector using an `endpoints` resource, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Kubernetes使用`endpoints`资源管理与标签选择器匹配的所有pod的IP地址，如下所示：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `endpoints` resource always keeps an up-to-date list of the IP addresses
    and ports of all the backing pods of a service. When pods are added, removed,
    or recreated with another IP address and port, the `endpoints` resource is updated.
    Now, let's see what types of services are available in Kubernetes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`endpoints`资源始终保持支持服务的所有pod的IP地址和端口的最新列表。当添加、删除或重新创建具有另一个IP地址和端口的pod时，将更新`endpoints`资源。现在，让我们看看Kubernetes中有哪些类型的服务。'
- en: Service types in Kubernetes
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的服务类型
- en: 'Kubernetes services always have a type. It''s important to understand when
    to use each type of service. Let''s go over the various service types and the
    differences between them:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes服务始终具有类型。了解何时使用每种类型的服务非常重要。让我们来看看各种服务类型及其之间的区别：
- en: '**ClusterIP (default)**: The ClusterIP type means that the service is only
    accessible inside the cluster. This is the default, and it''s perfect for microservices
    to communicate with each other. For testing purposes, you can expose such services
    using `kube-proxy` or `port-forwarding`. It is also a good way to view the Kubernetes
    dashboard or other UIs of internal services, such as Argo CD in Delinkcious.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ClusterIP（默认）：ClusterIP类型意味着服务只能在集群内部访问。这是默认设置，非常适合微服务之间的通信。为了测试目的，您可以使用`kube-proxy`或`port-forwarding`来暴露这样的服务。这也是查看Kubernetes仪表板或内部服务的其他UI（例如Delinkcious中的Argo
    CD）的好方法。
- en: If you don't specify a type of ClusterIP, set the `ClusterIP` to `None`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不指定ClusterIP的类型，请将`ClusterIP`设置为`None`。
- en: '**NodePort**: A service that''s of the NodePort type is exposed to the world
    through a dedicated port on all the nodes. You can access the service through
    `<Node IP>:<NodePort>`. The NodePort will be selected from a range you can control
    via `--service-node-port-range` to the Kubernetes API server if you run it yourself
    (by default, this is 30000-32767).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NodePort：NodePort类型的服务通过所有节点上的专用端口向世界公开。您可以通过`<Node IP>:<NodePort>`访问服务。如果您自己运行Kubernetes
    API服务器，则可以通过`--service-node-port-range`控制范围来选择NodePort（默认情况下为30000-32767）。
- en: You can also explicitly specify NodePort in your service definition. If you
    have a lot of services exposed via node ports that you specify, you'll have to
    manage those ports carefully to avoid conflicts. When a request comes into any
    node though the dedicated NodePort, the kubelet will take care of forwarding it
    to a node that has one of the backing pods on it (you can find it via endpoints).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在服务定义中明确指定NodePort。如果您通过指定的节点端口暴露了大量服务，则必须小心管理这些端口，以避免冲突。当请求通过专用NodePort进入任何节点时，kubelet将负责将其转发到具有其中一个支持pod的节点（您可以通过endpoints找到它）。
- en: '**LoadBalancer**: This type of service is most common when your Kubernetes
    cluster runs on a cloud platform that provides load balancer support. Although
    there are Kubernetes-aware load balancers for on-premise clusters too, the external
    load balancer will be in charge of accepting external requests and routing them
    through the service to the backing pods. There are often cloud provider-specific
    intricacies such as special annotations or having to create dual services to handle
    internal and external requests. We will use the LoadBalancer type to expose Delinkcious
    to the world of minikube, which provides a load balancer emulation.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LoadBalancer**：当您的Kubernetes集群在提供负载均衡器支持的云平台上运行时，这种类型的服务最常见。尽管在本地集群中也有适用于Kubernetes的负载均衡器，但外部负载均衡器将负责接受外部请求并将其通过服务路由到后端Pod。通常存在云提供商特定的复杂性，例如特殊注释或必须创建双重服务来处理内部和外部请求。我们将使用LoadBalancer类型来将Delinkcious暴露给minikube的世界，该世界提供了负载均衡器仿真。'
- en: '**ExternalName**: These services just resolve requests to the service to an
    externally provided DNS name. This is useful if your services need to talk to
    external services not running in the cluster, but you still want to be able to
    find them as if they were Kubernetes services. This may be useful if you plan
    to migrate those external services to the cluster at some point.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ExternalName**：这些服务只是将请求解析到外部提供的DNS名称。如果您的服务需要与集群外部未运行的外部服务通信，但仍希望能够像它们是Kubernetes服务一样找到它们，这将非常有用。如果您计划将这些外部服务迁移到集群中，这可能会很有用。'
- en: Now that we understand what services are all about, let's discuss the differences
    between cross-service communication inside the cluster and exposing services outside
    the cluster.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了服务的全部内容，让我们讨论一下集群内部的跨服务通信和将服务暴露到集群外部之间的区别。
- en: East-west versus north-south communication
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 东西通信与南北通信
- en: East-west communication is when services/pods/containers communicate with each
    other inside the cluster. As you may recall, Kubernetes exposes all the services
    inside the cluster via both DNS and environment variables. This solves the service
    discovery problem inside the cluster. It is up to you to impose further restrictions
    via network policies or other mechanisms. For example, in [Chapter 5](0d340a5c-b2da-41ab-a50d-56bd985c10f2.xhtml),* Configuring
    Microservices with Kubernetes*, we established mutual authentication between the
    link service and the social graph service.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 东西通信是指服务/Pod/容器在集群内部相互通信。正如您可能还记得的那样，Kubernetes通过DNS和环境变量公开了集群内的所有服务。这解决了集群内部的服务发现问题。您可以通过网络策略或其他机制来进一步施加限制。例如，在[第5章](0d340a5c-b2da-41ab-a50d-56bd985c10f2.xhtml)中，*使用Kubernetes配置微服务*，我们在链接服务和社交图服务之间建立了相互认证。
- en: 'North-south communication is about exposing services to the world. In theory,
    you could expose just your services via NodePort, but this approach is beset by
    numerous problems, including the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 南北通信是指向世界暴露服务。理论上，您可以仅通过NodePort暴露您的服务，但这种方法存在许多问题，包括以下问题：
- en: You have to deal with secure/encrypted transport yourself
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须自行处理安全/加密传输
- en: You can't control which pods will actually service requests
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法控制哪些Pod实际上会为请求提供服务
- en: You have to either let Kubernetes choose random ports for your services or manage
    port conflicts carefully
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须让Kubernetes为您的服务选择随机端口，或者仔细管理端口冲突。
- en: Only one service can be exposed via each port (for example, the coveted port
    `80` can't be reused)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个端口只能暴露一个服务（例如，令人垂涎的端口`80`不能被重用）
- en: The production approved methods for exposing your services are used via an ingress
    controller and/or a load balancer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 批准生产的暴露服务的方法是通过入口控制器和/或负载均衡器使用。
- en: Understanding ingress and load balancing
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解入口和负载均衡
- en: 'The ingress concept in Kubernetes is about controlling access to your services
    and potentially providing additional features, such as the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的入口概念是关于控制对您的服务的访问，并可能提供其他功能，例如以下内容：
- en: SSL termination
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSL终止
- en: Authentication
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身份验证
- en: Routing to multiple services
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由到多个服务
- en: There is an ingress resource that defines routing rules for other relevant information,
    and there is also an ingress controller that reads all the ingress resources defined
    in the cluster (across all namespaces). The ingress resource receives all the
    requests and routes to the target services that distribute them to the backing
    pods. The ingress controller serves as a cluster-wide software load balancer and
    router. Often, there will be a hardware load balancer that sits in front of the
    cluster and sends all traffic to the ingress controller.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个入口资源，定义其他相关信息的路由规则，还有一个入口控制器，它读取集群中定义的所有入口资源（跨所有命名空间）。入口资源接收所有请求并路由到分发它们到后台pod的目标服务。入口控制器充当集群范围的软件负载均衡器和路由器。通常，会有一个硬件负载均衡器坐在集群前面，并将所有流量发送到入口控制器。
- en: Let's go ahead and put all of these concepts together and expose Delinkcious
    to the world by adding a public API gateway.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把所有这些概念放在一起，通过添加一个公共API网关来向世界展示Delinkcious。
- en: Providing and consuming a public REST API
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供和使用公共REST API
- en: In this section, we will build a whole new service in Python (API gateway) to
    demonstrate that Kubernetes is really language-agnostic. Then, we will add user
    authentication via OAuth2 and expose the API gateway service externally.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将构建一个全新的Python服务（API网关），以证明Kubernetes实际上是与语言无关的。然后，我们将通过OAuth2添加用户身份验证，并将API网关服务暴露给外部。
- en: Building a Python-based API gateway service
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基于Python的API网关服务
- en: 'The API gateway service is designed to receive all requests from outside the
    cluster and route them to the proper services. Here is the directory''s structure:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: API网关服务旨在接收来自集群外部的所有请求，并将它们路由到适当的服务。以下是目录结构：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is a little different from the Go services. The code is under the `api_gateway_service`
    directory, which is also a Python package. The Kubernetes resources are under
    the `k8s` subdirectory, and there is a `tests` subdirectory too. In the top directory,
    the `run.py` file is the entry point, as defined in the `Dockerfile`. The `main()`
    function in `run.py` invokes the `app.run()` method of the app that''s imported
    from the `api.py` module:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这与Go服务有些不同。代码位于`api_gateway_service`目录下，这也是一个Python包。Kubernetes资源位于`k8s`子目录下，还有一个`tests`子目录。在顶级目录中，`run.py`文件是入口点，如`Dockerfile`中定义的那样。`run.py`中的`main()`函数调用了从`api.py`模块导入的`app.run()`方法：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `api.py` module is responsible for creating the app, hooking up the routing,
    and implementing social login.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`api.py`模块负责创建应用程序，连接路由，并实现社交登录。'
- en: Implementing social login
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现社交登录
- en: The `api-gateway` service utilizes several Python packages to assist in implementing
    social login via GitHub. Later, we will cover the user flow, but first, we will
    take a look at the code that implements it. The `login()` method is reaching out
    to GitHub and requesting authorization to the current user, who must be logged
    in to GitHub and give authorization to Delinkcious.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`api-gateway`服务利用了几个Python包来帮助通过GitHub实现社交登录。稍后，我们将介绍用户流程，但首先，我们将看一下实现它的代码。`login()`方法正在与GitHub联系，并请求对当前用户进行授权，该用户必须已登录GitHub并授权给Delinkcious。'
- en: 'The `logout()` method just removed the access token from the current session. The
    `authorized()` method is getting called by GitHub as a redirect after a successful
    login attempt and provides an access token that is displayed for the user in their
    browser. This access token must be passed as a header to all future requests to
    the API gateway:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`logout()`方法只是从当前会话中删除访问令牌。`authorized()`方法在GitHub成功登录尝试后被调用，并提供一个访问令牌，该令牌将在用户的浏览器中显示。这个访问令牌必须作为标头传递给API网关的所有未来请求：'
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When a user is passing a valid access token, Delinkcious can retrieve their
    name and email from GitHub. If the access token is missing or invalid, the request
    will be rejected with a 401 access denied error. This happens in the `_get_user()`
    function in `resources.py`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户传递有效的访问令牌时，Delinkcious可以从GitHub检索他们的姓名和电子邮件。如果访问令牌丢失或无效，请求将被拒绝，并显示401访问被拒绝错误。这发生在`resources.py`中的`_get_user()`函数中：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The GitHub object is created and initialized in the `create_app()` function
    of the `api.py` module. First, it imports a few third-party libraries, that is,
    `Flask`, `OAuth`, and `Api` class:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub对象是在`api.py`模块的`create_app()`函数中创建和初始化的。首先，它导入了一些第三方库，即`Flask`、`OAuth`和`Api`类：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, it initializes the `Flask` app with a GitHub `Oauth` provider:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它使用GitHub `Oauth`提供程序初始化`Flask`应用程序：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, it sets the routing map and stores the initialized `app` object:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它设置路由映射并存储初始化的`app`对象：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Routing traffic to internal microservices
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将流量路由到内部微服务
- en: The main job of the API gateway service is to implement the API gateway pattern
    we discussed in [Chapter 2](d4214218-a4e9-4df8-813c-e00df71da935.xhtml), *Getting
    Started with Microservices*. For example, here is how it routes the get links
    requests to the proper method of the link microservice.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: API网关服务的主要工作是实现我们在[第2章](d4214218-a4e9-4df8-813c-e00df71da935.xhtml)中讨论的API网关模式，*开始使用微服务*。例如，它是如何将获取链接请求路由到链接微服务的适当方法的。
- en: The `Link` class is derived from the `Resource` base class. It gets the host
    and port from the environment and constructs the base URL.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`Link`类是从`Resource`基类派生的。它从环境中获取主机和端口，并构造基本URL。'
- en: 'The `get()` method is called when a GET request for the `links` endpoint comes
    in. It extracts the username from the GitHub token in the `_get_user()` function
    and parses the query part of the request URL for the other parameter. Then, it
    makes its own request to the link manager service:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当GET请求`links`端点时，将调用`get()`方法。它从`_get_user()`函数中的GitHub令牌中提取用户名，并解析请求URL的查询部分以获取其他参数。然后，它会向链接管理器服务发出自己的请求：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Utilizing base Docker images to reduce build time
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用基础Docker镜像来减少构建时间
- en: 'When we built Go microservices for Delinkcious, we used the scratch image as
    the base and just copied the Go binary. The images are super lightweight, at less
    than 10 MB. However, the API gateway is almost 500 MB, even when using `python:alpine`,
    which is much lighter than the standard Debian-based Python image:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为Delinkcious构建Go微服务时，我们使用了scratch镜像作为基础，只是复制了Go二进制文件。这些镜像非常轻量，不到10MB。然而，即使使用`python:alpine`，API网关也几乎有500MB，这比标准的基于Debian的Python镜像要轻得多：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In addition, the API gateway needs to build some bindings to native libraries.
    Installing the C/C++ toolchain and then building the native libraries takes a
    long time (more than 15 minutes). Docker shines here with reusable layers and
    base images. We can put all the heavyweight stuff into a separate base image at
    `svc/shared/docker/python_flask_grpc/Dockerfile`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，API网关需要构建一些与本地库的绑定。安装C/C++工具链，然后构建本地库需要很长时间（超过15分钟）。Docker在这里表现出色，具有可重用的层和基础镜像。我们可以将所有繁重的东西放入一个单独的基础镜像中，位于`svc/shared/docker/python_flask_grpc/Dockerfile`：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `requirements.txt` file contains the dependencies for `Flask` applications
    that execute social login and need to consume a gRPC service (more on this later):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`requirements.txt`文件包含执行社交登录并需要使用gRPC服务的`Flask`应用程序的依赖项（稍后详细介绍）：'
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With all of this in place, we can build the base image, and then the API gateway
    Dockerfile can be based on it. The following is the super-simple build script
    at `svc/shared/docker/python_flask_grpc/build.sh` that builds the base image and
    pushes it to DockerHub:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们可以构建基础镜像，然后API网关Dockerfile可以基于它。以下是在`svc/shared/docker/python_flask_grpc/build.sh`中的超级简单构建脚本，用于构建基础镜像并将其推送到DockerHub：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s take a look at the Dockerfile for the API gateway service at `svc/api_gateway_service/Dockerfile`.
    It is based on our base image. Then, it copies the `api_gate_service` directory,
    exposes the `5000` port, and executes the `run.py` script:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`svc/api_gateway_service/Dockerfile`中API网关服务的Dockerfile。它基于我们的基础镜像。然后，它复制`api_gate_service`目录，公开`5000`端口，并执行`run.py`脚本：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The benefit is that as long as the heavy base image doesn't change, then making
    changes to the actual API service gateway code will result in lightning fast Docker
    image builds. We're talking a few seconds compared to 15 minutes. At this point,
    we have a nice and quick build-test-debug-deploy for the API gateway service.
    Now is a good time to add ingress to the cluster.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 好处是只要重型基础镜像不改变，对实际API服务网关代码进行更改将导致闪电般快速的Docker镜像构建。我们说的是几秒钟，而不是15分钟。在这一点上，我们对API网关服务有了一个不错而快速的构建-测试-调试-部署。现在是向集群添加入口的好时机。
- en: Adding ingress
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加入口
- en: 'On Minikube, you must enable the ingress add-on:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在Minikube上，您必须启用入口附加组件：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: On other Kubernetes clusters, you may want to install your own favorite ingress
    controller (such as Contour, Traefik, or Ambassador).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他Kubernetes集群上，您可能希望安装自己喜欢的入口控制器（例如Contour、Traefik或Ambassador）。
- en: 'The following code is for the ingress manifest for the API gateway service.
    By using this pattern, our entire cluster will have a single ingress that funnels
    every request to our API gateway service, which will route it to the proper internal
    service:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是API网关服务的入口清单。通过使用这种模式，我们的整个集群将有一个单一的入口，将每个请求引导到我们的API网关服务，然后将其路由到适当的内部服务：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The single ingress service is simple and effective. On most cloud platforms,
    you pay per ingress resource, since a load balancer is created for each ingress
    resource. You can scale the number of API gateway instances easily since it is
    totally stateless.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 单个入口服务简单而有效。在大多数云平台上，您按入口资源付费，因为为每个入口资源创建了一个负载均衡器。您可以轻松扩展API网关实例的数量，因为它完全无状态。
- en: Minikube does a lot of magic under the covers with networking, simulating load
    balancers, and tunneling traffic. I don't recommend using Minikube to test ingress
    to the cluster. Instead, we will use a service of the LoadBalancer type and access
    it through the Minikube cluster IP.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube在网络下面做了很多魔术，模拟负载均衡器，并隧道流量。我不建议使用Minikube来测试对集群的入口。相反，我们将使用LoadBalancer类型的服务，并通过Minikube集群IP访问它。
- en: Verifying that the API gateway is available outside the cluster
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证API网关在集群外部是否可用
- en: Delinkcious uses GitHub as a social login provider. You must have a GitHub account
    to follow along.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Delinkcious使用GitHub作为社交登录提供程序。您必须拥有GitHub帐户才能跟随。
- en: 'The user flow is as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 用户流程如下：
- en: Find the Delinkcious URL (on Minikube, this will change frequently).
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找Delinkcious URL（在Minikube上，这将经常更改）。
- en: Log in and get an access token.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录并获取访问令牌。
- en: Hit the Delinkcious API gateway from outside the cluster.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从集群外部访问Delinkcious API网关。
- en: Let's dive in and go over this in detail.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入详细讨论一下。
- en: Finding the Delinkcious URL
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找Delinkcious URL
- en: 'In a production cluster, you''ll have a well-known DNS name configured and
    a load balancer hooked up to that name. With Minikube, we can get the API gateway
    service URL using the following command:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产集群中，您将配置一个众所周知的 DNS 名称，并连接一个负载均衡器到该名称。使用 Minikube，我们可以使用以下命令获取 API 网关服务的
    URL：
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'It''s convenient to store it in an environment variable for interactive use
    with commands, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与命令进行交互使用，将其存储在环境变量中是方便的，如下所示：
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Getting an access token
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取访问令牌
- en: 'Here are the steps for getting an access token:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 获取访问令牌的步骤如下：
- en: 'Now that we have the API gateway URL, we can browse to the login endpoint,
    that is, `http://192.168.99.138:31658/login`. If you''re signed into your GitHub
    account, you''ll see the following dialog box:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了 API 网关 URL，我们可以浏览到登录端点，即 `http://192.168.99.138:31658/login`。如果您已登录到您的
    GitHub 帐户，您将看到以下对话框：
- en: '![](assets/806ac3a0-d918-4aef-9c87-de521ba70753.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/806ac3a0-d918-4aef-9c87-de521ba70753.png)'
- en: 'Next, if this is the first time your logging in to Delinkcious, GitHub will
    ask you to authorize Delinkcious to get access to your email and name:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，如果这是您第一次登录 Delinkcious，GitHub 将要求您授权 Delinkcious 获取访问您的电子邮件和姓名：
- en: '![](assets/975e7472-5cc3-4b1c-ad44-32550e819dae.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/975e7472-5cc3-4b1c-ad44-32550e819dae.png)'
- en: 'If you approve of this, then you''ll be redirected to a page that will show
    you a lot of information about your GitHub profile, but, most importantly, provide
    you with an access token, as shown in the following screenshot:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您同意，那么您将被重定向到一个页面，该页面将向您显示有关您的 GitHub 个人资料的大量信息，但更重要的是，向您提供一个访问令牌，如下截图所示：
- en: '![](assets/d2ca79b6-c9ee-4133-b2cc-621775611cb3.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/d2ca79b6-c9ee-4133-b2cc-621775611cb3.png)'
- en: 'Let''s store the access token in an environment variable, too:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也将访问令牌存储在环境变量中：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that we have all the information we need to access Delinkcious from the
    outside, let's take it for a test drive.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了从外部访问 Delinkcious 所需的所有信息，让我们来试一试。
- en: Hitting the Delinkcious API gateway from outside the cluster
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从集群外部访问 Delinkcious API 网关
- en: 'We''ll use HTTPie to hit the API gateway endpoint at `${DELINKCIOUS_URL}/v1.0/links`.
    To authenticate, we must provide the access token as a header, that is, `"Access-Token:
    ${DELINKCIOUS_TOKEN}"`.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将使用 HTTPie 命中 `${DELINKCIOUS_URL}/v1.0/links` 的 API 网关端点。要进行身份验证，我们必须将访问令牌作为标头提供，即
    `"Access-Token: ${DELINKCIOUS_TOKEN}"`。'
- en: 'Starting with a clean slate, let''s verify that there are no links whatsoever:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始，让我们验证一下是否没有任何链接：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Alright – so far, so good. Let''s add a couple of links by sending a POST request
    to the `/v1.0/links` endpoint. Here is the first link:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，到目前为止一切都很顺利。让我们通过向 `/v1.0/links` 端点发送 POST 请求来添加一些链接。这是第一个链接：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And here is the second link:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第二个链接：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'No errors. That''s great. By getting the links again, we can see the new links
    we just added:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 没有错误。太好了。通过再次获取链接，我们可以看到我们刚刚添加的新链接：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have successfully established an end-to-end flow, including user authentication,
    thus hitting a Python API gateway service that talks to a Go microservice via
    its internal HTTP REST API and stores information in a relational DB. Now, let's
    up the ante and add yet another service.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功建立了端到端的流程，包括用户身份验证，因此通过其内部 HTTP REST API 与 Go 微服务通信的 Python API 网关服务，并将信息存储在关系型数据库中。现在，让我们提高赌注并添加另一个服务。
- en: This time, it will be a Go microservice that uses a gRPC transport.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，将使用 gRPC 传输的 Go 微服务。
- en: Providing and consuming an internal gRPC API
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供和使用内部 gRPC API
- en: The service we will implement in this section is called the news service. Its
    job is to keep track of link events, such as link added or link updated, and return
    new events to users.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中实现的服务称为新闻服务。它的工作是跟踪链接事件，如添加链接或更新链接，并向用户返回新事件。
- en: Defining the NewsManager interface
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义 NewsManager 接口
- en: 'This interface exposes a single `GetNews()` method. Users may invoke it and
    receive a list of link events from users they follow. Here is the Go interface
    and related structs. It doesn''t get much simpler: a single method with a request
    struct with `username` and `token` fields, as well as a result struct. The resulting
    struct contains a list of `Event` structs with the following information: `EventType`,
    `Username`, `Url`, and `Timestamp`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此接口公开了一个`GetNews()`方法。用户可以调用它并从他们关注的用户那里收到链接事件列表。以下是Go接口和相关结构。它并不复杂：一个带有`username`和`token`字段的请求结构体，以及一个结果结构体。结果结构体包含一个`Event`结构体列表，其中包含以下信息：`EventType`、`Username`、`Url`和`Timestamp`：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Implementing the news manager package
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现新闻管理器包
- en: The implementation of the core logic service is in `pkg/news_manager`. Let's
    take a look at the `new_manager.go` file. The `NewsManager` struct has an `InMemoryNewsStore`
    called `eventStore` that implements the `GetNews()` method for the `NewsManager`
    interface. It delegates the work of actually fetching the news to the store.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 核心逻辑服务的实现在`pkg/news_manager`中。让我们看一下`new_manager.go`文件。`NewsManager`结构有一个名为`eventStore`的`InMemoryNewsStore`，它实现了`NewsManager`接口的`GetNews()`方法。它将实际获取新闻的工作委托给存储。
- en: 'However, it is aware of pagination and takes care of converting the token from
    a string into an integer to match the store preferences:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，它知道分页并负责将令牌从字符串转换为整数以匹配存储偏好：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The store is very basic and just keeps a map between usernames and all their
    events, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 存储非常基础，只是在用户名和所有事件之间保持映射，如下所示：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The store implements its own `GetNews()` method (a different signature from
    the `interface` method). It just returns the requested slice for the target user
    based on the start index and the maximum page size:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 存储实现了自己的`GetNews()`方法（与`interface`方法的签名不同）。它只是根据起始索引和最大页面大小返回目标用户请求的切片：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'It also has a method for adding new events:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 它还有一种添加新事件的方法：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now that we've implemented the core logic of storing and providing news to users,
    let's look at how to expose this functionality as a gRPC service.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了存储和向用户提供新闻的核心逻辑，让我们看看如何将这个功能公开为gRPC服务。
- en: Exposing NewsManager as a gRPC service
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将NewsManager公开为gRPC服务
- en: 'Before diving into the gRPC implementation of the news service, let''s see
    what all the fuss is about. The gRPC is a collection of a wire protocol, payload
    format, conceptual framework, and code generation facilities for interconnecting
    services and applications. It originated in Google (hence the g in gRPC) and is
    a highly performant and mature RPC framework. It has many things going for it,
    such as the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解新闻服务的gRPC实现之前，让我们看看到底是怎么回事。gRPC是一组用于连接服务和应用程序的传输协议、有效载荷格式、概念框架和代码生成工具。它起源于Google（因此在gRPC中有g），是一个高性能且成熟的RPC框架。它有很多优点，比如以下：
- en: Cross-platform
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨平台
- en: Wide spread adoption by industry
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行业广泛采用
- en: Idiomatic client libraries for all relevant programming languages
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有相关编程语言的惯用客户端库
- en: Extremely efficient wire protocols
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极其高效的传输协议
- en: Google protocol buffers for strongly typed contracts
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google协议缓冲区用于强类型合同
- en: HTTP/2 support enables bi-directional streaming
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP/2支持实现双向流
- en: Highly extensible (customize your own authentication, authorization, load balancing,
    and health checking)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可扩展（自定义您自己的身份验证、授权、负载均衡和健康检查）
- en: Excellent documentation
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出色的文档
- en: The bottom line is that for internal microservices, it is superior in almost
    every way to HTTP-based REST APIs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，对于内部微服务而言，它在几乎所有方面都优于基于HTTP的REST API。
- en: For Delinkcious, it's a great fit because Go-kit, which we selected as our microservice
    framework, has great support for gRPC.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Delinkcious来说，这非常合适，因为我们选择的微服务框架Go-kit对gRPC有很好的支持。
- en: Defining the gRPC service contract
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义gRPC服务契约
- en: gRPC requires that you define a contract for your service in a special DSL inspired
    by protocol buffers. It is pretty intuitive and lets gRPC generate a lot of boilerplate
    code for you. I chose to locate the contract and the generated code in a separate
    top-level directory called **pb** (common short name for **protocol buffers**)
    because different parts of the generated code will be used by services and consumers.
    In these cases, it is often best to put the shared code in a separate location
    and not arbitrarily throw it into the service or the client.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC要求您使用受协议缓冲区启发的特殊DSL为您的服务定义契约。它非常直观，并且让gRPC为您生成大量样板代码。我选择将契约和生成的代码放在一个名为**pb**（协议缓冲区的常用简称）的单独顶级目录中，因为生成的代码的不同部分将被服务和消费者使用。在这些情况下，最好将共享代码放在一个单独的位置，而不是随意地将其放入服务或客户端。
- en: 'Here is the `pb/new-service/pb/news.proto` file:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`pb/new-service/pb/news.proto`文件：
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We don't need to go over the syntax and meaning of each and every line. The
    short version is that requests and responses are always messages. Service-level
    errors need to be embedded in the response message. Other errors, such as network
    or invalid payloads, will be reported separately. One interesting tidbit is that,
    in addition to primitive data types and embedded messages, you can use other high-level
    types, such as the `google.protobuf.Timestamp` data type. This elevates the abstraction
    level significantly and brings the benefits of strong typing for things such as
    dates and timestamps that you always have to serialize and deserialize yourself
    when working with JSON over HTTP/REST.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要逐行讨论每一行的语法和含义。简而言之，请求和响应始终是消息。服务级错误需要嵌入在响应消息中。其他错误，如网络或无效的有效载荷，将被单独报告。一个有趣的细节是，除了原始数据类型和嵌入的消息之外，您还可以使用其他高级类型，例如`google.protobuf.Timestamp`数据类型。这显著提高了抽象级别，并为诸如日期和时间戳之类的事物带来了强类型化的好处，这些事物在使用JSON进行HTTP/REST工作时，您总是需要自己进行序列化和反序列化。
- en: The service definition is cool, but we need some actual code to connect the
    dots. Let's see how gRPC can help with this task.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 服务定义很酷，但我们需要一些实际的代码来连接这些点。让我们看看gRPC如何帮助完成这个任务。
- en: Generating service stubs and client libraries with gRPC
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用gRPC生成服务存根和客户端库
- en: The gRPC model is used to generate both service stubs and client libraries using
    a tool called `protoc`. We need to generate both Go code for the news service
    itself and Python code for the API gateway that consumes it.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC模型用于使用一个名为`protoc`的工具生成服务存根和客户端库。我们需要为新闻服务本身生成Go代码，以及为消费它的API网关生成Python代码。
- en: 'You can generate `news.pb.go` by running the following command:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行以下命令生成`news.pb.go`：
- en: '[PRE33]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You can generate `news_pb2.py` and `news_pb2_grpc.py` by running the following command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行以下命令生成`news_pb2.py`和`news_pb2_grpc.py`：
- en: '[PRE34]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: At this point, both the Go client code and Python client code can be used to
    call the news service from Go code or from Python code.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，Go客户端代码和Python客户端代码都可以用于从Go代码或Python代码调用新闻服务。
- en: Using Go-kit to build the NewsManager service
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Go-kit构建NewsManager服务
- en: 'Here is the implementation of the service itself in `news_service.go`. It looks
    very similar to an HTTP service. Let''s dissect the important sections. First,
    it imports some libraries, including the generated gRPC code in `pb/news-service-pb`,
    `pkg/news_manager`, and a general gRPC library called `google.golang.org/grpc`.
    At the beginning of the `Run()` function, it gets the `service` port to listen
    from the environment:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在`news_service.go`中服务本身的实现。它看起来非常类似于HTTP服务。让我们分解一下重要的部分。首先，它导入一些库，包括在`pb/news-service-pb`、`pkg/news_manager`和一个名为`google.golang.org/grpc`的一般gRPC库中生成的gRPC代码。在`Run()`函数的开头，它从环境中获取`service`端口来监听：
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, we need to create a standard TCP listener on the target port:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要在目标端口上创建一个标准的TCP监听器：
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Furthermore, we have to connect to a NATS message queue service. We''ll discuss
    this in detail in the next section:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们必须连接到一个NATS消息队列服务。我们将在下一节中详细讨论这个问题：
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here comes the main initialization code. It instantiates a new news manager,
    creates a new gRPC server, creates a news manager object, and registers the news
    manager with the gRPC server. The `pb.RegisterNewsManager()` method was generated
    by gRPC from the `news.proto` file:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是主要的初始化代码。它实例化一个新的新闻管理器，创建一个新的gRPC服务器，创建一个新闻管理器对象，并将新闻管理器注册到gRPC服务器。`pb.RegisterNewsManager()`方法是由gRPC从`news.proto`文件生成的：
- en: '[PRE38]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, the gRPC server starts listening on the TCP listener:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，gRPC服务器开始在TCP监听器上监听：
- en: '[PRE39]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Implementing the gRPC transport
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现gRPC传输
- en: The last piece of the puzzle is implementing the gRPC transport in the `transport.go`
    file. It is similar, conceptually, to the HTTP transport, but there are a few
    details that are different. Let's break it down so it's clear how all the pieces
    fit together.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 拼图的最后一部分是在`transport.go`文件中实现gRPC传输。在概念上，它类似于HTTP传输，但有一些不同的细节。让我们分解一下，以便清楚地了解所有部分是如何组合在一起的。
- en: 'First, all the relevant packages are imported, including the gRPC transport
    from go-kit. Note that in `news_service.go`, there is no mention of go-kit anywhere.
    You can definitely implement a gRPC service directly in Go with the general gRPC
    libraries. However, here, go-kit will help make this much easier via its service
    and endpoints concepts:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入所有相关的包，包括来自go-kit的gRPC传输。请注意，在`news_service.go`中，没有任何地方提到go-kit。您肯定可以直接在Go中使用一般的gRPC库实现gRPC服务。然而，在这里，通过其服务和端点的概念，go-kit将帮助使这变得更容易：
- en: '[PRE40]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `newEvent()` function is a helper that adopts `om.Event` from our abstract
    object model to the gRPC-generated event object. The most important part is translating
    the event type and the timestamp:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`newEvent()`函数是一个辅助函数，它从我们的抽象对象模型中采用`om.Event`到gRPC生成的事件对象。最重要的部分是翻译事件类型和时间戳：'
- en: '[PRE41]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Decoding the request and encoding the response is pretty trivial – there''s
    no need to serialize or deserialize any JSON code:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 解码请求和编码响应非常简单 - 没有必要序列化或反序列化任何JSON代码：
- en: '[PRE42]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Creating the endpoint is similar to the HTTP transport you''ve seen with other
    services. It invokes the actual service implementation and then translates the
    response and handles errors, if there are any:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 创建端点类似于您在其他服务中看到的HTTP传输。它调用实际的服务实现，然后翻译响应并处理错误（如果有的话）：
- en: '[PRE43]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The handler implements the gRPC news interface from the code generated:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 处理程序实现了从生成的代码中的gRPC新闻接口：
- en: '[PRE44]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The `newNewsServer()` function ties everything together. It returns a gRPC
    handler wrapped in a Go-kit handler that hooks up the endpoint, the request decoder,
    and the response encoder:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`newNewsServer()`函数将所有内容联系在一起。它返回一个包装在Go-kit处理程序中的gRPC处理程序，连接端点、请求解码器和响应编码器：'
- en: '[PRE45]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This may seem very confusing, with all the layers and nested functions, but
    the bottom line is that you have to write very little glue code (and can generate
    it, which is ideal) and end up with a very clean, safe (strongly typed), and efficient
    gRPC service.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来非常混乱，有着各种层和嵌套函数，但底线是你只需要编写很少的粘合代码（并且可以生成它，这是理想的），最终得到一个非常干净、安全（强类型）和高效的gRPC服务。
- en: Now that we have a gRPC news service that can serve the news, let's see how
    we can feed it the news.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个可以提供新闻的gRPC新闻服务，让我们看看如何为其提供新闻。
- en: Sending and receiving events via a message queue
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过消息队列发送和接收事件
- en: 'The news service needs to store link events for each user. The link service
    knows when links are added, updated, or deleted by different users. One approach
    to solve this problem is to add another API to the news service and have the link
    service invoke this API and notify the news service for each relevant event. However,
    this approach creates a tight coupling between the link service and the news service.
    The link service doesn''t really care about the news service since it doesn''t
    need anything from it. Instead, let''s go for a loosely-coupled solution. The
    link service will just send events to a general-purpose message queue service.
    Then, independently, the news service will subscribe to receive messages from
    that messages queue. There are several benefits to this approach, as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻服务需要为每个用户存储链接事件。链接服务知道不同用户何时添加、更新或删除链接。解决这个问题的一种方法是向新闻服务添加另一个API，并让链接服务调用此API，并通知新闻服务每个相关事件。然而，这种方法会在链接服务和新闻服务之间创建紧密耦合。链接服务并不真正关心新闻服务，因为它不需要任何来自新闻服务的东西。相反，让我们选择一种松散耦合的解决方案。链接服务只会向一个通用消息队列服务发送事件。然后，独立地，新闻服务将订阅从该消息队列接收消息。这种方法有几个好处，如下所示：
- en: No need for more complicated service code
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要更复杂的服务代码
- en: Fits perfectly with the interaction model of event notification
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与事件通知的交互模型完美契合
- en: Easy to add additional listeners to the same events without changing the code
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很容易在不改变代码的情况下添加额外的监听器到相同的事件
- en: The terms that I used here, that is, *message*, *event*, and *notification*,
    are interchangeable. The idea is that a source has some information to share with
    the world in a fire-and-forget way.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里使用的术语，即*消息*、*事件*和*通知*，是可以互换的。这个想法是，源有一些信息以一种即时即忘的方式与世界分享。
- en: It doesn't need to know who is interested in the information (this could be
    nobody or multiple listeners) and whether it was processed successfully. Delinkcious
    uses the NATS messaging system for loosely coupled communication between services.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 它不需要知道谁对信息感兴趣（可能是没有人或多个监听器），以及是否成功处理。Delinkcious使用NATS消息系统进行服务之间的松散耦合通信。
- en: What is NATS?
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NATS是什么？
- en: 'NATS ([https://nats.io/](https://nats.io/)) is an open source message queue
    service. It is a **Cloud Native Computing Foundation** (**CNCF**) project that''s
    implemented in Go and is considered one of the top contenders when you need a
    message queue in Kubernetes. NATS supports multiple models of message passing,
    such as the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: NATS（[https://nats.io/](https://nats.io/)）是一个开源消息队列服务。它是一个**Cloud Native Computing
    Foundation**（**CNCF**）项目，用Go实现，被认为是在Kubernetes中需要消息队列时的顶级竞争者之一。NATS支持多种消息传递模型，如下所示：
- en: Publish-subscribe
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布-订阅
- en: Request-reply
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求-回复
- en: Queueing
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排队
- en: 'NATS is very versatile and can be used for many use cases. It can also run
    in a highly available cluster. For Delinkcious, we will use the publish-subscribe
    model. The following diagram illustrates the pub-sub message passing model. A
    publisher publishes a message and all the subscribers receive the same message:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: NATS非常灵活，可以用于许多用例。它也可以在高可用的集群中运行。对于Delinkcious，我们将使用发布-订阅模型。以下图表说明了发布-订阅消息传递模型。发布者发布一条消息，所有订阅者都会收到相同的消息：
- en: '![](assets/f623f1c6-9276-40f2-b18b-989601014789.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/f623f1c6-9276-40f2-b18b-989601014789.png)'
- en: Let's deploy NATS in our cluster.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的集群中部署NATS。
- en: Deploying NATS in the cluster
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在集群中部署NATS
- en: 'First, let''s install the NATS operator ([https://github.com/nats-io/nats-operator](https://github.com/nats-io/nats-operator)).
    The NATS operator helps you to manage NATS clusters in Kubernetes. Here are the
    commands to install it:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们安装NATS操作员（[https://github.com/nats-io/nats-operator](https://github.com/nats-io/nats-operator)）。NATS操作员可以帮助您在Kubernetes中管理NATS集群。以下是安装它的命令：
- en: '[PRE46]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The NATS operator provides a NatsCluster **Custom Resource Definition** (**CRD**)
    that we will use to deploy NATS in our Kubernetes cluster. Don''t get confused
    by this NATS cluster within the Kubernetes cluster relationship. This is really
    nice since we can deploy the NATS cluster just like built-in Kubernetes resources.
    Here is the YAML manifest that''s available in `svc/shared/k8s/nats_cluster.yaml`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: NATS操作员提供了一个NatsCluster **自定义资源定义**（**CRD**），我们将使用它在我们的Kubernetes集群中部署NATS。不要被Kubernetes集群内的NATS集群关系所困扰。这真的很好，因为我们可以像内置的Kubernetes资源一样部署NATS集群。以下是在`svc/shared/k8s/nats_cluster.yaml`中可用的YAML清单：
- en: '[PRE47]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let''s deploy it using `kubectl` and verify that it was deployed properly:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`kubectl`部署它，并验证它是否被正确部署：
- en: '[PRE48]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This looks good. The `nats-cluster` service listening on port `4222` is the
    NATS server. The other service is a management service. Let's send some events
    to the NATS server.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错。监听端口`4222`的`nats-cluster`服务是NATS服务器。另一个服务是管理服务。让我们向NATS服务器发送一些事件。
- en: Sending link events with NATS
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NATS发送链接事件
- en: 'As you may recall, we defined a `LinkManagerEvents` interface in our object
    model:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能记得的，我们在我们的对象模型中定义了一个`LinkManagerEvents`接口：
- en: '[PRE49]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The `LinkManager` package receives this event link in its `NewLinkManager()`
    method:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`LinkManager`包在其`NewLinkManager()`方法中接收此事件链接：'
- en: '[PRE50]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Later, when a link is added, updated, or deleted, `LinkManager` will call the
    corresponding `OnLinkXXX()` method. For example, when `AddLink()` is called, the
    `OnLinkAdded()` method is called on the sink for each follower:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，当链接被添加、更新或删除时，`LinkManager`将调用相应的`OnLinkXXX()`方法。例如，当调用`AddLink()`时，对于每个关注者，都会在接收器上调用`OnLinkAdded()`方法：
- en: '[PRE51]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This is great, but how are these events going to get to the NATS server? That''s
    where the link service comes into the picture. When instantiating the `LinkManager`
    object, it will pass a dedicated event sender object as the sink that implements
    `LinkManagerEvents`. Whenever it receives an event such as `OnLinkAdded()` or
    `OnLinkUpdated()`, it publishes the event to the NATS server on the `link-events`
    subject. It ignores the `OnLinkDeleted()` event for now. This object lives in
    `pkg/link_manager_events package/sender.go`:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这很棒，但这些事件将如何传送到NATS服务器？这就是链接服务发挥作用的地方。在实例化`LinkManager`对象时，它将传递一个专用的事件发送对象作为实现`LinkManagerEvents`的接收器。每当它接收到诸如`OnLinkAdded()`或`OnLinkUpdated()`之类的事件时，它会将事件发布到`link-events`主题的NATS服务器上。它暂时忽略`OnLinkDeleted()`事件。这个对象位于`pkg/link_manager_events
    package/sender.go`中：
- en: '[PRE52]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Here is the implementation of the `OnLinkAdded()`, `OnLinkUpdated()`, and `OnLinkDeleted()` methods:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`OnLinkAdded()`、`OnLinkUpdated()`和`OnLinkDeleted()`方法的实现：
- en: '[PRE53]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `NewEventSender()` factory function accepts the URL of the NATS service
    it will send the events to and returns a `LinkManagerEvents` interface that can
    serve as a sink for `LinkManager`:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`NewEventSender()`工厂函数接受NATS服务的URL，将事件发送到NATS服务，并返回一个`LinkManagerEvents`接口，可以作为`LinkManager`的接收端：'
- en: '[PRE54]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, all the link service has to do is figure out the URL for the NATS server.
    Since the NATS server runs as a Kubernetes service, its hostname and port are
    available through environment variables, just like the Delinkcious microservices.
    The following is the relevant code from the `Run()` function of the link service:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，链接服务所需做的就是找出NATS服务器的URL。由于NATS服务器作为Kubernetes服务运行，其主机名和端口可以通过环境变量获得，就像Delinkcious微服务一样。以下是链接服务的`Run()`函数中的相关代码：
- en: '[PRE55]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: At this point, whenever a new link is added or updated for a user, `LinkManager`
    will invoke the `OnLinkAdded()` or `OnLinkUpdated()` method for each of the followers,
    which will result in that event being sent to the NATS server on the `link-events`
    topic, where all the subscribers will receive it and can handle it. The next step
    is for the news service to subscribe to these events.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，每当为用户添加或更新新链接时，`LinkManager`将为每个关注者调用`OnLinkAdded()`或`OnLinkUpdated()`方法，这将导致该事件被发送到`link-events`主题的NATS服务器上，所有订阅者都将收到并处理它。下一步是新闻服务订阅这些事件。
- en: Subscribing to link events with NATS
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NATS订阅链接事件
- en: 'The news service uses the `Listen()` function from `pkg/link_manager_events/listener.go`.
    It accepts the NATS server URL and an event sink that implements the `LinkManagerEvents`
    interface. It connects to the NATS server and then subscribes to the `link-events`
    subject. This is the same subject that the event sender is sending those events
    to:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻服务使用`pkg/link_manager_events/listener.go`中的`Listen()`函数。它接受NATS服务器的URL和实现`LinkManagerEvents`接口的事件接收端。它连接到NATS服务器，然后订阅`link-events`主题。这与事件发送器发送这些事件的主题相同：
- en: '[PRE56]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, let''s look at the `nats.go` file that defines the `link-events` subject,
    as well as the `connect()` function that''s used by both the event sender and
    the `Listen()` function. The connect function uses the `go-nats` client to establish
    a connection and then wraps it with a JSON encoder, which allows it to send and
    receive Go structs that get serialized automatically. This is pretty neat:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下定义`link-events`主题的`nats.go`文件，以及`connect()`函数，该函数被事件发送器和`Listen()`函数使用。连接函数使用`go-nats`客户端建立连接，然后用JSON编码器包装它，这使它能够自动序列化发送和接收Go结构。这很不错：
- en: '[PRE57]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The news service calls the `Listen()` function in its `NewNewsManager()` factory
    function. First, it instantiates the news manager object that implements `LinkManagerEvents`.
    Then, `if` composes a NATS server URL if a NATS hostname was provided and calls
    the `Listen()` function, thereby passing the news manager object as the sink:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻服务在其`NewNewsManager()`工厂函数中调用`Listen()`函数。首先，它实例化实现`LinkManagerEvents`的新闻管理器对象。然后，如果提供了NATS主机名，则组合NATS服务器URL并调用`Listen()`函数，从而将新闻管理器对象作为接收端传递：
- en: '[PRE58]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The next step is to do something with the incoming events.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对传入的事件进行处理。
- en: Handling link events
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理链接事件
- en: 'The news manager was subscribed to link events by the `NewNewsManager()` function,
    and the result is that those events will arrive as calls on `OnLinkAdded()` and
    `OnlinkUpdated()` (delete link events are ignored). The news manager creates an
    `Event` object that''s defined in the abstract object model, populates it with `EventType`,
    `Username`, `Url`, and `Timestamp`, and then calls the event store''s `AddEvent()`
    function. Here is the `OnLinkAdded()` method:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻管理器通过`NewNewsManager()`函数订阅链接事件，结果是这些事件将作为对`OnLinkAdded()`和`OnlinkUpdated()`的调用到达（删除链接事件被忽略）。新闻管理器创建了一个在抽象对象模型中定义的`Event`对象，用`EventType`、`Username`、`Url`和`Timestamp`填充它，然后调用事件存储的`AddEvent()`函数。这是`OnLinkAdded()`方法：
- en: '[PRE59]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Here is the `OnLinkUpdated()` method:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`OnLinkUpdated()`方法：
- en: '[PRE60]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let''s see what the store does in its `AddEvent()` method. It''s pretty simple:
    the subscribed user is located in the `userEvents` map. If they don''t exist yet,
    then an empty entry is created and the new event added. If the target user calls
    `GetNews()`, they''ll receive the events that have been collected for them:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看存储在其`AddEvent()`方法中做了什么。这很简单：订阅用户位于`userEvents`映射中。如果他们还不存在，那么将创建一个空条目并添加新事件。如果目标用户调用`GetNews()`，他们将收到为他们收集的事件：
- en: '[PRE61]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'That concludes our coverage of the news service and its interactions with the
    link manager via the NATS service. This is an application of the **command query
    responsibility segregation** (**CQRS**) pattern we discussed in [Chapter 2](d4214218-a4e9-4df8-813c-e00df71da935.xhtml),
    *Getting Started with Microservices*. Here is what the Delinkcious system looks
    like now:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对新闻服务及其通过NATS服务与链接管理器的交互的覆盖。这是我们在[第2章](d4214218-a4e9-4df8-813c-e00df71da935.xhtml)中讨论的**命令查询责任分离**（**CQRS**）模式的应用，*使用微服务入门*。现在Delinkcious系统看起来是这样的：
- en: '![](assets/47cae881-3b57-49f8-84b6-7fd50313a172.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/47cae881-3b57-49f8-84b6-7fd50313a172.png)'
- en: Now that we understand how events are handled in Delinkcious, let's take a quick
    look at service meshes.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了Delinkcious中如何处理事件，让我们快速看一下服务网格。
- en: Understanding service meshes
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解服务网格
- en: A service mesh is another layer of management that's running in your cluster.
    We will look into service meshes and Istio in particular in [Chapter 13](b39834c8-859c-42a5-846a-e48b76dfd6cc.xhtml),
    *Service Mesh – Working with Istio*. At this point, I just want to mention that
    a service mesh often takes the role of the ingress controller too.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格是在您的集群中运行的另一层管理。我们将在[第13章](b39834c8-859c-42a5-846a-e48b76dfd6cc.xhtml)中详细了解服务网格和特别是Istio，*服务网格-使用Istio*。在这一点上，我只想提一下，服务网格经常也承担入口控制器的角色。
- en: 'One of the primary reasons to use a service mesh for ingress is that the built-in
    ingress resource, being very generic, is limited and suffers from multiple issues,
    such as the following:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用服务网格进行入口的主要原因之一是，内置的入口资源非常通用，受到多个问题的限制，例如以下问题：
- en: No good way to validate the rules
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有很好的方法来验证规则
- en: Ingress resources can conflict with one other
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口资源可能会相互冲突
- en: Working with specific ingress controllers is often complicated and requires
    custom annotations
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特定的入口控制器通常很复杂，并且需要自定义注释。
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we accomplished many tasks and connected all the dots. In particular,
    we implemented two microservices design patterns (API gateway and CQRS), added
    a whole new service implemented in Python (including a split Docker base image),
    added a gRPC service, added an open source message queue system (NATS) to our
    cluster and integrated it with pub-sub message passing, and, finally, opened up
    our cluster to the world and demonstrated end-to-end interaction by adding and
    fetching links from Delinkcious.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们完成了许多任务并连接了所有要点。特别是，我们实现了两种微服务设计模式（API网关和CQRS），添加了一个用Python实现的全新服务（包括一个分割的Docker基础镜像），添加了一个gRPC服务，向我们的集群添加了一个开源消息队列系统（NATS）并将其与发布-订阅消息传递集成，最后，我们向世界打开了我们的集群，并通过向Delinkcious添加和获取链接来演示端到端的交互。
- en: At this point, Delinkcious can be considered Alpha-grade software. It's functional,
    but not even close to production ready. In the next chapter, we will start making
    Delinkcious more robust by taking care of the most valuable commodity of any software
    system – the data. Kubernetes provides many facilities for managing data and stateful
    services that we will put to good use.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，Delinkcious可以被视为Alpha级软件。它是功能性的，但离生产就绪还差得远。在下一章中，我们将通过处理任何软件系统的最有价值的商品
    - 数据，使Delinkcious更加健壮。Kubernetes提供了许多管理数据和有状态服务的设施，我们将充分利用它们。
- en: Further reading
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can refer to the following sources for more information regarding what
    was covered in this chapter:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下来源，了解本章涵盖的更多信息：
- en: '**The Kubernetes service**: [https://kubernetes.io/docs/concepts/services-networking/service/](https://kubernetes.io/docs/concepts/services-networking/service/)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes服务**：[https://kubernetes.io/docs/concepts/services-networking/service/](https://kubernetes.io/docs/concepts/services-networking/service/)'
- en: '**Exposing your app as a service**: [https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/](https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的应用程序公开为服务：[https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/](https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/)
- en: '**Building Oauth apps**: [https://developer.github.com/apps/building-oauth-apps/](https://developer.github.com/apps/building-oauth-apps/)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建Oauth应用程序：[https://developer.github.com/apps/building-oauth-apps/](https://developer.github.com/apps/building-oauth-apps/)
- en: '**High-performance gRPC**: [https://grpc.io/](https://grpc.io/)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高性能gRPC**：[https://grpc.io/](https://grpc.io/)'
- en: '[http://www.devx.com/architect/high-performance-services-with-grpc.html](http://www.devx.com/architect/high-performance-services-with-grpc.html)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.devx.com/architect/high-performance-services-with-grpc.html](http://www.devx.com/architect/high-performance-services-with-grpc.html)'
- en: '**NATS message broker**: [https://nats.io/](https://nats.io/)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NATS消息代理**：[https://nats.io/](https://nats.io/)'
