- en: Disaster Recovery
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 灾难恢复
- en: In every production system, **disaster recovery** (**DR**) and **business continuity**
    (**BC**) are the key concepts that you will have to bear in mind in order to provide
    the availability of your application workloads. You have to take them into account
    at an early stage in planning your cluster architecture. The proverb *by failing
    to prepare, you are preparing to fail *cannot be more true for operating distributed
    systems, such as Kubernetes. This chapter will focus on DR when running Kubernetes
    clusters. BC best practices, such as multiregion Deployments and asynchronous
    replication of persistent volumes, are not included in the scope of the chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个生产系统中，**灾难恢复**（**DR**）和**业务连续性**（**BC**）是您必须牢记的关键概念，以确保应用工作负载的可用性。您必须在早期阶段考虑它们，以规划您的集群架构。谚语*未能准备，实际上是在准备失败*对于操作Kubernetes等分布式系统来说再合适不过了。本章将重点介绍运行Kubernetes集群时的灾难恢复。本章的范围不包括多区部署和持久卷的异步复制等BC最佳实践。
- en: 'Disaster recovery, in general, consists of a set of policies, tools, and procedures
    to enable the recovery or continuation of vital technology infrastructure and
    systems following a natural or human-induced disaster. You can read more about
    the concepts involved in planning for DR in an excellent article from Google at [https://cloud.google.com/solutions/dr-scenarios-planning-guide](https://cloud.google.com/solutions/dr-scenarios-planning-guide).
    The main difference between DR and BC is that DR focuses on getting the infrastructure
    up and running following an outage, whereas BC deals with keeping business scenarios
    running during a major incident. The important thing about DR in Kubernetes is
    that you can essentially focus on data and state protectionfor your cluster: you
    need to have a backup-and-restore strategy for the stateful components. In a Kubernetes
    cluster, the most important stateful component is the etcd cluster, which is the
    storage layer for the Kubernetes API server.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，灾难恢复包括一套政策、工具和程序，以使关键技术基础设施和系统在自然或人为灾难后能够恢复或继续运行。您可以在Google的一篇优秀文章中了解更多关于灾难恢复规划涉及的概念：[https://cloud.google.com/solutions/dr-scenarios-planning-guide](https://cloud.google.com/solutions/dr-scenarios-planning-guide)。灾难恢复和业务连续性的主要区别在于，灾难恢复侧重于在停机后使基础设施恢复运行，而业务连续性则处理在重大事件期间保持业务场景运行。在Kubernetes中，灾难恢复的重要之处在于，您可以基本上专注于对集群的数据和状态进行保护：您需要为有状态的组件制定备份和恢复策略。在Kubernetes集群中，最重要的有状态组件是etcd集群，它是Kubernetes
    API服务器的存储层。
- en: 'In this chapter, we are going to cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Kubernetes cluster backup strategy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes集群备份策略
- en: Backing up the etcd cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份etcd集群
- en: Restoring the etcd cluster backup
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恢复etcd集群备份
- en: Automating backup
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化备份
- en: Replacing a failed etcd cluster member
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替换失败的etcd集群成员
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter you will need the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您将需要以下内容：
- en: Windows 10 Pro, Enterprise, or Education (version 1903 or later, 64-bit) installed
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了Windows 10 Pro、企业版或教育版（1903版或更高版本，64位）
- en: SSH client installed on your Windows machine
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的Windows机器上安装SSH客户端
- en: Azure account
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure帐户
- en: Multimaster Windows/Linux Kubernetes cluster deployed using AKS Engine or an
    on-premise cluster (valid for some scenarios)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AKS Engine或本地集群部署的多主Windows/Linux Kubernetes集群（适用于某些场景）
- en: To follow along, you will need your own Azure account in order to create Azure
    resources for Kubernetes clusters. If you haven't already created the account
    for earlier chapters, you can read more about how to obtain a limited free account
    for personal use at [https://azure.microsoft.com/en-us/free/](https://azure.microsoft.com/en-us/free/).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟着做，您需要自己的Azure账户以创建Kubernetes集群的Azure资源。如果您之前没有为早期章节创建账户，您可以阅读有关如何获取个人使用的有限免费账户的更多信息[https://azure.microsoft.com/en-us/free/](https://azure.microsoft.com/en-us/free/)。
- en: Deploying a Kubernetes cluster using AKS Engine has been covered in [Chapter
    8](ab695a0d-05dc-48f8-8c41-bbd167cfbfa6.xhtml), *Deploying Hybrid Azure Kubernetes
    Service Engine Cluster*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AKS Engine部署Kubernetes集群已在[第8章](ab695a0d-05dc-48f8-8c41-bbd167cfbfa6.xhtml)中进行了介绍，*部署混合Azure
    Kubernetes服务引擎集群*。
- en: You can download the latest code samples for this chapter from the official
    GitHub repository at [https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter15](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter15).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从官方GitHub存储库[https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter15](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/tree/master/Chapter15)下载本章的最新代码示例。
- en: Kubernetes cluster backup strategy
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes集群备份策略
- en: 'Disaster recovery for Kubernetes essentially involves creating a cluster state
    backup-and-restore strategy. Let''s first take a look at what stateful components
    are in Kubernetes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的灾难恢复基本上涉及创建集群状态备份和恢复策略。让我们首先看看Kubernetes中有哪些有状态的组件：
- en: Etcd cluster ([https://etcd.io/](https://etcd.io/)) that persists the state
    for Kubernetes API server resources.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd集群（[https://etcd.io/](https://etcd.io/)）用于持久化Kubernetes API服务器资源的状态。
- en: Persistent volumes used by pods.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod使用的持久卷。
- en: And surprisingly (or *not*), that is all! For the master node components and
    pods running on worker nodes, you don't have any nonrecoverable state involved;
    if you provision a new replacement node, Kubernetes can easily move the workload
    to the new nodes, providing full business continuity. When your etcd cluster is
    restored, Kubernetes will take care of reconciling the cluster component's state.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是（或者*不*），就是这样！对于主节点组件和运行在工作节点上的pod，您不涉及任何不可恢复的状态；如果您提供了一个新的替换节点，Kubernetes可以轻松地将工作负载移动到新节点，提供完整的业务连续性。当您的etcd集群被恢复时，Kubernetes将负责协调集群组件的状态。
- en: Let's take a look at how to back up and restore persistent volumes. It all depends
    on how your persistent volumes are provisioned. You can rely on standard filesystem
    backups stored externally or, in the case of cloud-backed PVs, you can use disk
    snapshots and manage them as part of the cloud services. There is also an interesting
    snapshot-and-restore feature (currently in the alpha state) for PVs provisioned
    using CSI plugins. This will provide better backup-and-restore integration directly
    at the Kubernetes-cluster level.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何备份和恢复持久卷。这完全取决于您的持久卷是如何提供的。您可以依赖于存储在外部的标准文件系统备份，或者在云支持的PV的情况下，您可以使用磁盘快照并将其作为云服务的一部分进行管理。还有一个有趣的快照和恢复功能（目前处于alpha状态），用于使用CSI插件提供的PV。这将直接在Kubernetes集群级别提供更好的备份和恢复集成。
- en: There is a general rule of thumb to keep your cluster workloads as stateless
    as possible. Think about using external, managed services to store your data (for
    example, Azure blob storage, Azure Cosmos DB) for which availability and data
    reliability are guaranteed by SLAs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个通用的经验法则，尽量使您的集群工作负载尽可能无状态。考虑使用外部托管服务来存储您的数据（例如，Azure blob存储，Azure Cosmos
    DB），这些服务的可用性和数据可靠性由SLA保证。
- en: 'For an etcd cluster, the backup-and-recovery strategy depends on two factors:
    how you store etcd data and what your high-availability topology for Kubernetes
    masters is. In the case of etcd data storage, the situation is similar to persistent
    volumes. If you mount the storage using a cloud volume, you can rely on disk snapshots
    from your cloud service provider (which is the case for AKS Engine), and for self-managed
    disks, you can employ standard filesystem backup strategies. In all cases, you
    also have a third option: you can use the snapshot feature of etcd itself. We
    will later show you how to perform a snapshot-and-restore of etcd using the `etcdctl`
    command.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于etcd集群，备份和恢复策略取决于两个因素：您如何存储etcd数据以及Kubernetes主节点的高可用性拓扑是什么。在etcd数据存储的情况下，情况类似于持久卷。如果您使用云卷挂载存储，可以依赖云服务提供商的磁盘快照（这是AKS
    Engine的情况），对于自管理磁盘，可以采用标准的文件系统备份策略。在所有情况下，您还有第三个选择：您可以使用etcd本身的快照功能。我们稍后将向您展示如何使用`etcdctl`命令执行etcd的快照和恢复。
- en: 'Regarding the high-availability topology for your Kubernetes masters, as mentioned
    in [Chapter 4](118e3c89-786e-4718-ba67-6c38928e2a42.xhtml), *Kubernetes Concepts
    and Windows Support*, you can run a **stacked** topology or an **external** topology
    for etcd. In a stacked topology, the etcd member is running as a Kubernetes pod
    on *every* master node. For the external topology, you run an etcd cluster outside
    your Kubernetes cluster. It may be fully external, deployed on separate, dedicated
    hosts, or it may share the same hosts as the master node. The latter is the case
    for AKS Engine: it runs the external topology, but each master node hosts an etcd
    member as a native Linux service. For both of these topologies, you can perform
    backups in the same way; the only difference lies in how you perform the restore.
    In the stacked topology, which is commonly used for **kubeadm** Deployments, you
    need to perform a `kubeadm init` on a new node overriding the local etcd storage.
    For an external topology, you can simply use the `etcdctl` command.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Kubernetes主节点的高可用性拓扑，如[第4章](118e3c89-786e-4718-ba67-6c38928e2a42.xhtml)中所述，*Kubernetes概念和Windows支持*，您可以运行**堆叠**拓扑或**外部**拓扑用于etcd。在堆叠拓扑中，etcd成员作为Kubernetes
    pod在*每个*主节点上运行。对于外部拓扑，您在Kubernetes集群之外运行etcd集群。它可能是完全外部的，部署在单独的专用主机上，也可能与主节点共享相同的主机。后者是AKS
    Engine的情况：它运行外部拓扑，但每个主节点都托管一个etcd成员作为本机Linux服务。对于这两种拓扑，您可以以相同的方式执行备份；唯一的区别在于如何执行恢复。在堆叠拓扑中，通常用于**kubeadm**部署，您需要在新节点上执行`kubeadm
    init`覆盖本地etcd存储。对于外部拓扑，您可以简单地使用`etcdctl`命令。
- en: An external topology for an etcd cluster has more components but is generally
    better at providing business continuity and disaster recovery.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: etcd集群的外部拓扑具有更多组件，但通常更好地提供业务连续性和灾难恢复。
- en: Additionally, if you are running an AKS Engine cluster, you may consider using
    an Azure Cosmos DB ([https://azure.microsoft.com/en-us/services/cosmos-db/](https://azure.microsoft.com/en-us/services/cosmos-db/))
    instead of a self-managed etcd cluster. Cosmos DB supports exposing an etcd API
    and can be used as the backing storage for Kubernetes in exactly the same way
    as a local etcd cluster. In this way, you receive global distribution, high availability,
    elastic scaling, and data reliability at the levels defined in the SLA. On top
    of that, you have automatic, online backups with georeplication. You can learn
    more about this feature and how to configure it in a cluster apimodel in the official
    documentation at [https://github.com/Azure/aks-engine/tree/master/examples/cosmos-etcd](https://github.com/Azure/aks-engine/tree/master/examples/cosmos-etcd).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您运行的是AKS Engine集群，您可以考虑使用Azure Cosmos DB（[https://azure.microsoft.com/en-us/services/cosmos-db/](https://azure.microsoft.com/en-us/services/cosmos-db/)）而不是自行管理的etcd集群。Cosmos
    DB支持暴露etcd API，并且可以像本地etcd集群一样用作Kubernetes的后备存储。这样，您可以获得全球分发、高可用性、弹性扩展和SLA中定义的数据可靠性。此外，您还可以获得具有地理复制的自动在线备份。您可以在官方文档的cluster
    apimodel中了解更多关于此功能以及如何配置它的信息，网址为[https://github.com/Azure/aks-engine/tree/master/examples/cosmos-etcd](https://github.com/Azure/aks-engine/tree/master/examples/cosmos-etcd)。
- en: Now, we will take a look at how you can back up your etcd cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看如何备份您的etcd集群。
- en: Backing up an etcd cluster
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 备份etcd集群
- en: 'The process of backing up an etcd cluster is straightforward, but there are
    multiple ways that you can approach this task:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 备份etcd集群的过程很简单，但有多种方法可以完成这项任务：
- en: Create a backup or snapshot of the storage disk for etcd. This is especially
    valid in cloud scenarios where you can easily manage backups outside your Kubernetes
    cluster.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建etcd存储磁盘的备份或快照。这在云场景中尤为重要，您可以轻松地在Kubernetes集群之外管理备份。
- en: 'Perform a manual snapshot of etcd using the `etcdctl` command. You need to
    manage the backup files yourself: upload them to external storage, and apply a
    retention policy.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`etcdctl`命令手动对etcd进行快照。您需要自行管理备份文件：将它们上传到外部存储，并应用保留策略。
- en: Use **Velero** (formerly Heptio Ark ([https://velero.io/](https://velero.io/))),
    which can perform snapshots, manage them in external storage, and restore them
    if needed. Additionally, it can be used to perform backups of persistent volumes
    using **Restic** integration ([https://velero.io/docs/master/restic/](https://velero.io/docs/master/restic/)).
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Velero**（原名Heptio Ark ([https://velero.io/](https://velero.io/)）），它可以执行快照，管理外部存储中的快照，并在需要时恢复它们。此外，它还可以使用**Restic**集成（[https://velero.io/docs/master/restic/](https://velero.io/docs/master/restic/)）来执行持久卷的备份。
- en: Use **etcd-operator** ([https://github.com/coreos/etcd-operator](https://github.com/coreos/etcd-operator))
    to provision etcd clusters on top of Kubernetes. You can easily manage etcd clusters
    and perform backup–restore operations. Use this approach if you plan to manage
    multiple Kubernetes clusters in your environment.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**etcd-operator**（[https://github.com/coreos/etcd-operator](https://github.com/coreos/etcd-operator)）在Kubernetes之上提供etcd集群。您可以轻松管理etcd集群并执行备份和恢复操作。如果您计划在环境中管理多个Kubernetes集群，可以使用这种方法。
- en: We are going to demonstrate the second option, performing manual snapshots of
    etcd—it is generally good to know what exactly is happening under the hood before
    switching to advanced automations, such as Velero. For this task, you will need
    a multimaster Kubernetes cluster; you can create one using AKS Engine. As in the
    previous chapters, you can use a ready apimodel definition from the Github repository
    at [https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/blob/master/Chapter15/01_multimaster-aks-engine/kubernetes-windows-template.json](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/blob/master/Chapter15/01_multimaster-aks-engine/kubernetes-windows-template.json)
    and deploy it using our usual PowerShell script ([https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/blob/master/Chapter15/01_multimaster-aks-engine/CreateAKSEngineClusterWithWindowsNodes.ps1](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/blob/master/Chapter15/01_multimaster-aks-engine/CreateAKSEngineClusterWithWindowsNodes.ps1)).
    This definition will deploy three master nodes together with one Linux worker
    node and one Windows node.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将演示第二个选项，即手动快照etcd——在切换到高级自动化（如Velero）之前，了解底层发生了什么通常是很重要的。为此任务，您将需要一个多主Kubernetes集群；您可以使用AKS
    Engine创建一个。与之前的章节一样，您可以使用Github存储库中的准备好的apimodel定义[https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/blob/master/Chapter15/01_multimaster-aks-engine/kubernetes-windows-template.json]，并使用我们通常的PowerShell脚本[https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/blob/master/Chapter15/01_multimaster-aks-engine/CreateAKSEngineClusterWithWindowsNodes.ps1]部署它。此定义将部署三个主节点以及一个Linux工作节点和一个Windows节点。
- en: Please make sure that you check the estimated costs of hosting a five-node Kubernetes
    cluster on Azure. The price will depend on the region in which you deploy it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您检查在Azure上托管五节点Kubernetes集群的预估成本。价格将取决于您部署的区域。
- en: 'When your cluster is ready, deploy an application workload—for example, the
    Voting application from the previous chapters. Then, go through the following
    steps to create an etcd snapshot:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的集群准备好后，部署一个应用工作负载，例如，之前章节中的投票应用。然后，按照以下步骤创建etcd快照：
- en: 'Open the PowerShell window and SSH into one of the master nodes using the following
    command:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开PowerShell窗口，并使用以下命令SSH到其中一个主节点：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Inspect your Kubernetes cluster configuration. Use the `kubectl cluster-info
    dump` command to learn more about the etcd setup. You will see that each master
    node is running its own  local instance (but external to the cluster) of etcd,
    which is passed to the Kubernetes API server as arguments:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查您的Kubernetes集群配置。使用`kubectl cluster-info dump`命令了解更多关于etcd设置的信息。您将看到每个主节点都在运行其自己的本地实例（但是外部到集群）的etcd，并将其作为参数传递给Kubernetes
    API服务器：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Use the `etcdctl` command to get the topology of the etcd cluster, which has
    members on the master nodes:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`etcdctl`命令获取etcd集群的拓扑结构，该集群在主节点上有成员：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can check that these are the private IP addresses of the master nodes in
    the Azure Portal.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Azure门户中检查这些是否是主节点的私有IP地址。
- en: 'Execute the following commands in order to create a snapshot of etcd:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按顺序执行以下命令以创建etcd的快照：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After a short while, the backup should be finished. You can inspect the status
    of the backup using the following command:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 备份应该在短时间内完成。您可以使用以下命令检查备份的状态：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Additionally, you should back up the certificates and keys used to access the
    etcd cluster. In our scenario, it will not be needed because we are going to restore
    the same master machines. For a general disaster-recovery scenario, you will need
    them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，您应该备份用于访问etcd集群的证书和密钥。在我们的情况下，这是不需要的，因为我们将恢复相同的主节点机器。但是在一般的灾难恢复场景中，您将需要它们。
- en: 'With the backup ready, let''s see how to upload the file to Azure blob storage.
    Please note that you *shouldn''t* perform these operations directly on production
    masters, especially when installing Azure CLI *the quick way*. We are demonstrating
    this in order to later create a Kubernetes **CronJob**, which will run a Docker
    container to execute these operations. Please go through the following steps for
    your development cluster:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 备份准备就绪，让我们看看如何将文件上传到Azure blob存储。请注意，*不应*直接在生产主节点上执行这些操作，特别是在*快速*安装Azure CLI时。我们演示这一点是为了之后创建一个Kubernetes
    **CronJob**，它将运行一个Docker容器来执行这些操作。请按照以下步骤操作您的开发集群。
- en: Open a PowerShell window on your local machine and log in to Azure using the `az
    login` command.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地计算机上打开一个PowerShell窗口，并使用`az login`命令登录到Azure。
- en: 'Create a service principal that we will use to upload the backup to the Azure
    blob storage container:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个服务主体，我们将用它来上传备份到Azure blob存储容器：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Copy `appId`, `password`, and `tenant` for further use.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 复制`appId`、`password`和`tenant`以供进一步使用。
- en: 'Execute the following command to create a dedicated `aksenginebackups` storage
    account to handle backups. Choose the same Azure location as your AKS Engine cluster:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令创建一个专用的`aksenginebackups`存储账户来处理备份。选择与您的AKS Engine集群相同的Azure位置：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'List the account keys for the new account and copy the value of `key1` for
    further use:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出新账户的账户密钥，并复制`key1`的值以供进一步使用：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Continue using the SSH session from the previous paragraph for your development
    AKS Engine cluster master node. Execute the following command to install the Azure
    CLI:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续使用上一段的SSH会话来操作您的开发AKS Engine集群主节点。执行以下命令安装Azure CLI：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Log in to Azure using the `appId`, `password`, and `tenant` for your service
    principal:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用服务主体的`appId`、`password`和`tenant`登录到Azure：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create a new container for backups of our AKS Engine cluster. You can use any
    name—for example, the DNS prefix for the cluster:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为我们的AKS Engine集群创建一个新的备份容器。您可以使用任何名称，例如集群的DNS前缀：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Create a blob with the backup that we created in the previous paragraph:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含我们在上一段中创建的备份的blob：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Remove the backup file from the local disk:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从本地磁盘中删除备份文件：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: For the creation of the service principal and storage account, we have provided
    a PowerShell script in the GitHub repository at [https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/blob/master/Chapter15/02_CreateBlobContainerForBackups.ps1](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/blob/master/Chapter15/02_CreateBlobContainerForBackups.ps1).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建服务主体和存储账户，我们在GitHub仓库中提供了一个PowerShell脚本，网址为[https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/blob/master/Chapter15/02_CreateBlobContainerForBackups.ps1](https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Windows/blob/master/Chapter15/02_CreateBlobContainerForBackups.ps1)。
- en: You have successfully created an etcd snapshot and uploaded it to the Azure
    blob storage. Now, we will demonstrate how you can restore the backup that we
    have just created.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您已成功创建了etcd快照并将其上传到Azure blob存储。现在，我们将演示如何恢复我们刚刚创建的备份。
- en: Restoring the etcd cluster backup
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恢复etcd集群备份
- en: To demonstrate the restore scenario of etcd for an existing AKS Engine cluster,
    we first need to modify some Kubernetes objects to later prove that the backup
    restore has worked. Please note that all the commands shown in this section assume
    that you are running AKS Engine where an external etcd topology is used with etcd
    members running on the same machines that host Kubernetes master components. For
    other clusters, such as an on-premise kubeadm setup, the structure of the directories
    will be different.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示对现有AKS Engine集群进行etcd恢复的场景，我们首先需要修改一些Kubernetes对象，以后证明备份恢复已经生效。请注意，本节中显示的所有命令都假定您正在运行使用外部etcd拓扑的AKS
    Engine，etcd成员在托管Kubernetes主控组件的相同机器上运行。对于其他集群，比如本地kubeadm设置，目录的结构将会有所不同。
- en: 'First, let''s introduce some changes to the cluster state. For example, if
    you have our Voting application running, delete the associated `Deployment` object
    using the following command:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们介绍一些对集群状态的更改。例如，如果您的投票应用程序正在运行，请使用以下命令删除相关的`Deployment`对象：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: After a while, all the pods will be terminated—let's assume that this was our
    **disaster event** that has left the cluster unuseable. We are going to restore
    the backup named `kubernetes-etcd-snapshot_20191208_182555.db`, which we created
    earlier and uploaded to Azure blob storage!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 过一段时间，所有的pod都将被终止——假设这是我们的**灾难事件**，导致集群无法使用。我们将要恢复之前创建并上传到Azure Blob存储的名为`kubernetes-etcd-snapshot_20191208_182555.db`的备份！
- en: If you have deleted the SQL Server Deployment together with the PVCs, then the
    restore will not be fully successful. As we mentioned in previous sections, for
    PVs you need to have a separate backup strategy that is coordinated with etcd
    backups. Then you can restore both the etcd snapshot and associated PV snapshots.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经删除了SQL Server Deployment以及PVCs，那么恢复将不会完全成功。正如我们在前面的章节中提到的，对于PVs，您需要有一个与etcd备份协调的单独的备份策略。然后您可以同时恢复etcd快照和相关的PV快照。
- en: 'To perform the restore, you will need to connect to all three Kubernetes nodes
    at once. This operation can be performed sequentially, but stopping and starting
    the etcd service on the hosts must be performed simultaneously. Please go through
    the following steps:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行恢复操作，您需要同时连接到所有三个Kubernetes节点。这个操作可以按顺序执行，但是主机上停止和启动etcd服务必须同时进行。请按照以下步骤进行：
- en: Open three PowerShell windows (try to have them all open and visible at the
    same time to make issuing commands easier). Each window will be used for a separate
    Kubernetes master.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开三个PowerShell窗口（尽量让它们同时打开并可见，以便更容易地发出命令）。每个窗口将用于一个单独的Kubernetes主控。
- en: In the Azure Portal, find the private IPs of the master nodes. You can also
    do this using the Azure CLI. They should follow the convention that master `0`
    is `10.255.255.5`, master `1` is `10.255.255.6`, and master `2` is `10.255.255.7`.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Azure门户中，找到主控节点的私有IP。您也可以使用Azure CLI来完成这个操作。它们应该遵循这样的约定，即主控`0`是`10.255.255.5`，主控`1`是`10.255.255.6`，主控`2`是`10.255.255.7`。
- en: 'In the first PowerShell window, execute the following command to connect to
    one of the master nodes (behind the Azure load balancer) and additionally use
    port forwarding from your local port `5500` to the SSH port for master `0`, port `5501` to
    the SSH port for master `1`, and port `5502` to the SSH port for master `2`:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个PowerShell窗口中，执行以下命令连接到一个主控节点（在Azure负载均衡器后面），并额外使用端口转发，将本地端口`5500`转发到主控`0`的SSH端口，端口`5501`转发到主控`1`的SSH端口，端口`5502`转发到主控`2`的SSH端口：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In this way, you can connect to any Kubernetes master you want from your local
    machine. Check to see which master node you have already connected to and create
    SSH connections to the *other two* nodes in the remaining PowerShell windows—for
    example:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过这种方式，您可以从本地机器连接到任何您想要的Kubernetes主节点。检查您已经连接到哪个主节点，并在剩余的PowerShell窗口中创建SSH连接到*其他两个*节点，例如：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, you have a set of PowerShell windows where you can manage each master
    node separately. The first step is installing the Azure CLI. Execute the following
    command *on all masters*:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您有一组PowerShell窗口，可以分别管理每个主节点。第一步是安装Azure CLI。在*所有主节点*上执行以下命令：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Log in to Azure using the `appId`, `password`, and `tenant` for your service
    principal, as you did previously. Execute the following command *on all masters*:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用服务主体的`appId`、`password`和`tenant`登录到Azure，就像之前一样。在*所有主节点*上执行以下命令：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Download the `kubernetes-etcd-snapshot_20191208_182555.db` snapshot file. Execute
    the following command on all master nodes :'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载`kubernetes-etcd-snapshot_20191208_182555.db`快照文件。在所有主节点上执行以下命令：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'All etcd members must be restored from the same snapshot file. This means that
    you have to perform a similar operation on all nodes, just with different parameters.
    On each master, determine the startup parameters for the etcd service (for AKS
    Engine, it is running as a systemd service). Execute the following command to
    get the parameters for each master:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有etcd成员必须从相同的快照文件中恢复。这意味着您必须在所有节点上执行类似的操作，只是使用不同的参数。在每个主节点上，确定etcd服务的启动参数（对于AKS
    Engine，它作为systemd服务运行）。执行以下命令以获取每个主节点的参数：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You need to capture `--name`, `--initial-cluster`, `--initial-cluster-token`,
    and `--initial-advertise-peer-urls` for each node. More precisely, `--initial-cluster` and `--initial-cluster-token`
    will be the same for all masters. We will use these values to initialize a *new*
    etcd member on each master—for example, for master `0` in our cluster, these parameters
    are as follows:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要捕获每个节点的`--name`、`--initial-cluster`、`--initial-cluster-token`和`--initial-advertise-peer-urls`。更确切地说，`--initial-cluster`和`--initial-cluster-token`对于所有主节点都是相同的。我们将使用这些值在每个主节点上初始化一个*新*的etcd成员，例如，在我们的集群中，对于主节点`0`，这些参数如下：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can proceed with restoring the data for each etcd cluster member. This restore
    operation only creates a new data directory. The original data directory that
    the cluster is currently using is `/var/lib/etcddisk` (which is mounted from a
    cloud volume). We are going to restore the data to `/var/lib/etcdisk-restored` and
    later swap the contents. Using the parameters from the previous step, execute
    this command using matching parameters for each master:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以继续为每个etcd集群成员恢复数据。此恢复操作仅创建一个新的数据目录。集群当前正在使用的原始数据目录是`/var/lib/etcddisk`（它是从云卷挂载的）。我们将把数据恢复到`/var/lib/etcdisk-restored`，然后交换内容。使用上一步的参数，使用匹配的参数为每个主节点执行此命令：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The snapshot data is ready to be used in a new etcd cluster. But first, we need
    to gracefully stop the existing Kubernetes master components; otherwise, you will
    arrive at an inconsistent state after the restore.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 快照数据已准备好用于新的etcd集群。但首先，我们需要优雅地停止现有的Kubernetes主组件；否则，在恢复后，您将处于不一致的状态。
- en: 'Kubelet observes the `/etc/kubernetes/manifests` directory where manifest files
    for master components are stored. Any change to these manifests is applied by
    kubelet to the cluster; this is how the Kubernetes master is bootstrapped when
    there is no Kubernetes API server yet. To stop the master components, including
    the Kubernetes API server, simply move the manifest files to a different directory
    and execute the command on all masters:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubelet观察`/etc/kubernetes/manifests`目录，其中存储了主组件的清单文件。对这些清单的任何更改都将由kubelet应用于集群；这就是在没有Kubernetes
    API服务器的情况下引导Kubernetes主节点的方式。要停止主组件，包括Kubernetes API服务器，只需将清单文件移动到另一个目录并在所有主节点上执行以下命令：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: After a few seconds, you will see that the Docker containers for master components
    are being stopped (use the `docker ps` command to see this).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，您将看到主组件的Docker容器正在停止（使用`docker ps`命令来查看）。
- en: 'Now, stop the etcd service on all masters:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在所有主节点上停止etcd服务：
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Stop the kubelet service on all masters:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有主节点上停止kubelet服务：
- en: '[PRE24]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The final step in preparing the masters for the restore is to remove all other
    Docker containers that run on the masters, but that are not started using the `/etc/kubernetes/manifests`
    directory. Execute the following command on all masters:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备好恢复的主节点的最后一步是删除在主节点上运行但未使用`/etc/kubernetes/manifests`目录启动的所有其他Docker容器。在所有主节点上执行以下命令：
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Perform the actual data directory restore for etcd members. Execute the following
    commands on all masters.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对etcd成员执行实际的数据目录恢复。在所有主节点上执行以下命令。
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can now start bootstrapping the cluster with the restored snapshot. The
    first step is starting the etcd cluster. Execute the following command on all
    master nodes:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以开始使用恢复的快照引导集群。第一步是启动etcd集群。在所有主节点上执行以下命令：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You can verify the health of your etcd cluster using the `sudo -E etcdctl cluster-health`
    command.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`sudo -E etcdctl cluster-health`命令验证etcd集群的健康状况。
- en: 'Move the stopped manifest files back to their original location on all masters.
    They will be picked up by kubelet once it starts:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将停止的清单文件移回到所有主节点的原始位置。一旦kubelet启动，它们将被kubelet捡起：
- en: '[PRE28]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, perform the last step: start the kubelet service on all masters:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，执行最后一步：在所有主节点上启动kubelet服务：
- en: '[PRE29]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You can quickly verify that the containers for the master components are being
    started using the `docker ps` command.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`docker ps`命令快速验证主组件的容器是否正在启动。
- en: 'You can check in a new PowerShell window whether the cluster is already working
    hard to reconcile the restored state:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以在新的PowerShell窗口中检查集群是否已经努力协调恢复的状态：
- en: '[PRE30]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Our Voting application Deployment is recreating. That is *great news*: the snapshot
    restore has been *successful*. After a few minutes, all the pods will be ready
    and you can navigate to the external IP in the web browser and enjoy the application
    again.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的投票应用程序部署正在重新创建。这是一个*好消息*：快照恢复已经*成功*。几分钟后，所有的pod都将准备就绪，您可以在Web浏览器中导航到外部IP并再次享受应用程序。
- en: The **Openshift** distribution of Kubernetes implements a native etcd snapshot
    restore functionality. You can see the details in the scripts in the repository
    at [https://github.com/openshift/machine-config-operator/blob/master/templates/master/00-master/_base/files/usr-local-bin-etcd-snapshot-restore-sh.yaml](https://github.com/openshift/machine-config-operator/blob/master/templates/master/00-master/_base/files/usr-local-bin-etcd-snapshot-restore-sh.yaml).
    The steps there are roughly similar to what we have done in this section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的**Openshift**发行版实现了本地etcd快照恢复功能。您可以在存储库中的脚本中查看详细信息[https://github.com/openshift/machine-config-operator/blob/master/templates/master/00-master/_base/files/usr-local-bin-etcd-snapshot-restore-sh.yaml](https://github.com/openshift/machine-config-operator/blob/master/templates/master/00-master/_base/files/usr-local-bin-etcd-snapshot-restore-sh.yaml)。那里的步骤大致类似于我们在本节中所做的。
- en: As you can see, the manual restore scenario is a bit complicated and can be
    prone to error. In production scenarios, you should use this method when everything
    else fails; generally, it is better to use automated backup controllers, such
    as Velero ([https://velero.io/](https://velero.io/)).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，手动恢复场景有点复杂，并且容易出错。在生产场景中，当其他方法失败时，您应该使用此方法；通常最好使用自动化备份控制器，例如Velero（[https://velero.io/](https://velero.io/)）。
- en: In the next section, you will learn how you can automate the backup procedure
    on AKS Engine using Kubernetes CronJobs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习如何使用Kubernetes CronJobs在AKS Engine上自动化备份过程。
- en: Automating backup
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化备份
- en: In this section, we will demonstrate how to automate the backup procedure for
    an etcd cluster using Kubernetes CronJob. For this, we will need a Dockerfile
    that has `etcdctl` and Azure CLI installed for the image in order to create the
    snapshot and upload it to a selected Azure blob container—exactly as we demonstrated
    in manual steps. All the configuration and service principal secrets will be injected
    using environment variables that can be set using Kubernetes secret.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用Kubernetes CronJob自动化etcd集群的备份过程。为此，我们需要一个Dockerfile，其中安装了`etcdctl`和Azure
    CLI，以便为图像创建快照并将其上传到选定的Azure blob容器，就像我们在手动步骤中演示的那样。所有配置和服务主体密码将使用环境变量注入，可以使用Kubernetes
    secret设置。
- en: 'To create the Docker image for the etcd snapshot worker, go through the following
    steps:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要为etcd快照工作程序创建Docker镜像，请按照以下步骤进行：
- en: Use a Linux machine or switch to the Linux containers in Docker Desktop for
    Windows.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Linux机器或切换到Windows的Docker桌面中的Linux容器。
- en: Open a new PowerShell window.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的PowerShell窗口。
- en: Create a new directory for your source code and navigate there.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为您的源代码创建一个新目录并导航到该目录。
- en: 'Create a `Dockerfile` file with the following contents:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`Dockerfile`的文件，内容如下：
- en: '[PRE31]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This Dockerfile is based on the Ubuntu 18.04 Docker image and installs the `etcdctl`
    command from the official release of etcd. Additionally, we install the Azure
    CLI and set the `ENTRYPOINT` to a custom shell script that will perform the snapshot
    operation when the container is started.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此Dockerfile基于Ubuntu 18.04 Docker镜像，并从etcd的官方发布中安装`etcdctl`命令。此外，我们安装了Azure CLI，并将`ENTRYPOINT`设置为一个自定义shell脚本，该脚本在容器启动时执行快照操作。
- en: Now, create a `docker-entrypoint.sh` file with the following contents.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，创建一个名为`docker-entrypoint.sh`的文件，内容如下。
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The preceding script automates the steps that we have provided in the previous
    sections. The idea here is that all configurations and credentials that are injected
    using environment variables, certificates, and keys for accessing an etcd cluster
    must be mounted as a host volume to the specified location: `/etc/kubernetes/certs/`.
    For AKS Engine masters, this mapping will be one-to-one.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 上述脚本自动化了我们在前几节中提供的步骤。这里的想法是，使用环境变量、证书和密钥注入的所有配置和凭据，用于访问etcd集群的主机卷必须挂载到指定位置：`/etc/kubernetes/certs/`。对于AKS
    Engine主节点，此映射将是一对一的。
- en: 'Build the image using a tag containing your Docker ID—we will use `packtpubkubernetesonwindows/aks-engine-etcd-snapshot-azure-blob-job`:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用包含您的Docker ID的标签构建图像 - 我们将使用`packtpubkubernetesonwindows/aks-engine-etcd-snapshot-azure-blob-job`：
- en: '[PRE33]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Tag the image with version `1.0.0` and push the image, along with all of the
    tags, to Docker Hub:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用版本`1.0.0`标记图像并将图像与所有标记一起推送到Docker Hub：
- en: '[PRE34]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can optionally test your Docker image by running it directly on the AKS
    Engine master node in a development environment. SSH to the node and execute the
    following command:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以选择在开发环境中直接在AKS Engine主节点上运行Docker镜像进行测试。SSH到节点并执行以下命令：
- en: '[PRE35]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: After a short while, the job will end and the snapshot will be uploaded to the
    container that we created earlier.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 过一段时间后，作业将结束，并且快照将上传到我们之前创建的容器中。
- en: 'With the Docker image ready, we can create a dedicated Kubernetes **CronJob**
    to run this operation periodically. Please note that we are providing a minimal
    setup for this job; you should consider using a dedicated service account and
    set up RBAC in a production environment. Using a Helm chart to efficiently manage
    this job is also recommended. To create the CronJob, please go through the following
    steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有了Docker镜像准备好后，我们可以创建一个专用的Kubernetes **CronJob**来定期运行此操作。请注意，我们为此工作提供了一个最小的设置；您应该考虑在生产环境中使用专用的服务帐户并设置RBAC。还建议使用Helm图表有效地管理此作业。要创建CronJob，请按照以下步骤进行：
- en: 'Define a *local* file, `etcd-snapshot-secrets.txt`, that will be used to define
    the secret object for your CronJob:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个*本地*文件`etcd-snapshot-secrets.txt`，该文件将用于为您的CronJob定义秘密对象：
- en: '[PRE36]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create the `etcd-snapshot-azure-blob-job-secrets` secret object using the `etcd-snapshot-secrets.txt`
    file:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`etcd-snapshot-secrets.txt`文件创建`etcd-snapshot-azure-blob-job-secrets`秘密对象：
- en: '[PRE37]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, create the `etcd-snapshot-cronjob.yaml` manifest file for the CronJob
    itself:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为CronJob本身创建`etcd-snapshot-cronjob.yaml`清单文件：
- en: '[PRE38]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In this manifest file, the most important part is the part that defines an appropriate
    `schedule` **(1)**. We use a `0 */6 * * *` cron expression that will perform the
    snapshots *every 6 hours*. For testing purposes, you can set it to `* * * * *`
    in order to schedule the job *every minute*. Next, we need to ensure that the
    pod for the CronJob can be scheduled on the master node. We do this by using `tolerations`
    for taints and a `nodeSelector` **(2)**. The reason for this is that we need access
    to the etcd certificates and keys, which must be mounted from the master host
    filesystem. We define the pod to use the `packtpubkubernetesonwindows/aks-engine-etcd-snapshot-azure-blob-job:1.0.0`
    image that we have just created **(3)**. To populate the environment variables
    for the container, we use `secretRef` for our secret object, `etcd-snapshot-azure-blob-job-secrets`
    **(4)**. Lastly, we need to mount the *host* directory, `/etc/kubernetes/certs`,
    to the pod container so that the worker can access the certificates and keys **(5)**.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个清单文件中，最重要的部分是定义适当的`schedule` **(1)** 的部分。我们使用了`0 */6 * * *` cron表达式，这将每6小时执行一次快照。为了测试目的，您可以将其设置为`*
    * * * *`，以便将作业安排在*每分钟*执行一次。接下来，我们需要确保CronJob的pod可以在主节点上调度。我们通过使用`taints`的`tolerations`和`nodeSelector`来实现这一点
    **(2)**。这是因为我们需要访问etcd证书和密钥，这些必须从主机文件系统中挂载。我们定义pod使用我们刚刚创建的`packtpubkubernetesonwindows/aks-engine-etcd-snapshot-azure-blob-job:1.0.0`镜像
    **(3)**。为了为容器填充环境变量，我们使用我们的秘密对象`etcd-snapshot-azure-blob-job-secrets`的`secretRef`
    **(4)**。最后，我们需要将*主机*目录`/etc/kubernetes/certs`挂载到pod容器中，以便工作节点可以访问证书和密钥 **(5)**。
- en: Apply the manifest file using the `kubectl apply -f .\etcd-snapshot-cronjob.yaml`
    command.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kubectl apply -f .\etcd-snapshot-cronjob.yaml`命令应用清单文件。
- en: 'Wait for the first job execution:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待第一次作业执行：
- en: '[PRE39]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'When the job is finished, you can check the logs for the associated pod and
    also verify in Azure Portal ([https://portal.azure.com/](https://portal.azure.com/))
    that the snapshots are uploaded to your Azure blob container:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当作业完成时，您可以检查相关pod的日志，并在Azure Portal ([https://portal.azure.com/](https://portal.azure.com/))中验证快照是否已上传到您的Azure
    blob容器：
- en: '![](assets/88207fd0-d5f1-4773-8ce3-bb97fa40063c.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/88207fd0-d5f1-4773-8ce3-bb97fa40063c.png)'
- en: When working with multiple etcd clusters (for multiple Kubernetes Deployments)
    you can achieve a similar result using the *etcd-operator* ([https://github.com/coreos/etcd-operator](https://github.com/coreos/etcd-operator)).
    For a small cluster, such as the one in this demonstration, it doesn't make sense
    to use such a complex solution.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用多个etcd集群（用于多个Kubernetes部署）时，您可以使用*etcd-operator* ([https://github.com/coreos/etcd-operator](https://github.com/coreos/etcd-operator))
    来实现类似的结果。对于像本演示中的小集群，使用这样一个复杂的解决方案是没有意义的。
- en: Congratulations! You have successfully set up an automated CronJob for creating
    etcd cluster snapshots and automatically uploaded them to the Azure blob container.
    Now, we are going to demonstrate how you can replace a failed etcd member in order
    to restore the full operations of the etcd cluster.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功设置了一个自动化的CronJob，用于创建etcd集群快照并自动将它们上传到Azure blob容器。现在，我们将演示如何替换失败的etcd成员，以恢复etcd集群的全部操作。
- en: Replacing a failed etcd cluster member
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替换失败的etcd集群成员
- en: 'As a highly-available database, etcd tolerates minority failures, which means
    a partial failure where the majority of cluster members are still available and
    healthy; however, it is a good practice to replace the failed members as soon
    as possible in order to improve the overall cluster health and minimize the risk
    of majority failure. It is also highly recommended that you always keep the cluster
    size greater than two members in production. In order to recover from a minority
    failure, you need to perform two steps:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个高可用的数据库，etcd可以容忍少数故障，这意味着大多数集群成员仍然可用和健康；但是，最好尽快替换失败的成员，以改善整体集群健康状况并最小化多数故障的风险。在生产环境中，始终建议将集群大小保持在两个以上的成员。为了从少数故障中恢复，您需要执行两个步骤：
- en: Remove the failed member from the cluster.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从集群中删除失败的成员。
- en: Add a new replacement member. If there is more than one failed member, replace
    them sequentially.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个新的替换成员。如果有多个失败的成员，依次替换它们。
- en: The etcd documentation provides a list of use cases for runtime configuration
    changes, as you can see at [https://etcd.io/docs/v3.3.12/op-guide/runtime-configuration/](https://etcd.io/docs/v3.3.12/op-guide/runtime-configuration/).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: etcd文档提供了运行时配置更改的用例列表，您可以在[https://etcd.io/docs/v3.3.12/op-guide/runtime-configuration/](https://etcd.io/docs/v3.3.12/op-guide/runtime-configuration/)中查看。
- en: The way that you create a new member depends on what exactly failed. If it is
    a disk failure or data corruption on the machine that hosts the member, you may
    consider reusing the same host but with the data directory on a *different* disk.
    In the case of total host failure, you may need to provision a new machine and
    use it as a new replacement member. We will demonstrate a case in AKS Engine where
    we *reuse* the same host and create a member with a different data directory.
    This is a rather specific use case, but the overall procedure will be the same
    in all cases.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新成员的方式取决于失败的具体情况。如果是主机上的磁盘故障或数据损坏，您可以考虑重用相同的主机，但使用*不同*磁盘上的数据目录。在主机完全故障的情况下，您可能需要提供一个新的机器并将其用作新的替换成员。我们将演示在AKS
    Engine中*重用*相同主机并创建具有不同数据目录的成员的情况。这是一个非常特定的用例，但在所有情况下，整体流程都是相同的。
- en: 'First, let''s simulate the failure of an etcd cluster member. To do that, go
    through the following steps:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们模拟etcd集群成员的故障。为此，请按照以下步骤进行：
- en: 'Connect to one of the Kubernetes master nodes using SSH:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SSH连接到Kubernetes主节点之一：
- en: '[PRE40]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Let's assume that we connected to master `0` with the private IP `10.255.255.5`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们连接到具有私有IP`10.255.255.5`的主节点`0`。
- en: 'Verify the cluster health:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证集群健康状态：
- en: '[PRE41]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Stop the etcd service on master `0` using the following command. This will
    simulate our failure of a member in the cluster:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在主节点`0`上停止etcd服务。这将模拟集群中成员的故障：
- en: '[PRE42]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Check the cluster health again, but this time provide only the endpoints for
    master `1` and master `2`, which are functioning properly:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次检查集群健康状况，但这次只提供主节点`1`和主节点`2`的端点，它们正常运行：
- en: '[PRE43]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Take note of the failed member ID, which in our case is `b3a6773c0e93604`.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录失败成员的ID，在我们的案例中是`b3a6773c0e93604`。
- en: 'Now, let''s demonstrate how to replace the failed member. Please go through
    the following steps:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们演示如何替换失败的成员。请按照以下步骤进行：
- en: Determine the ID of the failed member. We already have that information from
    the previous commands, but in general you can use the `sudo etcdctl --endpoints=https://10.255.255.6:2379,https://10.255.255.7:2379
    member list` command.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定失败成员的ID。我们已经从之前的命令中获得了这些信息，但通常您可以使用`sudo etcdctl --endpoints=https://10.255.255.6:2379,https://10.255.255.7:2379
    member list`命令。
- en: SSH into the machine with the failed member.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过SSH登录到具有失败成员的机器。
- en: 'Remove the failed member from the cluster using its ID:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用其ID从集群中删除失败的成员：
- en: '[PRE44]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Add a new member to the cluster with the name `k8s-master-50659983-0-replace-0`;
    you can use any name, but in general it is good to follow a convention. In our
    case, the member will have the same IP address as before:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向集群添加一个名为`k8s-master-50659983-0-replace-0`的新成员；您可以使用任何名称，但通常最好遵循一致的约定。在我们的情况下，该成员将具有与以前相同的IP地址：
- en: '[PRE45]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now, you need to modify the etcd service startup parameters in order to reflect
    the change of the member on this machine. Open `/etc/default/etcd` as the root
    using a text editor—for example, `vim`.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您需要修改etcd服务启动参数，以反映此机器上成员的更改。使用文本编辑器（例如`vim`）以root身份打开`/etc/default/etcd`。
- en: Modify the `--name` parameter to `k8s-master-50659983-0-replace-0`.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`--name`参数修改为`k8s-master-50659983-0-replace-0`。
- en: Modify the `--initial-cluster` parameter to `k8s-master-50659983-2=https://10.255.255.7:2380,k8s-master-50659983-1=https://10.255.255.6:2380,k8s-master-50659983-0-replace-0=https://10.255.255.5:2380`.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`--initial-cluster`参数修改为`k8s-master-50659983-2=https://10.255.255.7:2380,k8s-master-50659983-1=https://10.255.255.6:2380,k8s-master-50659983-0-replace-0=https://10.255.255.5:2380`。
- en: Modify the `--initial-cluster-state` parameter to `existing`.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`--initial-cluster-state`参数修改为`existing`。
- en: Finally, modify the data directory parameter `--data-dir` to a different one—for
    example, `/var/lib/etcddisk-replace-0`.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，修改数据目录参数`--data-dir`为另一个目录，例如`/var/lib/etcddisk-replace-0`。
- en: Save the file.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存文件。
- en: 'Create the data directory, ensuring that it is owned by `etcd`:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据目录，确保其归`etcd`所有：
- en: '[PRE46]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Start the etcd service:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动etcd服务：
- en: '[PRE47]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'After a while, check the cluster health:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过一段时间后，检查集群健康状态：
- en: '[PRE48]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Success! The new member is `healthy` and the overall status of the cluster is
    also `healthy`!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！新成员是`健康的`，集群的整体状态也是`健康的`！
- en: If you need to use a new machine with a *different* IP address for your etcd
    replacement member, remember to change the `--etcd-servers` argument for the Kubernetes
    API server and, if you use a load balancer in front of etcd, don't forget to update
    the load balancer configuration.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要使用具有*不同*IP地址的新机器作为etcd替换成员，请记得为Kubernetes API服务器更改`--etcd-servers`参数，并且如果您在etcd前面使用负载均衡器，请不要忘记更新负载均衡器配置。
- en: Congratulations! You have successfully performed the replacement of a failed
    member in an etcd cluster. Even though the new member is hosted on the same virtual
    machine, it has a new ID (`1f5a8b7d5b2a5b68`) and is treated as a completely new
    member in the cluster.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功替换了etcd集群中的一个失败成员。即使新成员托管在同一台虚拟机上，它也具有一个新的ID（`1f5a8b7d5b2a5b68`），并且在集群中被视为一个全新的成员。
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have learned the key points you should bear in mind when
    preparing for Kubernetes DR. You have learned about stateful components in the
    whole Kubernetes cluster and the fact that they require a backup-and-restore strategy
    using etcd clusters and persistent volumes. Next, you learned how to manually
    perform a snapshot for your Kubernetes etcd cluster and upload it to an Azure
    blob container. Then, we used this snapshot to restore a Kubernetes cluster to
    a previous state and verified that the restore was successful. On top of this,
    you utilized all your new knowledge in order to create a Docker image for a snapshot
    worker that created a snapshot of etcd (for AKS Engine) and uploaded it to the
    Azure blob container. We used this Docker image to create a Kubernetes CronJob for
    performing backups, which is done every six hours. The last topic that we looked
    at was how to replace a failed etcd member in AKS Engine. With this knowledge,
    you should be able to create a reliable disaster-recovery plan for your Kubernetes
    cluster.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经了解了为准备Kubernetes DR时应牢记的关键要点。您已经了解了整个Kubernetes集群中的有状态组件，以及它们需要使用etcd集群和持久卷进行备份和恢复策略的事实。接下来，您将学习如何手动为Kubernetes
    etcd集群执行快照，并将其上传到Azure blob容器。然后，我们使用此快照将Kubernetes集群恢复到先前的状态，并验证了恢复成功。除此之外，您利用了所有新知识，以创建一个用于创建etcd快照（用于AKS
    Engine）并将其上传到Azure blob容器的Docker镜像的快照工作者。我们使用此Docker镜像创建了一个Kubernetes CronJob来执行备份，每6小时执行一次。我们最后讨论的主题是如何替换AKS
    Engine中失败的etcd成员。有了这些知识，您应该能够为Kubernetes集群创建可靠的灾难恢复计划。
- en: The last chapter of this book will focus on production considerations for running
    Kubernetes. You can treat this chapter as a set of loosely-coupled recommendations
    and best practices for different production scenarios.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的最后一章将重点讨论运行Kubernetes的生产考虑。您可以将本章视为一组松散耦合的建议和不同生产场景的最佳实践。
- en: Questions
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the difference between disaster recovery (DC) and business continuity
    (BC) and how are they related?
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 灾难恢复（DC）和业务连续性（BC）之间有什么区别，它们之间的关系是什么？
- en: Which components do you need to back up in Kubernetes to ensure the possibility
    of recovering the cluster state?
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kubernetes中需要备份哪些组件，以确保能够恢复集群状态的可能性？
- en: What is an etcd snapshot?
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是etcd快照？
- en: What are Velero and etcd-operators and what are their use cases?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Velero和etcd-operators是什么，它们的用例是什么？
- en: What are high-level steps for recovering an etcd snapshot?
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 恢复etcd快照的高级步骤是什么？
- en: What is a Kubernetes CronJob and how can you use it to automate your backup
    strategy for etcd clusters?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes CronJob是什么，您如何使用它来自动化etcd集群的备份策略？
- en: What are high-level steps for replacing a failed etcd cluster member?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 替换失败的etcd集群成员的高级步骤是什么？
- en: You can find answers to these questions in *Assessments* of this book.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的*评估*中找到这些问题的答案。
- en: Further reading
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information about Kubernetes features and disaster recovery in general,
    please refer to the following Packt books:'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关Kubernetes功能和灾难恢复的更多信息，请参考以下Packt图书：
- en: '*The Complete Kubernetes Guide* ([https://www.packtpub.com/virtualization-and-cloud/complete-kubernetes-guide](https://www.packtpub.com/virtualization-and-cloud/complete-kubernetes-guide)).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完整的Kubernetes指南* ([https://www.packtpub.com/virtualization-and-cloud/complete-kubernetes-guide](https://www.packtpub.com/virtualization-and-cloud/complete-kubernetes-guide))。'
- en: '*Getting Started with Kubernetes - Third Edition* ([https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition)).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开始使用Kubernetes-第三版* ([https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition](https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition))。'
- en: '*Kubernetes for Developers* ([https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers)).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*面向开发人员的Kubernetes* ([https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers)).'
- en: If you are interested in the details about etcd itself and how to handle disaster
    recovery, you can refer to the official documentation at [https://etcd.io/docs/v3.4.0/op-guide/recovery/](https://etcd.io/docs/v3.4.0/op-guide/recovery/).
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您对etcd本身的细节以及如何处理灾难恢复感兴趣，可以参考官方文档[https://etcd.io/docs/v3.4.0/op-guide/recovery/](https://etcd.io/docs/v3.4.0/op-guide/recovery/)。
- en: 'Additionally, we recommend watching the following excellent webinars from the
    **Cloud Native Computing Foundation** (**CNCF**) regarding Kubernetes backup strategies
    using Velero, and operating etcd in production:'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们建议观看**Cloud Native Computing Foundation** (**CNCF**)关于使用Velero的Kubernetes备份策略以及在生产环境中操作etcd的优秀网络研讨会：
- en: '[https://www.cncf.io/webinars/kubernetes-backup-and-migration-strategies-using-project-velero/](https://www.cncf.io/webinars/kubernetes-backup-and-migration-strategies-using-project-velero/)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cncf.io/webinars/kubernetes-backup-and-migration-strategies-using-project-velero/](https://www.cncf.io/webinars/kubernetes-backup-and-migration-strategies-using-project-velero/)'
- en: '[https://www.cncf.io/webinars/kubernetes-in-production-operating-etcd-with-etcdadm/](https://www.cncf.io/webinars/kubernetes-in-production-operating-etcd-with-etcdadm/)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cncf.io/webinars/kubernetes-in-production-operating-etcd-with-etcdadm/](https://www.cncf.io/webinars/kubernetes-in-production-operating-etcd-with-etcdadm/)'
