# 服务网格-与 Istio 合作

在本章中，我们将回顾服务网格的热门话题，特别是 Istio。这很令人兴奋，因为服务网格是一个真正的游戏规则改变者。它们将许多复杂的任务从服务中移除到独立的代理中。这是一个巨大的胜利，尤其是在多语言环境中，不同的服务用不同的编程语言实现，或者如果您需要将一些遗留应用程序迁移到您的集群中。

我们将在本章中讨论以下主题:

*   什么是服务网格
*   我带来了什么
*   美味可口
*   Istio 的替代品

# 技术要求

在本章中，我们将与 Istio 合作。我选择在这一章使用**谷歌 Kubernetes 引擎** ( **GKE** )是因为 Istio 可以作为一个附加组件在 GKE 启用，不需要你安装。这有以下两个好处:

*   它节省了安装时间
*   它证明了 Delinkcious 可以在云中运行，而不仅仅是在本地

要安装 Istio，您只需在 GKE 控制台中启用它，并选择 mTLS 模式，这是服务之间的相互身份验证。我选择了 permissive，这意味着默认情况下，集群内部的内部通信不加密，服务将接受加密和非加密连接。您可以根据服务覆盖它。对于生产集群，我建议使用严格的 mTLS 模式，其中所有连接都必须加密:

![](assets/d238fcb7-04c2-42cd-afcc-b988619fbac2.png)

Istio 安装在自己的`istio-system`命名空间中，如下所示:

```
$ kubectl -n istio-system get po

NAME READY STATUS RESTARTS AGE
istio-citadel-6995f7bd9-69qhw 1/1 Running 0 11h
istio-cleanup-secrets-6xkjx 0/1 Completed 0 11h
istio-egressgateway-57b96d87bd-8lld5 1/1 Running 0 11h
istio-galley-6d7dd498f6-pm8zz 1/1 Running 0 11h
istio-ingressgateway-ddd557db7-b4mqq 1/1 Running 0 11h
istio-pilot-5765d76b8c-l9n5n 2/2 Running 0 11h
istio-policy-5b47b88467-tfq4b 2/2 Running 0 11h
istio-sidecar-injector-6b9fbbfcf6-vv2pt 1/1 Running 0 11h
istio-telemetry-65dcd9ff85-dxrhf 2/2 Running 0 11h
promsd-7b49dcb96c-cn49l 2/2 Running 1 11h
```

# 代码

你可以在[https://github.com/the-gigi/delinkcious/releases/tag/v0.11](https://github.com/the-gigi/delinkcious/releases/tag/v0.11)找到更新的德令状应用。

# 什么是服务网格？

让我们从回顾微服务与单片相比面临的问题开始，看看服务网格是如何解决这些问题的，然后你就会明白为什么我对它们如此兴奋。当设计和编写 Delinkcious 时，应用程序代码相当简单。我们跟踪用户、他们的链接以及他们的追随者/追随者关系。我们也做一些链接检查，并在新闻服务中存储最近的链接。最后，我们通过一个应用编程接口公开所有这些功能。

# 比较单片和微服务

在一个整体中实现所有这些功能会非常容易。部署、监控和调试一个德令状的整体也非常简单。然而，随着 Delinkcious 在功能、用户和开发团队方面的发展，整体应用程序的缺点变得更加明显。这就是为什么我们开始了基于微服务方法的旅程。然而，在这一过程中，我们不得不编写大量的代码，安装大量的附加工具，并配置许多与 Delinkcious 应用程序本身无关的组件。我们明智地利用了 Kubernetes 和 Go 工具包，将所有这些额外的关注点从德林契斯域代码中清晰地分离出来，但这是一项艰巨的工作。

例如，如果安全性是高优先级，您可能希望在您的系统中对服务间呼叫进行身份验证和授权。我们在 Delinkcious 中通过在链接服务和社交图服务之间引入一个共同的秘密来做到这一点。我们必须配置一个秘密，确保它只能被这两个服务访问，并添加代码来验证每个调用都来自正确的服务。维护(例如，轮换秘密)并在许多服务中发展这一点并不是一件容易的事情。

这方面的另一个例子是分布式跟踪。在单块中，整个调用链可以被堆栈跟踪捕获。在 Delinkcious 中，您必须安装分布式跟踪服务，如 Jaeger，并修改代码以记录跨度。

整块中的集中日志记录是微不足道的，因为整块已经是一个单一的集中实体。

归根结底，微服务带来了很多好处，但它们更难管理和控制。

# 使用共享库管理微服务的交叉问题

最常见的方法之一是在一个或一组库中实现所有这些关注点。所有微服务都包含或依赖于共享库，该库负责所有这些交叉方面，例如配置、日志记录、秘密管理、跟踪、速率限制和容错。这在理论上听起来很棒；让服务处理应用程序域，让一个或多个共享库处理常见的问题。来自网飞的海斯特里克斯是负责管理延迟和容错的 Java 库的一个很好的例子。推特上的 Finagle 是 Scala 库(针对 JVM)的另一个好例子。许多组织使用这些库的集合，并经常编写自己的库。

然而，实际上，这种方法有严重的缺点。第一个问题是，作为一个编程语言库，它自然是用一种特定的语言实现的(例如，在海斯特里克斯的例子中是 Java)。您的系统可能有多种语言的微服务(甚至德林契斯也有 Go 和 Python 服务)。用不同的编程语言实现微服务是最大的好处之一。共享库极大地阻碍了这一方面。这是因为您最终会得到几个不吸引人的选项，如下所示:

*   您将所有的微服务限制为单一的编程语言。
*   您为您使用的每种行为相同的编程语言维护跨语言共享库。
*   您接受不同的服务将与您的集中式服务进行不同的交互(例如，不同的日志格式或缺少跟踪)。

所有这些选择都很糟糕。但这还不是结束；假设您已经选择了前面选项的组合。这很可能包括大量的定制代码，因为没有现成的库能为您提供所需的一切。现在，您想要更新您的共享代码库。由于它由您的所有或大部分服务共享，这意味着您必须对所有服务进行全面升级。然而，很可能你不能关闭你的系统，一次升级所有的服务。

相反，您必须以滚动更新的形式来完成。即使蓝绿色部署也不能跨多个服务即时完成。问题是，共享代码通常与您如何管理服务之间的相互秘密或身份验证有关。例如，如果服务 A 升级到共享库的新版本，而服务 B 仍在以前的版本上，它们可能无法通信。这会导致停机，这可能会级联并影响许多服务。您可以找到一种以向后兼容的方式引入更改的方法，但这更困难，也更容易出错。

好吧，所有服务的共享库都很有用，但是很难管理。让我们来看看服务网格如何提供帮助。

# 使用服务网格来管理微服务的交叉问题

服务网格是一组智能代理和附加控制基础架构组件。代理部署在集群中的每个节点上。代理拦截服务之间的所有通信，并可以代表您做许多以前必须由服务(或服务使用的共享库)完成的工作。服务网格的一些职责如下:

*   通过重试和自动故障转移在服务之间可靠地传递请求
*   延迟感知负载平衡
*   基于灵活动态的路由规则路由请求(这也称为流量整形)
*   突破最后期限
*   服务对服务的身份验证和授权
*   报告度量和对分布式跟踪的支持

所有这些功能对于许多大规模云原生应用程序都很重要。从服务中卸载它们是一个巨大的胜利。智能流量整形等功能需要在没有服务网格的情况下构建专用且可靠的服务。

下图说明了服务网格是如何嵌入到 Kubernetes 集群中的:

![](assets/3c4edd51-3de0-4ed9-bedf-806b3153a883.png)

服务网格听起来确实是革命性的。让我们来看看它们是如何融入 Kubernetes 的。

# 了解 Kubernetes 和服务网格之间的关系

乍一看，服务网格听起来与 Kubernetes 本身非常相似。Kubernetes 将 kubelet 和 kube 代理部署到每个节点中，服务网格部署自己的代理。Kubernetes 有一个 kubelet/kube 代理与之交互的控制平面，服务网格有自己的控制平面，网格代理与之交互。

我喜欢将服务网格视为 Kubernetes 的补充。Kubernetes 主要负责调度 pods，并为其提供平面网络模型和服务发现，因此不同的 pods 和服务可以相互通信。这就是服务网格接管并以更细粒度的方式管理这种服务到服务的通信的地方。围绕负载平衡和网络策略的职责有一层薄薄的重叠，但总的来说，服务网格是对 Kubernetes 的一个很好的补充。

同样重要的是，要意识到这两项令人惊叹的技术并不相互依赖。显然，您可以在没有服务网格的情况下运行 Kubernetes 集群。此外，许多服务网格可以与其他非 Kubernetes 平台一起工作，例如 Mesos、Nomad、Cloud Foundry 和基于 Consul 的部署。

现在我们已经了解了什么是服务网格，让我们看一个具体的例子。

# Istio 给餐桌带来了什么？

Istio 是一个服务网格，最初由谷歌、IBM 和 Lyft 开发。它于 2017 年年中推出，像火箭一样起飞。它带来了一个具有控制和数据平面的连贯模型，围绕特使代理构建，势头强劲，并且已经成为其他项目的基础。当然是开源的，也是**云原生计算基金会** ( **CNCF** )项目。在 Kubernetes 中，每个特使代理作为一个边车容器被注入到参与网格的每个吊舱中。

让我们探索 Istio 架构，然后深入了解它提供的服务。

# 了解 Istio 架构

Istio 是一个提供大量功能的大型框架，它有多个部分，这些部分可以相互交互，也可以与 Kubernetes 组件交互(大多是间接的，不引人注目的)。它分为控制平面和数据平面。数据平面是一组代理(每个 pod 一个)。他们的控制平面是一组负责配置代理和收集遥测数据的组件。

下图说明了 Istio 的不同部分，它们是如何相互关联的，以及它们之间交换了哪些信息:

![](assets/bcf2a6b9-cc37-4580-9cb4-28ad191e5c04.png)

让我们更深入地了解每个组件，从特使代理开始。

# 使者

特使是一个用 C++实现的高性能代理。它是由 Lyft 开发的，用作 Istio 的数据平面，但它也是一个独立的 CNCF 项目，可以单独使用。对于服务网格中的每一个吊舱，Istio(自动或通过`istioctl` CLI)注入一个特使侧容器，该容器进行重举:

*   吊舱之间的代理 HTTP、HTTP/2 和 gRPC 流量
*   复杂的负载平衡
*   mTLS 终端
*   HTTP/2 和 gRPC 代理
*   提供健康服务
*   不健康服务的断路
*   基于百分比的流量整形
*   注入测试故障
*   详细指标

特使代理控制到其吊舱的所有传入和传出通信。到目前为止，它是 Istio 最重要的组成部分。特使的配置不是微不足道的，这是 Istio 控制平面处理的很大一部分。

下一个组件是 Pilot。

# 飞行员

Pilot 负责平台无关的服务发现、动态负载平衡和路由。它将高级路由规则和弹性从自己的规则应用编程接口转换为特使配置。这个抽象层允许 Istio 在多个编排平台上运行。Pilot 获取所有特定于平台的信息，将其转换为特使数据平面配置格式，并使用特使数据平面 API 将其传播到每个特使代理。飞行员是无状态的；在 Kubernetes 中，所有配置都存储为 etcd 上的**自定义资源定义** ( **CRDs** )。

# 搅拌器

混合器负责抽象度量集合和策略。这些方面通常通过直接访问特定后端的 API 在服务中实现。这样做的好处是减轻了服务开发人员的负担，并将控制权交给了配置 Istio 的运营商。它还允许您在不改变代码的情况下轻松切换后端。混音器可以使用的后端类型包括以下几种:

*   记录
*   批准
*   配额
*   遥感勘测
*   演员表

特使代理和混合器之间的交互很简单——在每个请求之前，代理调用混合器进行前提条件检查，这可能会导致请求被拒绝；在每个请求之后，代理向 Mixer 报告度量。Mixer 有一个适配器 API，便于对任意基础设施后端进行扩展。这是它设计的主要部分。

# 城堡

Citadel 负责 Istio 中的证书和密钥管理。它与各种平台集成，并与它们的身份机制保持一致。例如，在 Kubernetes 中，它使用服务帐户；在 AWS 上，它使用 AWS IAM 在 GCP/GKE，它可以使用 GCP IAM。Istio 公钥基础设施基于 Citadel。它使用 SPIFEE 格式的 X.509 证书作为服务身份的载体。

以下是 Kubernetes 的工作流程:

*   Citadel 为现有服务帐户创建证书和密钥对。
*   Citadel 监视 Kubernetes 应用编程接口服务器上的新服务帐户，为其提供证书和密钥对。
*   Citadel 将证书和密钥存储为 Kubernetes 机密。
*   Kubernetes 将秘密装载到与服务帐户相关联的每个新 pod 中(这是标准的 Kubernetes 实践)。
*   当证书过期时，城堡会自动轮换库本内特斯的秘密。
*   Pilot 生成安全的命名信息，将服务帐户与 Istio 服务相关联。然后，Pilot 将安全命名信息传递给特使代理。

我们将介绍的最后一个主要组件是厨房。

# 活版盘

厨房是一个相对简单的组件。它的工作是抽象出不同平台上的用户配置。它向先导和混合器提供摄取的配置。

现在我们已经将 Istio 分解成了它的主要组件，让我们来看看它是如何完成其作为服务网格的职责的。首要能力是交通管理。

# 使用 Istio 管理流量

Istio 在集群内部服务之间的网络级别上运行，并管理如何向世界公开您的服务。它提供了许多功能，例如请求路由、负载平衡、自动重试和故障注入。让我们回顾一下所有这些，从路由请求开始。

# 路由请求

Istio 推出了自己的虚拟服务作为 CRD。Istio 服务有一个版本概念，对于 Kubernetes 服务来说是不存在的。同一个映像可以部署为虚拟服务的不同版本。例如，您可以将生产或暂存环境表示为同一服务的不同版本。Istio 允许您配置规则，以确定如何将流量路由到不同版本的服务。

其工作方式是 Pilot 向代理发送入口和出口规则，以确定应该在哪里处理请求。然后你在库本内斯把规则定义为 CRD。下面是一个简单的例子，它为`link-manager`服务定义了一个虚拟服务:

```
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
 name: link-manager
spec:
  hosts:
  - link-manager # same as link-manager.default.svc.cluster.local
  http:
  - route:
    - destination:
        host: link-manager
```

让我们看看 Istio 是如何实现负载平衡的。

# 负载平衡

Istio 有自己的独立于平台的服务发现，带有底层平台的适配器(例如，Kubernetes)。它依赖于底层平台管理的服务注册中心的存在，并删除不健康的实例以更新其负载平衡池。目前有三种受支持的负载平衡算法:

*   一系列
*   随意
*   加权最小请求

特使还有几个算法，比如磁悬浮、环形哈希和加权循环法，Istio 还不支持。

Istio 还会执行定期运行状况检查，以验证池中的实例是否实际运行状况良好，如果它们未达到配置的运行状况检查阈值，可以暂时将其从负载平衡中删除。

您可以在单独的`DestinationRule` CRD 通过目的地规则配置负载平衡，如下所示:

```
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: link-manager
spec:
  host: link-manager
  trafficPolicy:
    loadBalancer:
      simple: ROUND_ROBIN
```

您可以按端口指定不同的算法，如下所示:

```
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: link-manager
spec:
  host: link-manager
  trafficPolicy:
    portLevelSettings:
    - port:
        number: 80
      loadBalancer:
        simple: LEAST_CONN
    - port:
        number: 8080
      loadBalancer:
        simple: ROUND_ROBIN
```

现在，让我们看看 Istio 如何帮助我们自动处理失败。

# 处理故障

Istio 提供了许多处理故障的机制，包括以下机制:

*   超时设定
*   重试次数(包括回退和抖动)
*   限速
*   健康检查
*   断路器

所有这些都可以通过 Istio CRDs 进行配置。

例如，下面的代码演示了如何在 TCP 级别设置`link-manager`服务的连接限制和超时(也支持 HTTP):

```
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
   name: link-manager
spec:
  host: link-manager
  trafficPolicy:
     connectionPool:
       tcp:
         maxConnections: 200
         connectTimeout: 45ms
         tcpKeepalive:
           time: 3600s
           interval: 75s
```

断路是通过明确检查给定时间段内的应用程序错误(例如，5XX HTTP 状态代码)来完成的。这是在`outlierDetection`部分完成的。以下示例每 2 分钟检查 10 个连续错误。如果服务超过此阈值，实例将在 5 分钟内从池中弹出:

```
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: link-manager
spec:
  host: link-manager
  trafficPolicy:
     outlierDetection:
       consecutiveErrors: 10
       interval: 2m
       baseEjectionTime: 5m
```

请注意，就 Kubernetes 而言，该服务可能很好，因为容器正在运行。

Istio 提供了如此多的方法来处理操作层面的错误和失败，这很好。当测试分布式系统时，测试某些组件失败时的行为是很重要的。Istio 支持这个用例，允许你有目的地注入错误。

# 注入测试故障

Istio 的故障处理机制不会神奇地修复错误。自动重试可以自动解决间歇性故障，但有些故障需要由应用程序甚至操作员来处理。事实上，Istio 故障处理的错误配置本身会导致故障(例如，配置太短的超时)。测试系统在出现故障时的行为可以通过人工注入故障来完成。Istio 可以注入两种类型的故障:中止和延迟。您可以在虚拟服务级别配置故障注入。

下面是一个例子，其中 5 秒的延迟被添加到对`link-manager`服务的所有请求的 10%，以便模拟系统上的重负载:

```
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
   name: link-manager
 spec:
   hosts:
   - link-manager
   http:
   - fault:
       delay:
         percent: 10
         fixedDelay: 5s
```

在压力下和有故障的情况下进行测试是一个巨大的福音，但是所有的测试都是不完整的。当需要部署新版本时，您可能希望将其部署给一小部分用户，或者让新版本处理一小部分请求。这就是金丝雀部署的好处。

# 做金丝雀部署

我们之前发现了如何在 Kubernetes 中执行加那利部署。如果我们想将 10%的请求转移到我们的加那利版本，我们必须部署当前版本的九个吊舱和一个加那利吊舱来获得正确的比率。Kubernetes 的负载平衡与部署的吊舱紧密耦合。这是次优的。Istio 有一个更好的负载平衡方法，因为它运行在网络级别。您可以简单地配置服务的两个版本，并决定每个版本的请求百分比，而不管每个版本运行多少个 pod。

这里有一个例子，Istio 将拆分流量，并将 95%的流量发送到服务 v1，5%的流量发送到服务 v2:

```
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: link-service
spec:
  hosts:
    - reviews
  http:
  - route:
    - destination:
        host: link-service
        subset: v1
       weight: 95
     - destination:
         host: reviews
         subset: v2
       weight: 5
```

名为 v1 和 v2 的子集是在基于标签的目标规则中定义的。在这种情况下，标签为`version: v1`和`version: v2`:

```
apiVersion: networking.istio.io/v1alpha3
 kind: DestinationRule
 metadata:
   name: link-manager
 spec:
   host: link-manager
   subsets:
   - name: v1
     labels:
       version: v1
   - name: v2
     labels:
       version: v2
```

这是对 Istio 流量管理能力的一个相当全面的覆盖，但还有更多要发现的。让我们把注意力转向安全。

# 使用 Istio 保护您的集群

Istio 的安全模型围绕三个主题:身份、认证和授权。

# 理解我的身份

Istio 管理自己的身份模型，它可以代表人类用户、服务或服务组。在 Kubernetes 中，Istio 使用 Kubernetes 的服务帐户来表示身份。Istio 使用它的公钥基础设施(通过 Citadel)为它管理的每个 pod 创建一个强加密身份。它为每个服务帐户创建一个 x.509 证书(采用 SPIFEE 格式)和一个密钥对，并将它们作为机密注入 pod。Pilot 管理 DNS 服务名称和允许运行它们的身份之间的映射。当客户端调用服务时，它们可以验证服务确实是由允许的身份运行的，并且可以检测恶意服务。有了一个强大的身份，让我们看看身份验证是如何与 Istio 一起工作的。

# 使用 Istio 验证用户

Istio 身份验证基于策略。有两种策略:名称空间策略和网格策略。命名空间策略适用于单个命名空间。网状策略适用于整个集群。一种`MeshPolicy`只能有一个网状策略，必须命名为`default`。下面是一个网格策略的示例，它要求所有服务都使用 mTLS:

```
apiVersion: "authentication.istio.io/v1alpha1"
 kind: "MeshPolicy"
 metadata:
   name: "default"
 spec:
   peers:
   - mtls: {}
```

命名空间策略有一种`Policy`。如果您没有指定名称空间，那么它将应用于默认名称空间。每个命名空间只能有一个策略，它也必须被称为`default`。以下策略使用目标选择器仅适用于链接服务的`api-gateway`服务和端口`8080`:

```
apiVersion: "authentication.istio.io/v1alpha1"
 kind: "Policy"
 metadata:
   name: "default"
   namespace: "some-ns"
 spec:
   targets:
    - name: api-gateway
    - name: link-manager
      ports:
      - number: 8080
```

这个想法是为了避免歧义；策略从服务到名称空间再到网格。如果存在狭隘的政策，则优先考虑。

Istio 通过 mTLS 提供对等认证，或者通过 JWT 提供原产地认证。

您可以通过`peers`部分配置对等身份验证，如下所示:

```
peers:
   - mtls: {}
```

您可以通过`origins`部分配置原点，如下所示:

```
origins:
 - jwt:
     issuer: "https://accounts.google.com"
     jwksUri: "https://www.googleapis.com/oauth2/v3/certs"
     trigger_rules:
     - excluded_paths:
       - exact: /healthcheck
```

如您所见，可以为特定路径配置源身份验证(通过包含或排除路径)。在前面的示例中，`/healthcheck`路径免于身份验证，这对于经常需要从负载平衡器或远程监控服务调用的健康检查端点来说是有意义的。

默认情况下，如果有对等部分，则使用对等身份验证。如果没有，则不会设置身份验证。要强制原始身份验证，您可以将以下内容添加到策略中:

```
principalBinding: USE_ORIGIN
```

既然我们已经发现了 Istio 是如何验证请求的，那么让我们来看看它是如何进行授权的。

# 使用 Istio 授权请求

服务通常公开多个端点。可能只允许服务 A 调用服务 B 的特定端点。服务 A 必须首先针对服务 B 进行身份验证，然后特定请求也必须经过授权。Istio 通过扩展 Kubernetes 用来授权对其 API 服务器的请求的基于角色的访问控制来支持这一点。

需要注意的是，授权在默认情况下是关闭的。要打开它，可以创建一个`ClusterRbacConfig`对象。该模式控制如何启用授权，如下所示:

*   `OFF`表示授权被禁用(默认)。
*   `ON`表示为整个网格中的所有服务启用授权。
*   `ON_WITH_INCLUSION`表示对所有包含的名称空间和服务启用授权。
*   `ON_WITH_EXCLUSION`表示除了排除的名称空间和服务之外，所有名称空间和服务都启用了授权。

以下是在除`kube-system`和`development`之外的所有名称空间上启用授权的示例:

```
apiVersion: "rbac.istio.io/v1alpha1"
 kind: ClusterRbacConfig
 metadata:
   name: default
 spec:
   mode: 'ON_WITH_EXCLUSION'
   exclusion:
     namespaces: ["kube-system", "development"]
```

实际授权在服务级别运行，非常类似于 Kubernetes 的 RBAC 模型。在库本内特斯有`Role`、`ClusterRole`、`RoleBinding`和`ClusterRoleBinding`的地方，在伊斯特奥有`ServiceRole`和`ServiceRoleBinding`的地方。

粒度的基本层次是`namespace/service/path/method`。您可以使用通配符进行分组。例如，以下角色授予 GET 和 HEAD 对默认命名空间中的所有 Delinkcious 管理器和 API 网关的访问权限:

```
apiVersion: "rbac.istio.io/v1alpha1"
 kind: ServiceRole
 metadata:
   name: full-access-reader
   namespace: default
 spec:
   rules:
   - services: ["*-manager", "api-gateway"]
     paths:
     methods: ["GET", "HEAD"]
```

然而，Istio 提供了更进一步的约束和属性控制。您可以通过源命名空间或 IP、标签、请求头和其他属性来限制规则。

详情可参考[https://istio . io/docs/reference/config/authorization-and-properties/](https://istio.io/docs/reference/config/authorization/constraints-and-properties/)。

一旦你有了一个`ServiceRole`，你需要把它和一个被允许执行所请求的操作的主体列表(比如服务帐户或者人类用户)联系起来。以下是如何定义`ServiceRoleBinding`:

```
apiVersion: "rbac.istio.io/v1alpha1"
 kind: ServiceRoleBinding
 metadata:
   name: test-binding-products
   namespace: default
 spec:
   subjects:
   - user: "service-account-delinkcious"
   - user: "istio-ingress-service-account"
     properties:
       request.auth.claims[email]: "the.gigi@gmail.com"
   roleRef:
     kind: ServiceRole
     name: "full-access-reader"
```

通过将主体用户设置为`*`，您可以将角色公开给经过身份验证或未经身份验证的用户。

我的授权有很多我们可以在这里涵盖的。您可以阅读以下主题:

*   对 TCP 协议的授权
*   许可模式(实验)
*   调试授权问题
*   通过特使过滤器授权

一旦请求在那里被授权，如果它不符合策略检查，它仍然可能被拒绝。

# 用 Istio 执行策略

Istio 策略执行类似于 Kubernetes 中准入控制员的工作方式。混合器有一组适配器，在处理请求之前和之后调用。在我们深入探讨之前，需要注意的是，默认情况下策略强制是禁用的。如果使用 helm 安装 Istio，可以通过提供以下标志来启用它:

```
--set global.disablePolicyChecks=false.
```

在 GKE，它是启用的；下面是如何检查这一点:

```
$ kubectl -n istio-system get cm istio -o jsonpath="{@.data.mesh}" | grep disablePolicyChecks
disablePolicyChecks: false
```

如果结果是`disablePolicyChecks: false`，那么已经启用了。否则，通过编辑 Istio 配置映射并将其设置为 false 来启用它。

一种常见的政策是限制利率。您可以通过配置配额对象、将它们绑定到特定服务以及定义混合器规则来实施速率限制。Istio 演示应用程序的一个很好的例子可以在[https://raw . githubusercontent . com/Istio/Istio/release-1.1/samples/bookinfo/policy/mixer-rule-product page-rate limit . YAML](https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/policy/mixer-rule-productpage-ratelimit.yaml)找到。

您也可以通过创建混合器适配器来添加自己的策略。有三种内置类型的适配器，如下所示:

*   支票
*   配额
*   报告

这不是小事；您将不得不实现一个可以处理专用模板中指定的数据的 gRPC 服务。现在，让我们看看 Istio 为我们收集的指标。

# 使用 Istio 收集指标

Istio 在每次请求后都会收集指标。度量被发送到混合器。特使是度量的主要生产者，但是如果您愿意，您可以添加自己的度量。度量的配置模型基于多个 Istio 概念:属性、实例、模板、处理程序、规则和混合器适配器。

下面是一个示例实例，它对所有请求进行计数，并将它们报告为`request-count`度量:

```
apiVersion: config.istio.io/v1alpha2
 kind: instance
 metadata:
   name: request-count
   namespace: istio-system
 spec:
   compiledTemplate: metric
   params:
     value: "1" # count each request
     dimensions:
       reporter: conditional((context.reporter.kind | "inbound") == "outbound", "client", "server")
       source: source.workload.name | "unknown"
       destination: destination.workload.name | "unknown"
       message: '"counting requests..."'
     monitored_resource_type: '"UNSPECIFIED"'```
```

现在，我们可以配置一个普罗米修斯处理器来接收度量。普罗米修斯是一个编译的适配器(它是 Mixer 的一部分)，所以我们可以在规范中使用它。`spec | params | metrics`部分有一种`COUNTER`，普罗米修斯度量名称(`request_count`)，最重要的是，我们刚刚定义的实例名称，它是度量的来源:

```
apiVersion: config.istio.io/v1alpha2
 kind: handler
 metadata:
   name: request-count-handler
   namespace: istio-system
 spec:
   compiledAdapter: prometheus
   params:
     metrics:
     - name: request_count # Prometheus metric name
       instance_name: request-count.instance.istio-system # Mixer instance name (fully-qualified)
       kind: COUNTER
       label_names:
       - reporter
       - source
       - destination
       - message
```

最后，我们用一个规则把它们联系在一起，如下所示:

```
apiVersion: config.istio.io/v1alpha2
 kind: rule
 metadata:
   name: prom-request-counter
   namespace: istio-system
 spec:
   actions:
   - handler: request-count-handler
     instances: [ request-count ]
```

好吧，那么 Istio 是惊人的强大。但是有什么情况是不应该用 Istio 的吗？

# 你应该什么时候避开 Istio？

Istio 提供了很多价值。然而，这个价值并非没有代价。Istio 的侵入性及其复杂性有一些明显的缺点。在采用 Istio 之前，您应该考虑这些缺点:

*   在已经很复杂的 Kubernetes 之上的额外概念和管理系统使得学习曲线非常陡峭。
*   排除配置问题具有挑战性。
*   与其他项目的集成可能缺失或不完整(例如，NATS 和网真)。
*   代理会增加延迟并消耗 CPU 和内存资源。

如果您刚刚开始使用 Kubernetes，我建议您等到掌握了它之后再考虑使用 Istio。

现在我们了解了 Istio 的全部内容，让我们来探索德令奇如何从 Istio 中获益。

# 美味可口

有了 Istio，Delinkcious 可以潜在地甩掉很多额外的包袱。那么，为什么将这个功能从 Delinkcious 服务或 Go kit 中间件转移到 Istio 是一个好主意呢？

原因是这个功能通常与应用程序域无关。我们投入了大量的工作来小心地分离关注点，并从部署和管理它们的方式中分离出非信任域。然而，只要微服务本身解决了所有这些问题，我们就需要对代码进行更改，并在每次想要进行操作更改时重新构建它们。即使这些都是数据驱动的，也很难解决和调试问题，因为当故障发生时，并不总是容易确定它是由于域代码还是操作代码中的错误。

让我们看一些具体的例子，Istio 可以简化 Delinkcious。

# 删除服务之间的相互身份验证

大家可能还记得，在[第 6 章](06.html)、*保护 Kubernetes* 上的微服务时，我们在`link-manager`服务和`social-graph-manager`服务之间创建了一个共同的秘密:

```
$ kubectl get secret | grep mutual
link-mutual-auth             Opaque          1      9d
 social-graph-mutual-auth    Opaque          1      5d19h
```

编码秘密需要大量的协调和明确的工作，然后将秘密装入容器:

```
    spec:
       containers:
       - name: link-manager
         image: g1g1/delinkcious-link:0.3
         imagePullPolicy: Always
         ports:
         - containerPort: 8080
         envFrom:
         - configMapRef:
             name: link-manager-config
         volumeMounts:
         - name: mutual-auth
           mountPath: /etc/delinkcious
           readOnly: true
       volumes:
       - name: mutual-auth
         secret:
           secretName: link-mutual-auth
```

然后，链接管理器必须通过我们必须实现的`auth_util`包获取这个秘密，并将其作为请求的头部注入:

```
// encodeHTTPGenericRequest is a transport/http.EncodeRequestFunc that
 // JSON-encodes any request to the request body. Primarily useful in a client.
 func encodeHTTPGenericRequest(_ context.Context, r *http.Request, request interface{}) error {
     var buf bytes.Buffer
     if err := json.NewEncoder(&buf).Encode(request); err != nil {
         return err
     }
     r.Body = ioutil.NopCloser(&buf)

     if os.Getenv("DELINKCIOUS_MUTUAL_AUTH") != "false" {
         token := auth_util.GetToken(SERVICE_NAME)
         r.Header["Delinkcious-Caller-Token"] = []string{token}
     }

     return nil
 }
```

最后，社交图管理器必须知道这个方案，并明确检查调用者是否被允许:

```
func decodeGetFollowersRequest(_ context.Context, r *http.Request) (interface{}, error){ 
    if os.Getenv("DELINKCIOUS_MUTUAL_AUTH") != "false" { 
        token := r.Header["Delinkcious-Caller-Token"] 
        if len(token) == 0 || token[0] == "" { 
            return nil, errors.New("Missing caller token") 
        }
        if !auth_util.HasCaller("link-manager", token[0]) {
         return nil, errors.New("Unauthorized caller")
        }
    }
 ...
}
```

这是大量与服务本身无关的工作。想象一下，用成千上万种方法来管理对数百个交互微服务的访问。这种方法很麻烦，容易出错，并且每当您添加或删除交互时，都需要对两个服务进行代码更改。

有了 Istio，我们可以把这个完全外化为一个角色和一个角色绑定。这里有一个角色允许你调用`/following`端点的 GET 方法:

```
apiVersion: "rbac.istio.io/v1alpha1"
 kind: ServiceRole
 metadata:
   name: get-following
   namespace: default
 spec:
   rules:
   - services: ["social-graph.default.svc.cluster.local"]
     paths: ["/following"]
     methods: ["GET"]
```

为了只允许链接服务调用方法，我们可以将角色绑定到`link-manager`服务账户作为主体用户:

```
apiVersion: "rbac.istio.io/v1alpha1"
 kind: ServiceRoleBinding
 metadata:
   name: get-following
   namespace: default
 spec:
   subjects:
   - user: "cluster.local/ns/default/sa/link-manager"
   roleRef:
     kind: ServiceRole
     name: "get-following"
```

如果以后我们需要允许其他服务调用`/following`端点，我们可以给这个角色绑定添加更多的主体。社会服务本身不需要知道什么服务被允许调用它的方法。调用服务不需要显式提供任何凭据。服务网格负责所有这些。

Istio 能够真正帮助德令计划的另一个领域是金丝雀的部署。

# 利用更好的金丝雀部署

在[第 11 章](11.html)、*部署微服务*中，我们使用 Kubernetes 部署和服务来进行金丝雀部署。为了将 10%的流量转移到 canary 版本，我们将当前版本扩展到 9 个副本，并创建了一个 canary 部署，其中一个副本用于新版本。我们在两个部署中使用了相同的标签(`svc: link`和`app: manager`)。

两个部署前的`link-manager`服务将负载平均分配给所有吊舱，形成了我们想要的 90/10 分割:

```
$ kubectl scale --replicas=9 deployment/green-link-manager
 deployment.extensions/green-link-manager scaled

 $ kubectl get po -l svc=link,app=manager
 NAME                                 READY  STATUS    RESTARTS   AGE
 green-link-manager-5874c6cd4f-2ldfn   1/1   Running   10         15h
 green-link-manager-5874c6cd4f-9csxz   1/1   Running   0          52s
 green-link-manager-5874c6cd4f-c5rqn   1/1   Running   0          52s
 green-link-manager-5874c6cd4f-mvm5v   1/1   Running   10         15h
 green-link-manager-5874c6cd4f-qn4zj   1/1   Running   0          52s
 green-link-manager-5874c6cd4f-r2jxf   1/1   Running   0          52s
 green-link-manager-5874c6cd4f-rtwsj   1/1   Running   0          52s
 green-link-manager-5874c6cd4f-sw27r   1/1   Running   0          52s
 green-link-manager-5874c6cd4f-vcj9s   1/1   Running   10         15h
 yellow-link-manager-67847d6b85-n97b5  1/1   Running   4          6m20s
```

这是可行的，但是它将 canary 部署与扩展部署结合在一起。这可能很昂贵，尤其是如果您需要运行一段时间的加那利部署，直到您确信它是正确的。理想情况下，你不需要为了将一定比例的流量转移到新版本而创建更多的豆荚。

Istio 子集概念的流量整形功能完美地解决了这个用例。以下虚拟服务将流量分成 90/10 的比例，一个子集称为`v0.5`，另一个子集称为`canary`:

```
apiVersion: networking.istio.io/v1alpha3
 kind: VirtualService
 metadata:
   name: social-graph-manager
 spec:
   hosts:
     - social-graph-manager
   http:
   - route:
     - destination:
         host: social-graph-manager
         subset: v0.5
       weight: 90
     - destination:
         host: social-graph-manager
         subset: canary
       weight: 10
```

用 Istio 的虚拟服务和子集进行金丝雀部署对 Delinkcious 来说非常棒。Istio 也可以帮助记录和报告错误。

# 自动记录和错误报告

当使用 Istio 插件在 GKE 运行 Delinkcious 时，您可以自动与 Stackdriver 集成，stack driver 是一个一站式监控工具，包括度量、集中日志记录、错误报告和分布式跟踪。当您搜索`link-manager`日志时，这里是 Stackdriver 日志查看器:

![](assets/6099c1f0-231e-4ae7-ac23-1e811b7183a1.png)

或者，您可以通过下拉列表按服务名称进行筛选。下面是指定 api 网关时的样子:

![](assets/1e2866bd-d11d-4642-a65f-be8ee5b65c24.png)

有时，您需要错误报告视图:

![](assets/0f5d208d-1511-4be7-ae49-cf0d143da468.png)

然后，您可以深入查看任何错误，并获得许多其他信息，这些信息将帮助您了解哪里出错以及如何修复:

![](assets/87161def-792a-4732-88dc-266613cd7ba3.png)

虽然 Istio 提供了很多价值，而且在 Stackdriver 的情况下，您也可以从自动设置中受益，但它并不总是平稳行驶，它有一些局限性和粗糙的边缘。

# 迁就 NATS

我在将 Istio 部署到 Delinkcious 集群中时发现的一个限制是，NATS 不能与 Istio 一起工作，因为它需要直接连接，当特使代理劫持通信时，它就会中断。解决办法是防止 Istio 向边车容器注射，并接受 NATS 将不受管理。将`NatsCluster` CRD 添加到吊舱规格的以下注释中对我们来说是可行的:`sidecar.istio.io/inject: "false"`:

```
apiVersion: nats.io/v1alpha2
 kind: NatsCluster
 metadata:
   name: nats-cluster
 spec:
   pod:
     # Disable istio on nats pods
     annotations:
       sidecar.istio.io/inject: "false"
   size: 1
   version: "1.4.0"
```

前面的代码是完整的`NatsCluster`资源定义，并带有适当的注释。

# 检查 Istio 足迹

Istio 在集群中部署了很多东西，所以让我们回顾一下其中的一些。幸运的是，Istio 控制平面被隔离在它自己的`istio-system`命名空间中，但是 CRD 总是集群范围的，Istio 没有吝啬这些:

```
$ kubectl get crd -l k8s-app=istio -o custom-columns="NAME:.metadata.name"

 NAME
 adapters.config.istio.io
 apikeys.config.istio.io
 attributemanifests.config.istio.io
 authorizations.config.istio.io
 bypasses.config.istio.io
 checknothings.config.istio.io
 circonuses.config.istio.io
 deniers.config.istio.io
 destinationrules.networking.istio.io
 edges.config.istio.io
 envoyfilters.networking.istio.io
 fluentds.config.istio.io
 gateways.networking.istio.io
 handlers.config.istio.io
 httpapispecbindings.config.istio.io
 httpapispecs.config.istio.io
 instances.config.istio.io
 kubernetesenvs.config.istio.io
 kuberneteses.config.istio.io
 listcheckers.config.istio.io
 listentries.config.istio.io
 logentries.config.istio.io
 memquotas.config.istio.io
 metrics.config.istio.io
 noops.config.istio.io
 opas.config.istio.io
 prometheuses.config.istio.io
 quotas.config.istio.io
 quotaspecbindings.config.istio.io
 quotaspecs.config.istio.io
 rbacconfigs.rbac.istio.io
 rbacs.config.istio.io
 redisquotas.config.istio.io
 reportnothings.config.istio.io
 rules.config.istio.io
 servicecontrolreports.config.istio.io
 servicecontrols.config.istio.io
 serviceentries.networking.istio.io
 servicerolebindings.rbac.istio.io
 serviceroles.rbac.istio.io
 signalfxs.config.istio.io
 solarwindses.config.istio.io
 stackdrivers.config.istio.io
 statsds.config.istio.io
 stdios.config.istio.io
 templates.config.istio.io
 tracespans.config.istio.io
 virtualservices.networking.istio.io
```

除了所有这些 CRD，Istio 还将其所有组件安装到 Istio 命名空间中:

```
$ kubectl -n istio-system get all -o name
 pod/istio-citadel-6995f7bd9-7c7x9
 pod/istio-egressgateway-57b96d87bd-cnc2s
 pod/istio-galley-6d7dd498f6-b29sk
 pod/istio-ingressgateway-ddd557db7-glwm2
 pod/istio-pilot-5765d76b8c-d9hq7
 pod/istio-policy-5b47b88467-x7pqf
 pod/istio-sidecar-injector-6b9fbbfcf6-fhc4k
 pod/istio-telemetry-65dcd9ff85-bkjtd
 pod/promsd-7b49dcb96c-wrfs8
 service/istio-citadel
 service/istio-egressgateway
 service/istio-galley
 service/istio-ingressgateway
 service/istio-pilot
 service/istio-policy
 service/istio-sidecar-injector
 service/istio-telemetry
 service/promsd
 deployment.apps/istio-citadel
 deployment.apps/istio-egressgateway
 deployment.apps/istio-galley
 deployment.apps/istio-ingressgateway
 deployment.apps/istio-pilot
 deployment.apps/istio-policy
 deployment.apps/istio-sidecar-injector
 deployment.apps/istio-telemetry
 deployment.apps/promsd
 replicaset.apps/istio-citadel-6995f7bd9
 replicaset.apps/istio-egressgateway-57b96d87bd
 replicaset.apps/istio-galley-6d7dd498f6
 replicaset.apps/istio-ingressgateway-ddd557db7
 replicaset.apps/istio-pilot-5765d76b8c
 replicaset.apps/istio-policy-5b47b88467
 replicaset.apps/istio-sidecar-injector-6b9fbbfcf6
 replicaset.apps/istio-telemetry-65dcd9ff85
 replicaset.apps/promsd-7b49dcb96c
 horizontalpodautoscaler.autoscaling/istio-egressgateway
 horizontalpodautoscaler.autoscaling/istio-ingressgateway
 horizontalpodautoscaler.autoscaling/istio-pilot
 horizontalpodautoscaler.autoscaling/istio-policy
 horizontalpodautoscaler.autoscaling/istio-telemetry
```

最后，当然，Istio 会在每个吊舱中安装它的 sidecar 代理(除了 Nats，我们禁用了它)。可以看到，默认命名空间中的每个 pod 都有两个容器(2/2 在`READY`列下)。一个容器完成工作，另一个是 Istio 代理边车容器:

```
$ kubectl get po
 NAME READY STATUS RESTARTS AGE
 api-gateway-5497d95c74-zlgnm 2/2 Running 0 4d11h
 link-db-7445d6cbf7-wdfsb 2/2 Running 0 4d22h
 link-manager-54968ff8cf-vtpqr 2/2 Running 1 4d13h
 nats-cluster-1 1/1 Running 0 4d20h
 nats-operator-55dfdc6868-2b57q 2/2 Running 3 4d22h
 news-manager-7f447f5c9f-n2v2v 2/2 Running 1 4d20h
 news-manager-redis-0 2/2 Running 0 4d22h
 social-graph-db-7d8ffb877b-nrzxh 2/2 Running 0 4d11h
 social-graph-manager-59b464456f-48lrn 2/2 Running 1 4d11h
 trouble-64554479d-rjszv 2/2 Running 0 4d17h
 user-db-0 2/2 Running 0 4d22h
 user-manager-699458447-9h64n 2/2 Running 2 4d22h
```

如果你认为 Istio 太大太复杂，你可能仍然想通过追求其中一种选择来享受服务网格的好处。

# Istio 的替代品

Istio 势头很大，但它不一定是适合您的最佳服务网格。让我们看看其他一些服务网格，并考虑它们的属性。

# 左 2.0

浮力是在 2016 年创造了术语*服务网格*的公司，并推出了第一个服务网格——Linkerd。它基于推特的 Finagle，并在 Scala 中实现。从那以后，浮力开发了一个新的服务网格，专注于 Kubernetes，称为 Conduit(在 Rust 和 Go 中实现)，后来(2018 年 7 月)将其更名为 Linkerd 2.0。这是一个像 Istio 一样的 CNCF 项目。Linkerd 2.0 还使用了可以自动或手动注射的 sidecar 容器。

由于其轻量级设计和 Rust 中数据平面代理的更紧密实现，Linkerd 2.0 应该优于 Istio，并且在控制平面中消耗更少的资源。有关更多信息，您可以参考以下资源:

*   **CPU 和内存**:[https://istio . io/docs/concepts/performance-and-scalability/# CPU 和内存](https://istio.io/docs/concepts/performance-and-scalability/#cpu-and-memory)
*   **Linkerd 2.0 和 Istio 性能基准**:[https://medium . com/@ ihcsim/Linkerd-2-0 和-Istio-性能基准-df290101c2bb](https://medium.com/@ihcsim/linkerd-2-0-and-istio-performance-benchmark-df290101c2bb)
*   **对标 Istio 和 Linkerd CPU**:[https://medium . com/@ Michael _ 87395/对标-Istio-Linkerd-CPU-c 36287 e 32781](https://medium.com/@michael_87395/benchmarking-istio-linkerd-cpu-c36287e32781)

浮力是一家规模较小的公司，在功能上似乎略落后于 Istio。

# 使者

Istio 数据平面是特使，它完成所有繁重的工作。你可能会觉得 Istio 控制平面太复杂，更喜欢去掉这一层间接性，构建自己的控制平面直接与特使交互。这在某些特殊情况下会很有用；例如，如果您想使用特使提供的 Istio 不支持的负载平衡算法。

# hashicorp 领事

Consul 并不勾选服务网格的所有复选框，但是它提供服务发现、服务标识和 mTLS 授权。它不是库本内特特有的，也没有得到 CNCF 的认可。如果您已经使用了领事或其他哈希公司的产品，您可能更喜欢使用它作为服务网格。

# 自动气象站应用网格

如果您在 AWS 上运行您的基础架构，您应该考虑 AWS 应用网格。这是一个较新的项目，特定于 AWS，也使用特使作为其数据平面。可以肯定的是，它将最好地与 AWS IAM 网络和监控技术相结合。目前还不清楚 AWS 应用网格是否会成为 Kubernetes 更好的服务网格，或者它的主要目的是为 ECS——AWS 的专有容器编排解决方案——提供服务网格优势。

# 其他人

还有一些其他的服务网格。我只在这里提一下，这样如果你感兴趣的话，你可以进一步研究。其中一些与 Istio 有某种形式的集成。它们的价值并不总是明确的，因为它们并不开放:

*   阿斯彭网格
*   孔网
*   AVI 网络通用服务网格

# 无网格选项

您总是可以完全避开服务网格，并使用库，如 Go kit、Hystrix 或 Finagle。您可能会失去外部服务网格的优势，但是如果您严格控制所有的微服务，并且它们都使用相同的编程语言，那么库方法可能对您来说很合适。它在概念上和操作上更简单，并且将管理交叉问题的责任转移给了开发人员。

# 摘要

在这一章中，我们特别关注了服务网格和 Istio。Istio 是一个复杂的项目；它位于 Kubernetes 之上，用它的代理创建了一种影子集群。Istio 有突出的特点；它可以在非常细粒度的级别上塑造流量，提供复杂的身份验证和授权，实施高级策略，收集大量信息，并帮助扩展集群。

我们介绍了 Istio 架构及其强大的功能，并探讨了 Delinkcious 如何从这些功能中获益。

然而，Istio 并不简单。它创建了过多的定制资源，并以复杂的方式重叠和扩展了现有的 Kubernetes 资源(VirtualService 对 Service)。

我们还回顾了 Istio 的替代产品，包括 Linkerd 2.0、直特使、AWS App Mesh 和 Consul。

在这一点上，您应该很好地理解服务网格的好处以及 Istio 可以为您的项目做些什么。你可能需要做一些额外的阅读和实验来做出明智的决定，是应该立即将 Istio 合并到你的系统中，考虑其中一个替代方案，还是只是等待。

我相信服务网格，尤其是 Istio，将非常重要，并将成为纳入大型 Kubernetes 集群的标准最佳实践。

在下一章，也就是最后一章，我们将继续讨论微服务、Kubernetes 和其他新兴趋势(如无服务器)的未来。

# 进一步阅读

有关本章内容的更多信息，您可以参考以下资源:

*   **浪费** ： [https://istio.io](https://istio.io)
*   **hystrix**:[https://github . com/Netflix/hystrix](https://github.com/Netflix/Hystrix)
*   **菲纳格尔**:[https://twitter.github.io/finagle/](https://twitter.github.io/finagle/)
*   **发送**:[https://www . send proxy . io/](https://www.envoyproxy.io/)
*   **窃听**:[https://窃听。我](https://spiffe.io)
*   **配置**:[https://istio.io/docs/reference/config/](https://istio.io/docs/reference/config/)